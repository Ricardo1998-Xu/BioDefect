{"project": "Scanpy", "commit_id": "100_scanpy_1.9.0__settings.py_categories_to_ignore.py", "target": 0, "func": "def categories_to_ignore(self, categories_to_ignore: Iterable[str]):\n        categories_to_ignore = list(categories_to_ignore)\n        for i, cat in enumerate(categories_to_ignore):\n            _type_check(cat, f\"categories_to_ignore[{i}]\", str)\n        self._categories_to_ignore = categories_to_ignore", "idx": 0}
{"project": "Scanpy", "commit_id": "101_scanpy_1.9.0__settings.py_set_figure_params.py", "target": 0, "func": "def set_figure_params(\n        self,\n        scanpy: bool = True,\n        dpi: int = 80,\n        dpi_save: int = 150,\n        frameon: bool = True,\n        vector_friendly: bool = True,\n        fontsize: int = 14,\n        figsize: Optional[int] = None,\n        color_map: Optional[str] = None,\n        format: _Format = \"pdf\",\n        facecolor: Optional[str] = None,\n        transparent: bool = False,\n        ipython_format: str = \"png2x\",\n    ):\n        \"\"\"\\\n        Set resolution/size, styling and format of figures.\n\n        Parameters\n        ----------\n        scanpy\n            Init default values for :obj:`matplotlib.rcParams` suited for Scanpy.\n        dpi\n            Resolution of rendered figures \u2013 this influences the size of figures in notebooks.\n        dpi_save\n            Resolution of saved figures. This should typically be higher to achieve\n            publication quality.\n        frameon\n            Add frames and axes labels to scatter plots.\n        vector_friendly\n            Plot scatter plots using `png` backend even when exporting as `pdf` or `svg`.\n        fontsize\n            Set the fontsize for several `rcParams` entries. Ignored if `scanpy=False`.\n        figsize\n            Set plt.rcParams['figure.figsize'].\n        color_map\n            Convenience method for setting the default color map. Ignored if `scanpy=False`.\n        format\n            This sets the default format for saving figures: `file_format_figs`.\n        facecolor\n            Sets backgrounds via `rcParams['figure.facecolor'] = facecolor` and\n            `rcParams['axes.facecolor'] = facecolor`.\n        transparent\n            Save figures with transparent back ground. Sets\n            `rcParams['savefig.transparent']`.\n        ipython_format\n            Only concerns the notebook/IPython environment; see\n            :func:`~IPython.display.set_matplotlib_formats` for details.\n        \"\"\"\n        if self._is_run_from_ipython():\n            import IPython\n\n            if isinstance(ipython_format, str):\n                ipython_format = [ipython_format]\n            IPython.display.set_matplotlib_formats(*ipython_format)\n\n        from matplotlib import rcParams\n\n        self._vector_friendly = vector_friendly\n        self.file_format_figs = format\n        if dpi is not None:\n            rcParams[\"figure.dpi\"] = dpi\n        if dpi_save is not None:\n            rcParams[\"savefig.dpi\"] = dpi_save\n        if transparent is not None:\n            rcParams[\"savefig.transparent\"] = transparent\n        if facecolor is not None:\n            rcParams['figure.facecolor'] = facecolor\n            rcParams['axes.facecolor'] = facecolor\n        if scanpy:\n            from .plotting._rcmod import set_rcParams_scanpy\n\n            set_rcParams_scanpy(fontsize=fontsize, color_map=color_map)\n        if figsize is not None:\n            rcParams['figure.figsize'] = figsize\n        self._frameon = frameon", "idx": 1}
{"project": "Scanpy", "commit_id": "102_scanpy_1.9.0__settings.py__is_run_from_ipython.py", "target": 0, "func": "def _is_run_from_ipython():\n        \"\"\"Determines whether we're currently in IPython.\"\"\"\n        import builtins\n\n        return getattr(builtins, \"__IPYTHON__\", False)", "idx": 2}
{"project": "Scanpy", "commit_id": "103_scanpy_1.9.0__settings.py___str__.py", "target": 0, "func": "def __str__(self) -> str:\n        return '\\n'.join(\n            f'{k} = {v!r}'\n            for k, v in inspect.getmembers(self)\n            if not k.startswith(\"_\") and not k == 'getdoc'", "idx": 3}
{"project": "Scanpy", "commit_id": "104_scanpy_1.9.0__datasets.py_blobs.py", "target": 0, "func": "def blobs(\n    n_variables: int = 11,\n    n_centers: int = 5,\n    cluster_std: float = 1.0,\n    n_observations: int = 640,\n) -> ad.AnnData:\n    \"\"\"\\\n    Gaussian Blobs.\n\n    Parameters\n    ----------\n    n_variables\n        Dimension of feature space.\n    n_centers\n        Number of cluster centers.\n    cluster_std\n        Standard deviation of clusters.\n    n_observations\n        Number of observations. By default, this is the same observation number\n        as in :func:`scanpy.datasets.krumsiek11`.\n\n    Returns\n    -------\n    Annotated data matrix containing a observation annotation 'blobs' that\n    indicates cluster identity.\n    \"\"\"\n    import sklearn.datasets\n\n    X, y = sklearn.datasets.make_blobs(\n        n_samples=n_observations,\n        n_features=n_variables,\n        centers=n_centers,\n        cluster_std=cluster_std,\n        random_state=0,\n    )\n    return ad.AnnData(X, obs=dict(blobs=y.astype(str)), dtype=X.dtype)", "idx": 4}
{"project": "Scanpy", "commit_id": "105_scanpy_1.9.0__datasets.py_burczynski06.py", "target": 0, "func": "def burczynski06() -> ad.AnnData:\n    \"\"\"\\\n    Bulk data with conditions ulcerative colitis (UC) and Crohn's disease (CD).\n\n    The study assesses transcriptional profiles in peripheral blood mononuclear\n    cells from 42 healthy individuals, 59 CD patients, and 26 UC patients by\n    hybridization to microarrays interrogating more than 22,000 sequences.\n\n    Reference\n    ---------\n    Burczynski et al., \"Molecular classification of Crohn's disease and\n    ulcerative colitis patients using transcriptional profiles in peripheral\n    blood mononuclear cells\"\n    J Mol Diagn 8, 51 (2006). PMID:16436634.\n    \"\"\"\n    filename = settings.datasetdir / 'burczynski06/GDS1615_full.soft.gz'\n    url = 'ftp://ftp.ncbi.nlm.nih.gov/geo/datasets/GDS1nnn/GDS1615/soft/GDS1615_full.soft.gz'\n    adata = read(filename, backup_url=url)\n    return adata", "idx": 5}
{"project": "Scanpy", "commit_id": "106_scanpy_1.9.0__datasets.py_krumsiek11.py", "target": 0, "func": "def krumsiek11() -> ad.AnnData:\n    \"\"\"\\\n    Simulated myeloid progenitors [Krumsiek11]_.\n\n    The literature-curated boolean network from [Krumsiek11]_ was used to\n    simulate the data. It describes development to four cell fates: 'monocyte',\n    'erythrocyte', 'megakaryocyte' and 'neutrophil'.\n\n    See also the discussion of this data in [Wolf19]_.\n\n    Simulate via :func:`~scanpy.tl.sim`.\n\n    Returns\n    -------\n    Annotated data matrix.\n    \"\"\"\n    filename = HERE / 'krumsiek11.txt'\n    verbosity_save = settings.verbosity\n    settings.verbosity = 'error'  # suppress output...\n    adata = read(filename, first_column_names=True)\n    settings.verbosity = verbosity_save\n    adata.uns['iroot'] = 0\n    fate_labels = {0: 'Stem', 159: 'Mo', 319: 'Ery', 459: 'Mk', 619: 'Neu'}\n    adata.uns['highlights'] = fate_labels\n    cell_type = np.array(['progenitor' for i in range(adata.n_obs)])\n    cell_type[80:160] = 'Mo'\n    cell_type[240:320] = 'Ery'\n    cell_type[400:480] = 'Mk'\n    cell_type[560:640] = 'Neu'\n    adata.obs['cell_type'] = cell_type\n    _utils.sanitize_anndata(adata)\n    return adata", "idx": 6}
{"project": "Scanpy", "commit_id": "107_scanpy_1.9.0__datasets.py_moignard15.py", "target": 0, "func": "def moignard15() -> ad.AnnData:\n    \"\"\"\\\n    Hematopoiesis in early mouse embryos [Moignard15]_.\n\n    Returns\n    -------\n    Annotated data matrix.\n    \"\"\"\n    filename = settings.datasetdir / 'moignard15/nbt.3154-S3.xlsx'\n    backup_url = 'https://static-content.springer.com/esm/art%3A10.1038%2Fnbt.3154/MediaObjects/41587_2015_BFnbt3154_MOESM4_ESM.xlsx'\n    adata = read(filename, sheet='dCt_values.txt', backup_url=backup_url)\n    # filter out 4 genes as in Haghverdi et al. (2016)\n    gene_subset = ~np.in1d(adata.var_names, ['Eif2b1', 'Mrpl19', 'Polr2a', 'Ubc'])\n    adata = adata[:, gene_subset].copy()  # retain non-removed genes\n    # choose root cell for DPT analysis as in Haghverdi et al. (2016)\n    adata.uns[\"iroot\"] = 532  # note that in Matlab/R, counting starts at 1\n    # annotate with Moignard et al. (2015) experimental cell groups\n    groups = {\n        'HF': '#D7A83E',\n        'NP': '#7AAE5D',\n        'PS': '#497ABC',\n        '4SG': '#AF353A',\n        '4SFG': '#765099',\n    }\n    # annotate each observation/cell\n    adata.obs['exp_groups'] = [\n        next(gname for gname in groups.keys() if sname.startswith(gname))\n        for sname in adata.obs_names\n    ]\n    # fix the order and colors of names in \"groups\"\n    adata.obs['exp_groups'] = pd.Categorical(\n        adata.obs['exp_groups'], categories=list(groups.keys())\n    )\n    adata.uns['exp_groups_colors'] = list(groups.values())\n    return adata", "idx": 7}
{"project": "Scanpy", "commit_id": "108_scanpy_1.9.0__datasets.py_paul15.py", "target": 1, "func": "def paul15() -> ad.AnnData:\n    \"\"\"\\\n    Development of Myeloid Progenitors [Paul15]_.\n\n    Non-logarithmized raw data.\n\n    The data has been sent out by Email from the Amit Lab. An R version for\n    loading the data can be found here\n    https://github.com/theislab/scAnalysisTutorial\n\n    Returns\n    -------\n    Annotated data matrix.\n    \"\"\"\n    logg.warning(\n        'In Scanpy 0.*, this returned logarithmized data. '\n        'Now it returns non-logarithmized data.'\n    )\n    import h5py\n\n    filename = settings.datasetdir / 'paul15/paul15.h5'\n    filename.parent.mkdir(exist_ok=True)\n    backup_url = 'http://falexwolf.de/data/paul15.h5'\n    _utils.check_presence_download(filename, backup_url)\n    with h5py.File(filename, 'r') as f:\n        X = f['data.debatched'][()]\n        gene_names = f['data.debatched_rownames'][()].astype(str)\n        cell_names = f['data.debatched_colnames'][()].astype(str)\n        clusters = f['cluster.id'][()].flatten().astype(int)\n        infogenes_names = f['info.genes_strings'][()].astype(str)\n    # each row has to correspond to a observation, therefore transpose\n    adata = ad.AnnData(X.transpose(), dtype=X.dtype)\n    adata.var_names = gene_names\n    adata.row_names = cell_names\n    # names reflecting the cell type identifications from the paper\n    cell_type = 6 * ['Ery']\n    cell_type += 'MEP Mk GMP GMP DC Baso Baso Mo Mo Neu Neu Eos Lymph'.split()\n    adata.obs['paul15_clusters'] = [f'{i}{cell_type[i-1]}' for i in clusters]\n    # make string annotations categorical (optional)\n    _utils.sanitize_anndata(adata)\n    # just keep the first of the two equivalent names per gene\n    adata.var_names = [gn.split(';')[0] for gn in adata.var_names]\n    # remove 10 corrupted gene names\n    infogenes_names = np.intersect1d(infogenes_names, adata.var_names)\n    # restrict data array to the 3461 informative genes\n    adata = adata[:, infogenes_names]\n    # usually we'd set the root cell to an arbitrary cell in the MEP cluster\n    # adata.uns['iroot'] = np.flatnonzero(adata.obs['paul15_clusters'] == '7MEP')[0]\n    # here, set the root cell as in Haghverdi et al. (2016)\n    # note that other than in Matlab/R, counting starts at 0\n    adata.uns['iroot'] = 840\n    return adata", "idx": 8}
{"project": "Scanpy", "commit_id": "109_scanpy_1.9.0__datasets.py_toggleswitch.py", "target": 0, "func": "def toggleswitch() -> ad.AnnData:\n    \"\"\"\\\n    Simulated toggleswitch.\n\n    Data obtained simulating a simple toggleswitch [Gardner00]_\n\n    Simulate via :func:`~scanpy.tl.sim`.\n\n    Returns\n    -------\n    Annotated data matrix.\n    \"\"\"\n    filename = HERE / 'toggleswitch.txt'\n    adata = read(filename, first_column_names=True)\n    adata.uns['iroot'] = 0\n    return adata", "idx": 9}
{"project": "Scanpy", "commit_id": "10_scanpy_1.9.0_function_images.py_insert_function_images.py", "target": 0, "func": "def insert_function_images(\n    app: Sphinx, what: str, name: str, obj: Any, options: Options, lines: List[str]\n):\n    path = app.config.api_dir / f'{name}.png'\n    if what != 'function' or not path.is_file():\n        return\n    lines[0:0] = [\n        f'.. image:: {path.name}',\n        '   :width: 200',\n        '   :align: right',\n        '',", "idx": 10}
{"project": "Scanpy", "commit_id": "110_scanpy_1.9.0__datasets.py_pbmc68k_reduced.py", "target": 0, "func": "def pbmc68k_reduced() -> ad.AnnData:\n    \"\"\"\\\n    Subsampled and processed 68k PBMCs.\n\n    10x PBMC 68k dataset from\n    https://support.10xgenomics.com/single-cell-gene-expression/datasets\n\n    The original PBMC 68k dataset was preprocessed using scanpy and was saved\n    keeping only 724 cells and 221 highly variable genes.\n\n    The saved file contains the annotation of cell types (key: `'bulk_labels'`),\n    UMAP coordinates, louvain clustering and gene rankings based on the\n    `bulk_labels`.\n\n    Returns\n    -------\n    Annotated data matrix.\n    \"\"\"\n\n    filename = HERE / '10x_pbmc68k_reduced.h5ad'\n    with warnings.catch_warnings():\n        warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"anndata\")\n        return read(filename)", "idx": 11}
{"project": "Scanpy", "commit_id": "111_scanpy_1.9.0__datasets.py_pbmc3k.py", "target": 0, "func": "def pbmc3k() -> ad.AnnData:\n    \"\"\"\\\n    3k PBMCs from 10x Genomics.\n\n    The data consists in 3k PBMCs from a Healthy Donor and is freely available\n    from 10x Genomics (`here\n    <http://cf.10xgenomics.com/samples/cell-exp/1.1.0/pbmc3k/pbmc3k_filtered_gene_bc_matrices.tar.gz>`__\n    from this `webpage\n    <https://support.10xgenomics.com/single-cell-gene-expression/datasets/1.1.0/pbmc3k>`__).\n\n    The exact same data is also used in Seurat's\n    `basic clustering tutorial <https://satijalab.org/seurat/pbmc3k_tutorial.html>`__.\n\n    .. note::\n\n        This downloads 5.9 MB of data upon the first call of the function and stores it in `./data/pbmc3k_raw.h5ad`.\n\n    The following code was run to produce the file.\n\n    .. code:: python\n\n        adata = sc.read_10x_mtx(\n            # the directory with the `.mtx` file\n            './data/filtered_gene_bc_matrices/hg19/',\n            # use gene symbols for the variable names (variables-axis index)\n            var_names='gene_symbols',\n            # write a cache file for faster subsequent reading\n            cache=True,\n        )\n\n        adata.var_names_make_unique()  # this is unnecessary if using 'gene_ids'\n        adata.write('write/pbmc3k_raw.h5ad', compression='gzip')\n\n    Returns\n    -------\n    Annotated data matrix.\n    \"\"\"\n    url = 'http://falexwolf.de/data/pbmc3k_raw.h5ad'\n    adata = read(settings.datasetdir / 'pbmc3k_raw.h5ad', backup_url=url)\n    return adata", "idx": 12}
{"project": "Scanpy", "commit_id": "112_scanpy_1.9.0__datasets.py_pbmc3k_processed.py", "target": 0, "func": "def pbmc3k_processed() -> ad.AnnData:\n    \"\"\"Processed 3k PBMCs from 10x Genomics.\n\n    Processed using the `basic tutorial <https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html>`__.\n\n    Returns\n    -------\n    Annotated data matrix.\n    \"\"\"\n    with warnings.catch_warnings():\n        warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"anndata\")\n        return read(\n            settings.datasetdir / 'pbmc3k_processed.h5ad',\n            backup_url='https://raw.githubusercontent.com/chanzuckerberg/cellxgene/main/example-dataset/pbmc3k.h5ad',", "idx": 13}
{"project": "Scanpy", "commit_id": "113_scanpy_1.9.0__datasets.py__download_visium_dataset.py", "target": 0, "func": "def _download_visium_dataset(\n    sample_id: str,\n    spaceranger_version: str,\n    base_dir: Optional[Path] = None,\n    download_image: bool = False,\n):\n    \"\"\"\n    Params\n    ------\n    sample_id\n        String name of example visium dataset.\n    base_dir\n        Where to download the dataset to.\n    download_image\n        Whether to download the high-resolution tissue section.\n    \"\"\"\n    import tarfile\n\n    if base_dir is None:\n        base_dir = settings.datasetdir\n\n    url_prefix = f'https://cf.10xgenomics.com/samples/spatial-exp/{spaceranger_version}/{sample_id}/'\n\n    sample_dir = base_dir / sample_id\n    sample_dir.mkdir(exist_ok=True)\n\n    # Download spatial data\n    tar_filename = f\"{sample_id}_spatial.tar.gz\"\n    tar_pth = sample_dir / tar_filename\n    _utils.check_presence_download(\n        filename=tar_pth, backup_url=url_prefix + tar_filename\n    )\n    with tarfile.open(tar_pth) as f:\n        for el in f:\n            if not (sample_dir / el.name).exists():\n                f.extract(el, sample_dir)\n\n    # Download counts\n    _utils.check_presence_download(\n        filename=sample_dir / \"filtered_feature_bc_matrix.h5\",\n        backup_url=url_prefix + f\"{sample_id}_filtered_feature_bc_matrix.h5\",\n    )\n\n    # Download image\n    if download_image:\n        _utils.check_presence_download(\n            filename=sample_dir / \"image.tif\",\n            backup_url=url_prefix + f\"{sample_id}_image.tif\",", "idx": 14}
{"project": "Scanpy", "commit_id": "114_scanpy_1.9.0__datasets.py_visium_sge.py", "target": 0, "func": "def visium_sge(\n    sample_id: Literal[\n        'V1_Breast_Cancer_Block_A_Section_1',\n        'V1_Breast_Cancer_Block_A_Section_2',\n        'V1_Human_Heart',\n        'V1_Human_Lymph_Node',\n        'V1_Mouse_Kidney',\n        'V1_Adult_Mouse_Brain',\n        'V1_Mouse_Brain_Sagittal_Posterior',\n        'V1_Mouse_Brain_Sagittal_Posterior_Section_2',\n        'V1_Mouse_Brain_Sagittal_Anterior',\n        'V1_Mouse_Brain_Sagittal_Anterior_Section_2',\n        'V1_Human_Brain_Section_1',\n        'V1_Human_Brain_Section_2',\n        'V1_Adult_Mouse_Brain_Coronal_Section_1',\n        'V1_Adult_Mouse_Brain_Coronal_Section_2',\n        # spaceranger version 1.2.0\n        'Targeted_Visium_Human_Cerebellum_Neuroscience',\n        'Parent_Visium_Human_Cerebellum',\n        'Targeted_Visium_Human_SpinalCord_Neuroscience',\n        'Parent_Visium_Human_SpinalCord',\n        'Targeted_Visium_Human_Glioblastoma_Pan_Cancer',\n        'Parent_Visium_Human_Glioblastoma',\n        'Targeted_Visium_Human_BreastCancer_Immunology',\n        'Parent_Visium_Human_BreastCancer',\n        'Targeted_Visium_Human_OvarianCancer_Pan_Cancer',\n        'Targeted_Visium_Human_OvarianCancer_Immunology',\n        'Parent_Visium_Human_OvarianCancer',\n        'Targeted_Visium_Human_ColorectalCancer_GeneSignature',\n        'Parent_Visium_Human_ColorectalCancer',\n    ] = 'V1_Breast_Cancer_Block_A_Section_1',\n    *,\n    include_hires_tiff: bool = False,\n) -> ad.AnnData:\n    \"\"\"\\\n    Processed Visium Spatial Gene Expression data from 10x Genomics.\n    Database: https://support.10xgenomics.com/spatial-gene-expression/datasets\n\n    Parameters\n    ----------\n    sample_id\n        The ID of the data sample in 10x\u2019s spatial database.\n    include_hires_tiff\n        Download and include the high-resolution tissue image (tiff) in `adata.uns[\"spatial\"][sample_id][\"metadata\"][\"source_image_path\"]`.\n\n    Returns\n    -------\n    Annotated data matrix.\n    \"\"\"\n    if \"V1_\" in sample_id:\n        spaceranger_version = \"1.1.0\"\n    else:\n        spaceranger_version = \"1.2.0\"\n    _download_visium_dataset(\n        sample_id, spaceranger_version, download_image=include_hires_tiff\n    )\n    if include_hires_tiff:\n        adata = read_visium(\n            settings.datasetdir / sample_id,\n            source_image_path=settings.datasetdir / sample_id / \"image.tif\",\n        )\n    else:\n        adata = read_visium(settings.datasetdir / sample_id)\n    return adata", "idx": 15}
{"project": "Scanpy", "commit_id": "115_scanpy_1.9.0__ebi_expression_atlas.py__filter_boring.py", "target": 0, "func": "def _filter_boring(dataframe: pd.DataFrame) -> pd.DataFrame:\n    unique_vals = dataframe.apply(lambda x: len(x.unique()))\n    is_boring = (unique_vals == 1) | (unique_vals == len(dataframe))\n    return dataframe.loc[:, ~is_boring]", "idx": 16}
{"project": "Scanpy", "commit_id": "116_scanpy_1.9.0__ebi_expression_atlas.py_sniff_url.py", "target": 0, "func": "def sniff_url(accession: str):\n    # Note that data is downloaded from gxa/sc/experiment, not experiments\n    base_url = f\"https://www.ebi.ac.uk/gxa/sc/experiments/{accession}/\"\n    try:\n        with urlopen(base_url):  # Check if server up/ dataset exists\n            pass\n    except HTTPError as e:\n        e.msg = f\"{e.msg} ({base_url})\"  # Report failed url\n        raise", "idx": 17}
{"project": "Scanpy", "commit_id": "117_scanpy_1.9.0__ebi_expression_atlas.py_download_experiment.py", "target": 0, "func": "def download_experiment(accession: str):\n    sniff_url(accession)\n\n    base_url = f\"https://www.ebi.ac.uk/gxa/sc/experiment/{accession}\"\n    design_url = f\"{base_url}/download?accessKey=&fileType=\"\n    mtx_url = f\"{base_url}/download/zip?accessKey=&fileType=\"\n\n    experiment_dir = settings.datasetdir / accession\n    experiment_dir.mkdir(parents=True, exist_ok=True)\n\n    _download(\n        design_url + \"experiment-design\",\n        experiment_dir / \"experimental_design.tsv\",\n    )\n    _download(\n        mtx_url + \"quantification-raw\",\n        experiment_dir / \"expression_archive.zip\",", "idx": 18}
{"project": "Scanpy", "commit_id": "118_scanpy_1.9.0__ebi_expression_atlas.py_read_mtx_from_stream.py", "target": 0, "func": "def read_mtx_from_stream(stream: BinaryIO) -> sparse.csr_matrix:\n    curline = stream.readline()\n    while curline.startswith(b\"%\"):\n        curline = stream.readline()\n    n, m, _ = (int(x) for x in curline[:-1].split(b\" \"))\n\n    max_int32 = np.iinfo(np.int32).max\n    if n > max_int32 or m > max_int32:\n        coord_dtype = np.int64\n    else:\n        coord_dtype = np.int32\n\n    data = pd.read_csv(\n        stream,\n        sep=r\"\\s+\",\n        header=None,\n        dtype={0: coord_dtype, 1: coord_dtype, 2: np.float32},\n    )\n    mtx = sparse.csr_matrix((data[2], (data[1] - 1, data[0] - 1)), shape=(m, n))\n    return mtx", "idx": 19}
{"project": "Scanpy", "commit_id": "119_scanpy_1.9.0__ebi_expression_atlas.py_read_expression_from_archive.py", "target": 0, "func": "def read_expression_from_archive(archive: ZipFile) -> anndata.AnnData:\n    info = archive.infolist()\n    assert len(info) == 3\n    mtx_data_info = next(i for i in info if i.filename.endswith(\".mtx\"))\n    mtx_rows_info = next(i for i in info if i.filename.endswith(\".mtx_rows\"))\n    mtx_cols_info = next(i for i in info if i.filename.endswith(\".mtx_cols\"))\n    with archive.open(mtx_data_info, \"r\") as f:\n        expr = read_mtx_from_stream(f)\n    with archive.open(mtx_rows_info, \"r\") as f:\n        # TODO: Check what other value could be\n        varname = pd.read_csv(f, sep=\"\\t\", header=None)[1]\n    with archive.open(mtx_cols_info, \"r\") as f:\n        obsname = pd.read_csv(f, sep=\"\\t\", header=None).iloc[:, 0]\n    adata = anndata.AnnData(expr)\n    adata.var_names = varname\n    adata.obs_names = obsname\n    return adata", "idx": 20}
{"project": "Scanpy", "commit_id": "11_scanpy_1.9.0_function_images.py_setup.py", "target": 0, "func": "def setup(app: Sphinx):\n    app.add_config_value('api_dir', Path(), 'env')\n    app.connect('autodoc-process-docstring', insert_function_images)", "idx": 21}
{"project": "Scanpy", "commit_id": "120_scanpy_1.9.0__ebi_expression_atlas.py_ebi_expression_atlas.py", "target": 0, "func": "def ebi_expression_atlas(\n    accession: str, *, filter_boring: bool = False\n) -> anndata.AnnData:\n    \"\"\"\\\n    Load a dataset from the `EBI Single Cell Expression Atlas\n    <https://www.ebi.ac.uk/gxa/sc/experiments>`__\n\n    Downloaded datasets are saved in the directory specified by\n    :attr:`~scanpy._settings.ScanpyConfig.datasetdir`.\n\n    Params\n    ------\n    accession\n        Dataset accession. Like ``E-GEOD-98816`` or ``E-MTAB-4888``.\n        This can be found in the url on the datasets page, for example\n        https://www.ebi.ac.uk/gxa/sc/experiments/E-GEOD-98816/results/tsne.\n    filter_boring\n        Whether boring labels in `.obs` should be automatically removed, such as\n        labels with a single or :attr:`~anndata.AnnData.n_obs` distinct values.\n\n    Example\n    -------\n    >>> import scanpy as sc\n    >>> adata = sc.datasets.ebi_expression_atlas(\"E-MTAB-4888\")\n    \"\"\"\n    experiment_dir = settings.datasetdir / accession\n    dataset_path = experiment_dir / f\"{accession}.h5ad\"\n    try:\n        adata = anndata.read(dataset_path)\n        if filter_boring:\n            adata.obs = _filter_boring(adata.obs)\n        return adata\n    except OSError:\n        # Dataset couldn't be read for whatever reason\n        pass\n\n    download_experiment(accession)\n\n    logg.info(f\"Downloaded {accession} to {experiment_dir.absolute()}\")\n\n    with ZipFile(experiment_dir / \"expression_archive.zip\", \"r\") as f:\n        adata = read_expression_from_archive(f)\n    obs = pd.read_csv(experiment_dir / \"experimental_design.tsv\", sep=\"\\t\", index_col=0)\n\n    adata.obs[obs.columns] = obs\n    adata.write(dataset_path, compression=\"gzip\")  # To be kind to disk space\n\n    if filter_boring:\n        adata.obs = _filter_boring(adata.obs)\n\n    return adata", "idx": 22}
{"project": "Scanpy", "commit_id": "121_scanpy_1.9.0__utils.py_check_datasetdir_exists.py", "target": 0, "func": "def check_datasetdir_exists(f):\n    @wraps(f)\n    def wrapper(*args, **kwargs):\n        settings.datasetdir.mkdir(exist_ok=True)\n        return f(*args, **kwargs)\n\n    return wrapper", "idx": 23}
{"project": "Scanpy", "commit_id": "122_scanpy_1.9.0__utils.py_filter_oldformatwarning.py", "target": 0, "func": "def filter_oldformatwarning(f):\n    \"\"\"\n    Filters anndata.OldFormatWarning from being thrown by the wrapped function.\n    \"\"\"\n\n    @wraps(f)\n    def wrapper(*args, **kwargs):\n        with warnings.catch_warnings():\n            if version.parse(ad.__version__).release >= (0, 8):\n                warnings.filterwarnings(\n                    \"ignore\", category=ad.OldFormatWarning, module=\"anndata\"\n                )\n            return f(*args, **kwargs)\n\n    return wrapper", "idx": 24}
{"project": "Scanpy", "commit_id": "123_scanpy_1.9.0__utils.py_wrapper.py", "target": 0, "func": "def wrapper(*args, **kwargs):\n        with warnings.catch_warnings():\n            if version.parse(ad.__version__).release >= (0, 8):\n                warnings.filterwarnings(\n                    \"ignore\", category=ad.OldFormatWarning, module=\"anndata\"\n                )\n            return f(*args, **kwargs)", "idx": 25}
{"project": "Scanpy", "commit_id": "124_scanpy_1.9.0__highly_variable_genes.py__highly_variable_pearson_residuals.py", "target": 1, "func": "def _highly_variable_pearson_residuals(\n    adata: AnnData,\n    theta: float = 100,\n    clip: Optional[float] = None,\n    n_top_genes: int = 1000,\n    batch_key: Optional[str] = None,\n    chunksize: int = 1000,\n    check_values: bool = True,\n    layer: Optional[str] = None,\n    subset: bool = False,\n    inplace: bool = True,\n) -> Optional[pd.DataFrame]:\n    \"\"\"\\\n    See `scanpy.experimental.pp.highly_variable_genes`.\n\n    Returns\n    -------\n    If `inplace=True`, `adata.var` is updated with the following fields. Otherwise,\n    returns the same fields as :class:`~pandas.DataFrame`.\n\n    highly_variable : bool\n        boolean indicator of highly-variable genes\n    means : float\n        means per gene\n    variances : float\n        variance per gene\n    residual_variances : float\n        Residual variance per gene. Averaged in the case of multiple batches.\n    highly_variable_rank : float\n        Rank of the gene according to residual variance, median rank in the case of multiple batches\n    highly_variable_nbatches : int\n        If `batch_key` given, denotes in how many batches genes are detected as HVG\n    highly_variable_intersection : bool\n        If `batch_key` given, denotes the genes that are highly variable in all batches\n    \"\"\"\n\n    view_to_actual(adata)\n    X = _get_obs_rep(adata, layer=layer)\n    computed_on = layer if layer else 'adata.X'\n\n    # Check for raw counts\n    if check_values and (check_nonnegative_integers(X) is False):\n        warnings.warn(\n            \"`flavor='pearson_residuals'` expects raw count data, but non-integers were found.\",\n            UserWarning,\n        )\n    # check theta\n    if theta <= 0:\n        # TODO: would \"underdispersion\" with negative theta make sense?\n        # then only theta=0 were undefined..\n        raise ValueError('Pearson residuals require theta > 0')\n    # prepare clipping\n\n    if batch_key is None:\n        batch_info = np.zeros(adata.shape[0], dtype=int)\n    else:\n        batch_info = adata.obs[batch_key].values\n    n_batches = len(np.unique(batch_info))\n\n    # Get pearson residuals for each batch separately\n    residual_gene_vars = []\n    for batch in np.unique(batch_info):\n\n        adata_subset_prefilter = adata[batch_info == batch]\n        X_batch_prefilter = _get_obs_rep(adata_subset_prefilter, layer=layer)\n\n        # Filter out zero genes\n        with settings.verbosity.override(Verbosity.error):\n            nonzero_genes = np.ravel(X_batch_prefilter.sum(axis=0)) != 0\n        adata_subset = adata_subset_prefilter[:, nonzero_genes]\n        X_batch = _get_obs_rep(adata_subset, layer=layer)\n\n        # Prepare clipping\n        if clip is None:\n            n = X_batch.shape[0]\n            clip = np.sqrt(n)\n        if clip < 0:\n            raise ValueError(\"Pearson residuals require `clip>=0` or `clip=None`.\")\n\n        if sp_sparse.issparse(X_batch):\n            sums_genes = np.sum(X_batch, axis=0)\n            sums_cells = np.sum(X_batch, axis=1)\n            sum_total = np.sum(sums_genes).squeeze()\n        else:\n            sums_genes = np.sum(X_batch, axis=0, keepdims=True)\n            sums_cells = np.sum(X_batch, axis=1, keepdims=True)\n            sum_total = np.sum(sums_genes)\n\n        # Compute pearson residuals in chunks\n        residual_gene_var = np.empty((X_batch.shape[1]))\n        for start in np.arange(0, X_batch.shape[1], chunksize):\n            stop = start + chunksize\n            mu = np.array(sums_cells @ sums_genes[:, start:stop] / sum_total)\n            X_dense = X_batch[:, start:stop].toarray()\n            residuals = (X_dense - mu) / np.sqrt(mu + mu**2 / theta)\n            residuals = np.clip(residuals, a_min=-clip, a_max=clip)\n            residual_gene_var[start:stop] = np.var(residuals, axis=0)\n\n        # Add 0 values for genes that were filtered out\n        unmasked_residual_gene_var = np.zeros(len(nonzero_genes))\n        unmasked_residual_gene_var[nonzero_genes] = residual_gene_var\n        residual_gene_vars.append(unmasked_residual_gene_var.reshape(1, -1))\n\n    residual_gene_vars = np.concatenate(residual_gene_vars, axis=0)\n\n    # Get rank per gene within each batch\n    # argsort twice gives ranks, small rank means most variable\n    ranks_residual_var = np.argsort(np.argsort(-residual_gene_vars, axis=1), axis=1)\n    ranks_residual_var = ranks_residual_var.astype(np.float32)\n    # count in how many batches a genes was among the n_top_genes\n    highly_variable_nbatches = np.sum(\n        (ranks_residual_var < n_top_genes).astype(int), axis=0\n    )\n    # set non-top genes within each batch to nan\n    ranks_residual_var[ranks_residual_var >= n_top_genes] = np.nan\n    ranks_masked_array = np.ma.masked_invalid(ranks_residual_var)\n    # Median rank across batches, ignoring batches in which gene was not selected\n    medianrank_residual_var = np.ma.median(ranks_masked_array, axis=0).filled(np.nan)\n\n    means, variances = materialize_as_ndarray(_get_mean_var(X))\n    df = pd.DataFrame.from_dict(\n        dict(\n            means=means,\n            variances=variances,\n            residual_variances=np.mean(residual_gene_vars, axis=0),\n            highly_variable_rank=medianrank_residual_var,\n            highly_variable_nbatches=highly_variable_nbatches.astype(np.int64),\n            highly_variable_intersection=highly_variable_nbatches == n_batches,\n        )\n    )\n    df = df.set_index(adata.var_names)\n\n    # Sort genes by how often they selected as hvg within each batch and\n    # break ties with median rank of residual variance across batches\n    df.sort_values(\n        ['highly_variable_nbatches', 'highly_variable_rank'],\n        ascending=[False, True],\n        na_position='last',\n        inplace=True,\n    )\n\n    high_var = np.zeros(df.shape[0], dtype=bool)\n    high_var[:n_top_genes] = True\n    df['highly_variable'] = high_var\n    df = df.loc[adata.var_names, :]\n\n    if inplace:\n        adata.uns['hvg'] = {'flavor': 'pearson_residuals', 'computed_on': computed_on}\n        logg.hint(\n            'added\\n'\n            '    \\'highly_variable\\', boolean vector (adata.var)\\n'\n            '    \\'highly_variable_rank\\', float vector (adata.var)\\n'\n            '    \\'highly_variable_nbatches\\', int vector (adata.var)\\n'\n            '    \\'highly_variable_intersection\\', boolean vector (adata.var)\\n'\n            '    \\'means\\', float vector (adata.var)\\n'\n            '    \\'variances\\', float vector (adata.var)\\n'\n            '    \\'residual_variances\\', float vector (adata.var)'\n        )\n        adata.var['means'] = df['means'].values\n        adata.var['variances'] = df['variances'].values\n        adata.var['residual_variances'] = df['residual_variances']\n        adata.var['highly_variable_rank'] = df['highly_variable_rank'].values\n        if batch_key is not None:\n            adata.var['highly_variable_nbatches'] = df[\n                'highly_variable_nbatches'\n            ].values\n            adata.var['highly_variable_intersection'] = df[\n                'highly_variable_intersection'\n            ].values\n        adata.var['highly_variable'] = df['highly_variable'].values\n\n        if subset:\n            adata._inplace_subset_var(df['highly_variable'].values)\n\n    else:\n        if batch_key is None:\n            df = df.drop(\n                ['highly_variable_nbatches', 'highly_variable_intersection'], axis=1\n            )\n        if subset:\n            df = df.iloc[df.highly_variable.values, :]\n\n        return df", "idx": 26}
{"project": "Scanpy", "commit_id": "125_scanpy_1.9.0__highly_variable_genes.py_highly_variable_genes.py", "target": 0, "func": "def highly_variable_genes(\n    adata: AnnData,\n    *,\n    theta: float = 100,\n    clip: Optional[float] = None,\n    n_top_genes: Optional[int] = None,\n    batch_key: Optional[str] = None,\n    chunksize: int = 1000,\n    flavor: Literal['pearson_residuals'] = 'pearson_residuals',\n    check_values: bool = True,\n    layer: Optional[str] = None,\n    subset: bool = False,\n    inplace: bool = True,\n) -> Optional[pd.DataFrame]:\n    \"\"\"\\\n    Select highly variable genes using analytic Pearson residuals [Lause21]_.\n\n    In [Lause21]_, Pearson residuals of a negative binomial offset model are computed\n    (with overdispersion `theta` shared across genes). By default, overdispersion\n    `theta=100` is used and residuals are clipped to `sqrt(n_obs)`. Finally, genes\n    are ranked by residual variance.\n\n    Expects raw count input.\n\n    Parameters\n    ----------\n    {adata}\n    {dist_params}\n    {genes_batch_chunk}\n    flavor\n        Choose the flavor for identifying highly variable genes. In this experimental\n        version, only 'pearson_residuals' is functional.\n    {check_values}\n    {layer}\n    subset\n        If `True`, subset the data to highly-variable genes after finding them.\n        Otherwise merely indicate highly variable genes in `adata.var` (see below).\n    {inplace}\n\n    Returns\n    -------\n    If `inplace=True`, `adata.var` is updated with the following fields. Otherwise,\n    returns the same fields as :class:`~pandas.DataFrame`.\n\n    highly_variable : bool\n        boolean indicator of highly-variable genes.\n    means : float\n        means per gene.\n    variances : float\n        variance per gene.\n    residual_variances : float\n        For `flavor='pearson_residuals'`, residual variance per gene. Averaged in the\n        case of multiple batches.\n    highly_variable_rank : float\n        For `flavor='pearson_residuals'`, rank of the gene according to residual.\n        variance, median rank in the case of multiple batches.\n    highly_variable_nbatches : int\n        If `batch_key` given, denotes in how many batches genes are detected as HVG.\n    highly_variable_intersection : bool\n        If `batch_key` given, denotes the genes that are highly variable in all batches.\n\n    Notes\n    -----\n    Experimental version of `sc.pp.highly_variable_genes()`\n    \"\"\"\n\n    logg.info('extracting highly variable genes')\n\n    if not isinstance(adata, AnnData):\n        raise ValueError(\n            '`pp.highly_variable_genes` expects an `AnnData` argument, '\n            'pass `inplace=False` if you want to return a `pd.DataFrame`.'\n        )\n\n    if flavor == 'pearson_residuals':\n        if n_top_genes is None:\n            raise ValueError(\n                \"`pp.highly_variable_genes` requires the argument `n_top_genes`\"\n                \" for `flavor='pearson_residuals'`\"\n            )\n        return _highly_variable_pearson_residuals(\n            adata,\n            layer=layer,\n            n_top_genes=n_top_genes,\n            batch_key=batch_key,\n            theta=theta,\n            clip=clip,\n            chunksize=chunksize,\n            subset=subset,\n            check_values=check_values,\n            inplace=inplace,\n        )\n    else:\n        raise ValueError(\n            \"This is an experimental API and only `flavor=pearson_residuals` is available.\"", "idx": 27}
{"project": "Scanpy", "commit_id": "126_scanpy_1.9.0__normalization.py__pearson_residuals.py", "target": 0, "func": "def _pearson_residuals(X, theta, clip, check_values, copy=False):\n\n    X = X.copy() if copy else X\n\n    # check theta\n    if theta <= 0:\n        # TODO: would \"underdispersion\" with negative theta make sense?\n        # then only theta=0 were undefined..\n        raise ValueError('Pearson residuals require theta > 0')\n    # prepare clipping\n    if clip is None:\n        n = X.shape[0]\n        clip = np.sqrt(n)\n    if clip < 0:\n        raise ValueError(\"Pearson residuals require `clip>=0` or `clip=None`.\")\n\n    if check_values and not check_nonnegative_integers(X):\n        warn(\n            \"`normalize_pearson_residuals()` expects raw count data, but non-integers were found.\",\n            UserWarning,\n        )\n\n    if issparse(X):\n        sums_genes = np.sum(X, axis=0)\n        sums_cells = np.sum(X, axis=1)\n        sum_total = np.sum(sums_genes).squeeze()\n    else:\n        sums_genes = np.sum(X, axis=0, keepdims=True)\n        sums_cells = np.sum(X, axis=1, keepdims=True)\n        sum_total = np.sum(sums_genes)\n\n    mu = np.array(sums_cells @ sums_genes / sum_total)\n    diff = np.array(X - mu)\n    residuals = diff / np.sqrt(mu + mu**2 / theta)\n\n    # clip\n    residuals = np.clip(residuals, a_min=-clip, a_max=clip)\n\n    return residuals", "idx": 28}
{"project": "Scanpy", "commit_id": "127_scanpy_1.9.0__normalization.py_normalize_pearson_residuals.py", "target": 0, "func": "def normalize_pearson_residuals(\n    adata: AnnData,\n    *,\n    theta: float = 100,\n    clip: Optional[float] = None,\n    check_values: bool = True,\n    layer: Optional[str] = None,\n    inplace: bool = True,\n    copy: bool = False,\n) -> Optional[Dict[str, np.ndarray]]:\n    \"\"\"\\\n    Applies analytic Pearson residual normalization, based on [Lause21]_.\n\n    The residuals are based on a negative binomial offset model with overdispersion\n    `theta` shared across genes. By default, residuals are clipped to `sqrt(n_obs)`\n    and overdispersion `theta=100` is used.\n\n    Expects raw count input.\n\n    Params\n    ------\n    {adata}\n    {dist_params}\n    {check_values}\n    {layer}\n    {inplace}\n    {copy}\n\n    Returns\n    -------\n    If `inplace=True`, `adata.X` or the selected layer in `adata.layers` is updated\n    with the normalized values. `adata.uns` is updated with the following fields.\n    If `inplace=False`, the same fields are returned as dictionary with the\n    normalized values in `results_dict['X']`.\n\n    `.uns['pearson_residuals_normalization']['theta']`\n         The used value of the overdisperion parameter theta.\n    `.uns['pearson_residuals_normalization']['clip']`\n         The used value of the clipping parameter.\n    `.uns['pearson_residuals_normalization']['computed_on']`\n         The name of the layer on which the residuals were computed.\n    \"\"\"\n\n    if copy:\n        if not inplace:\n            raise ValueError(\"`copy=True` cannot be used with `inplace=False`.\")\n        adata = adata.copy()\n\n    view_to_actual(adata)\n    X = _get_obs_rep(adata, layer=layer)\n    computed_on = layer if layer else 'adata.X'\n\n    msg = f'computing analytic Pearson residuals on {computed_on}'\n    start = logg.info(msg)\n\n    residuals = _pearson_residuals(X, theta, clip, check_values, copy=~inplace)\n    settings_dict = dict(theta=theta, clip=clip, computed_on=computed_on)\n\n    if inplace:\n        _set_obs_rep(adata, residuals, layer=layer)\n        adata.uns['pearson_residuals_normalization'] = settings_dict\n    else:\n        results_dict = dict(X=residuals, **settings_dict)\n\n    logg.info('    finished ({time_passed})', time=start)\n\n    if copy:\n        return adata\n    elif not inplace:\n        return results_dict", "idx": 29}
{"project": "Scanpy", "commit_id": "128_scanpy_1.9.0__normalization.py_normalize_pearson_residuals_pca.py", "target": 0, "func": "def normalize_pearson_residuals_pca(\n    adata: AnnData,\n    *,\n    theta: float = 100,\n    clip: Optional[float] = None,\n    n_comps: Optional[int] = 50,\n    random_state: Optional[float] = 0,\n    kwargs_pca: Optional[dict] = {},\n    use_highly_variable: Optional[bool] = None,\n    check_values: bool = True,\n    inplace: bool = True,\n) -> Optional[AnnData]:\n    \"\"\"\\\n    Applies analytic Pearson residual normalization and PCA, based on [Lause21]_.\n\n    The residuals are based on a negative binomial offset model with overdispersion\n    `theta` shared across genes. By default, residuals are clipped to `sqrt(n_obs)`,\n    overdispersion `theta=100` is used, and PCA is run with 50 components.\n\n    Operates on the subset of highly variable genes in `adata.var['highly_variable']`\n    by default. Expects raw count input.\n\n    Params\n    ------\n    {adata}\n    {dist_params}\n    {pca_chunk}\n    use_highly_variable\n        If `True`, uses gene selection present in `adata.var['highly_variable']` to\n        subset the data before normalizing (default). Otherwise, proceed on the full\n        dataset.\n    {check_values}\n    {inplace}\n\n    Returns\n    -------\n    If `inplace=False`, returns the Pearson residual-based PCA results (as :class:`~anndata.AnnData`\n    object). If `inplace=True`, updates `adata` with the following fields:\n\n    `.uns['pearson_residuals_normalization']['pearson_residuals_df']`\n         The subset of highly variable genes, normalized by Pearson residuals.\n    `.uns['pearson_residuals_normalization']['theta']`\n         The used value of the overdisperion parameter theta.\n    `.uns['pearson_residuals_normalization']['clip']`\n         The used value of the clipping parameter.\n\n    `.obsm['X_pca']`\n        PCA representation of data after gene selection (if applicable) and Pearson\n        residual normalization.\n    `.varm['PCs']`\n         The principal components containing the loadings. When `inplace=True` and\n         `use_highly_variable=True`, this will contain empty rows for the genes not\n         selected.\n    `.uns['pca']['variance_ratio']`\n         Ratio of explained variance.\n    `.uns['pca']['variance']`\n         Explained variance, equivalent to the eigenvalues of the covariance matrix.\n    \"\"\"\n\n    # check if HVG selection is there if user wants to use it\n    if use_highly_variable and 'highly_variable' not in adata.var_keys():\n        raise ValueError(\n            \"You passed `use_highly_variable=True`, but no HVG selection was found \"\n            \"(e.g., there was no 'highly_variable' column in adata.var).'\"\n        )\n\n    # default behavior: if there is a HVG selection, we will use it\n    if use_highly_variable is None and 'highly_variable' in adata.var_keys():\n        use_highly_variable = True\n\n    if use_highly_variable:\n        adata_sub = adata[:, adata.var['highly_variable']].copy()\n        adata_pca = AnnData(\n            adata_sub.X.copy(), obs=adata_sub.obs[[]], var=adata_sub.var[[]]\n        )\n    else:\n        adata_pca = AnnData(adata.X.copy(), obs=adata.obs[[]], var=adata.var[[]])\n\n    normalize_pearson_residuals(\n        adata_pca, theta=theta, clip=clip, check_values=check_values\n    )\n    pca(adata_pca, n_comps=n_comps, random_state=random_state, **kwargs_pca)\n\n    if inplace:\n        norm_settings = adata_pca.uns['pearson_residuals_normalization']\n        norm_dict = dict(**norm_settings, pearson_residuals_df=adata_pca.to_df())\n        if use_highly_variable:\n            adata.varm['PCs'] = np.zeros(shape=(adata.n_vars, n_comps))\n            adata.varm['PCs'][adata.var['highly_variable']] = adata_pca.varm['PCs']\n        else:\n            adata.varm['PCs'] = adata_pca.varm['PCs']\n        adata.uns['pca'] = adata_pca.uns['pca']\n        adata.uns['pearson_residuals_normalization'] = norm_dict\n        adata.obsm['X_pca'] = adata_pca.obsm['X_pca']\n        return None\n    else:\n        return adata_pca", "idx": 30}
{"project": "Scanpy", "commit_id": "129_scanpy_1.9.0__recipes.py_recipe_pearson_residuals.py", "target": 0, "func": "def recipe_pearson_residuals(\n    adata: AnnData,\n    *,\n    theta: float = 100,\n    clip: Optional[float] = None,\n    n_top_genes: int = 1000,\n    batch_key: Optional[str] = None,\n    chunksize: int = 1000,\n    n_comps: Optional[int] = 50,\n    random_state: Optional[float] = 0,\n    kwargs_pca: dict = {},\n    check_values: bool = True,\n    inplace: bool = True,\n) -> Optional[Tuple[AnnData, pd.DataFrame]]:\n    \"\"\"\\\n    Full pipeline for HVG selection and normalization by analytic Pearson residuals ([Lause21]_).\n\n    Applies gene selection based on Pearson residuals. On the resulting subset,\n    Pearson residual normalization and PCA are performed.\n\n    Expects raw count input.\n\n    Params\n    ------\n    {adata}\n    {dist_params}\n    {genes_batch_chunk}\n    {pca_chunk}\n    {check_values}\n    {inplace}\n\n    Returns\n    -------\n    If `inplace=False`, separately returns the gene selection results (as\n    :class:`~pandas.DataFrame`) and Pearson residual-based PCA results (as\n    :class:`~anndata.AnnData`). If `inplace=True`, updates `adata` with the\n    following fields for gene selection results:\n\n    `.var['highly_variable']` : bool\n        boolean indicator of highly-variable genes.\n    `.var['means']` : float\n        means per gene.\n    `.var['variances']` : float\n        variances per gene.\n    `.var['residual_variances']` : float\n        Pearson residual variance per gene. Averaged in the case of multiple\n        batches.\n    `.var['highly_variable_rank']` : float\n        Rank of the gene according to residual variance, median rank in the\n        case of multiple batches.\n    `.var['highly_variable_nbatches']` : int\n        If batch_key is given, this denotes in how many batches genes are\n        detected as HVG.\n    `.var['highly_variable_intersection']` : bool\n        If batch_key is given, this denotes the genes that are highly variable\n        in all batches.\n\n    The following fields contain Pearson residual-based PCA results and\n    normalization settings:\n\n    `.uns['pearson_residuals_normalization']['pearson_residuals_df']`\n         The subset of highly variable genes, normalized by Pearson residuals.\n    `.uns['pearson_residuals_normalization']['theta']`\n         The used value of the overdisperion parameter theta.\n    `.uns['pearson_residuals_normalization']['clip']`\n         The used value of the clipping parameter.\n\n    `.obsm['X_pca']`\n        PCA representation of data after gene selection and Pearson residual\n        normalization.\n    `.varm['PCs']`\n         The principal components containing the loadings. When `inplace=True` this\n         will contain empty rows for the genes not selected during HVG selection.\n    `.uns['pca']['variance_ratio']`\n         Ratio of explained variance.\n    `.uns['pca']['variance']`\n         Explained variance, equivalent to the eigenvalues of the covariance matrix.\n    \"\"\"\n\n    hvg_args = dict(\n        flavor='pearson_residuals',\n        n_top_genes=n_top_genes,\n        batch_key=batch_key,\n        theta=theta,\n        clip=clip,\n        chunksize=chunksize,\n        check_values=check_values,\n    )\n\n    if inplace:\n        experimental.pp.highly_variable_genes(adata, **hvg_args, inplace=True)\n        # TODO: are these copies needed?\n        adata_pca = adata[:, adata.var['highly_variable']].copy()\n    else:\n        hvg = experimental.pp.highly_variable_genes(adata, **hvg_args, inplace=False)\n        # TODO: are these copies needed?\n        adata_pca = adata[:, hvg['highly_variable']].copy()\n\n    experimental.pp.normalize_pearson_residuals(\n        adata_pca, theta=theta, clip=clip, check_values=check_values\n    )\n    pca(adata_pca, n_comps=n_comps, random_state=random_state, **kwargs_pca)\n\n    if inplace:\n        normalization_param = adata_pca.uns['pearson_residuals_normalization']\n        normalization_dict = dict(\n            **normalization_param, pearson_residuals_df=adata_pca.to_df()\n        )\n\n        adata.uns['pca'] = adata_pca.uns['pca']\n        adata.varm['PCs'] = np.zeros(shape=(adata.n_vars, n_comps))\n        adata.varm['PCs'][adata.var['highly_variable']] = adata_pca.varm['PCs']\n        adata.uns['pearson_residuals_normalization'] = normalization_dict\n        adata.obsm['X_pca'] = adata_pca.obsm['X_pca']\n        return None\n    else:\n        return adata_pca, hvg", "idx": 31}
{"project": "Scanpy", "commit_id": "12_scanpy_1.9.0_github_links.py_register_links.py", "target": 0, "func": "def register_links(app: Sphinx, config: Config):\n    gh_url = 'https://github.com/{github_user}/{github_repo}'.format_map(\n        config.html_context\n    )\n    app.add_role('pr', AutoLink('pr', f'{gh_url}/pull/{{}}', 'PR {}'))\n    app.add_role('issue', AutoLink('issue', f'{gh_url}/issues/{{}}', 'issue {}'))\n    app.add_role('noteversion', AutoLink('noteversion', f'{gh_url}/releases/tag/{{}}'))\n    # tutorial links\n    scanpy_tutorials_url = 'https://scanpy-tutorials.readthedocs.io/en/latest/'\n    app.add_role(\n        'tutorial',\n        AutoLink('tutorial', f'{scanpy_tutorials_url}{{}}.html', '\u2192 tutorial: {}'),", "idx": 32}
{"project": "Scanpy", "commit_id": "130_scanpy_1.9.0_exporting.py_spring_project.py", "target": 1, "func": "def spring_project(\n    adata: AnnData,\n    project_dir: Union[Path, str],\n    embedding_method: str,\n    subplot_name: Optional[str] = None,\n    cell_groupings: Union[str, Iterable[str], None] = None,\n    custom_color_tracks: Union[str, Iterable[str], None] = None,\n    total_counts_key: str = 'n_counts',\n    neighbors_key: Optional[str] = None,\n    overwrite: bool = False,\n):\n    \"\"\"\\\n    Exports to a SPRING project directory [Weinreb17]_.\n\n    Visualize annotation present in `adata`. By default, export all gene expression data\n    from `adata.raw` and categorical and continuous annotations present in `adata.obs`.\n\n    See `SPRING <https://github.com/AllonKleinLab/SPRING>`__ or [Weinreb17]_ for details.\n\n    Parameters\n    ----------\n    adata\n        Annotated data matrix: `adata.uns['neighbors']` needs to\n        be present.\n    project_dir\n        Path to directory for exported SPRING files.\n    embedding_method\n        Name of a 2-D embedding in `adata.obsm`\n    subplot_name\n        Name of subplot folder to be created at `project_dir+\"/\"+subplot_name`\n    cell_groupings\n        Instead of importing all categorical annotations when `None`,\n        pass a list of keys for `adata.obs`.\n    custom_color_tracks\n        Specify specific `adata.obs` keys for continuous coloring.\n    total_counts_key\n        Name of key for total transcript counts in `adata.obs`.\n    overwrite\n        When `True`, existing counts matrices in `project_dir` are overwritten.\n\n    Examples\n    --------\n    See this `tutorial <https://github.com/theislab/scanpy_usage/tree/master/171111_SPRING_export>`__.\n    \"\"\"\n\n    # need to get nearest neighbors first\n    if neighbors_key is None:\n        neighbors_key = 'neighbors'\n\n    if neighbors_key not in adata.uns:\n        raise ValueError('Run `sc.pp.neighbors` first.')\n\n    # check that requested 2-D embedding has been generated\n    if embedding_method not in adata.obsm_keys():\n        if 'X_' + embedding_method in adata.obsm_keys():\n            embedding_method = 'X_' + embedding_method\n        else:\n            if embedding_method in adata.uns:\n                embedding_method = (\n                    'X_'\n                    + embedding_method\n                    + '_'\n                    + adata.uns[embedding_method]['params']['layout']\n                )\n            else:\n                raise ValueError(\n                    'Run the specified embedding method `%s` first.' % embedding_method\n                )\n\n    coords = adata.obsm[embedding_method]\n\n    # Make project directory and subplot directory (subplot has same name as project)\n    # For now, the subplot is just all cells in adata\n    project_dir: Path = Path(project_dir)\n    subplot_dir: Path = (\n        project_dir.parent if subplot_name is None else project_dir / subplot_name\n    )\n    subplot_dir.mkdir(parents=True, exist_ok=True)\n    print(f'Writing subplot to {subplot_dir}')\n\n    # Write counts matrices as hdf5 files and npz if they do not already exist\n    # or if user requires overwrite.\n    # To do: check if Alex's h5sparse format will allow fast loading from just\n    # one file.\n    write_counts_matrices = True\n    base_dir_filelist = [\n        'counts_norm_sparse_genes.hdf5',\n        'counts_norm_sparse_cells.hdf5',\n        'counts_norm.npz',\n        'total_counts.txt',\n        'genes.txt',\n    ]\n    if all((project_dir / f).is_file() for f in base_dir_filelist):\n        if not overwrite:\n            logg.warning(\n                f'{project_dir} is an existing SPRING folder. A new subplot will be created, but '\n                'you must set `overwrite=True` to overwrite counts matrices.'\n            )\n            write_counts_matrices = False\n        else:\n            logg.warning(f'Overwriting the files in {project_dir}.')\n\n    # Ideally, all genes will be written from adata.raw\n    if adata.raw is not None:\n        E = adata.raw.X.tocsc()\n        gene_list = list(adata.raw.var_names)\n    else:\n        E = adata.X.tocsc()\n        gene_list = list(adata.var_names)\n\n    # Keep track of total counts per cell if present\n    if total_counts_key in adata.obs:\n        total_counts = np.array(adata.obs[total_counts_key])\n    else:\n        total_counts = E.sum(1).A1\n\n    # Write the counts matrices to project directory\n    if write_counts_matrices:\n        write_hdf5_genes(E, gene_list, project_dir / 'counts_norm_sparse_genes.hdf5')\n        write_hdf5_cells(E, project_dir / 'counts_norm_sparse_cells.hdf5')\n        write_sparse_npz(E, project_dir / 'counts_norm.npz')\n        with (project_dir / 'genes.txt').open('w') as o:\n            for g in gene_list:\n                o.write(g + '\\n')\n        np.savetxt(project_dir / 'total_counts.txt', total_counts)\n\n    # Get categorical and continuous metadata\n    categorical_extras = {}\n    continuous_extras = {}\n    if cell_groupings is None:\n        for obs_name in adata.obs:\n            if is_categorical(adata.obs[obs_name]):\n                categorical_extras[obs_name] = [str(x) for x in adata.obs[obs_name]]\n    else:\n        if isinstance(cell_groupings, str):\n            cell_groupings = [cell_groupings]\n        for obs_name in cell_groupings:\n            if obs_name not in adata.obs:\n                logg.warning(f'Cell grouping {obs_name!r} is not in adata.obs')\n            elif is_categorical(adata.obs[obs_name]):\n                categorical_extras[obs_name] = [str(x) for x in adata.obs[obs_name]]\n            else:\n                logg.warning(\n                    f'Cell grouping {obs_name!r} is not a categorical variable'\n                )\n    if custom_color_tracks is None:\n        for obs_name in adata.obs:\n            if not is_categorical(adata.obs[obs_name]):\n                continuous_extras[obs_name] = np.array(adata.obs[obs_name])\n    else:\n        if isinstance(custom_color_tracks, str):\n            custom_color_tracks = [custom_color_tracks]\n        for obs_name in custom_color_tracks:\n            if obs_name not in adata.obs:\n                logg.warning(f'Custom color track {obs_name!r} is not in adata.obs')\n            elif not is_categorical(adata.obs[obs_name]):\n                continuous_extras[obs_name] = np.array(adata.obs[obs_name])\n            else:\n                logg.warning(\n                    f'Custom color track {obs_name!r} is not a continuous variable'\n                )\n\n    # Write continuous colors\n    continuous_extras['Uniform'] = np.zeros(E.shape[0])\n    _write_color_tracks(continuous_extras, subplot_dir / 'color_data_gene_sets.csv')\n\n    # Create and write a dictionary of color profiles to be used by the visualizer\n    color_stats = {}\n    color_stats = _get_color_stats_genes(color_stats, E, gene_list)\n    color_stats = _get_color_stats_custom(color_stats, continuous_extras)\n    _write_color_stats(subplot_dir / 'color_stats.json', color_stats)\n\n    # Write categorical data\n    categorical_coloring_data = {}\n    categorical_coloring_data = _build_categ_colors(\n        categorical_coloring_data, categorical_extras\n    )\n    _write_cell_groupings(\n        subplot_dir / 'categorical_coloring_data.json', categorical_coloring_data\n    )\n\n    # Write graph in two formats for backwards compatibility\n    edges = _get_edges(adata, neighbors_key)\n    _write_graph(subplot_dir / 'graph_data.json', E.shape[0], edges)\n    _write_edges(subplot_dir / 'edges.csv', edges)\n\n    # Write cell filter; for now, subplots must be generated from within SPRING,\n    # so cell filter includes all cells.\n    np.savetxt(subplot_dir / 'cell_filter.txt', np.arange(E.shape[0]), fmt='%i')\n    np.save(subplot_dir / 'cell_filter.npy', np.arange(E.shape[0]))\n\n    # Write 2-D coordinates, after adjusting to roughly match SPRING's default d3js force layout parameters\n    coords = coords - coords.min(0)[None, :]\n    coords = (\n        coords * (np.array([1000, 1000]) / coords.ptp(0))[None, :]\n        + np.array([200, -200])[None, :]\n    )\n    np.savetxt(\n        subplot_dir / 'coordinates.txt',\n        np.hstack((np.arange(E.shape[0])[:, None], coords)),\n        fmt='%i,%.6f,%.6f',\n    )\n\n    # Write some useful intermediates, if they exist\n    if 'X_pca' in adata.obsm_keys():\n        np.savez_compressed(\n            subplot_dir / 'intermediates.npz',\n            Epca=adata.obsm['X_pca'],\n            total_counts=total_counts,\n        )\n\n    # Write PAGA data, if present\n    if 'paga' in adata.uns:\n        clusts = np.array(adata.obs[adata.uns['paga']['groups']].cat.codes)\n        uniq_clusts = adata.obs[adata.uns['paga']['groups']].cat.categories\n        paga_coords = [coords[clusts == i, :].mean(0) for i in range(len(uniq_clusts))]\n        _export_PAGA_to_SPRING(adata, paga_coords, subplot_dir / 'PAGA_data.json')", "idx": 33}
{"project": "Scanpy", "commit_id": "131_scanpy_1.9.0_exporting.py__get_edges.py", "target": 0, "func": "def _get_edges(adata, neighbors_key=None):\n    neighbors = NeighborsView(adata, neighbors_key)\n    if 'distances' in neighbors:  # these are sparse matrices\n        matrix = neighbors['distances']\n    else:\n        matrix = neighbors['connectivities']\n    matrix = matrix.tocoo()\n    edges = [(i, j) for i, j in zip(matrix.row, matrix.col)]\n\n    return edges", "idx": 34}
{"project": "Scanpy", "commit_id": "132_scanpy_1.9.0_exporting.py_write_hdf5_genes.py", "target": 0, "func": "def write_hdf5_genes(E, gene_list, filename):\n    '''SPRING standard: filename = main_spring_dir + \"counts_norm_sparse_genes.hdf5\"'''\n\n    E = E.tocsc()\n\n    hf = h5py.File(filename, 'w')\n    counts_group = hf.create_group('counts')\n    cix_group = hf.create_group('cell_ix')\n\n    hf.attrs['ncells'] = E.shape[0]\n    hf.attrs['ngenes'] = E.shape[1]\n\n    for iG, g in enumerate(gene_list):\n        counts = E[:, iG].A.squeeze()\n        cell_ix = np.nonzero(counts)[0]\n        counts = counts[cell_ix]\n        counts_group.create_dataset(g, data=counts)\n        cix_group.create_dataset(g, data=cell_ix)\n\n    hf.close()", "idx": 35}
{"project": "Scanpy", "commit_id": "133_scanpy_1.9.0_exporting.py_write_hdf5_cells.py", "target": 0, "func": "def write_hdf5_cells(E, filename):\n    '''SPRING standard: filename = main_spring_dir + \"counts_norm_sparse_cells.hdf5\"'''\n\n    E = E.tocsr()\n\n    hf = h5py.File(filename, 'w')\n    counts_group = hf.create_group('counts')\n    gix_group = hf.create_group('gene_ix')\n\n    hf.attrs['ncells'] = E.shape[0]\n    hf.attrs['ngenes'] = E.shape[1]\n\n    for iC in range(E.shape[0]):\n        counts = E[iC, :].A.squeeze()\n        gene_ix = np.nonzero(counts)[0]\n        counts = counts[gene_ix]\n        counts_group.create_dataset(str(iC), data=counts)\n        gix_group.create_dataset(str(iC), data=gene_ix)\n\n    hf.close()", "idx": 36}
{"project": "Scanpy", "commit_id": "134_scanpy_1.9.0_exporting.py_write_sparse_npz.py", "target": 0, "func": "def write_sparse_npz(E, filename, compressed=False):\n    '''SPRING standard: filename = main_spring_dir + \"/counts_norm.npz\"'''\n    E = E.tocsc()\n    scipy.sparse.save_npz(filename, E, compressed=compressed)", "idx": 37}
{"project": "Scanpy", "commit_id": "135_scanpy_1.9.0_exporting.py__write_graph.py", "target": 0, "func": "def _write_graph(filename, n_nodes, edges):\n    nodes = [{'name': int(i), 'number': int(i)} for i in range(n_nodes)]\n    edges = [{'source': int(i), 'target': int(j), 'distance': 0} for i, j in edges]\n    out = {'nodes': nodes, 'links': edges}\n    open(filename, 'w').write(json.dumps(out, indent=4, separators=(',', ': ')))", "idx": 38}
{"project": "Scanpy", "commit_id": "136_scanpy_1.9.0_exporting.py__write_edges.py", "target": 0, "func": "def _write_edges(filename, edges):\n    with open(filename, 'w') as f:\n        for e in edges:\n            f.write('%i;%i\\n' % (e[0], e[1]))", "idx": 39}
{"project": "Scanpy", "commit_id": "137_scanpy_1.9.0_exporting.py__write_color_tracks.py", "target": 0, "func": "def _write_color_tracks(ctracks, fname):\n    out = []\n    for name, score in ctracks.items():\n        line = name + ',' + ','.join(['%.3f' % x for x in score])\n        out += [line]\n    out = sorted(out, key=lambda x: x.split(',')[0])\n    open(fname, 'w').write('\\n'.join(out))", "idx": 40}
{"project": "Scanpy", "commit_id": "138_scanpy_1.9.0_exporting.py__frac_to_hex.py", "target": 0, "func": "def _frac_to_hex(frac):\n    rgb = tuple(np.array(np.array(plt.cm.jet(frac)[:3]) * 255, dtype=int))\n    return '#%02x%02x%02x' % rgb", "idx": 41}
{"project": "Scanpy", "commit_id": "139_scanpy_1.9.0_exporting.py__get_color_stats_genes.py", "target": 0, "func": "def _get_color_stats_genes(color_stats, E, gene_list):\n    means, variances = _get_mean_var(E)\n    stdevs = np.zeros(variances.shape, dtype=float)\n    stdevs[variances > 0] = np.sqrt(variances[variances > 0])\n    mins = E.min(0).todense().A1\n    maxes = E.max(0).todense().A1\n\n    pctl = 99.6\n    pctl_n = (100 - pctl) / 100.0 * E.shape[0]\n    pctls = np.zeros(E.shape[1], dtype=float)\n    for iG in range(E.shape[1]):\n        n_nonzero = E.indptr[iG + 1] - E.indptr[iG]\n        if n_nonzero > pctl_n:\n            pctls[iG] = np.percentile(\n                E.data[E.indptr[iG] : E.indptr[iG + 1]], 100 - 100 * pctl_n / n_nonzero\n            )\n        else:\n            pctls[iG] = 0\n        color_stats[gene_list[iG]] = tuple(\n            map(float, (means[iG], stdevs[iG], mins[iG], maxes[iG], pctls[iG]))\n        )\n    return color_stats", "idx": 42}
{"project": "Scanpy", "commit_id": "13_scanpy_1.9.0_github_links.py_setup.py", "target": 0, "func": "def setup(app: Sphinx):\n    app.connect('config-inited', register_links)", "idx": 43}
{"project": "Scanpy", "commit_id": "140_scanpy_1.9.0_exporting.py__get_color_stats_custom.py", "target": 0, "func": "def _get_color_stats_custom(color_stats, custom_colors):\n    for k, v in custom_colors.items():\n        color_stats[k] = tuple(\n            map(\n                float,\n                (np.mean(v), np.std(v), np.min(v), np.max(v), np.percentile(v, 99)),\n            )\n        )\n    return color_stats", "idx": 44}
{"project": "Scanpy", "commit_id": "141_scanpy_1.9.0_exporting.py__write_color_stats.py", "target": 0, "func": "def _write_color_stats(filename, color_stats):\n    with open(filename, 'w') as f:\n        f.write(json.dumps(color_stats, indent=4, sort_keys=True))  # .decode('utf-8'))", "idx": 45}
{"project": "Scanpy", "commit_id": "142_scanpy_1.9.0_exporting.py__build_categ_colors.py", "target": 0, "func": "def _build_categ_colors(categorical_coloring_data, cell_groupings):\n    for k, labels in cell_groupings.items():\n        label_colors = {\n            l: _frac_to_hex(float(i) / len(set(labels)))\n            for i, l in enumerate(list(set(labels)))\n        }\n        categorical_coloring_data[k] = {\n            'label_colors': label_colors,\n            'label_list': labels,\n        }\n    return categorical_coloring_data", "idx": 46}
{"project": "Scanpy", "commit_id": "143_scanpy_1.9.0_exporting.py__write_cell_groupings.py", "target": 0, "func": "def _write_cell_groupings(filename, categorical_coloring_data):\n    with open(filename, 'w') as f:\n        f.write(\n            json.dumps(categorical_coloring_data, indent=4, sort_keys=True)", "idx": 47}
{"project": "Scanpy", "commit_id": "144_scanpy_1.9.0_exporting.py__export_PAGA_to_SPRING.py", "target": 0, "func": "def _export_PAGA_to_SPRING(adata, paga_coords, outpath):\n    # retrieve node data\n    group_key = adata.uns['paga']['groups']\n    names = adata.obs[group_key].cat.categories\n    coords = [list(xy) for xy in paga_coords]\n\n    sizes = list(adata.uns[group_key + '_sizes'])\n    clus_labels = adata.obs[group_key].cat.codes.values\n    cell_groups = [\n        [int(j) for j in np.nonzero(clus_labels == i)[0]] for i in range(len(names))\n    ]\n\n    if group_key + '_colors' in adata.uns:\n        colors = list(adata.uns[group_key + '_colors'])\n    else:\n        import scanpy.plotting.utils\n\n        scanpy.plotting.utils.add_colors_for_categorical_sample_annotation(\n            adata, group_key\n        )\n        colors = list(adata.uns[group_key + '_colors'])\n\n    # retrieve edge level data\n    sources, targets = adata.uns['paga']['connectivities'].nonzero()\n    weights = np.sqrt(adata.uns['paga']['connectivities'].data) / 3\n\n    # save a threshold weight for showing edges so that by default,\n    # the number of edges shown is 8X the number of nodes\n    if len(names) * 8 > len(weights):\n        min_edge_weight_view = 0\n    else:\n        min_edge_weight_view = sorted(weights)[-len(names) * 8]\n\n    # save another threshold for even saving edges at all, with 100 edges per node\n    if len(weights) < 100 * len(names):\n        min_edge_weight_save = 0\n    else:\n        min_edge_weight_save = sorted(weights)[-len(names) * 100]\n\n    # make node list\n    nodes = []\n    for i, name, xy, color, size, cells in zip(\n        range(len(names)), names, coords, colors, sizes, cell_groups\n    ):\n        nodes.append(\n            {\n                'index': i,\n                'size': int(size),\n                'color': color,\n                'coordinates': xy,\n                'cells': cells,\n                'name': name,\n            }\n        )\n\n    # make link list, avoid redundant encoding (graph is undirected)\n    links = []\n    for source, target, weight in zip(sources, targets, weights):\n        if source < target and weight > min_edge_weight_save:\n            links.append(\n                {'source': int(source), 'target': int(target), 'weight': float(weight)}\n            )\n\n    # save data about edge weights\n    edge_weight_meta = {\n        'min_edge_weight': min_edge_weight_view,\n        'max_edge_weight': np.max(weights),\n    }\n\n    PAGA_data = {'nodes': nodes, 'links': links, 'edge_weight_meta': edge_weight_meta}\n\n    import json\n\n    json.dump(PAGA_data, open(outpath, 'w'), indent=4)\n\n    return None", "idx": 48}
{"project": "Scanpy", "commit_id": "145_scanpy_1.9.0_exporting.py_cellbrowser.py", "target": 0, "func": "def cellbrowser(\n    adata: AnnData,\n    data_dir: Union[Path, str],\n    data_name: str,\n    embedding_keys: Union[Iterable[str], Mapping[str, str], str, None] = None,\n    annot_keys: Union[Iterable[str], Mapping[str, str], None] = (\n        \"louvain\",\n        \"percent_mito\",\n        \"n_genes\",\n        \"n_counts\",\n    ),\n    cluster_field: str = \"louvain\",\n    nb_marker: int = 50,\n    skip_matrix: bool = False,\n    html_dir: Union[Path, str, None] = None,\n    port: Optional[int] = None,\n    do_debug: bool = False,\n):\n    \"\"\"\\\n    Export adata to a UCSC Cell Browser project directory. If `html_dir` is\n    set, subsequently build the html files from the project directory into\n    `html_dir`. If `port` is set, start an HTTP server in the background and\n    serve `html_dir` on `port`.\n\n    By default, export all gene expression data from `adata.raw`, the\n    annotations `louvain`, `percent_mito`, `n_genes` and `n_counts` and the top\n    `nb_marker` cluster markers. All existing files in data_dir are\n    overwritten, except `cellbrowser.conf`.\n\n    See `UCSC Cellbrowser <https://github.com/maximilianh/cellBrowser>`__ for\n    details.\n\n    Parameters\n    ----------\n    adata\n        Annotated data matrix\n    data_dir\n        Path to directory for exported Cell Browser files.\n        Usually these are the files `exprMatrix.tsv.gz`, `meta.tsv`,\n        coordinate files like `tsne.coords.tsv`,\n        and cluster marker gene lists like `markers.tsv`.\n        A file `cellbrowser.conf` is also created with pointers to these files.\n        As a result, each adata object should have its own project_dir.\n    data_name\n        Name of dataset in Cell Browser, a string without special characters.\n        This is written to `data_dir/cellbrowser.conf`.\n        Ideally this is a short unique name for the dataset,\n        like `\"pbmc3k\"` or `\"tabulamuris\"`.\n    embedding_keys\n        2-D embeddings in `adata.obsm` to export.\n        The prefix `X_` or `X_draw_graph_` is not necessary.\n        Coordinates missing from `adata` are skipped.\n        By default (or when specifying `'all'` or `None`), these keys are tried:\n        [`\"tsne\"`, `\"umap\"`, `\"pagaFa\"`, `\"pagaFr\"`, `\"pagaUmap\"`, `\"phate\"`,\n        `\"fa\"`, `\"fr\"`, `\"kk\"`, `\"drl\"`, `\"rt\"`, `\"trimap\"`].\n        For these, default display labels are automatically used.\n        For other values, you can specify a mapping from coordinate name to\n        display label, e.g. `{\"tsne\": \"t-SNE by Scanpy\"}`.\n    annot_keys\n        Annotations in `adata.obsm` to export.\n        Can be a mapping from annotation column name to display label.\n        Specify `None` for all available columns in `.obs`.\n    skip_matrix\n        Do not export the matrix.\n        If you had previously exported this adata into the same `data_dir`,\n        then there is no need to export the whole matrix again.\n        This option will make the export a lot faster,\n        e.g. when only coordinates or meta data were changed.\n    html_dir\n        If this variable is set, the export will build html\n        files from `data_dir` to `html_dir`, creating html/js/json files.\n        Usually there is one global html output directory for all datasets.\n        Often, `html_dir` is located under a webserver's (like Apache)\n        htdocs directory or is copied to one.\n        A directory `html_dir`/`project_name` will be created and\n        an index.html will be created under `html_dir` for all subdirectories.\n        Existing files will be overwritten.\n        If do not to use html_dir,\n        you can use the command line tool `cbBuild` to build the html directory.\n    port\n        If this variable and `html_dir` are set,\n        Python's built-in web server will be spawned as a daemon in the\n        background and serve the files under `html_dir`.\n        To kill the process, call `cellbrowser.cellbrowser.stop()`.\n    do_debug\n        Activate debugging output\n\n    Examples\n    --------\n    See this\n    `tutorial <https://github.com/theislab/scanpy_usage/tree/master/181126_Cellbrowser_exports>`__.\n    \"\"\"\n\n    try:\n        import cellbrowser.cellbrowser as cb\n    except ImportError:\n        logg.error(\n            \"The package cellbrowser is not installed. \"\n            \"Install with 'pip install cellbrowser' and retry.\"\n        )\n        raise\n\n    data_dir = str(data_dir)\n\n    cb.setDebug(do_debug)\n    cb.scanpyToCellbrowser(\n        adata,\n        data_dir,\n        data_name,\n        coordFields=embedding_keys,\n        metaFields=annot_keys,\n        clusterField=cluster_field,\n        nb_marker=nb_marker,\n        skipMatrix=skip_matrix,\n        doDebug=None,\n    )\n\n    if html_dir is not None:\n        html_dir = str(html_dir)\n        cb.build(data_dir, html_dir, doDebug=None)\n        if port is not None:\n            cb.serve(html_dir, port)", "idx": 49}
{"project": "Scanpy", "commit_id": "146_scanpy_1.9.0_pl.py_phate.py", "target": 0, "func": "def phate(adata, **kwargs) -> Union[List[Axes], None]:\n    \"\"\"\\\n    Scatter plot in PHATE basis.\n\n    Parameters\n    ----------\n    {adata_color_etc}\n    {edges_arrows}\n    {scatter_bulk}\n    {show_save_ax}\n\n    Returns\n    -------\n    If `show==False`, a list of :class:`~matplotlib.axes.Axes` objects.\n    Every second element corresponds to the 'right margin'\n    drawing area for color bars and legends.\n\n    Examples\n    --------\n    >>> from anndata import AnnData\n    >>> import scanpy.external as sce\n    >>> import phate\n    >>> data, branches = phate.tree.gen_dla(\n    ...     n_dim=100,\n    ...     n_branch=20,\n    ...     branch_length=100,\n    ... )\n    >>> data.shape\n    (2000, 100)\n    >>> adata = AnnData(data)\n    >>> adata.obs['branches'] = branches\n    >>> sce.tl.phate(adata, k=5, a=20, t=150)\n    >>> adata.obsm['X_phate'].shape\n    (2000, 2)\n    >>> sce.pl.phate(\n    ...     adata,\n    ...     color='branches',\n    ...     color_map='tab20',\n    ... )\n    \"\"\"\n    return embedding(adata, 'phate', **kwargs)", "idx": 50}
{"project": "Scanpy", "commit_id": "147_scanpy_1.9.0_pl.py_trimap.py", "target": 0, "func": "def trimap(adata, **kwargs) -> Union[Axes, List[Axes], None]:\n    \"\"\"\\\n    Scatter plot in TriMap basis.\n\n    Parameters\n    ----------\n    {adata_color_etc}\n    {edges_arrows}\n    {scatter_bulk}\n    {show_save_ax}\n\n    Returns\n    -------\n    If `show==False` a :class:`~matplotlib.axes.Axes` or a list of it.\n    \"\"\"\n    return embedding(adata, 'trimap', **kwargs)", "idx": 51}
{"project": "Scanpy", "commit_id": "148_scanpy_1.9.0_pl.py_harmony_timeseries.py", "target": 0, "func": "def harmony_timeseries(\n    adata, *, show: bool = True, return_fig: bool = False, **kwargs\n) -> Union[Axes, List[Axes], None]:\n    \"\"\"\\\n    Scatter plot in Harmony force-directed layout basis.\n\n    Parameters\n    ----------\n    {adata_color_etc}\n    {edges_arrows}\n    {scatter_bulk}\n    {show_save_ax}\n\n    Returns\n    -------\n    If `return_fig` is True, a :class:`~matplotlib.figure.Figure`.\n    If `show==False` a :class:`~matplotlib.axes.Axes` or a list of it.\n    \"\"\"\n\n    tp_name = adata.uns[\"harmony_timepoint_var\"]\n    tps = adata.obs[tp_name].unique()\n\n    fig, axes = plt.subplots(1, len(tps))\n    for i, tp in enumerate(tps):\n        p = embedding(\n            adata,\n            'harmony',\n            color=tp_name,\n            groups=tp,\n            title=tp,\n            show=False,\n            ax=axes[i],\n            legend_loc='none',\n        )\n        p.set_axis_off()\n    if return_fig:\n        return fig\n    elif not show:\n        return axes", "idx": 52}
{"project": "Scanpy", "commit_id": "149_scanpy_1.9.0_pl.py_sam.py", "target": 0, "func": "def sam(\n    adata: AnnData,\n    projection: Union[str, np.ndarray] = 'X_umap',\n    c: Optional[Union[str, np.ndarray]] = None,\n    cmap: str = 'Spectral_r',\n    linewidth: float = 0.0,\n    edgecolor: str = 'k',\n    axes: Optional[Axes] = None,\n    colorbar: bool = True,\n    s: float = 10.0,\n    **kwargs: Any,\n) -> Axes:\n    \"\"\"\\\n    Scatter plot using the SAM projection or another input projection.\n\n    Parameters\n    ----------\n    projection\n        A case-sensitive string indicating the projection to display (a key\n        in adata.obsm) or a 2D numpy array with cell coordinates. If None,\n        projection defaults to UMAP.\n    c\n        Cell color values overlaid on the projection. Can be a string from adata.obs\n        to overlay cluster assignments / annotations or a 1D numpy array.\n    axes\n        Plot output to the specified, existing axes. If None, create new\n        figure window.\n    kwargs\n        all keyword arguments in matplotlib.pyplot.scatter are eligible.\n    \"\"\"\n\n    if isinstance(projection, str):\n        try:\n            dt = adata.obsm[projection]\n        except KeyError:\n            raise ValueError(\n                'Please create a projection first using run_umap or run_tsne'\n            )\n    else:\n        dt = projection\n\n    if axes is None:\n        plt.figure()\n        axes = plt.gca()\n\n    if c is None:\n        axes.scatter(\n            dt[:, 0], dt[:, 1], s=s, linewidth=linewidth, edgecolor=edgecolor, **kwargs\n        )\n        return axes\n\n    if isinstance(c, str):\n        try:\n            c = np.array(list(adata.obs[c]))\n        except KeyError:\n            pass\n\n    if isinstance(c[0], (str, np.str_)) and isinstance(c, (np.ndarray, list)):\n        import samalg.utilities as ut\n\n        i = ut.convert_annotations(c)\n        ui, ai = np.unique(i, return_index=True)\n        cax = axes.scatter(\n            dt[:, 0],\n            dt[:, 1],\n            c=i,\n            cmap=cmap,\n            s=s,\n            linewidth=linewidth,\n            edgecolor=edgecolor,\n            **kwargs,\n        )\n\n        if colorbar:\n            cbar = plt.colorbar(cax, ax=axes, ticks=ui)\n            cbar.ax.set_yticklabels(c[ai])\n    else:\n        if not isinstance(c, (np.ndarray, list)):\n            colorbar = False\n        i = c\n\n        cax = axes.scatter(\n            dt[:, 0],\n            dt[:, 1],\n            c=i,\n            cmap=cmap,\n            s=s,\n            linewidth=linewidth,\n            edgecolor=edgecolor,\n            **kwargs,\n        )\n\n        if colorbar:\n            plt.colorbar(cax, ax=axes)\n    return axes", "idx": 53}
{"project": "Scanpy", "commit_id": "14_scanpy_1.9.0_github_links.py___call__.py", "target": 0, "func": "def __call__(\n        self,\n        name: str,\n        rawtext: str,\n        text: str,\n        lineno: int,\n        inliner: Inliner,\n        options: Mapping[str, Any] = MappingProxyType({}),\n        content: Sequence[str] = (),\n    ):\n        url = self.url_template.format(text)\n        title = self.title_template.format(text)\n        options = {**dict(classes=[self.class_name]), **options}\n        node = nodes.reference(rawtext, title, refuri=url, **options)\n        return [node], []", "idx": 54}
{"project": "Scanpy", "commit_id": "150_scanpy_1.9.0_pl.py_wishbone_marker_trajectory.py", "target": 0, "func": "def wishbone_marker_trajectory(\n    adata: AnnData,\n    markers: Collection[str],\n    no_bins: int = 150,\n    smoothing_factor: int = 1,\n    min_delta: float = 0.1,\n    show_variance: bool = False,\n    figsize: Optional[Tuple[float, float]] = None,\n    return_fig: bool = False,\n    show: bool = True,\n    save: Optional[Union[str, bool]] = None,\n    ax: Optional[Axes] = None,\n):\n    \"\"\"\\\n    Plot marker trends along trajectory, and return trajectory branches for further\n    analysis and visualization (heatmap, etc..)\n\n    Parameters\n    ----------\n    adata\n        Annotated data matrix.\n    markers\n        Iterable of markers/genes to be plotted.\n    show_variance\n        Logical indicating if the trends should be accompanied with variance.\n    no_bins\n        Number of bins for calculating marker density.\n    smoothing_factor\n        Parameter controlling the degree of smoothing.\n    min_delta\n        Minimum difference in marker expression after normalization to show\n        separate trends for the two branches.\n    figsize\n        width, height\n    return_fig\n        Return the matplotlib figure.\n    {show_save_ax}\n\n    Returns\n    -------\n    Updates `adata` with the following fields:\n\n    `trunk_wishbone` : :class:`pandas.DataFrame` (`adata.uns`)\n        Computed values before branching\n    `branch1_wishbone` : :class:`pandas.DataFrame` (`adata.uns`)\n        Computed values for the first branch\n    `branch2_wishbone` : :class:`pandas.DataFrame` (`adata.uns`)\n        Computed values for the second branch.\n    \"\"\"\n\n    wb = _anndata_to_wishbone(adata)\n\n    if figsize is None:\n        width = 2 * len(markers)\n        height = 0.75 * len(markers)\n    else:\n        width, height = figsize\n\n    if ax:\n        fig = ax.figure\n    else:\n        fig = plt.figure(figsize=(width, height))\n        ax = plt.gca()\n\n    ret_values, fig, ax = wb.plot_marker_trajectory(\n        markers=markers,\n        show_variance=show_variance,\n        no_bins=no_bins,\n        smoothing_factor=smoothing_factor,\n        min_delta=min_delta,\n        fig=fig,\n        ax=ax,\n    )\n\n    adata.uns['trunk_wishbone'] = ret_values['Trunk']\n    adata.uns['branch1_wishbone'] = ret_values['Branch1']\n    adata.uns['branch2_wishbone'] = ret_values['Branch2']\n\n    _utils.savefig_or_show('wishbone_trajectory', show=show, save=save)\n\n    if return_fig:\n        return fig\n    elif not show:\n        return ax", "idx": 55}
{"project": "Scanpy", "commit_id": "151_scanpy_1.9.0_pl.py_scrublet_score_distribution.py", "target": 0, "func": "def scrublet_score_distribution(\n    adata,\n    scale_hist_obs: str = 'log',\n    scale_hist_sim: str = 'linear',\n    figsize: Optional[Tuple[float, float]] = (8, 3),\n    return_fig: bool = False,\n    show: bool = True,\n    save: Optional[Union[str, bool]] = None,\n):\n    \"\"\"\\\n    Plot histogram of doublet scores for observed transcriptomes and simulated doublets.\n\n    The histogram for simulated doublets is useful for determining the correct doublet\n    score threshold.\n\n    Scrublet must have been run previously with the input object.\n\n    Parameters\n    ----------\n    adata\n        An annData object resulting from func:`~scanpy.external.scrublet`.\n    scale_hist_obs\n        Set y axis scale transformation in matplotlib for the plot of observed\n        transcriptomes (e.g. \"linear\", \"log\", \"symlog\", \"logit\")\n    scale_hist_sim\n        Set y axis scale transformation in matplotlib for the plot of simulated\n        doublets (e.g. \"linear\", \"log\", \"symlog\", \"logit\")\n    figsize\n        width, height\n    show\n         Show the plot, do not return axis.\n    save\n        If `True` or a `str`, save the figure.\n        A string is appended to the default filename.\n        Infer the filetype if ending on {`'.pdf'`, `'.png'`, `'.svg'`}.\n\n    Returns\n    -------\n    If `return_fig` is True, a :class:`~matplotlib.figure.Figure`.\n    If `show==False` a list of :class:`~matplotlib.axes.Axes`.\n\n    See also\n    --------\n    :func:`~scanpy.external.pp.scrublet`: Main way of running Scrublet, runs\n        preprocessing, doublet simulation and calling.\n    :func:`~scanpy.external.pp.scrublet_simulate_doublets`: Run Scrublet's doublet\n        simulation separately for advanced usage.\n    \"\"\"\n\n    def _plot_scores(\n        ax: plt.Axes, scores: np.ndarray, scale: str, title: str, threshold=None\n    ):\n        ax.hist(\n            scores,\n            np.linspace(0, 1, 50),\n            color='gray',\n            linewidth=0,\n            density=True,\n        )\n        ax.set_yscale(scale)\n        yl = ax.get_ylim()\n        ax.set_ylim(yl)\n\n        if threshold is not None:\n            ax.plot(threshold * np.ones(2), yl, c='black', linewidth=1)\n\n        ax.set_title(title)\n        ax.set_xlabel('Doublet score')\n        ax.set_ylabel('Prob. density')\n\n    if 'scrublet' not in adata.uns:\n        raise ValueError(\n            'Please run scrublet before trying to generate the scrublet plot.'\n        )\n\n    # If batched_by is populated, then we know Scrublet was run over multiple batches\n\n    if 'batched_by' in adata.uns['scrublet']:\n        batched_by = adata.uns['scrublet']['batched_by']\n        batches = adata.obs[batched_by].astype(\"category\", copy=False)\n        n_batches = len(batches.cat.categories)\n        figsize = (figsize[0], figsize[1] * n_batches)\n    else:\n        batches = pd.Series(\n            np.broadcast_to(0, adata.n_obs), dtype=\"category\", index=adata.obs_names\n        )\n        n_batches = 1\n\n    fig, axs = plt.subplots(n_batches, 2, figsize=figsize)\n\n    for idx, (batch_key, sub_obs) in enumerate(adata.obs.groupby(batches)):\n\n        # We'll need multiple rows if Scrublet was run in multiple batches\n        if 'batched_by' in adata.uns['scrublet']:\n\n            threshold = adata.uns[\"scrublet\"]['batches'][batch_key].get(\n                \"threshold\", None\n            )\n            doublet_scores_sim = adata.uns['scrublet']['batches'][batch_key][\n                'doublet_scores_sim'\n            ]\n            axis_lab_suffix = \" (%s)\" % batch_key\n            obs_ax = axs[idx][0]\n            sim_ax = axs[idx][1]\n\n        else:\n            threshold = adata.uns[\"scrublet\"].get(\"threshold\", None)\n            doublet_scores_sim = adata.uns['scrublet']['doublet_scores_sim']\n            axis_lab_suffix = ''\n            obs_ax = axs[0]\n            sim_ax = axs[1]\n\n        # Make the plots\n        _plot_scores(\n            obs_ax,\n            sub_obs[\"doublet_score\"],\n            scale=scale_hist_obs,\n            title=f\"Observed transcriptomes {axis_lab_suffix}\",\n            threshold=threshold,\n        )\n        _plot_scores(\n            sim_ax,\n            doublet_scores_sim,\n            scale=scale_hist_sim,\n            title=f\"Simulated doublets {axis_lab_suffix}\",\n            threshold=threshold,\n        )\n\n    fig.tight_layout()\n\n    _utils.savefig_or_show('scrublet_score_distribution', show=show, save=save)\n    if return_fig:\n        return fig\n    elif not show:\n        return axs", "idx": 56}
{"project": "Scanpy", "commit_id": "152_scanpy_1.9.0_pl.py__plot_scores.py", "target": 0, "func": "def _plot_scores(\n        ax: plt.Axes, scores: np.ndarray, scale: str, title: str, threshold=None\n    ):\n        ax.hist(\n            scores,\n            np.linspace(0, 1, 50),\n            color='gray',\n            linewidth=0,\n            density=True,\n        )\n        ax.set_yscale(scale)\n        yl = ax.get_ylim()\n        ax.set_ylim(yl)\n\n        if threshold is not None:\n            ax.plot(threshold * np.ones(2), yl, c='black', linewidth=1)\n\n        ax.set_title(title)\n        ax.set_xlabel('Doublet score')\n        ax.set_ylabel('Prob. density')", "idx": 57}
{"project": "Scanpy", "commit_id": "153_scanpy_1.9.0__bbknn.py_bbknn.py", "target": 0, "func": "def bbknn(\n    adata: AnnData,\n    batch_key: str = 'batch',\n    use_rep: str = 'X_pca',\n    approx: bool = True,\n    use_annoy: bool = True,\n    metric: Union[str, Callable, 'sklearn.neighbors.DistanceMetric'] = 'euclidean',\n    copy: bool = False,\n    *,\n    neighbors_within_batch: int = 3,\n    n_pcs: int = 50,\n    trim: Optional[int] = None,\n    annoy_n_trees: int = 10,\n    pynndescent_n_neighbors: int = 30,\n    pynndescent_random_state: int = 0,\n    use_faiss: bool = True,\n    set_op_mix_ratio: float = 1.0,\n    local_connectivity: int = 1,\n    **kwargs,\n) -> AnnData:\n    \"\"\"\\\n    Batch balanced kNN [Polanski19]_.\n\n    Batch balanced kNN alters the kNN procedure to identify each cell's top neighbours in\n    each batch separately instead of the entire cell pool with no accounting for batch.\n    The nearest neighbours for each batch are then merged to create a final list of\n    neighbours for the cell. Aligns batches in a quick and lightweight manner.\n\n    For use in the scanpy workflow as an alternative to :func:`~scanpy.pp.neighbors`.\n\n    .. note::\n\n        This is just a wrapper of :func:`bbknn.bbknn`: up to date docstring,\n        more information and bug reports there.\n\n    Params\n    ------\n    adata\n        Needs the PCA computed and stored in `adata.obsm[\"X_pca\"]`.\n    batch_key\n        `adata.obs` column name discriminating between your batches.\n    use_rep\n        The dimensionality reduction in `.obsm` to use for neighbour detection. Defaults to PCA.\n    approx\n        If `True`, use approximate neighbour finding - annoy or pyNNDescent. This results\n        in a quicker run time for large datasets while also potentially increasing the degree of\n        batch correction.\n    use_annoy\n        Only used when `approx=True`. If `True`, will use annoy for neighbour finding. If\n        `False`, will use pyNNDescent instead.\n    metric\n        What distance metric to use. The options depend on the choice of neighbour algorithm.\n\n        \"euclidean\", the default, is always available.\n\n        Annoy supports \"angular\", \"manhattan\" and \"hamming\".\n\n        PyNNDescent supports metrics listed in `pynndescent.distances.named_distances`\n        and custom functions, including compiled Numba code.\n\n        >>> pynndescent.distances.named_distances.keys()\n        dict_keys(['euclidean', 'l2', 'sqeuclidean', 'manhattan', 'taxicab', 'l1', 'chebyshev', 'linfinity',\n        'linfty', 'linf', 'minkowski', 'seuclidean', 'standardised_euclidean', 'wminkowski', 'weighted_minkowski',\n        'mahalanobis', 'canberra', 'cosine', 'dot', 'correlation', 'hellinger', 'haversine', 'braycurtis', 'spearmanr',\n        'kantorovich', 'wasserstein', 'tsss', 'true_angular', 'hamming', 'jaccard', 'dice', 'matching', 'kulsinski',\n        'rogerstanimoto', 'russellrao', 'sokalsneath', 'sokalmichener', 'yule'])\n\n        KDTree supports members of the `sklearn.neighbors.KDTree.valid_metrics` list, or parameterised\n        `sklearn.neighbors.DistanceMetric` `objects\n        <https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.DistanceMetric.html>`_:\n\n        >>> sklearn.neighbors.KDTree.valid_metrics\n        ['p', 'chebyshev', 'cityblock', 'minkowski', 'infinity', 'l2', 'euclidean', 'manhattan', 'l1']\n    copy\n        If `True`, return a copy instead of writing to the supplied adata.\n    neighbors_within_batch\n        How many top neighbours to report for each batch; total number of neighbours in\n        the initial k-nearest-neighbours computation will be this number times the number\n        of batches. This then serves as the basis for the construction of a symmetrical\n        matrix of connectivities.\n    n_pcs\n        How many dimensions (in case of PCA, principal components) to use in the analysis.\n    trim\n        Trim the neighbours of each cell to these many top connectivities. May help with\n        population independence and improve the tidiness of clustering. The lower the value the\n        more independent the individual populations, at the cost of more conserved batch effect.\n        If `None`, sets the parameter value automatically to 10 times `neighbors_within_batch`\n        times the number of batches. Set to 0 to skip.\n    annoy_n_trees\n        Only used with annoy neighbour identification. The number of trees to construct in the\n        annoy forest. More trees give higher precision when querying, at the cost of increased\n        run time and resource intensity.\n    pynndescent_n_neighbors\n        Only used with pyNNDescent neighbour identification. The number of neighbours to include\n        in the approximate neighbour graph. More neighbours give higher precision when querying,\n        at the cost of increased run time and resource intensity.\n    pynndescent_random_state\n        Only used with pyNNDescent neighbour identification. The RNG seed to use when creating\n        the graph.\n    use_faiss\n        If `approx=False` and the metric is \"euclidean\", use the faiss package to compute\n        nearest neighbours if installed. This improves performance at a minor cost to numerical\n        precision as faiss operates on float32.\n    set_op_mix_ratio\n        UMAP connectivity computation parameter, float between 0 and 1, controlling the\n        blend between a connectivity matrix formed exclusively from mutual nearest neighbour\n        pairs (0) and a union of all observed neighbour relationships with the mutual pairs\n        emphasised (1)\n    local_connectivity\n        UMAP connectivity computation parameter, how many nearest neighbors of each cell\n        are assumed to be fully connected (and given a connectivity value of 1)\n\n    Returns\n    -------\n    The `adata` with the batch-corrected graph.\n    \"\"\"\n    try:\n        from bbknn import bbknn\n    except ImportError:\n        raise ImportError('Please install bbknn: `pip install bbknn`.')\n    return bbknn(\n        adata=adata,\n        batch_key=batch_key,\n        use_rep=use_rep,\n        approx=approx,\n        use_annoy=use_annoy,\n        metric=metric,\n        copy=copy,\n        neighbors_within_batch=neighbors_within_batch,\n        n_pcs=n_pcs,\n        trim=trim,\n        annoy_n_trees=annoy_n_trees,\n        pynndescent_n_neighbors=pynndescent_n_neighbors,\n        pynndescent_random_state=pynndescent_random_state,\n        use_faiss=use_faiss,\n        set_op_mix_ratio=set_op_mix_ratio,\n        local_connectivity=local_connectivity,\n        **kwargs,", "idx": 58}
{"project": "Scanpy", "commit_id": "154_scanpy_1.9.0__dca.py_dca.py", "target": 0, "func": "def dca(\n    adata: AnnData,\n    mode: Literal['denoise', 'latent'] = 'denoise',\n    ae_type: _AEType = 'nb-conddisp',\n    normalize_per_cell: bool = True,\n    scale: bool = True,\n    log1p: bool = True,\n    # network args\n    hidden_size: Sequence[int] = (64, 32, 64),\n    hidden_dropout: Union[float, Sequence[float]] = 0.0,\n    batchnorm: bool = True,\n    activation: str = 'relu',\n    init: str = 'glorot_uniform',\n    network_kwds: Mapping[str, Any] = MappingProxyType({}),\n    # training args\n    epochs: int = 300,\n    reduce_lr: int = 10,\n    early_stop: int = 15,\n    batch_size: int = 32,\n    optimizer: str = 'RMSprop',\n    random_state: AnyRandom = 0,\n    threads: Optional[int] = None,\n    learning_rate: Optional[float] = None,\n    verbose: bool = False,\n    training_kwds: Mapping[str, Any] = MappingProxyType({}),\n    return_model: bool = False,\n    return_info: bool = False,\n    copy: bool = False,\n) -> Optional[AnnData]:\n    \"\"\"\\\n    Deep count autoencoder [Eraslan18]_.\n\n    Fits a count autoencoder to the raw count data given in the anndata object\n    in order to denoise the data and to capture hidden representation of\n    cells in low dimensions. Type of the autoencoder and return values are\n    determined by the parameters.\n\n    .. note::\n        More information and bug reports `here <https://github.com/theislab/dca>`__.\n\n    Parameters\n    ----------\n    adata\n        An anndata file with `.raw` attribute representing raw counts.\n    mode\n        `denoise` overwrites `adata.X` with denoised expression values.\n        In `latent` mode DCA adds `adata.obsm['X_dca']` to given adata\n        object. This matrix represent latent representation of cells via DCA.\n    ae_type\n        Type of the autoencoder. Return values and the architecture is\n        determined by the type e.g. `nb` does not provide dropout\n        probabilities. Types that end with \"-conddisp\", assumes that dispersion is mean dependant.\n    normalize_per_cell\n        If true, library size normalization is performed using\n        the `sc.pp.normalize_per_cell` function in Scanpy and saved into adata\n        object. Mean layer is re-introduces library size differences by\n        scaling the mean value of each cell in the output layer. See the\n        manuscript for more details.\n    scale\n        If true, the input of the autoencoder is centered using\n        `sc.pp.scale` function of Scanpy. Note that the output is kept as raw\n        counts as loss functions are designed for the count data.\n    log1p\n        If true, the input of the autoencoder is log transformed with a\n        pseudocount of one using `sc.pp.log1p` function of Scanpy.\n    hidden_size\n        Width of hidden layers.\n    hidden_dropout\n        Probability of weight dropout in the autoencoder (per layer if list\n        or tuple).\n    batchnorm\n        If true, batch normalization is performed.\n    activation\n        Activation function of hidden layers.\n    init\n        Initialization method used to initialize weights.\n    network_kwds\n        Additional keyword arguments for the autoencoder.\n    epochs\n        Number of total epochs in training.\n    reduce_lr\n        Reduces learning rate if validation loss does not improve in given number of epochs.\n    early_stop\n        Stops training if validation loss does not improve in given number of epochs.\n    batch_size\n        Number of samples in the batch used for SGD.\n    optimizer\n        Type of optimization method used for training.\n    random_state\n        Seed for python, numpy and tensorflow.\n    threads\n        Number of threads to use in training. All cores are used by default.\n    learning_rate\n        Learning rate to use in the training.\n    verbose\n        If true, prints additional information about training and architecture.\n    training_kwds\n        Additional keyword arguments for the training process.\n    return_model\n        If true, trained autoencoder object is returned. See \"Returns\".\n    return_info\n        If true, all additional parameters of DCA are stored in `adata.obsm` such as dropout\n        probabilities (obsm['X_dca_dropout']) and estimated dispersion values\n        (obsm['X_dca_dispersion']), in case that autoencoder is of type\n        zinb or zinb-conddisp.\n    copy\n        If true, a copy of anndata is returned.\n\n    Returns\n    -------\n    If `copy` is true and `return_model` is false, AnnData object is returned.\n\n    In \"denoise\" mode, `adata.X` is overwritten with the denoised values.\n    In \"latent\" mode, latent low dimensional representation of cells are stored\n    in `adata.obsm['X_dca']` and `adata.X` is not modified.\n    Note that these values are not corrected for library size effects.\n\n    If `return_info` is true, all estimated distribution parameters are stored\n    in AnnData like this:\n\n    `.obsm[\"X_dca_dropout\"]`\n        The mixture coefficient (pi) of the zero component in ZINB,\n        i.e. dropout probability (if `ae_type` is `zinb` or `zinb-conddisp`).\n    `.obsm[\"X_dca_dispersion\"]`\n        The dispersion parameter of NB.\n    `.uns[\"dca_loss_history\"]`\n        The loss history of the training.\n        See `.history` attribute of Keras History class for mode details.\n\n    Finally, the raw counts are stored in `.raw` attribute of AnnData object.\n\n    If `return_model` is given, trained model is returned.\n    When both `copy` and `return_model` are true,\n    a tuple of anndata and model is returned in that order.\n    \"\"\"\n\n    try:\n        from dca.api import dca\n    except ImportError:\n        raise ImportError('Please install dca package (>= 0.2.1) via `pip install dca`')\n\n    return dca(\n        adata,\n        mode=mode,\n        ae_type=ae_type,\n        normalize_per_cell=normalize_per_cell,\n        scale=scale,\n        log1p=log1p,\n        hidden_size=hidden_size,\n        hidden_dropout=hidden_dropout,\n        batchnorm=batchnorm,\n        activation=activation,\n        init=init,\n        network_kwds=network_kwds,\n        epochs=epochs,\n        reduce_lr=reduce_lr,\n        early_stop=early_stop,\n        batch_size=batch_size,\n        optimizer=optimizer,\n        random_state=random_state,\n        threads=threads,\n        learning_rate=learning_rate,\n        verbose=verbose,\n        training_kwds=training_kwds,\n        return_model=return_model,\n        return_info=return_info,\n        copy=copy,", "idx": 59}
{"project": "Scanpy", "commit_id": "155_scanpy_1.9.0__harmony_integrate.py_harmony_integrate.py", "target": 1, "func": "def harmony_integrate(\n    adata: AnnData,\n    key: str,\n    basis: str = \"X_pca\",\n    adjusted_basis: str = \"X_pca_harmony\",\n    **kwargs,\n):\n    \"\"\"\\\n    Use harmonypy [Korunsky19]_ to integrate different experiments.\n\n    Harmony [Korunsky19]_ is an algorithm for integrating single-cell\n    data from multiple experiments. This function uses the python\n    port of Harmony, ``harmonypy``, to integrate single-cell data\n    stored in an AnnData object. As Harmony works by adjusting the\n    principal components, this function should be run after performing\n    PCA but before computing the neighbor graph, as illustrated in the\n    example below.\n\n    Parameters\n    ----------\n    adata\n        The annotated data matrix.\n    key\n        The name of the column in ``adata.obs`` that differentiates\n        among experiments/batches.\n    basis\n        The name of the field in ``adata.obsm`` where the PCA table is\n        stored. Defaults to ``'X_pca'``, which is the default for\n        ``sc.tl.pca()``.\n    adjusted_basis\n        The name of the field in ``adata.obsm`` where the adjusted PCA\n        table will be stored after running this function. Defaults to\n        ``X_pca_harmony``.\n    kwargs\n        Any additional arguments will be passed to\n        ``harmonypy.run_harmony()``.\n\n    Returns\n    -------\n    Updates adata with the field ``adata.obsm[obsm_out_field]``,\n    containing principal components adjusted by Harmony such that\n    different experiments are integrated.\n\n    Example\n    -------\n    First, load libraries and example dataset, and preprocess.\n\n    >>> import scanpy as sc\n    >>> import scanpy.external as sce\n    >>> adata = sc.datasets.pbmc3k()\n    >>> sc.pp.recipe_zheng17(adata)\n    >>> sc.tl.pca(adata)\n\n    We now arbitrarily assign a batch metadata variable to each cell\n    for the sake of example, but during real usage there would already\n    be a column in ``adata.obs`` giving the experiment each cell came\n    from.\n\n    >>> adata.obs['batch'] = 1350*['a'] + 1350*['b']\n\n    Finally, run harmony. Afterwards, there will be a new table in\n    ``adata.obsm`` containing the adjusted PC's.\n\n    >>> sce.pp.harmony_integrate(adata, 'batch')\n    >>> 'X_pca_harmony' in adata.obsm\n    True\n    \"\"\"\n    try:\n        import harmonypy\n    except ImportError:\n        raise ImportError(\"\\nplease install harmonypy:\\n\\n\\tpip install harmonypy\")\n\n    harmony_out = harmonypy.run_harmony(adata.obsm[basis], adata.obs, key, **kwargs)\n\n    adata.obsm[adjusted_basis] = harmony_out.Z_corr.T", "idx": 60}
{"project": "Scanpy", "commit_id": "156_scanpy_1.9.0__hashsolo.py__calculate_log_likelihoods.py", "target": 0, "func": "def _calculate_log_likelihoods(data, number_of_noise_barcodes):\n    \"\"\"Calculate log likelihoods for each hypothesis, negative, singlet, doublet\n\n    Parameters\n    ----------\n    data : np.ndarray\n        cells by hashing counts matrix\n    number_of_noise_barcodes : int,\n        number of barcodes to used to calculated noise distribution\n\n    Returns\n    -------\n    log_likelihoods_for_each_hypothesis : np.ndarray\n        a 2d np.array log likelihood of each hypothesis\n    all_indices\n    counter_to_barcode_combo\n    \"\"\"\n\n    def gaussian_updates(data, mu_o, std_o):\n        \"\"\"Update parameters of your gaussian\n        https://www.cs.ubc.ca/~murphyk/Papers/bayesGauss.pdf\n\n        Parameters\n        ----------\n        data : np.array\n            1-d array of counts\n        mu_o : float,\n            global mean for hashing count distribution\n        std_o : float,\n            global std for hashing count distribution\n\n        Returns\n        -------\n        float\n            mean of gaussian\n        float\n            std of gaussian\n        \"\"\"\n        lam_o = 1 / (std_o**2)\n        n = len(data)\n        lam = 1 / np.var(data) if len(data) > 1 else lam_o\n        lam_n = lam_o + n * lam\n        mu_n = (\n            (np.mean(data) * n * lam + mu_o * lam_o) / lam_n if len(data) > 0 else mu_o\n        )\n        return mu_n, (1 / (lam_n / (n + 1))) ** (1 / 2)\n\n    eps = 1e-15\n    # probabilites for negative, singlet, doublets\n    log_likelihoods_for_each_hypothesis = np.zeros((data.shape[0], 3))\n\n    all_indices = np.empty(data.shape[0])\n    num_of_barcodes = data.shape[1]\n    number_of_non_noise_barcodes = (\n        num_of_barcodes - number_of_noise_barcodes\n        if number_of_noise_barcodes is not None\n        else 2\n    )\n\n    num_of_noise_barcodes = num_of_barcodes - number_of_non_noise_barcodes\n\n    # assume log normal\n    data = np.log(data + 1)\n    data_arg = np.argsort(data, axis=1)\n    data_sort = np.sort(data, axis=1)\n\n    # global signal and noise counts useful for when we have few cells\n    # barcodes with the highest number of counts are assumed to be a true signal\n    # barcodes with rank < k are considered to be noise\n    global_signal_counts = np.ravel(data_sort[:, -1])\n    global_noise_counts = np.ravel(data_sort[:, :-number_of_non_noise_barcodes])\n    global_mu_signal_o, global_sigma_signal_o = (\n        np.mean(global_signal_counts),\n        np.std(global_signal_counts),\n    )\n    global_mu_noise_o, global_sigma_noise_o = (\n        np.mean(global_noise_counts),\n        np.std(global_noise_counts),\n    )\n\n    noise_params_dict = {}\n    signal_params_dict = {}\n\n    # for each barcode get  empirical noise and signal distribution parameterization\n    for x in np.arange(num_of_barcodes):\n        sample_barcodes = data[:, x]\n        sample_barcodes_noise_idx = np.where(data_arg[:, :num_of_noise_barcodes] == x)[\n            0\n        ]\n        sample_barcodes_signal_idx = np.where(data_arg[:, -1] == x)\n\n        # get noise and signal counts\n        noise_counts = sample_barcodes[sample_barcodes_noise_idx]\n        signal_counts = sample_barcodes[sample_barcodes_signal_idx]\n\n        # get parameters of distribution, assuming lognormal do update from global values\n        noise_param = gaussian_updates(\n            noise_counts, global_mu_noise_o, global_sigma_noise_o\n        )\n        signal_param = gaussian_updates(\n            signal_counts, global_mu_signal_o, global_sigma_signal_o\n        )\n        noise_params_dict[x] = noise_param\n        signal_params_dict[x] = signal_param\n\n    counter_to_barcode_combo = {}\n    counter = 0\n\n    # for each combination of noise and signal barcode calculate probiltiy of in silico and real cell hypotheses\n    for noise_sample_idx, signal_sample_idx in product(\n        np.arange(num_of_barcodes), np.arange(num_of_barcodes)\n    ):\n        signal_subset = data_arg[:, -1] == signal_sample_idx\n        noise_subset = data_arg[:, -2] == noise_sample_idx\n        subset = signal_subset & noise_subset\n        if sum(subset) == 0:\n            continue\n\n        indices = np.where(subset)[0]\n        barcode_combo = \"_\".join([str(noise_sample_idx), str(signal_sample_idx)])\n        all_indices[np.where(subset)[0]] = counter\n        counter_to_barcode_combo[counter] = barcode_combo\n        counter += 1\n        noise_params = noise_params_dict[noise_sample_idx]\n        signal_params = signal_params_dict[signal_sample_idx]\n\n        # calculate probabilties for each hypothesis for each cell\n        data_subset = data[subset]\n        log_signal_signal_probs = np.log(\n            norm.pdf(\n                data_subset[:, signal_sample_idx],\n                *signal_params[:-2],\n                loc=signal_params[-2],\n                scale=signal_params[-1],\n            )\n            + eps\n        )\n        signal_noise_params = signal_params_dict[noise_sample_idx]\n        log_noise_signal_probs = np.log(\n            norm.pdf(\n                data_subset[:, noise_sample_idx],\n                loc=signal_noise_params[-2],\n                scale=signal_noise_params[-1],\n            )\n            + eps\n        )\n\n        log_noise_noise_probs = np.log(\n            norm.pdf(\n                data_subset[:, noise_sample_idx],\n                loc=noise_params[-2],\n                scale=noise_params[-1],\n            )\n            + eps\n        )\n        log_signal_noise_probs = np.log(\n            norm.pdf(\n                data_subset[:, signal_sample_idx],\n                loc=noise_params[-2],\n                scale=noise_params[-1],\n            )\n            + eps\n        )\n\n        probs_of_negative = np.sum(\n            [log_noise_noise_probs, log_signal_noise_probs], axis=0\n        )\n        probs_of_singlet = np.sum(\n            [log_noise_noise_probs, log_signal_signal_probs], axis=0\n        )\n        probs_of_doublet = np.sum(\n            [log_noise_signal_probs, log_signal_signal_probs], axis=0\n        )\n        log_probs_list = [probs_of_negative, probs_of_singlet, probs_of_doublet]\n\n        # each cell and each hypothesis probability\n        for prob_idx, log_prob in enumerate(log_probs_list):\n            log_likelihoods_for_each_hypothesis[indices, prob_idx] = log_prob\n    return (\n        log_likelihoods_for_each_hypothesis,\n        all_indices,\n        counter_to_barcode_combo,", "idx": 61}
{"project": "Scanpy", "commit_id": "157_scanpy_1.9.0__hashsolo.py__calculate_bayes_rule.py", "target": 0, "func": "def _calculate_bayes_rule(data, priors, number_of_noise_barcodes):\n    \"\"\"\n    Calculate bayes rule from log likelihoods\n\n    Parameters\n    ----------\n    data : np.array\n        Anndata object filled only with hashing counts\n    priors : list,\n        a list of your prior for each hypothesis\n        first element is your prior for the negative hypothesis\n        second element is your prior for the singlet hypothesis\n        third element is your prior for the doublet hypothesis\n        We use [0.01, 0.8, 0.19] by default because we assume the barcodes\n        in your cell hashing matrix are those cells which have passed QC\n        in the transcriptome space, e.g. UMI counts, pct mito reads, etc.\n    number_of_noise_barcodes : int\n        number of barcodes to used to calculated noise distribution\n\n    Returns\n    -------\n    bayes_dict_results : dict\n        \"most_likely_hypothesis\" key is a 1d np.array of the most likely hypothesis\n        \"probs_hypotheses\" key is a 2d np.array probability of each hypothesis\n        \"log_likelihoods_for_each_hypothesis\" key is a 2d np.array log likelihood of each hypothesis\n    \"\"\"\n    priors = np.array(priors)\n    log_likelihoods_for_each_hypothesis, _, _ = _calculate_log_likelihoods(\n        data, number_of_noise_barcodes\n    )\n    probs_hypotheses = (\n        np.exp(log_likelihoods_for_each_hypothesis)\n        * priors\n        / np.sum(\n            np.multiply(np.exp(log_likelihoods_for_each_hypothesis), priors),\n            axis=1,\n        )[:, None]\n    )\n    most_likely_hypothesis = np.argmax(probs_hypotheses, axis=1)\n    return {\n        \"most_likely_hypothesis\": most_likely_hypothesis,\n        \"probs_hypotheses\": probs_hypotheses,\n        \"log_likelihoods_for_each_hypothesis\": log_likelihoods_for_each_hypothesis,", "idx": 62}
{"project": "Scanpy", "commit_id": "158_scanpy_1.9.0__hashsolo.py_hashsolo.py", "target": 1, "func": "def hashsolo(\n    adata: anndata.AnnData,\n    cell_hashing_columns: list,\n    priors: list = [0.01, 0.8, 0.19],\n    pre_existing_clusters: str = None,\n    number_of_noise_barcodes: int = None,\n    inplace: bool = True,\n):\n    \"\"\"Probabilistic demultiplexing of cell hashing data using HashSolo [Bernstein20]_.\n\n    .. note::\n        More information and bug reports `here <https://github.com/calico/solo>`__.\n\n    Parameters\n    ----------\n    adata\n        Anndata object with cell hashes in .obs columns\n    cell_hashing_columns\n        list specifying which columns in adata.obs\n        are cell hashing counts\n    priors\n        a list of your prior for each hypothesis\n        first element is your prior for the negative hypothesis\n        second element is your prior for the singlet hypothesis\n        third element is your prior for the doublet hypothesis\n        We use [0.01, 0.8, 0.19] by default because we assume the barcodes\n        in your cell hashing matrix are those cells which have passed QC\n        in the transcriptome space, e.g. UMI counts, pct mito reads, etc.\n    pre_existing_clusters\n        column in adata.obs for how to break up demultiplexing\n        for example leiden or cell types, not batches though\n    number_of_noise_barcodes\n        Use this if you wish change the number of barcodes used to create the\n        noise distribution. The default is number of cell hashes - 2.\n    inplace\n        To do operation in place\n\n    Returns\n    -------\n    adata\n        if inplace is False returns AnnData with demultiplexing results\n        in .obs attribute otherwise does is in place\n\n    Examples\n    -------\n    >>> import anndata\n    >>> import scanpy.external as sce\n    >>> data = anndata.read(\"data.h5ad\")\n    >>> sce.pp.hashsolo(data, ['Hash1', 'Hash2', 'Hash3'])\n    >>> data.obs.head()\n    \"\"\"\n    print(\n        \"Please cite HashSolo paper:\\nhttps://www.cell.com/cell-systems/fulltext/S2405-4712(20)30195-2\"\n    )\n\n    data = adata.obs[cell_hashing_columns].values\n    if not check_nonnegative_integers(data):\n        raise ValueError(\"Cell hashing counts must be non-negative\")\n    if (number_of_noise_barcodes is not None) and (\n        number_of_noise_barcodes >= len(cell_hashing_columns)\n    ):\n        raise ValueError(\n            \"number_of_noise_barcodes must be at least one less \\\n        than the number of samples you have as determined by the number of \\\n        cell_hashing_columns you've given as input  \"\n        )\n    num_of_cells = adata.shape[0]\n    results = pd.DataFrame(\n        np.zeros((num_of_cells, 6)),\n        columns=[\n            \"most_likely_hypothesis\",\n            \"probs_hypotheses\",\n            \"cluster_feature\",\n            \"negative_hypothesis_probability\",\n            \"singlet_hypothesis_probability\",\n            \"doublet_hypothesis_probability\",\n        ],\n        index=adata.obs_names,\n    )\n    if pre_existing_clusters is not None:\n        cluster_features = pre_existing_clusters\n        unique_cluster_features = np.unique(adata.obs[cluster_features])\n        for cluster_feature in unique_cluster_features:\n            cluster_feature_bool_vector = adata.obs[cluster_features] == cluster_feature\n            posterior_dict = _calculate_bayes_rule(\n                data[cluster_feature_bool_vector],\n                priors,\n                number_of_noise_barcodes,\n            )\n            results.loc[\n                cluster_feature_bool_vector, \"most_likely_hypothesis\"\n            ] = posterior_dict[\"most_likely_hypothesis\"]\n            results.loc[\n                cluster_feature_bool_vector, \"cluster_feature\"\n            ] = cluster_feature\n            results.loc[\n                cluster_feature_bool_vector, \"negative_hypothesis_probability\"\n            ] = posterior_dict[\"probs_hypotheses\"][:, 0]\n            results.loc[\n                cluster_feature_bool_vector, \"singlet_hypothesis_probability\"\n            ] = posterior_dict[\"probs_hypotheses\"][:, 1]\n            results.loc[\n                cluster_feature_bool_vector, \"doublet_hypothesis_probability\"\n            ] = posterior_dict[\"probs_hypotheses\"][:, 2]\n    else:\n        posterior_dict = _calculate_bayes_rule(data, priors, number_of_noise_barcodes)\n        results.loc[:, \"most_likely_hypothesis\"] = posterior_dict[\n            \"most_likely_hypothesis\"\n        ]\n        results.loc[:, \"cluster_feature\"] = 0\n        results.loc[:, \"negative_hypothesis_probability\"] = posterior_dict[\n            \"probs_hypotheses\"\n        ][:, 0]\n        results.loc[:, \"singlet_hypothesis_probability\"] = posterior_dict[\n            \"probs_hypotheses\"\n        ][:, 1]\n        results.loc[:, \"doublet_hypothesis_probability\"] = posterior_dict[\n            \"probs_hypotheses\"\n        ][:, 2]\n\n    adata.obs[\"most_likely_hypothesis\"] = results.loc[\n        adata.obs_names, \"most_likely_hypothesis\"\n    ]\n    adata.obs[\"cluster_feature\"] = results.loc[adata.obs_names, \"cluster_feature\"]\n    adata.obs[\"negative_hypothesis_probability\"] = results.loc[\n        adata.obs_names, \"negative_hypothesis_probability\"\n    ]\n    adata.obs[\"singlet_hypothesis_probability\"] = results.loc[\n        adata.obs_names, \"singlet_hypothesis_probability\"\n    ]\n    adata.obs[\"doublet_hypothesis_probability\"] = results.loc[\n        adata.obs_names, \"doublet_hypothesis_probability\"\n    ]\n\n    adata.obs[\"Classification\"] = None\n    adata.obs.loc[\n        adata.obs[\"most_likely_hypothesis\"] == 2, \"Classification\"\n    ] = \"Doublet\"\n    adata.obs.loc[\n        adata.obs[\"most_likely_hypothesis\"] == 0, \"Classification\"\n    ] = \"Negative\"\n    all_sings = adata.obs[\"most_likely_hypothesis\"] == 1\n    singlet_sample_index = np.argmax(\n        adata.obs.loc[all_sings, cell_hashing_columns].values, axis=1\n    )\n    adata.obs.loc[all_sings, \"Classification\"] = adata.obs[\n        cell_hashing_columns\n    ].columns[singlet_sample_index]\n\n    return adata if not inplace else None", "idx": 63}
{"project": "Scanpy", "commit_id": "159_scanpy_1.9.0__hashsolo.py_gaussian_updates.py", "target": 0, "func": "def gaussian_updates(data, mu_o, std_o):\n        \"\"\"Update parameters of your gaussian\n        https://www.cs.ubc.ca/~murphyk/Papers/bayesGauss.pdf\n\n        Parameters\n        ----------\n        data : np.array\n            1-d array of counts\n        mu_o : float,\n            global mean for hashing count distribution\n        std_o : float,\n            global std for hashing count distribution\n\n        Returns\n        -------\n        float\n            mean of gaussian\n        float\n            std of gaussian\n        \"\"\"\n        lam_o = 1 / (std_o**2)\n        n = len(data)\n        lam = 1 / np.var(data) if len(data) > 1 else lam_o\n        lam_n = lam_o + n * lam\n        mu_n = (\n            (np.mean(data) * n * lam + mu_o * lam_o) / lam_n if len(data) > 0 else mu_o\n        )\n        return mu_n, (1 / (lam_n / (n + 1))) ** (1 / 2)", "idx": 64}
{"project": "Scanpy", "commit_id": "15_scanpy_1.9.0_has_attr_test.py_has_attr.py", "target": 0, "func": "def has_attr(obj_path: str, attr: str) -> bool:\n    # https://jinja.palletsprojects.com/en/3.0.x/api/#custom-tests\n    obj = import_string(obj_path)\n    return hasattr(obj, attr)", "idx": 65}
{"project": "Scanpy", "commit_id": "160_scanpy_1.9.0__magic.py_magic.py", "target": 0, "func": "def magic(\n    adata: AnnData,\n    name_list: Union[Literal['all_genes', 'pca_only'], Sequence[str], None] = None,\n    *,\n    knn: int = 5,\n    decay: Optional[float] = 1,\n    knn_max: Optional[int] = None,\n    t: Union[Literal['auto'], int] = 3,\n    n_pca: Optional[int] = 100,\n    solver: Literal['exact', 'approximate'] = 'exact',\n    knn_dist: str = 'euclidean',\n    random_state: AnyRandom = None,\n    n_jobs: Optional[int] = None,\n    verbose: bool = False,\n    copy: Optional[bool] = None,\n    **kwargs,\n) -> Optional[AnnData]:\n    \"\"\"\\\n    Markov Affinity-based Graph Imputation of Cells (MAGIC) API [vanDijk18]_.\n\n    MAGIC is an algorithm for denoising and transcript recover of single cells\n    applied to single-cell sequencing data. MAGIC builds a graph from the data\n    and uses diffusion to smooth out noise and recover the data manifold.\n\n    The algorithm implemented here has changed primarily in two ways\n    compared to the algorithm described in [vanDijk18]_. Firstly, we use\n    the adaptive kernel described in Moon et al, 2019 [Moon17]_ for\n    improved stability. Secondly, data diffusion is applied\n    in the PCA space, rather than the data space, for speed and\n    memory improvements.\n\n    More information and bug reports\n    `here <https://github.com/KrishnaswamyLab/MAGIC>`__. For help, visit\n    <https://krishnaswamylab.org/get-help>.\n\n    Parameters\n    ----------\n    adata\n        An anndata file with `.raw` attribute representing raw counts.\n    name_list\n        Denoised genes to return. The default `'all_genes'`/`None`\n        may require a large amount of memory if the input data is sparse.\n        Another possibility is `'pca_only'`.\n    knn\n        number of nearest neighbors on which to build kernel.\n    decay\n        sets decay rate of kernel tails.\n        If None, alpha decaying kernel is not used.\n    knn_max\n        maximum number of nearest neighbors with nonzero connection.\n        If `None`, will be set to 3 * `knn`.\n    t\n        power to which the diffusion operator is powered.\n        This sets the level of diffusion. If 'auto', t is selected\n        according to the Procrustes disparity of the diffused data.\n    n_pca\n        Number of principal components to use for calculating\n        neighborhoods. For extremely large datasets, using\n        n_pca < 20 allows neighborhoods to be calculated in\n        roughly log(n_samples) time. If `None`, no PCA is performed.\n    solver\n        Which solver to use. \"exact\" uses the implementation described\n        in van Dijk et al. (2018) [vanDijk18]_. \"approximate\" uses a faster\n        implementation that performs imputation in the PCA space and then\n        projects back to the gene space. Note, the \"approximate\" solver may\n        return negative values.\n    knn_dist\n        recommended values: 'euclidean', 'cosine', 'precomputed'\n        Any metric from `scipy.spatial.distance` can be used\n        distance metric for building kNN graph. If 'precomputed',\n        `data` should be an n_samples x n_samples distance or\n        affinity matrix.\n    random_state\n        Random seed. Defaults to the global `numpy` random number generator.\n    n_jobs\n        Number of threads to use in training. All cores are used by default.\n    verbose\n        If `True` or an integer `>= 2`, print status messages.\n        If `None`, `sc.settings.verbosity` is used.\n    copy\n        If true, a copy of anndata is returned. If `None`, `copy` is True if\n        `genes` is not `'all_genes'` or `'pca_only'`. `copy` may only be False\n        if `genes` is `'all_genes'` or `'pca_only'`, as the resultant data\n        will otherwise have different column names from the input data.\n    kwargs\n        Additional arguments to `magic.MAGIC`.\n\n    Returns\n    -------\n    If `copy` is True, AnnData object is returned.\n\n    If `subset_genes` is not `all_genes`, PCA on MAGIC values of cells are\n    stored in `adata.obsm['X_magic']` and `adata.X` is not modified.\n\n    The raw counts are stored in `.raw` attribute of AnnData object.\n\n    Examples\n    --------\n    >>> import scanpy as sc\n    >>> import scanpy.external as sce\n    >>> adata = sc.datasets.paul15()\n    >>> sc.pp.normalize_per_cell(adata)\n    >>> sc.pp.sqrt(adata)  # or sc.pp.log1p(adata)\n    >>> adata_magic = sce.pp.magic(adata, name_list=['Mpo', 'Klf1', 'Ifitm1'], knn=5)\n    >>> adata_magic.shape\n    (2730, 3)\n    >>> sce.pp.magic(adata, name_list='pca_only', knn=5)\n    >>> adata.obsm['X_magic'].shape\n    (2730, 100)\n    >>> sce.pp.magic(adata, name_list='all_genes', knn=5)\n    >>> adata.X.shape\n    (2730, 3451)\n    \"\"\"\n\n    try:\n        from magic import MAGIC, __version__\n    except ImportError:\n        raise ImportError(\n            'Please install magic package via `pip install --user '\n            'git+git://github.com/KrishnaswamyLab/MAGIC.git#subdirectory=python`'\n        )\n    else:\n        if not version.parse(__version__) >= version.parse(MIN_VERSION):\n            raise ImportError(\n                'scanpy requires magic-impute >= '\n                f'v{MIN_VERSION} (detected: v{__version__}). '\n                'Please update magic package via `pip install --user '\n                '--upgrade magic-impute`'\n            )\n\n    start = logg.info('computing MAGIC')\n    all_or_pca = isinstance(name_list, (str, type(None)))\n    if all_or_pca and name_list not in {\"all_genes\", \"pca_only\", None}:\n        raise ValueError(\n            \"Invalid string value for `name_list`: \"\n            \"Only `'all_genes'` and `'pca_only'` are allowed.\"\n        )\n    if copy is None:\n        copy = not all_or_pca\n    elif not all_or_pca and not copy:\n        raise ValueError(\n            \"Can only perform MAGIC in-place with `name_list=='all_genes' or \"\n            f\"`name_list=='pca_only'` (got {name_list}). Consider setting \"\n            \"`copy=True`\"\n        )\n    adata = adata.copy() if copy else adata\n    n_jobs = settings.n_jobs if n_jobs is None else n_jobs\n\n    X_magic = MAGIC(\n        knn=knn,\n        decay=decay,\n        knn_max=knn_max,\n        t=t,\n        n_pca=n_pca,\n        solver=solver,\n        knn_dist=knn_dist,\n        random_state=random_state,\n        n_jobs=n_jobs,\n        verbose=verbose,\n        **kwargs,\n    ).fit_transform(adata, genes=name_list)\n    logg.info(\n        '    finished',\n        time=start,\n        deep=(\n            \"added\\n    'X_magic', PCA on MAGIC coordinates (adata.obsm)\"\n            if name_list == \"pca_only\"\n            else ''\n        ),\n    )\n    # update AnnData instance\n    if name_list == \"pca_only\":\n        # special case \u2013 update adata.obsm with smoothed values\n        adata.obsm[\"X_magic\"] = X_magic.X\n    elif copy:\n        # just return X_magic\n        X_magic.raw = adata\n        adata = X_magic\n    else:\n        # replace data with smoothed data\n        adata.raw = adata\n        adata.X = X_magic.X\n\n    if copy:\n        return adata", "idx": 66}
{"project": "Scanpy", "commit_id": "161_scanpy_1.9.0__mnn_correct.py_mnn_correct.py", "target": 1, "func": "def mnn_correct(\n    *datas: Union[AnnData, np.ndarray],\n    var_index: Optional[Collection[str]] = None,\n    var_subset: Optional[Collection[str]] = None,\n    batch_key: str = 'batch',\n    index_unique: str = '-',\n    batch_categories: Optional[Collection[Any]] = None,\n    k: int = 20,\n    sigma: float = 1.0,\n    cos_norm_in: bool = True,\n    cos_norm_out: bool = True,\n    svd_dim: Optional[int] = None,\n    var_adj: bool = True,\n    compute_angle: bool = False,\n    mnn_order: Optional[Sequence[int]] = None,\n    svd_mode: Literal['svd', 'rsvd', 'irlb'] = 'rsvd',\n    do_concatenate: bool = True,\n    save_raw: bool = False,\n    n_jobs: Optional[int] = None,\n    **kwargs,\n) -> Tuple[\n    Union[np.ndarray, AnnData],\n    List[pd.DataFrame],\n    Optional[List[Tuple[Optional[float], int]]],\n]:\n    \"\"\"\\\n    Correct batch effects by matching mutual nearest neighbors [Haghverdi18]_ [Kang18]_.\n\n    This uses the implementation of `mnnpy\n    <https://github.com/chriscainx/mnnpy>`__ [Kang18]_.\n\n    Depending on `do_concatenate`, returns matrices or `AnnData` objects in the\n    original order containing corrected expression values or a concatenated\n    matrix or AnnData object.\n\n    Be reminded that it is not advised to use the corrected data matrices for\n    differential expression testing.\n\n    More information and bug reports `here <https://github.com/chriscainx/mnnpy>`__.\n\n    Parameters\n    ----------\n    datas\n        Expression matrices or AnnData objects. Matrices should be shaped like\n        n_obs \u00d7 n_vars (n_cell \u00d7 n_gene) and have consistent number of columns.\n        AnnData objects should have same number of variables.\n    var_index\n        The index (list of str) of vars (genes). Necessary when using only a\n        subset of vars to perform MNN correction, and should be supplied with\n        `var_subset`. When `datas` are AnnData objects, `var_index` is ignored.\n    var_subset\n        The subset of vars (list of str) to be used when performing MNN\n        correction. Typically, a list of highly variable genes (HVGs).\n        When set to `None`, uses all vars.\n    batch_key\n        The `batch_key` for :meth:`~anndata.AnnData.concatenate`.\n        Only valid when `do_concatenate` and supplying `AnnData` objects.\n    index_unique\n        The `index_unique` for :meth:`~anndata.AnnData.concatenate`.\n        Only valid when `do_concatenate` and supplying `AnnData` objects.\n    batch_categories\n        The `batch_categories` for :meth:`~anndata.AnnData.concatenate`.\n        Only valid when `do_concatenate` and supplying AnnData objects.\n    k\n        Number of mutual nearest neighbors.\n    sigma\n        The bandwidth of the Gaussian smoothing kernel used to compute the\n        correction vectors. Default is 1.\n    cos_norm_in\n        Whether cosine normalization should be performed on the input data prior\n        to calculating distances between cells.\n    cos_norm_out\n        Whether cosine normalization should be performed prior to computing corrected expression values.\n    svd_dim\n        The number of dimensions to use for summarizing biological substructure\n        within each batch. If None, biological components will not be removed\n        from the correction vectors.\n    var_adj\n        Whether to adjust variance of the correction vectors. Note this step\n        takes most computing time.\n    compute_angle\n        Whether to compute the angle between each cell\u2019s correction vector and\n        the biological subspace of the reference batch.\n    mnn_order\n        The order in which batches are to be corrected. When set to None, datas\n        are corrected sequentially.\n    svd_mode\n        `'svd'` computes SVD using a non-randomized SVD-via-ID algorithm,\n        while `'rsvd'` uses a randomized version. `'irlb'` perfores\n        truncated SVD by implicitly restarted Lanczos bidiagonalization\n        (forked from https://github.com/airysen/irlbpy).\n    do_concatenate\n        Whether to concatenate the corrected matrices or AnnData objects. Default is True.\n    save_raw\n        Whether to save the original expression data in the\n        :attr:`~anndata.AnnData.raw` attribute.\n    n_jobs\n        The number of jobs. When set to `None`, automatically uses\n        :attr:`scanpy._settings.ScanpyConfig.n_jobs`.\n    kwargs\n        optional keyword arguments for irlb.\n\n    Returns\n    -------\n    datas\n        Corrected matrix/matrices or AnnData object/objects, depending on the\n        input type and `do_concatenate`.\n    mnn_list\n        A list containing MNN pairing information as DataFrames in each iteration step.\n    angle_list\n        A list containing angles of each batch.\n    \"\"\"\n    if len(datas) < 2:\n        return datas, [], []\n\n    try:\n        from mnnpy import mnn_correct\n    except ImportError:\n        raise ImportError(\n            'Please install the package mnnpy '\n            '(https://github.com/chriscainx/mnnpy). '\n        )\n\n    n_jobs = settings.n_jobs if n_jobs is None else n_jobs\n    datas, mnn_list, angle_list = mnn_correct(\n        *datas,\n        var_index=var_index,\n        var_subset=var_subset,\n        batch_key=batch_key,\n        index_unique=index_unique,\n        batch_categories=batch_categories,\n        k=k,\n        sigma=sigma,\n        cos_norm_in=cos_norm_in,\n        cos_norm_out=cos_norm_out,\n        svd_dim=svd_dim,\n        var_adj=var_adj,\n        compute_angle=compute_angle,\n        mnn_order=mnn_order,\n        svd_mode=svd_mode,\n        do_concatenate=do_concatenate,\n        save_raw=save_raw,\n        n_jobs=n_jobs,\n        **kwargs,\n    )\n    return datas, mnn_list, angle_list", "idx": 67}
{"project": "Scanpy", "commit_id": "162_scanpy_1.9.0__scanorama_integrate.py_scanorama_integrate.py", "target": 0, "func": "def scanorama_integrate(\n    adata: AnnData,\n    key: str,\n    basis: str = 'X_pca',\n    adjusted_basis: str = 'X_scanorama',\n    knn: int = 20,\n    sigma: float = 15,\n    approx: bool = True,\n    alpha: float = 0.10,\n    batch_size: int = 5000,\n    **kwargs,\n):\n    \"\"\"\\\n    Use Scanorama [Hie19]_ to integrate different experiments.\n\n    Scanorama [Hie19]_ is an algorithm for integrating single-cell\n    data from multiple experiments stored in an AnnData object. This\n    function should be run after performing PCA but before computing\n    the neighbor graph, as illustrated in the example below.\n\n    This uses the implementation of `scanorama\n    <https://github.com/brianhie/scanorama>`__ [Hie19]_.\n\n    Parameters\n    ----------\n    adata\n        The annotated data matrix.\n    key\n        The name of the column in ``adata.obs`` that differentiates\n        among experiments/batches. Cells from the same batch must be\n        contiguously stored in ``adata``.\n    basis\n        The name of the field in ``adata.obsm`` where the PCA table is\n        stored. Defaults to ``'X_pca'``, which is the default for\n        ``sc.tl.pca()``.\n    adjusted_basis\n        The name of the field in ``adata.obsm`` where the integrated\n        embeddings will be stored after running this function. Defaults\n        to ``X_scanorama``.\n    knn\n        Number of nearest neighbors to use for matching.\n    sigma\n        Correction smoothing parameter on Gaussian kernel.\n    approx\n        Use approximate nearest neighbors with Python ``annoy``;\n        greatly speeds up matching runtime.\n    alpha\n        Alignment score minimum cutoff.\n    batch_size\n        The batch size used in the alignment vector computation. Useful\n        when integrating very large (>100k samples) datasets. Set to\n        large value that runs within available memory.\n    kwargs\n        Any additional arguments will be passed to\n        ``scanorama.integrate()``.\n\n    Returns\n    -------\n    Updates adata with the field ``adata.obsm[adjusted_basis]``,\n    containing Scanorama embeddings such that different experiments\n    are integrated.\n\n    Example\n    -------\n    First, load libraries and example dataset, and preprocess.\n\n    >>> import scanpy as sc\n    >>> import scanpy.external as sce\n    >>> adata = sc.datasets.pbmc3k()\n    >>> sc.pp.recipe_zheng17(adata)\n    >>> sc.tl.pca(adata)\n\n    We now arbitrarily assign a batch metadata variable to each cell\n    for the sake of example, but during real usage there would already\n    be a column in ``adata.obs`` giving the experiment each cell came\n    from.\n\n    >>> adata.obs['batch'] = 1350*['a'] + 1350*['b']\n\n    Finally, run Scanorama. Afterwards, there will be a new table in\n    ``adata.obsm`` containing the Scanorama embeddings.\n\n    >>> sce.pp.scanorama_integrate(adata, 'batch')\n    >>> 'X_scanorama' in adata.obsm\n    True\n    \"\"\"\n    try:\n        import scanorama\n    except ImportError:\n        raise ImportError(\"\\nplease install Scanorama:\\n\\n\\tpip install scanorama\")\n\n    # Get batch indices in linear time.\n    curr_batch = None\n    batch_names = []\n    name2idx = {}\n    for idx in range(adata.X.shape[0]):\n        batch_name = adata.obs[key][idx]\n        if batch_name != curr_batch:\n            curr_batch = batch_name\n            if batch_name in batch_names:\n                # Contiguous batches important for preserving cell order.\n                raise ValueError('Detected non-contiguous batches.')\n            batch_names.append(batch_name)  # Preserve name order.\n            name2idx[batch_name] = []\n        name2idx[batch_name].append(idx)\n\n    # Separate batches.\n    datasets_dimred = [\n        adata.obsm[basis][name2idx[batch_name]] for batch_name in batch_names\n    ]\n\n    # Integrate.\n    integrated = scanorama.assemble(\n        datasets_dimred,  # Assemble in low dimensional space.\n        knn=knn,\n        sigma=sigma,\n        approx=approx,\n        alpha=alpha,\n        ds_names=batch_names,\n        **kwargs,\n    )\n\n    adata.obsm[adjusted_basis] = np.concatenate(integrated)", "idx": 68}
{"project": "Scanpy", "commit_id": "163_scanpy_1.9.0__scrublet.py_scrublet.py", "target": 0, "func": "def scrublet(\n    adata: AnnData,\n    adata_sim: Optional[AnnData] = None,\n    batch_key: str = None,\n    sim_doublet_ratio: float = 2.0,\n    expected_doublet_rate: float = 0.05,\n    stdev_doublet_rate: float = 0.02,\n    synthetic_doublet_umi_subsampling: float = 1.0,\n    knn_dist_metric: str = 'euclidean',\n    normalize_variance: bool = True,\n    log_transform: bool = False,\n    mean_center: bool = True,\n    n_prin_comps: int = 30,\n    use_approx_neighbors: bool = True,\n    get_doublet_neighbor_parents: bool = False,\n    n_neighbors: Optional[int] = None,\n    threshold: Optional[float] = None,\n    verbose: bool = True,\n    copy: bool = False,\n    random_state: int = 0,\n) -> Optional[AnnData]:\n    \"\"\"\\\n    Predict doublets using Scrublet [Wolock19]_.\n\n    Predict cell doublets using a nearest-neighbor classifier of observed\n    transcriptomes and simulated doublets. Works best if the input is a raw\n    (unnormalized) counts matrix from a single sample or a collection of\n    similar samples from the same experiment.\n    This function is a wrapper around functions that pre-process using Scanpy\n    and directly call functions of Scrublet(). You may also undertake your own\n    preprocessing, simulate doublets with\n    scanpy.external.pp.scrublet_simulate_doublets(), and run the core scrublet\n    function scanpy.external.pp.scrublet.scrublet().\n\n    .. note::\n        More information and bug reports `here\n        <https://github.com/swolock/scrublet>`__.\n\n    Parameters\n    ----------\n    adata\n        The annotated data matrix of shape ``n_obs`` \u00d7 ``n_vars``. Rows\n        correspond to cells and columns to genes. Expected to be un-normalised\n        where adata_sim is not supplied, in which case doublets will be\n        simulated and pre-processing applied to both objects. If adata_sim is\n        supplied, this should be the observed transcriptomes processed\n        consistently (filtering, transform, normalisaton, hvg) with adata_sim.\n    adata_sim\n        (Advanced use case) Optional annData object generated by\n        sc.external.pp.scrublet_simulate_doublets(), with same number of vars\n        as adata. This should have been built from adata_obs after\n        filtering genes and cells and selcting highly-variable genes.\n    batch_key\n        Optional `adata.obs` column name discriminating between batches.\n    sim_doublet_ratio\n        Number of doublets to simulate relative to the number of observed\n        transcriptomes.\n    expected_doublet_rate\n        Where adata_sim not suplied, the estimated doublet rate for the\n        experiment.\n    stdev_doublet_rate\n        Where adata_sim not suplied, uncertainty in the expected doublet rate.\n    synthetic_doublet_umi_subsampling\n        Where adata_sim not suplied, rate for sampling UMIs when creating\n        synthetic doublets. If 1.0, each doublet is created by simply adding\n        the UMI counts from two randomly sampled observed transcriptomes. For\n        values less than 1, the UMI counts are added and then randomly sampled\n        at the specified rate.\n    knn_dist_metric\n        Distance metric used when finding nearest neighbors. For list of\n        valid values, see the documentation for annoy (if `use_approx_neighbors`\n        is True) or sklearn.neighbors.NearestNeighbors (if `use_approx_neighbors`\n        is False).\n    normalize_variance\n        If True, normalize the data such that each gene has a variance of 1.\n        `sklearn.decomposition.TruncatedSVD` will be used for dimensionality\n        reduction, unless `mean_center` is True.\n    log_transform\n        Whether to use :func:``~scanpy.pp.log1p`` to log-transform the data\n        prior to PCA.\n    mean_center\n        If True, center the data such that each gene has a mean of 0.\n        `sklearn.decomposition.PCA` will be used for dimensionality\n        reduction.\n    n_prin_comps\n        Number of principal components used to embed the transcriptomes prior\n        to k-nearest-neighbor graph construction.\n    use_approx_neighbors\n        Use approximate nearest neighbor method (annoy) for the KNN\n        classifier.\n    get_doublet_neighbor_parents\n        If True, return (in .uns) the parent transcriptomes that generated the\n        doublet neighbors of each observed transcriptome. This information can\n        be used to infer the cell states that generated a given doublet state.\n    n_neighbors\n        Number of neighbors used to construct the KNN graph of observed\n        transcriptomes and simulated doublets. If ``None``, this is\n        automatically set to ``np.round(0.5 * np.sqrt(n_obs))``.\n    threshold\n        Doublet score threshold for calling a transcriptome a doublet. If\n        `None`, this is set automatically by looking for the minimum between\n        the two modes of the `doublet_scores_sim_` histogram. It is best\n        practice to check the threshold visually using the\n        `doublet_scores_sim_` histogram and/or based on co-localization of\n        predicted doublets in a 2-D embedding.\n    verbose\n        If True, print progress updates.\n    copy\n        If ``True``, return a copy of the input ``adata`` with Scrublet results\n        added. Otherwise, Scrublet results are added in place.\n    random_state\n        Initial state for doublet simulation and nearest neighbors.\n\n    Returns\n    -------\n    adata : anndata.AnnData\n        if ``copy=True`` it returns or else adds fields to ``adata``. Those fields:\n\n        ``.obs['doublet_score']``\n            Doublet scores for each observed transcriptome\n\n        ``.obs['predicted_doublets']``\n            Boolean indicating predicted doublet status\n\n        ``.uns['scrublet']['doublet_scores_sim']``\n            Doublet scores for each simulated doublet transcriptome\n\n        ``.uns['scrublet']['doublet_parents']``\n            Pairs of ``.obs_names`` used to generate each simulated doublet\n            transcriptome\n\n        ``.uns['scrublet']['parameters']``\n            Dictionary of Scrublet parameters\n\n    See also\n    --------\n    :func:`~scanpy.external.pp.scrublet_simulate_doublets`: Run Scrublet's doublet\n        simulation separately for advanced usage.\n    :func:`~scanpy.external.pl.scrublet_score_distribution`: Plot histogram of doublet\n        scores for observed transcriptomes and simulated doublets.\n    \"\"\"\n    try:\n        import scrublet as sl\n    except ImportError:\n        raise ImportError(\n            'Please install scrublet: `pip install scrublet` or `conda install scrublet`.'\n        )\n\n    if copy:\n        adata = adata.copy()\n\n    start = logg.info('Running Scrublet')\n\n    adata_obs = adata.copy()\n\n    def _run_scrublet(ad_obs, ad_sim=None):\n\n        # With no adata_sim we assume the regular use case, starting with raw\n        # counts and simulating doublets\n\n        if ad_sim is None:\n\n            pp.filter_genes(ad_obs, min_cells=3)\n            pp.filter_cells(ad_obs, min_genes=3)\n\n            # Doublet simulation will be based on the un-normalised counts, but on the\n            # selection of genes following normalisation and variability filtering. So\n            # we need to save the raw and subset at the same time.\n\n            ad_obs.layers['raw'] = ad_obs.X.copy()\n            pp.normalize_total(ad_obs)\n\n            # HVG process needs log'd data.\n\n            logged = pp.log1p(ad_obs, copy=True)\n            pp.highly_variable_genes(logged)\n            ad_obs = ad_obs[:, logged.var['highly_variable']]\n\n            # Simulate the doublets based on the raw expressions from the normalised\n            # and filtered object.\n\n            ad_sim = scrublet_simulate_doublets(\n                ad_obs,\n                layer='raw',\n                sim_doublet_ratio=sim_doublet_ratio,\n                synthetic_doublet_umi_subsampling=synthetic_doublet_umi_subsampling,\n            )\n\n            if log_transform:\n                pp.log1p(ad_obs)\n                pp.log1p(ad_sim)\n\n            # Now normalise simulated and observed in the same way\n\n            pp.normalize_total(ad_obs, target_sum=1e6)\n            pp.normalize_total(ad_sim, target_sum=1e6)\n\n        ad_obs = _scrublet_call_doublets(\n            adata_obs=ad_obs,\n            adata_sim=ad_sim,\n            n_neighbors=n_neighbors,\n            expected_doublet_rate=expected_doublet_rate,\n            stdev_doublet_rate=stdev_doublet_rate,\n            mean_center=mean_center,\n            normalize_variance=normalize_variance,\n            n_prin_comps=n_prin_comps,\n            use_approx_neighbors=use_approx_neighbors,\n            knn_dist_metric=knn_dist_metric,\n            get_doublet_neighbor_parents=get_doublet_neighbor_parents,\n            threshold=threshold,\n            random_state=random_state,\n            verbose=verbose,\n        )\n\n        return {'obs': ad_obs.obs, 'uns': ad_obs.uns['scrublet']}\n\n    if batch_key is not None:\n        if batch_key not in adata.obs.keys():\n            raise ValueError(\n                '`batch_key` must be a column of .obs in the input annData object.'\n            )\n\n        # Run Scrublet independently on batches and return just the\n        # scrublet-relevant parts of the objects to add to the input object\n\n        batches = np.unique(adata.obs[batch_key])\n        scrubbed = [\n            _run_scrublet(\n                adata_obs[\n                    adata_obs.obs[batch_key] == batch,\n                ],\n                adata_sim,\n            )\n            for batch in batches\n        ]\n        scrubbed_obs = pd.concat([scrub['obs'] for scrub in scrubbed])\n\n        # Now reset the obs to get the scrublet scores\n\n        adata.obs = scrubbed_obs.loc[adata.obs_names.values]\n\n        # Save the .uns from each batch separately\n\n        adata.uns['scrublet'] = {}\n        adata.uns['scrublet']['batches'] = dict(\n            zip(batches, [scrub['uns'] for scrub in scrubbed])\n        )\n\n        # Record that we've done batched analysis, so e.g. the plotting\n        # function knows what to do.\n\n        adata.uns['scrublet']['batched_by'] = batch_key\n\n    else:\n        scrubbed = _run_scrublet(adata_obs, adata_sim)\n\n        # Copy outcomes to input object from our processed version\n\n        adata.obs['doublet_score'] = scrubbed['obs']['doublet_score']\n        adata.obs['predicted_doublet'] = scrubbed['obs']['predicted_doublet']\n        adata.uns['scrublet'] = scrubbed['uns']\n\n    logg.info('    Scrublet finished', time=start)\n\n    if copy:\n        return adata\n    else:\n        return None", "idx": 69}
{"project": "Scanpy", "commit_id": "164_scanpy_1.9.0__scrublet.py__scrublet_call_doublets.py", "target": 1, "func": "def _scrublet_call_doublets(\n    adata_obs: AnnData,\n    adata_sim: AnnData,\n    n_neighbors: Optional[int] = None,\n    expected_doublet_rate: float = 0.05,\n    stdev_doublet_rate: float = 0.02,\n    mean_center: bool = True,\n    normalize_variance: bool = True,\n    n_prin_comps: int = 30,\n    use_approx_neighbors: bool = True,\n    knn_dist_metric: str = 'euclidean',\n    get_doublet_neighbor_parents: bool = False,\n    threshold: Optional[float] = None,\n    random_state: int = 0,\n    verbose: bool = True,\n) -> AnnData:\n    \"\"\"\\\n    Core function for predicting doublets using Scrublet [Wolock19]_.\n\n    Predict cell doublets using a nearest-neighbor classifier of observed\n    transcriptomes and simulated doublets. This is a wrapper around the core\n    functions of `Scrublet <https://github.com/swolock/scrublet>`__ to allow\n    for flexibility in applying Scanpy filtering operations upstream. Unless\n    you know what you're doing you should use the main scrublet() function.\n\n    .. note::\n        More information and bug reports `here\n        <https://github.com/swolock/scrublet>`__.\n\n    Parameters\n    ----------\n    adata_obs\n        The annotated data matrix of shape ``n_obs`` \u00d7 ``n_vars``. Rows\n        correspond to cells and columns to genes. Should be normalised with\n        scanpy.pp.normalize_total() and filtered to include only highly\n        variable genes.\n    adata_sim\n        Anndata object generated by\n        sc.external.pp.scrublet_simulate_doublets(), with same number of vars\n        as adata_obs. This should have been built from adata_obs after\n        filtering genes and cells and selcting highly-variable genes.\n    n_neighbors\n        Number of neighbors used to construct the KNN graph of observed\n        transcriptomes and simulated doublets. If ``None``, this is\n        automatically set to ``np.round(0.5 * np.sqrt(n_obs))``.\n    expected_doublet_rate\n        The estimated doublet rate for the experiment.\n    stdev_doublet_rate\n        Uncertainty in the expected doublet rate.\n    mean_center\n        If True, center the data such that each gene has a mean of 0.\n        `sklearn.decomposition.PCA` will be used for dimensionality\n        reduction.\n    normalize_variance\n        If True, normalize the data such that each gene has a variance of 1.\n        `sklearn.decomposition.TruncatedSVD` will be used for dimensionality\n        reduction, unless `mean_center` is True.\n    n_prin_comps\n        Number of principal components used to embed the transcriptomes prior\n        to k-nearest-neighbor graph construction.\n    use_approx_neighbors\n        Use approximate nearest neighbor method (annoy) for the KNN\n        classifier.\n    knn_dist_metric\n        Distance metric used when finding nearest neighbors. For list of\n        valid values, see the documentation for annoy (if `use_approx_neighbors`\n        is True) or sklearn.neighbors.NearestNeighbors (if `use_approx_neighbors`\n        is False).\n    get_doublet_neighbor_parents\n        If True, return the parent transcriptomes that generated the\n        doublet neighbors of each observed transcriptome. This information can\n        be used to infer the cell states that generated a given\n        doublet state.\n    threshold\n        Doublet score threshold for calling a transcriptome a doublet. If\n        `None`, this is set automatically by looking for the minimum between\n        the two modes of the `doublet_scores_sim_` histogram. It is best\n        practice to check the threshold visually using the\n        `doublet_scores_sim_` histogram and/or based on co-localization of\n        predicted doublets in a 2-D embedding.\n    random_state\n        Initial state for doublet simulation and nearest neighbors.\n    verbose\n        If True, print progress updates.\n\n    Returns\n    -------\n    adata : anndata.AnnData\n        if ``copy=True`` it returns or else adds fields to ``adata``:\n\n        ``.obs['doublet_score']``\n            Doublet scores for each observed transcriptome\n\n        ``.obs['predicted_doublets']``\n            Boolean indicating predicted doublet status\n\n        ``.uns['scrublet']['doublet_scores_sim']``\n            Doublet scores for each simulated doublet transcriptome\n\n        ``.uns['scrublet']['doublet_parents']``\n            Pairs of ``.obs_names`` used to generate each simulated doublet transcriptome\n\n        ``.uns['scrublet']['parameters']``\n            Dictionary of Scrublet parameters\n    \"\"\"\n    try:\n        import scrublet as sl\n    except ImportError:\n        raise ImportError(\n            'Please install scrublet: `pip install scrublet` or `conda install scrublet`.'\n        )\n\n    # Estimate n_neighbors if not provided, and create scrublet object.\n\n    if n_neighbors is None:\n        n_neighbors = int(round(0.5 * np.sqrt(adata_obs.shape[0])))\n\n    # Note: Scrublet() will sparse adata_obs.X if it's not already, but this\n    # matrix won't get used if we pre-set the normalised slots.\n\n    scrub = sl.Scrublet(\n        adata_obs.X,\n        n_neighbors=n_neighbors,\n        expected_doublet_rate=expected_doublet_rate,\n        stdev_doublet_rate=stdev_doublet_rate,\n        random_state=random_state,\n    )\n\n    # Ensure normalised matrix sparseness as Scrublet does\n    # https://github.com/swolock/scrublet/blob/67f8ecbad14e8e1aa9c89b43dac6638cebe38640/src/scrublet/scrublet.py#L100\n\n    scrub._E_obs_norm = sparse.csc_matrix(adata_obs.X)\n    scrub._E_sim_norm = sparse.csc_matrix(adata_sim.X)\n\n    scrub.doublet_parents_ = adata_sim.obsm['doublet_parents']\n\n    # Call scrublet-specific preprocessing where specified\n\n    if mean_center and normalize_variance:\n        sl.pipeline_zscore(scrub)\n    elif mean_center:\n        sl.pipeline_mean_center(scrub)\n    elif normalize_variance:\n        sl.pipeline_normalize_variance(scrub)\n\n    # Do PCA. Scrublet fits to the observed matrix and decomposes both observed\n    # and simulated based on that fit, so we'll just let it do its thing rather\n    # than trying to use Scanpy's PCA wrapper of the same functions.\n\n    if mean_center:\n        logg.info('Embedding transcriptomes using PCA...')\n        sl.pipeline_pca(\n            scrub, n_prin_comps=n_prin_comps, random_state=scrub.random_state\n        )\n    else:\n        logg.info('Embedding transcriptomes using Truncated SVD...')\n        sl.pipeline_truncated_svd(\n            scrub, n_prin_comps=n_prin_comps, random_state=scrub.random_state\n        )\n\n    # Score the doublets\n\n    scrub.calculate_doublet_scores(\n        use_approx_neighbors=use_approx_neighbors,\n        distance_metric=knn_dist_metric,\n        get_doublet_neighbor_parents=get_doublet_neighbor_parents,\n    )\n\n    # Actually call doublets\n\n    scrub.call_doublets(threshold=threshold, verbose=verbose)\n\n    # Store results in AnnData for return\n\n    adata_obs.obs['doublet_score'] = scrub.doublet_scores_obs_\n\n    # Store doublet Scrublet metadata\n\n    adata_obs.uns['scrublet'] = {\n        'doublet_scores_sim': scrub.doublet_scores_sim_,\n        'doublet_parents': adata_sim.obsm['doublet_parents'],\n        'parameters': {\n            'expected_doublet_rate': expected_doublet_rate,\n            'sim_doublet_ratio': (\n                adata_sim.uns.get('scrublet', {})\n                .get('parameters', {})\n                .get('sim_doublet_ratio', None)\n            ),\n            'n_neighbors': n_neighbors,\n            'random_state': random_state,\n        },\n    }\n\n    # If threshold hasn't been located successfully then we couldn't make any\n    # predictions. The user will get a warning from Scrublet, but we need to\n    # set the boolean so that any downstream filtering on\n    # predicted_doublet=False doesn't incorrectly filter cells. The user can\n    # still use this object to generate the plot and derive a threshold\n    # manually.\n\n    if hasattr(scrub, 'threshold_'):\n        adata_obs.uns['scrublet']['threshold'] = scrub.threshold_\n        adata_obs.obs['predicted_doublet'] = scrub.predicted_doublets_\n    else:\n        adata_obs.obs['predicted_doublet'] = False\n\n    if get_doublet_neighbor_parents:\n        adata_obs.uns['scrublet'][\n            'doublet_neighbor_parents'\n        ] = scrub.doublet_neighbor_parents_\n\n    return adata_obs", "idx": 70}
{"project": "Scanpy", "commit_id": "165_scanpy_1.9.0__scrublet.py_scrublet_simulate_doublets.py", "target": 0, "func": "def scrublet_simulate_doublets(\n    adata: AnnData,\n    layer=None,\n    sim_doublet_ratio: float = 2.0,\n    synthetic_doublet_umi_subsampling: float = 1.0,\n    random_seed: int = 0,\n) -> AnnData:\n    \"\"\"\\\n    Simulate doublets by adding the counts of random observed transcriptome pairs.\n\n    Parameters\n    ----------\n    adata\n        The annotated data matrix of shape ``n_obs`` \u00d7 ``n_vars``. Rows\n        correspond to cells and columns to genes. Genes should have been\n        filtered for expression and variability, and the object should contain\n        raw expression of the same dimensions.\n    layer\n        Layer of adata where raw values are stored, or 'X' if values are in .X.\n    sim_doublet_ratio\n        Number of doublets to simulate relative to the number of observed\n        transcriptomes. If `None`, self.sim_doublet_ratio is used.\n    synthetic_doublet_umi_subsampling\n        Rate for sampling UMIs when creating synthetic doublets. If 1.0,\n        each doublet is created by simply adding the UMIs from two randomly\n        sampled observed transcriptomes. For values less than 1, the\n        UMI counts are added and then randomly sampled at the specified\n        rate.\n\n    Returns\n    -------\n    adata : anndata.AnnData with simulated doublets in .X\n        Adds fields to ``adata``:\n\n        ``.obsm['scrublet']['doublet_parents']``\n            Pairs of ``.obs_names`` used to generate each simulated doublet transcriptome\n\n        ``.uns['scrublet']['parameters']``\n            Dictionary of Scrublet parameters\n\n    See also\n    --------\n    :func:`~scanpy.external.pp.scrublet`: Main way of running Scrublet, runs\n        preprocessing, doublet simulation (this function) and calling.\n    :func:`~scanpy.external.pl.scrublet_score_distribution`: Plot histogram of doublet\n        scores for observed transcriptomes and simulated doublets.\n    \"\"\"\n    try:\n        import scrublet as sl\n    except ImportError:\n        raise ImportError(\n            'Please install scrublet: `pip install scrublet` or `conda install scrublet`.'\n        )\n\n    X = _get_obs_rep(adata, layer=layer)\n    scrub = sl.Scrublet(X)\n\n    scrub.simulate_doublets(\n        sim_doublet_ratio=sim_doublet_ratio,\n        synthetic_doublet_umi_subsampling=synthetic_doublet_umi_subsampling,\n    )\n\n    adata_sim = AnnData(scrub._E_sim)\n    adata_sim.obs['n_counts'] = scrub._total_counts_sim\n    adata_sim.obsm['doublet_parents'] = scrub.doublet_parents_\n    adata_sim.uns['scrublet'] = {'parameters': {'sim_doublet_ratio': sim_doublet_ratio}}\n    return adata_sim", "idx": 71}
{"project": "Scanpy", "commit_id": "166_scanpy_1.9.0__scrublet.py__run_scrublet.py", "target": 0, "func": "def _run_scrublet(ad_obs, ad_sim=None):\n\n        # With no adata_sim we assume the regular use case, starting with raw\n        # counts and simulating doublets\n\n        if ad_sim is None:\n\n            pp.filter_genes(ad_obs, min_cells=3)\n            pp.filter_cells(ad_obs, min_genes=3)\n\n            # Doublet simulation will be based on the un-normalised counts, but on the\n            # selection of genes following normalisation and variability filtering. So\n            # we need to save the raw and subset at the same time.\n\n            ad_obs.layers['raw'] = ad_obs.X.copy()\n            pp.normalize_total(ad_obs)\n\n            # HVG process needs log'd data.\n\n            logged = pp.log1p(ad_obs, copy=True)\n            pp.highly_variable_genes(logged)\n            ad_obs = ad_obs[:, logged.var['highly_variable']]\n\n            # Simulate the doublets based on the raw expressions from the normalised\n            # and filtered object.\n\n            ad_sim = scrublet_simulate_doublets(\n                ad_obs,\n                layer='raw',\n                sim_doublet_ratio=sim_doublet_ratio,\n                synthetic_doublet_umi_subsampling=synthetic_doublet_umi_subsampling,\n            )\n\n            if log_transform:\n                pp.log1p(ad_obs)\n                pp.log1p(ad_sim)\n\n            # Now normalise simulated and observed in the same way\n\n            pp.normalize_total(ad_obs, target_sum=1e6)\n            pp.normalize_total(ad_sim, target_sum=1e6)\n\n        ad_obs = _scrublet_call_doublets(\n            adata_obs=ad_obs,\n            adata_sim=ad_sim,\n            n_neighbors=n_neighbors,\n            expected_doublet_rate=expected_doublet_rate,\n            stdev_doublet_rate=stdev_doublet_rate,\n            mean_center=mean_center,\n            normalize_variance=normalize_variance,\n            n_prin_comps=n_prin_comps,\n            use_approx_neighbors=use_approx_neighbors,\n            knn_dist_metric=knn_dist_metric,\n            get_doublet_neighbor_parents=get_doublet_neighbor_parents,\n            threshold=threshold,\n            random_state=random_state,\n            verbose=verbose,\n        )\n\n        return {'obs': ad_obs.obs, 'uns': ad_obs.uns['scrublet']}", "idx": 72}
{"project": "Scanpy", "commit_id": "167_scanpy_1.9.0__harmony_timeseries.py_harmony_timeseries.py", "target": 0, "func": "def harmony_timeseries(\n    adata: AnnData,\n    tp: str,\n    n_neighbors: int = 30,\n    n_components: Optional[int] = 1000,\n    n_jobs: int = -2,\n    copy: bool = False,\n) -> Optional[AnnData]:\n    \"\"\"\\\n    Harmony time series for data visualization with augmented affinity matrix\n    at discrete time points [Nowotschin18i]_.\n\n    Harmony time series is a framework for data visualization, trajectory\n    detection and interpretation for scRNA-seq data measured at discrete\n    time points. Harmony constructs an augmented affinity matrix by augmenting\n    the kNN graph affinity matrix with mutually nearest neighbors between\n    successive time points. This augmented affinity matrix forms the basis for\n    generated a force directed layout for visualization and also serves as input\n    for computing the diffusion operator which can be used for trajectory\n    detection using Palantir_.\n\n    .. _Palantir: https://github.com/dpeerlab/Palantir\n\n    .. note::\n       More information and bug reports `here\n       <https://github.com/dpeerlab/Harmony>`__.\n\n    Parameters\n    ----------\n    adata\n        Annotated data matrix of shape n_obs `\u00d7` n_vars. Rows correspond to\n        cells and columns to genes. Rows represent two or more time points,\n        where replicates of the same time point are consecutive in order.\n    tp\n        key name of observation annotation `.obs` representing time points. Time\n        points should be categorical of `dtype=category`. The unique categories for\n        the categorical will be used as the time points to construct the timepoint\n        connections.\n    n_neighbors\n        Number of nearest neighbors for graph construction.\n    n_components\n        Minimum number of principal components to use. Specify `None` to use\n        pre-computed components. The higher the value the better to capture 85% of the\n        variance.\n    n_jobs\n        Nearest Neighbors will be computed in parallel using n_jobs.\n    copy\n        Return a copy instead of writing to `adata`.\n\n    Returns\n    -------\n    Depending on `copy`, returns or updates `.obsm`, `.obsp` and `.uns` with the following:\n\n    **X_harmony** - :class:`~numpy.ndarray` (:attr:`~anndata.AnnData.obsm`, dtype `float`)\n        force directed layout\n    **harmony_aff** - :class:`~scipy.sparse.spmatrix` (:attr:`~anndata.AnnData.obsp`, dtype `float`)\n        affinity matrix\n    **harmony_aff_aug** - :class:`~scipy.sparse.spmatrix` (:attr:`~anndata.AnnData.obsp`, dtype `float`)\n        augmented affinity matrix\n    **harmony_timepoint_var** - `str` (:attr:`~anndata.AnnData.uns`)\n        The name of the variable passed as `tp`\n    **harmony_timepoint_connections** - :class:`~numpy.ndarray` (:attr:`~anndata.AnnData.uns`, dtype `str`)\n        The links between time points\n\n    Example\n    -------\n\n    >>> from itertools import product\n    >>> import pandas as pd\n    >>> from anndata import AnnData\n    >>> import scanpy as sc\n    >>> import scanpy.external as sce\n\n    **Load** `AnnData`\n\n    A sample with real data is available here_.\n\n    .. _here: https://github.com/dpeerlab/Harmony/tree/master/data\n\n    Random data sets of three time points with two replicates each:\n\n    >>> adata_ref = sc.datasets.pbmc3k()\n    >>> start = [596, 615, 1682, 1663, 1409, 1432]\n    >>> adata = AnnData.concatenate(\n    ...     *(adata_ref[i : i + 1000] for i in start),\n    ...     join=\"outer\",\n    ...     batch_key=\"sample\",\n    ...     batch_categories=[f\"sa{i}_Rep{j}\" for i, j in product((1, 2, 3), (1, 2))],\n    ... )\n    >>> time_points = adata.obs[\"sample\"].str.split(\"_\", expand=True)[0]\n    >>> adata.obs[\"time_points\"] = pd.Categorical(\n    ....    time_points, categories=['sa1', 'sa2', 'sa3']\n    ... )\n\n    Normalize and filter for highly expressed genes\n\n    >>> sc.pp.normalize_total(adata, target_sum=10000)\n    >>> sc.pp.log1p(adata)\n    >>> sc.pp.highly_variable_genes(adata, n_top_genes=1000, subset=True)\n\n    Run harmony_timeseries\n\n    >>> sce.tl.harmony_timeseries(adata, tp=\"time_points\", n_components=500)\n\n    Plot time points:\n\n    >>> sce.pl.harmony_timeseries(adata)\n\n    For further demonstration of Harmony visualizations please follow the notebook\n    `Harmony_sample_notebook.ipynb\n    <https://github.com/dpeerlab/Harmony/blob/master/notebooks/\n    Harmony_sample_notebook.ipynb>`_.\n    It provides a comprehensive guide to draw *gene expression trends*,\n    amongst other things.\n    \"\"\"\n\n    try:\n        import harmony\n    except ImportError:\n        raise ImportError(\"\\nplease install harmony:\\n\\n\\tpip install harmonyTS\")\n\n    adata = adata.copy() if copy else adata\n    logg.info(\"Harmony augmented affinity matrix\")\n\n    if adata.obs[tp].dtype.name != 'category':\n        raise ValueError(f'{tp!r} column does not contain Categorical data')\n    timepoints = adata.obs[tp].cat.categories.tolist()\n    timepoint_connections = pd.DataFrame(np.array([timepoints[:-1], timepoints[1:]]).T)\n\n    # compute the augmented and non-augmented affinity matrices\n    aug_aff, aff = harmony.core.augmented_affinity_matrix(\n        data_df=adata.to_df(),\n        timepoints=adata.obs[tp],\n        timepoint_connections=timepoint_connections,\n        n_neighbors=n_neighbors,\n        n_jobs=n_jobs,\n        pc_components=n_components,\n    )\n\n    # Force directed layouts\n    layout = harmony.plot.force_directed_layout(aug_aff, adata.obs.index)\n\n    adata.obsm[\"X_harmony\"] = np.asarray(layout)\n    adata.obsp[\"harmony_aff\"] = aff\n    adata.obsp[\"harmony_aff_aug\"] = aug_aff\n    adata.uns[\"harmony_timepoint_var\"] = tp\n    adata.uns[\"harmony_timepoint_connections\"] = np.asarray(timepoint_connections)\n\n    return adata if copy else None", "idx": 73}
{"project": "Scanpy", "commit_id": "168_scanpy_1.9.0__palantir.py_palantir.py", "target": 0, "func": "def palantir(\n    adata: AnnData,\n    n_components: int = 10,\n    knn: int = 30,\n    alpha: float = 0,\n    use_adjacency_matrix: bool = False,\n    distances_key: Optional[str] = None,\n    n_eigs: int = None,\n    impute_data: bool = True,\n    n_steps: int = 3,\n    copy: bool = False,\n) -> Optional[AnnData]:\n    \"\"\"\\\n    Run Diffusion maps using the adaptive anisotropic kernel [Setty18]_.\n\n    Palantir is an algorithm to align cells along differentiation trajectories.\n    Palantir models differentiation as a stochastic process where stem cells\n    differentiate to terminally differentiated cells by a series of steps through\n    a low dimensional phenotypic manifold. Palantir effectively captures the\n    continuity in cell states and the stochasticity in cell fate determination.\n    Palantir has been designed to work with multidimensional single cell data\n    from diverse technologies such as Mass cytometry and single cell RNA-seq.\n\n    .. note::\n       More information and bug reports `here <https://github.com/dpeerlab/Palantir>`__.\n\n    Parameters\n    ----------\n    adata\n        An AnnData object.\n    n_components\n        Number of diffusion components.\n    knn\n        Number of nearest neighbors for graph construction.\n    alpha\n        Normalization parameter for the diffusion operator.\n    use_adjacency_matrix\n        Use adaptive anisotropic adjacency matrix, instead of PCA projections\n        (default) to compute diffusion components.\n    distances_key\n        With `use_adjacency_matrix=True`, use the indicated distances key for `.obsp`.\n        If `None`, `'distances'`.\n    n_eigs\n        Number of eigen vectors to use. If `None` specified, the number of eigen\n        vectors will be determined using eigen gap. Passed to\n        `palantir.utils.determine_multiscale_space`.\n    impute_data\n        Impute data using MAGIC.\n    n_steps\n        Number of steps in the diffusion operator. Passed to\n        `palantir.utils.run_magic_imputation`.\n    copy\n        Return a copy instead of writing to `adata`.\n\n    Returns\n    -------\n    Depending on `copy`, returns or updates `adata` with the following fields:\n\n    **Diffusion maps**,\n        used for magic imputation, and to generate multi-scale data matrix,\n\n        - X_palantir_diff_comp - :class:`~numpy.ndarray` (:attr:`~anndata.AnnData.obsm`, dtype `float`)\n            Array of Diffusion components.\n        - palantir_EigenValues - :class:`~numpy.ndarray` (:attr:`~anndata.AnnData.uns`, dtype `float`)\n            Array of corresponding eigen values.\n        - palantir_diff_op - :class:`~scipy.sparse.spmatrix` (:attr:`~anndata.AnnData.obsp`, dtype `float`)\n            The diffusion operator matrix.\n\n    **Multi scale space results**,\n        used to build tsne on diffusion components, and to compute branch probabilities\n        and waypoints,\n\n        - X_palantir_multiscale - :class:`~numpy.ndarray` (:attr:`~anndata.AnnData.obsm`, dtype `float`)\n            Multi scale data matrix.\n\n    **MAGIC imputation**,\n        used for plotting gene expression on tsne, and gene expression trends,\n\n        - palantir_imp - :class:`~numpy.ndarray` (:attr:`~anndata.AnnData.layers`, dtype `float`)\n            Imputed data matrix (MAGIC imputation).\n\n    Example\n    -------\n    >>> import scanpy.external as sce\n    >>> import scanpy as sc\n\n    A sample data is available `here <https://github.com/dpeerlab/Palantir/tree/master/data>`_.\n\n    **Load sample data**\n\n    >>> adata = sc.read_csv(filename=\"Palantir/data/marrow_sample_scseq_counts.csv.gz\")\n\n    *Cleanup and normalize*\n\n    >>> sc.pp.filter_cells(adata, min_counts=1000)\n    >>> sc.pp.filter_genes(adata, min_counts=10)\n    >>> sc.pp.normalize_per_cell(adata)\n    >>> sc.pp.log1p(adata)\n\n    **Data preprocessing**\n\n    Palantir builds diffusion maps using one of two optional inputs:\n\n    *Principal component analysis*\n\n    >>> sc.tl.pca(adata, n_comps=300)\n\n    or,\n\n    *Nearist neighbors graph*\n\n    >>> sc.pp.neighbors(adata, knn=30)\n\n    *Diffusion maps*\n\n    Palantir determines the diffusion maps of the data as an estimate of the low\n    dimensional phenotypic manifold of the data.\n\n    >>> sce.tl.palantir(adata, n_components=5, knn=30)\n\n    if pre-computed distances are to be used,\n\n    >>> sce.tl.palantir(\n    ...     adata,\n    ...     n_components=5,\n    ...     knn=30,\n    ...     use_adjacency_matrix=True,\n    ...     distances_key=\"distances\",\n    ... )\n\n    **Visualizing Palantir results**\n\n    *tSNE visualization*\n\n    important for Palantir!\n\n    Palantir constructs the tSNE map in the embedded space since these maps better\n    represent the differentiation trajectories.\n\n    >>> sc.tl.tsne(adata, n_pcs=2, use_rep='X_palantir_multiscale', perplexity=150)\n\n    *tsne by cell size*\n\n    >>> sc.pl.tsne(adata, color=\"n_counts\")\n\n    *Imputed gene expression visualized on tSNE maps*\n\n    >>> sc.pl.tsne(\n    ...     adata,\n    ...     gene_symbols=['CD34', 'MPO', 'GATA1', 'IRF8'],\n    ...     layer='palantir_imp',\n    ...     color=['CD34', 'MPO', 'GATA1', 'IRF8']\n    ... )\n\n    **Running Palantir**\n\n    Palantir can be run by specifying an approximate early cell. While Palantir\n    automatically determines the terminal states, they can also be specified using the\n    `termine_states` parameter.\n\n    >>> start_cell = 'Run5_164698952452459'\n    >>> pr_res = sce.tl.palantir_results(\n    ...     adata,\n    ...     early_cell=start_cell,\n    ...     ms_data='X_palantir_multiscale',\n    ...     num_waypoints=500,\n    ... )\n\n    .. note::\n       A `start_cell` must be defined for every data set. The start cell for\n       this dataset was chosen based on high expression of CD34.\n\n    At this point the returned Palantir object `pr_res` can be used for all downstream\n    analysis and plotting. Please consult this notebook\n    `Palantir_sample_notebook.ipynb\n    <https://github.com/dpeerlab/Palantir/blob/master/notebooks/Palantir_sample_notebook.ipynb>`_.\n    It provides a comprehensive guide to draw *gene expression trends*, amongst other\n    things.\n    \"\"\"\n\n    _check_import()\n    from palantir.utils import (\n        run_diffusion_maps,\n        determine_multiscale_space,\n        run_magic_imputation,\n    )\n\n    adata = adata.copy() if copy else adata\n\n    logg.info('Palantir Diffusion Maps in progress ...')\n\n    if use_adjacency_matrix:\n        df = adata.obsp[distances_key] if distances_key else adata.obsp[\"distances\"]\n    else:\n        df = pd.DataFrame(adata.obsm['X_pca'], index=adata.obs_names)\n\n    # Diffusion maps\n    dm_res = run_diffusion_maps(\n        data_df=df,\n        n_components=n_components,\n        knn=knn,\n        alpha=alpha,\n    )\n    # Determine the multi scale space of the data\n    ms_data = determine_multiscale_space(dm_res=dm_res, n_eigs=n_eigs)\n\n    # MAGIC imputation\n    if impute_data:\n        imp_df = run_magic_imputation(\n            data=adata.to_df(), dm_res=dm_res, n_steps=n_steps\n        )\n        adata.layers['palantir_imp'] = imp_df\n\n    (\n        adata.obsm['X_palantir_diff_comp'],\n        adata.uns['palantir_EigenValues'],\n        adata.obsp['palantir_diff_op'],\n        adata.obsm['X_palantir_multiscale'],\n    ) = (\n        dm_res['EigenVectors'].to_numpy(),\n        dm_res['EigenValues'].to_numpy(),\n        dm_res['T'],\n        ms_data.to_numpy(),\n    )\n\n    return adata if copy else None", "idx": 74}
{"project": "Scanpy", "commit_id": "169_scanpy_1.9.0__palantir.py_palantir_results.py", "target": 0, "func": "def palantir_results(\n    adata: AnnData,\n    early_cell: str,\n    ms_data: str = 'X_palantir_multiscale',\n    terminal_states: List = None,\n    knn: int = 30,\n    num_waypoints: int = 1200,\n    n_jobs: int = -1,\n    scale_components: bool = True,\n    use_early_cell_as_start: bool = False,\n    max_iterations: int = 25,\n) -> Optional[AnnData]:\n    \"\"\"\\\n    **Running Palantir**\n\n    A convenience function that wraps `palantir.core.run_palantir` to compute branch\n    probabilities and waypoints.\n\n    Parameters\n    ----------\n    adata\n        An AnnData object.\n    early_cell\n        Start cell for pseudotime construction.\n    ms_data\n        Palantir multi scale data matrix,\n    terminal_states\n        List of user defined terminal states\n    knn\n        Number of nearest neighbors for graph construction.\n    num_waypoints\n        Number of waypoints to sample.\n    n_jobs\n        Number of jobs for parallel processing.\n    scale_components\n        Transform features by scaling each feature to a given range. Consult the\n        documentation for `sklearn.preprocessing.minmax_scale`.\n    use_early_cell_as_start\n        Use `early_cell` as `start_cell`, instead of determining it from the boundary\n        cells closest to the defined `early_cell`.\n    max_iterations\n        Maximum number of iterations for pseudotime convergence.\n\n    Returns\n    -------\n    PResults\n        PResults object with pseudotime, entropy, branch probabilities and waypoints.\n    \"\"\"\n    logg.info('Palantir computing waypoints..')\n\n    _check_import()\n    from palantir.core import run_palantir\n\n    ms_data = pd.DataFrame(adata.obsm[ms_data], index=adata.obs_names)\n    pr_res = run_palantir(\n        ms_data=ms_data,\n        early_cell=early_cell,\n        terminal_states=terminal_states,\n        knn=knn,\n        num_waypoints=num_waypoints,\n        n_jobs=n_jobs,\n        scale_components=scale_components,\n        use_early_cell_as_start=use_early_cell_as_start,\n        max_iterations=max_iterations,\n    )\n\n    return pr_res", "idx": 75}
{"project": "Scanpy", "commit_id": "16_scanpy_1.9.0_has_attr_test.py_setup.py", "target": 0, "func": "def setup(app: Sphinx):\n    DEFAULT_NAMESPACE[\"has_attr\"] = has_attr", "idx": 76}
{"project": "Scanpy", "commit_id": "170_scanpy_1.9.0__palantir.py__check_import.py", "target": 0, "func": "def _check_import():\n    try:\n        import palantir\n    except ImportError:\n        raise ImportError('\\nplease install palantir:\\n\\tpip install palantir')", "idx": 77}
{"project": "Scanpy", "commit_id": "171_scanpy_1.9.0__phate.py_phate.py", "target": 0, "func": "def phate(\n    adata: AnnData,\n    n_components: int = 2,\n    k: int = 5,\n    a: int = 15,\n    n_landmark: int = 2000,\n    t: Union[int, str] = 'auto',\n    gamma: float = 1.0,\n    n_pca: int = 100,\n    knn_dist: str = 'euclidean',\n    mds_dist: str = 'euclidean',\n    mds: Literal['classic', 'metric', 'nonmetric'] = 'metric',\n    n_jobs: Optional[int] = None,\n    random_state: AnyRandom = None,\n    verbose: Union[bool, int, None] = None,\n    copy: bool = False,\n    **kwargs,\n) -> Optional[AnnData]:\n    \"\"\"\\\n    PHATE [Moon17]_.\n\n    Potential of Heat-diffusion for Affinity-based Trajectory Embedding (PHATE)\n    embeds high dimensional single-cell data into two or three dimensions for\n    visualization of biological progressions.\n\n    For more information and access to the object-oriented interface, read the\n    `PHATE documentation <https://phate.readthedocs.io/>`__.  For\n    tutorials, bug reports, and R/MATLAB implementations, visit the `PHATE\n    GitHub page <https://github.com/KrishnaswamyLab/PHATE/>`__. For help\n    using PHATE, go `here <https://krishnaswamylab.org/get-help>`__.\n\n    Parameters\n    ----------\n    adata\n        Annotated data matrix.\n    n_components\n        number of dimensions in which the data will be embedded\n    k\n        number of nearest neighbors on which to build kernel\n    a\n        sets decay rate of kernel tails.\n        If None, alpha decaying kernel is not used\n    n_landmark\n        number of landmarks to use in fast PHATE\n    t\n        power to which the diffusion operator is powered\n        sets the level of diffusion. If 'auto', t is selected\n        according to the knee point in the Von Neumann Entropy of\n        the diffusion operator\n    gamma\n        Informational distance constant between -1 and 1.\n        `gamma=1` gives the PHATE log potential, `gamma=0` gives\n        a square root potential.\n    n_pca\n        Number of principal components to use for calculating\n        neighborhoods. For extremely large datasets, using\n        n_pca < 20 allows neighborhoods to be calculated in\n        log(n_samples) time.\n    knn_dist\n        recommended values: 'euclidean' and 'cosine'\n        Any metric from `scipy.spatial.distance` can be used\n        distance metric for building kNN graph\n    mds_dist\n        recommended values: 'euclidean' and 'cosine'\n        Any metric from `scipy.spatial.distance` can be used\n        distance metric for MDS\n    mds\n        Selects which MDS algorithm is used for dimensionality reduction.\n    n_jobs\n        The number of jobs to use for the computation.\n        If `None`, `sc.settings.n_jobs` is used.\n        If -1 all CPUs are used. If 1 is given, no parallel computing code is\n        used at all, which is useful for debugging.\n        For n_jobs below -1, (n_cpus + 1 + n_jobs) are used. Thus for\n        n_jobs = -2, all CPUs but one are used\n    random_state\n        Random seed. Defaults to the global `numpy` random number generator\n    verbose\n        If `True` or an `int`/`Verbosity` \u2265 2/`hint`, print status messages.\n        If `None`, `sc.settings.verbosity` is used.\n    copy\n        Return a copy instead of writing to `adata`.\n    kwargs\n        Additional arguments to `phate.PHATE`\n\n    Returns\n    -------\n    Depending on `copy`, returns or updates `adata` with the following fields.\n\n    **X_phate** : `np.ndarray`, (`adata.obs`, shape=[n_samples, n_components], dtype `float`)\n        PHATE coordinates of data.\n\n    Examples\n    --------\n    >>> from anndata import AnnData\n    >>> import scanpy.external as sce\n    >>> import phate\n    >>> tree_data, tree_clusters = phate.tree.gen_dla(\n    ...     n_dim=100,\n    ...     n_branch=20,\n    ...     branch_length=100,\n    ... )\n    >>> tree_data.shape\n    (2000, 100)\n    >>> adata = AnnData(tree_data)\n    >>> sce.tl.phate(adata, k=5, a=20, t=150)\n    >>> adata.obsm['X_phate'].shape\n    (2000, 2)\n    >>> sce.pl.phate(adata)\n    \"\"\"\n    start = logg.info('computing PHATE')\n    adata = adata.copy() if copy else adata\n    verbosity = settings.verbosity if verbose is None else verbose\n    verbose = verbosity if isinstance(verbosity, bool) else verbosity >= 2\n    n_jobs = settings.n_jobs if n_jobs is None else n_jobs\n    try:\n        import phate\n    except ImportError:\n        raise ImportError(\n            'You need to install the package `phate`: please run `pip install '\n            '--user phate` in a terminal.'\n        )\n    X_phate = phate.PHATE(\n        n_components=n_components,\n        k=k,\n        a=a,\n        n_landmark=n_landmark,\n        t=t,\n        gamma=gamma,\n        n_pca=n_pca,\n        knn_dist=knn_dist,\n        mds_dist=mds_dist,\n        mds=mds,\n        n_jobs=n_jobs,\n        random_state=random_state,\n        verbose=verbose,\n        **kwargs,\n    ).fit_transform(adata)\n    # update AnnData instance\n    adata.obsm['X_phate'] = X_phate  # annotate samples with PHATE coordinates\n    logg.info(\n        '    finished',\n        time=start,\n        deep=('added\\n' \"    'X_phate', PHATE coordinates (adata.obsm)\"),\n    )\n    return adata if copy else None", "idx": 78}
{"project": "Scanpy", "commit_id": "172_scanpy_1.9.0__phenograph.py_phenograph.py", "target": 1, "func": "def phenograph(\n    adata: Union[AnnData, np.ndarray, spmatrix],\n    clustering_algo: Optional[Literal['louvain', 'leiden']] = 'louvain',\n    k: int = 30,\n    directed: bool = False,\n    prune: bool = False,\n    min_cluster_size: int = 10,\n    jaccard: bool = True,\n    primary_metric: Literal[\n        'euclidean',\n        'manhattan',\n        'correlation',\n        'cosine',\n    ] = \"euclidean\",\n    n_jobs: int = -1,\n    q_tol: float = 1e-3,\n    louvain_time_limit: int = 2000,\n    nn_method: Literal['kdtree', 'brute'] = 'kdtree',\n    partition_type: Optional[Type[MutableVertexPartition]] = None,\n    resolution_parameter: float = 1,\n    n_iterations: int = -1,\n    use_weights: bool = True,\n    seed: Optional[int] = None,\n    copy: bool = False,\n    **kargs: Any,\n) -> Tuple[Optional[np.ndarray], spmatrix, Optional[float]]:\n    \"\"\"\\\n    PhenoGraph clustering [Levine15]_.\n\n    **PhenoGraph** is a clustering method designed for high-dimensional single-cell\n    data. It works by creating a graph (\"network\") representing phenotypic similarities\n    between cells and then identifying communities in this graph. It supports both\n    Louvain_ and Leiden_ algorithms for community detection.\n\n    .. _Louvain: https://louvain-igraph.readthedocs.io/en/latest/\n\n    .. _Leiden: https://leidenalg.readthedocs.io/en/latest/reference.html\n\n    .. note::\n       More information and bug reports `here\n       <https://github.com/dpeerlab/PhenoGraph>`__.\n\n    Parameters\n    ----------\n    adata\n        AnnData, or Array of data to cluster, or sparse matrix of k-nearest neighbor\n        graph. If ndarray, n-by-d array of n cells in d dimensions. if sparse matrix,\n        n-by-n adjacency matrix.\n    clustering_algo\n        Choose between `'Louvain'` or `'Leiden'` algorithm for clustering.\n    k\n        Number of nearest neighbors to use in first step of graph construction.\n    directed\n        Whether to use a symmetric (default) or asymmetric (`'directed'`) graph.\n        The graph construction process produces a directed graph, which is symmetrized\n        by one of two methods (see `prune` below).\n    prune\n        `prune=False`, symmetrize by taking the average between the graph and its\n        transpose. `prune=True`, symmetrize by taking the product between the graph\n        and its transpose.\n    min_cluster_size\n        Cells that end up in a cluster smaller than min_cluster_size are considered\n        outliers and are assigned to -1 in the cluster labels.\n    jaccard\n        If `True`, use Jaccard metric between k-neighborhoods to build graph. If\n        `False`, use a Gaussian kernel.\n    primary_metric\n        Distance metric to define nearest neighbors. Note that performance will be\n        slower for correlation and cosine.\n    n_jobs\n        Nearest Neighbors and Jaccard coefficients will be computed in parallel using\n        n_jobs. If 1 is given, no parallelism is used. If set to -1, all CPUs are used.\n        For n_jobs below -1, `n_cpus + 1 + n_jobs` are used.\n    q_tol\n        Tolerance, i.e. precision, for monitoring modularity optimization.\n    louvain_time_limit\n        Maximum number of seconds to run modularity optimization. If exceeded the best\n        result so far is returned.\n    nn_method\n        Whether to use brute force or kdtree for nearest neighbor search.\n        For very large high-dimensional data sets, brute force, with parallel\n        computation, performs faster than kdtree.\n    partition_type\n        Defaults to :class:`~leidenalg.RBConfigurationVertexPartition`. For the\n        available options, consult the documentation for\n        :func:`~leidenalg.find_partition`.\n    resolution_parameter\n        A parameter value controlling the coarseness of the clustering in Leiden. Higher\n        values lead to more clusters. Set to `None` if overriding `partition_type` to\n        one that does not accept a `resolution_parameter`.\n    n_iterations\n        Number of iterations to run the Leiden algorithm. If the number of iterations is\n        negative, the Leiden algorithm is run until an iteration in which there was no\n        improvement.\n    use_weights\n        Use vertices in the Leiden computation.\n    seed\n        Leiden initialization of the optimization.\n    copy\n        Return a copy or write to `adata`.\n    kargs\n        Additional arguments passed to :func:`~leidenalg.find_partition` and the\n        constructor of the `partition_type`.\n\n    Returns\n    -------\n    Depending on `copy`, returns or updates `adata` with the following fields:\n\n    **communities** - :class:`~numpy.ndarray` (:attr:`~anndata.AnnData.obs`, dtype `int`)\n        integer array of community assignments for each row in data.\n\n    **graph** - :class:`~scipy.sparse.spmatrix` (:attr:`~anndata.AnnData.obsp`, dtype `float`)\n        the graph that was used for clustering.\n\n    **Q** - `float` (:attr:`~anndata.AnnData.uns`, dtype `float`)\n        the modularity score for communities on graph.\n\n    Example\n    -------\n    >>> from anndata import AnnData\n    >>> import scanpy as sc\n    >>> import scanpy.external as sce\n    >>> import numpy as np\n    >>> import pandas as pd\n\n    With annotated data as input:\n\n    >>> adata = sc.datasets.pbmc3k()\n    >>> sc.pp.normalize_per_cell(adata)\n\n    Then do PCA:\n\n    >>> sc.tl.pca(adata, n_comps=100)\n\n    Compute phenograph clusters:\n\n    **Louvain** community detection\n\n    >>> sce.tl.phenograph(adata, clustering_algo=\"louvain\", k=30)\n\n    **Leiden** community detection\n\n    >>> sce.tl.phenograph(adata, clustering_algo=\"leiden\", k=30)\n\n    Return only `Graph` object\n\n    >>> sce.tl.phenograph(adata, clustering_algo=None, k=30)\n\n    Now to show phenograph on tSNE (for example):\n\n    Compute tSNE:\n\n    >>> sc.tl.tsne(adata, random_state=7)\n\n    Plot phenograph clusters on tSNE:\n\n    >>> sc.pl.tsne(\n    ...     adata, color = [\"pheno_louvain\", \"pheno_leiden\"], s = 100,\n    ...     palette = sc.pl.palettes.vega_20_scanpy, legend_fontsize = 10\n    ... )\n\n    Cluster and cluster centroids for input Numpy ndarray\n\n    >>> df = np.random.rand(1000, 40)\n    >>> dframe = pd.DataFrame(df)\n    >>> dframe.index, dframe.columns = (map(str, dframe.index), map(str, dframe.columns))\n    >>> adata = AnnData(dframe)\n    >>> sc.tl.pca(adata, n_comps=20)\n    >>> sce.tl.phenograph(adata, clustering_algo=\"leiden\", k=50)\n    >>> sc.tl.tsne(adata, random_state=1)\n    >>> sc.pl.tsne(\n    ...     adata, color=['pheno_leiden'], s=100,\n    ...     palette=sc.pl.palettes.vega_20_scanpy, legend_fontsize=10\n    ... )\n    \"\"\"\n    start = logg.info(\"PhenoGraph clustering\")\n\n    try:\n        import phenograph\n\n        assert phenograph.__version__ >= \"1.5.3\"\n    except (ImportError, AssertionError, AttributeError):\n        raise ImportError(\n            \"please install the latest release of phenograph:\\n\\t\"\n            \"pip install -U PhenoGraph\"\n        )\n\n    if isinstance(adata, AnnData):\n        try:\n            data = adata.obsm[\"X_pca\"]\n        except KeyError:\n            raise KeyError(\"Please run `sc.tl.pca` on `adata` and try again!\")\n    else:\n        data = adata\n        copy = True\n\n    comm_key = (\n        \"pheno_{}\".format(clustering_algo)\n        if clustering_algo in [\"louvain\", \"leiden\"]\n        else ''\n    )\n    ig_key = \"pheno_{}_ig\".format(\"jaccard\" if jaccard else \"gaussian\")\n    q_key = \"pheno_{}_q\".format(\"jaccard\" if jaccard else \"gaussian\")\n\n    communities, graph, Q = phenograph.cluster(\n        data=data,\n        clustering_algo=clustering_algo,\n        k=k,\n        directed=directed,\n        prune=prune,\n        min_cluster_size=min_cluster_size,\n        jaccard=jaccard,\n        primary_metric=primary_metric,\n        n_jobs=n_jobs,\n        q_tol=q_tol,\n        louvain_time_limit=louvain_time_limit,\n        nn_method=nn_method,\n        partition_type=partition_type,\n        resolution_parameter=resolution_parameter,\n        n_iterations=n_iterations,\n        use_weights=use_weights,\n        seed=seed,\n        **kargs,\n    )\n\n    logg.info(\"    finished\", time=start)\n\n    if copy:\n        return communities, graph, Q\n    else:\n        adata.obsp[ig_key] = graph\n        if comm_key:\n            adata.obs[comm_key] = pd.Categorical(communities)\n        if Q:\n            adata.uns[q_key] = Q", "idx": 79}
{"project": "Scanpy", "commit_id": "173_scanpy_1.9.0__pypairs.py_sandbag.py", "target": 0, "func": "def sandbag(\n    adata: Union[AnnData],\n    annotation: Optional[Mapping[str, Genes]] = None,\n    *,\n    fraction: float = 0.65,\n    filter_genes: Optional[Genes] = None,\n    filter_samples: Optional[Genes] = None,\n) -> Dict[str, List[Tuple[str, str]]]:\n    \"\"\"\\\n    Calculate marker pairs of genes. [Scialdone15]_ [Fechtner18]_.\n\n    Calculates the pairs of genes serving as marker pairs for each phase,\n    based on a matrix of gene counts and an annotation of known phases.\n\n    This reproduces the approach of [Scialdone15]_ in the implementation of\n    [Fechtner18]_.\n\n    More information and bug reports `here\n    <https://github.com/rfechtner/pypairs>`__.\n\n    Parameters\n    ----------\n    adata\n        The annotated data matrix.\n    annotation\n        Mapping from category to genes, e.g. `{'phase': [Gene1, ...]}`.\n        Defaults to ``data.vars['category']``.\n    fraction\n        Fraction of cells per category where marker criteria must be satisfied.\n    filter_genes\n        Genes for sampling the reference set. Defaults to all genes.\n    filter_samples\n        Cells for sampling the reference set. Defaults to all samples.\n\n    Returns\n    -------\n    A dict mapping from category to lists of marker pairs, e.g.:\n    `{'Category_1': [(Gene_1, Gene_2), ...], ...}`.\n\n    Examples\n    --------\n    >>> from scanpy.external.tl import sandbag\n    >>> from pypairs import datasets\n    >>> adata = datasets.leng15()\n    >>> marker_pairs = sandbag(adata, fraction=0.5)\n    \"\"\"\n    _check_import()\n    from pypairs.pairs import sandbag\n    from pypairs import settings as pp_settings\n\n    pp_settings.verbosity = settings.verbosity\n    pp_settings.n_jobs = settings.n_jobs\n    pp_settings.writedir = settings.writedir\n    pp_settings.cachedir = settings.cachedir\n    pp_settings.logfile = settings.logfile\n\n    return sandbag(\n        data=adata,\n        annotation=annotation,\n        fraction=fraction,\n        filter_genes=filter_genes,\n        filter_samples=filter_samples,", "idx": 80}
{"project": "Scanpy", "commit_id": "174_scanpy_1.9.0__pypairs.py_cyclone.py", "target": 0, "func": "def cyclone(\n    adata: AnnData,\n    marker_pairs: Optional[Mapping[str, Collection[Tuple[str, str]]]] = None,\n    *,\n    iterations: int = 1000,\n    min_iter: int = 100,\n    min_pairs: int = 50,\n) -> pd.DataFrame:\n    \"\"\"\\\n    Assigns scores and predicted class to observations [Scialdone15]_ [Fechtner18]_.\n\n    Calculates scores for each observation and each phase and assigns prediction\n    based on marker pairs indentified by :func:`~scanpy.external.tl.sandbag`.\n\n    This reproduces the approach of [Scialdone15]_ in the implementation of\n    [Fechtner18]_.\n\n    Parameters\n    ----------\n    adata\n        The annotated data matrix.\n    marker_pairs\n        Mapping of categories to lists of marker pairs.\n        See :func:`~scanpy.external.tl.sandbag` output.\n    iterations\n        An integer scalar specifying the number of\n        iterations for random sampling to obtain a cycle score.\n    min_iter\n        An integer scalar specifying the minimum number of iterations\n        for score estimation.\n    min_pairs\n        An integer scalar specifying the minimum number of pairs\n        for score estimation.\n\n    Returns\n    -------\n    A :class:`~pandas.DataFrame` with samples as index and categories as columns\n    with scores for each category for each sample and a additional column with\n    the name of the max scoring category for each sample.\n\n    If `marker_pairs` contains only the cell cycle categories G1, S and G2M an\n    additional column `pypairs_cc_prediction` will be added.\n    Where category S is assigned to samples where G1 and G2M score are < 0.5.\n    \"\"\"\n    _check_import()\n    from pypairs.pairs import cyclone\n    from pypairs import settings as pp_settings\n\n    pp_settings.verbosity = settings.verbosity\n    pp_settings.n_jobs = settings.n_jobs\n    pp_settings.writedir = settings.writedir\n    pp_settings.cachedir = settings.cachedir\n    pp_settings.logfile = settings.logfile\n\n    return cyclone(\n        data=adata,\n        marker_pairs=marker_pairs,\n        iterations=iterations,\n        min_iter=min_iter,\n        min_pairs=min_pairs,", "idx": 81}
{"project": "Scanpy", "commit_id": "175_scanpy_1.9.0__pypairs.py__check_import.py", "target": 0, "func": "def _check_import():\n    try:\n        import pypairs\n    except ImportError:\n        raise ImportError('You need to install the package `pypairs`.')\n\n    min_version = version.parse(\"3.0.9\")\n    if version.parse(pypairs.__version__) < min_version:\n        raise ImportError(f'Please only use `pypairs` >= {min_version}')", "idx": 82}
{"project": "Scanpy", "commit_id": "176_scanpy_1.9.0__sam.py_sam.py", "target": 0, "func": "def sam(\n    adata: AnnData,\n    max_iter: int = 10,\n    num_norm_avg: int = 50,\n    k: int = 20,\n    distance: str = 'correlation',\n    standardization: Literal['Normalizer', 'StandardScaler', 'None'] = 'StandardScaler',\n    weight_pcs: bool = False,\n    sparse_pca: bool = False,\n    n_pcs: Optional[int] = 150,\n    n_genes: Optional[int] = 3000,\n    projection: Literal['umap', 'tsne', 'None'] = 'umap',\n    inplace: bool = True,\n    verbose: bool = True,\n) -> Union[SAM, Tuple[SAM, AnnData]]:\n    \"\"\"\\\n    Self-Assembling Manifolds single-cell RNA sequencing analysis tool [Tarashansky19]_.\n\n    SAM iteratively rescales the input gene expression matrix to emphasize\n    genes that are spatially variable along the intrinsic manifold of the data.\n    It outputs the gene weights, nearest neighbor matrix, and a 2D projection.\n\n    The AnnData input should contain unstandardized, non-negative values.\n    Preferably, the data should be log-normalized and no genes should be filtered out.\n\n\n    Parameters\n    ----------\n\n    k\n        The number of nearest neighbors to identify for each cell.\n\n    distance\n        The distance metric to use when identifying nearest neighbors.\n        Can be any of the distance metrics supported by\n        :func:`~scipy.spatial.distance.pdist`.\n\n    max_iter\n        The maximum number of iterations SAM will run.\n\n    projection\n        If 'tsne', generates a t-SNE embedding. If 'umap', generates a UMAP\n        embedding. If 'None', no embedding will be generated.\n\n    standardization\n        If 'Normalizer', use sklearn.preprocessing.Normalizer, which\n        normalizes expression data prior to PCA such that each cell has\n        unit L2 norm. If 'StandardScaler', use\n        sklearn.preprocessing.StandardScaler, which normalizes expression\n        data prior to PCA such that each gene has zero mean and unit\n        variance. Otherwise, do not normalize the expression data. We\n        recommend using 'StandardScaler' for large datasets with many\n        expected cell types and 'Normalizer' otherwise. If 'None', no\n        transformation is applied.\n\n    num_norm_avg\n        The top 'num_norm_avg' dispersions are averaged to determine the\n        normalization factor when calculating the weights. This prevents\n        genes with large spatial dispersions from skewing the distribution\n        of weights.\n\n    weight_pcs\n        If True, scale the principal components by their eigenvalues. In\n        datasets with many expected cell types, setting this to False might\n        improve the resolution as these cell types might be encoded by lower-\n        variance principal components.\n\n    sparse_pca\n        If True, uses an implementation of PCA that accepts sparse inputs.\n        This way, we no longer need a temporary dense copy of the sparse data.\n        However, this implementation is slower and so is only worth using when\n        memory constraints become noticeable.\n\n    n_pcs\n        Determines the number of top principal components selected at each\n        iteration of the SAM algorithm. If None, this number is chosen\n        automatically based on the size of the dataset. If weight_pcs is\n        set to True, this parameter primarily affects the runtime of the SAM\n        algorithm (more PCs = longer runtime).\n\n    n_genes\n        Determines the number of top SAM-weighted genes to use at each iteration\n        of the SAM algorithm. If None, this number is chosen automatically\n        based on the size of the dataset. This parameter primarily affects\n        the runtime of the SAM algorithm (more genes = longer runtime). For\n        extremely homogeneous datasets, decreasing `n_genes` may improve\n        clustering resolution.\n\n    inplace\n        Set fields in `adata` if True. Otherwise, returns a copy.\n\n    verbose\n        If True, displays SAM log statements.\n\n    Returns\n    -------\n    sam_obj if inplace is True or (sam_obj,AnnData) otherwise\n\n    adata - AnnData\n        `.var['weights']`\n            SAM weights for each gene.\n        `.var['spatial_dispersions']`\n            Spatial dispersions for each gene (these are used to compute the\n            SAM weights)\n        `.uns['sam']`\n            Dictionary of SAM-specific outputs, such as the parameters\n            used for preprocessing ('preprocess_args') and running\n            ('run_args') SAM.\n        `.uns['neighbors']`\n            A dictionary with key 'connectivities' containing the kNN adjacency\n            matrix output by SAM. If built-in scanpy dimensionality reduction\n            methods are to be used using the SAM-output AnnData, users\n            should recompute the neighbors using `.obs['X_pca']` with\n            `scanpy.pp.neighbors`.\n        `.obsm['X_pca']`\n            The principal components output by SAM.\n        `.obsm['X_umap']`\n            The UMAP projection output by SAM.\n        `.layers['X_disp']`\n            The expression matrix used for nearest-neighbor averaging.\n        `.layers['X_knn_avg']`\n            The nearest-neighbor-averaged expression data used for computing the\n            spatial dispersions of genes.\n\n    Example\n    -------\n    >>> import scanpy.external as sce\n    >>> import scanpy as sc\n\n    *** Running SAM ***\n\n    Assuming we are given an AnnData object called `adata`, we can run the SAM\n    algorithm as follows:\n\n    >>> sam_obj = sce.tl.sam(adata,inplace=True)\n\n    The input AnnData object should contain unstandardized, non-negative\n    expression values. Preferably, the data should be log-normalized and no\n    genes should be filtered out.\n\n    Please see the documentation for a description of all available parameters.\n\n    For more detailed tutorials, please visit the original Github repository:\n    https://github.com/atarashansky/self-assembling-manifold/tree/master/tutorial\n\n    *** Plotting ***\n\n    To visualize the output, we can use:\n\n    >>> sce.pl.sam(adata,projection='X_umap')\n\n    `sce.pl.sam` accepts all keyword arguments used in the\n    `matplotlib.pyplot.scatter` function.\n\n    *** SAMGUI ***\n\n    SAM comes with the SAMGUI module, a graphical-user interface written with\n    `Plotly` and `ipythonwidgets` for interactively exploring and annotating\n    the scRNAseq data and running SAM.\n\n    Dependencies can be installed with Anaconda by following the instructions in\n    the self-assembling-manifold Github README:\n    https://github.com/atarashansky/self-assembling-manifold\n\n    In a Jupyter notebook, execute the following to launch the interface:\n\n    >>> from samalg.gui import SAMGUI\n    >>> sam_gui = SAMGUI(sam_obj) # sam_obj is your SAM object\n    >>> sam_gui.SamPlot\n\n    This can also be enabled in Jupyer Lab by following the instructions in the\n    self-assembling-manifold README.\n\n    \"\"\"\n\n    try:\n        from samalg import SAM\n    except ImportError:\n        raise ImportError(\n            '\\nplease install sam-algorithm: \\n\\n'\n            '\\tgit clone git://github.com/atarashansky/self-assembling-manifold.git\\n'\n            '\\tcd self-assembling-manifold\\n'\n            '\\tpip install .'\n        )\n\n    logg.info('Self-assembling manifold')\n\n    s = SAM(counts=adata, inplace=inplace)\n\n    logg.info('Running SAM')\n    s.run(\n        max_iter=max_iter,\n        num_norm_avg=num_norm_avg,\n        k=k,\n        distance=distance,\n        preprocessing=standardization,\n        weight_PCs=weight_pcs,\n        npcs=n_pcs,\n        n_genes=n_genes,\n        projection=projection,\n        sparse_pca=sparse_pca,\n        verbose=verbose,\n    )\n\n    s.adata.uns['sam'] = {}\n    for attr in ['nnm', 'preprocess_args', 'run_args', 'ranked_genes']:\n        s.adata.uns['sam'][attr] = s.adata.uns.pop(attr, None)\n\n    return s if inplace else (s, s.adata)", "idx": 83}
{"project": "Scanpy", "commit_id": "177_scanpy_1.9.0__trimap.py_trimap.py", "target": 0, "func": "def trimap(\n    adata: AnnData,\n    n_components: int = 2,\n    n_inliers: int = 10,\n    n_outliers: int = 5,\n    n_random: int = 5,\n    metric: Literal['angular', 'euclidean', 'hamming', 'manhattan'] = 'euclidean',\n    weight_adj: float = 500.0,\n    lr: float = 1000.0,\n    n_iters: int = 400,\n    verbose: Union[bool, int, None] = None,\n    copy: bool = False,\n) -> Optional[AnnData]:\n    \"\"\"\\\n    TriMap: Large-scale Dimensionality Reduction Using Triplets [Amid19]_.\n\n    TriMap is a dimensionality reduction method that uses triplet constraints\n    to form a low-dimensional embedding of a set of points. The triplet\n    constraints are of the form \"point i is closer to point j than point k\".\n    The triplets are sampled from the high-dimensional representation of the\n    points and a weighting scheme is used to reflect the importance of each\n    triplet.\n\n    TriMap provides a significantly better global view of the data than the\n    other dimensionality reduction methods such t-SNE, LargeVis, and UMAP.\n    The global structure includes relative distances of the clusters, multiple\n    scales in the data, and the existence of possible outliers. We define a\n    global score to quantify the quality of an embedding in reflecting the\n    global structure of the data.\n\n    Parameters\n    ----------\n    adata\n        Annotated data matrix.\n    n_components\n        Number of dimensions of the embedding.\n    n_inliers\n        Number of inlier points for triplet constraints.\n    n_outliers\n        Number of outlier points for triplet constraints.\n    n_random\n        Number of random triplet constraints per point.\n    metric\n        Distance measure: 'angular', 'euclidean', 'hamming', 'manhattan'.\n    weight_adj\n        Adjusting the weights using a non-linear transformation.\n    lr\n        Learning rate.\n    n_iters\n        Number of iterations.\n    verbose\n        If `True`, print the progress report.\n        If `None`, `sc.settings.verbosity` is used.\n    copy\n        Return a copy instead of writing to `adata`.\n\n    Returns\n    -------\n    Depending on `copy`, returns or updates `adata` with the following fields.\n\n    **X_trimap** : :class:`~numpy.ndarray`, (:attr:`~anndata.AnnData.obsm`, shape=(n_samples, n_components), dtype `float`)\n        TriMap coordinates of data.\n\n    Example\n    -------\n\n    >>> import scanpy as sc\n    >>> import scanpy.external as sce\n    >>> pbmc = sc.datasets.pbmc68k_reduced()\n    >>> pbmc = sce.tl.trimap(pbmc, copy=True)\n    >>> sce.pl.trimap(pbmc, color=['bulk_labels'], s=10)\n    \"\"\"\n\n    try:\n        from trimap import TRIMAP\n    except ImportError:\n        raise ImportError('\\nplease install trimap: \\n\\n\\tsudo pip install trimap')\n    adata = adata.copy() if copy else adata\n    start = logg.info('computing TriMap')\n    adata = adata.copy() if copy else adata\n    verbosity = settings.verbosity if verbose is None else verbose\n    verbose = verbosity if isinstance(verbosity, bool) else verbosity > 0\n\n    if 'X_pca' in adata.obsm:\n        n_dim_pca = adata.obsm['X_pca'].shape[1]\n        X = adata.obsm['X_pca'][:, : min(n_dim_pca, 100)]\n    else:\n        X = adata.X\n        if scp.issparse(X):\n            raise ValueError(\n                'trimap currently does not support sparse matrices. Please'\n                'use a dense matrix or apply pca first.'\n            )\n        logg.warning('`X_pca` not found. Run `sc.pp.pca` first for speedup.')\n    X_trimap = TRIMAP(\n        n_dims=n_components,\n        n_inliers=n_inliers,\n        n_outliers=n_outliers,\n        n_random=n_random,\n        lr=lr,\n        distance=metric,\n        weight_adj=weight_adj,\n        n_iters=n_iters,\n        verbose=verbose,\n    ).fit_transform(X)\n    adata.obsm['X_trimap'] = X_trimap\n    logg.info(\n        '    finished',\n        time=start,\n        deep=\"added\\n    'X_trimap', TriMap coordinates (adata.obsm)\",\n    )\n    return adata if copy else None", "idx": 84}
{"project": "Scanpy", "commit_id": "178_scanpy_1.9.0__wishbone.py_wishbone.py", "target": 0, "func": "def wishbone(\n    adata: AnnData,\n    start_cell: str,\n    branch: bool = True,\n    k: int = 15,\n    components: Iterable[int] = (1, 2, 3),\n    num_waypoints: Union[int, Collection] = 250,\n):\n    \"\"\"\\\n    Wishbone identifies bifurcating developmental trajectories from single-cell data\n    [Setty16]_.\n\n    Wishbone is an algorithm for positioning single cells along bifurcating\n    developmental trajectories with high resolution. Wishbone uses multi-dimensional\n    single-cell data, such as mass cytometry or RNA-Seq data, as input and orders cells\n    according to their developmental progression, and it pinpoints bifurcation points\n    by labeling each cell as pre-bifurcation or as one of two post-bifurcation cell\n    fates.\n\n    .. note::\n       More information and bug reports `here\n       <https://github.com/dpeerlab/wishbone>`__.\n\n    Parameters\n    ----------\n    adata\n        Annotated data matrix.\n    start_cell\n        Desired start cell from `obs_names`.\n    branch\n        Use True for Wishbone and False for Wanderlust.\n    k\n        Number of nearest neighbors for graph construction.\n    components\n        Components to use for running Wishbone.\n    num_waypoints\n        Number of waypoints to sample.\n\n    Returns\n    -------\n    Updates `adata` with the following fields:\n\n    `trajectory_wishbone` : (`adata.obs`, dtype `float64`)\n        Computed trajectory positions.\n    `branch_wishbone` : (`adata.obs`, dtype `int64`)\n        Assigned branches.\n\n    Example\n    -------\n\n    >>> import scanpy.external as sce\n    >>> import scanpy as sc\n\n    **Loading Data and Pre-processing**\n\n    >>> adata = sc.datasets.pbmc3k()\n    >>> sc.pp.normalize_per_cell(adata)\n    >>> sc.pp.pca(adata)\n    >>> sc.tl.tsne(adata=adata, n_pcs=5, perplexity=30)\n    >>> sc.pp.neighbors(adata, n_pcs=15, n_neighbors=10)\n    >>> sc.tl.diffmap(adata, n_comps=10)\n\n    **Running Wishbone Core Function**\n\n    Usually, the start cell for a dataset should be chosen based on high expression of\n    the gene of interest:\n\n    >>> sce.tl.wishbone(\n    ...     adata=adata, start_cell='ACAAGAGACTTATC-1',\n    ...     components=[2, 3], num_waypoints=150,\n    ... )\n\n    **Visualizing Wishbone results**\n\n    >>> sc.pl.tsne(adata, color=['trajectory_wishbone', 'branch_wishbone'])\n    >>> markers = ['C1QA', 'PSAP', 'CD79A', 'CD79B', 'CST3', 'LYZ', 'MALAT1']\n    >>> sce.pl.wishbone_marker_trajectory(adata, markers, show=True)\n\n    For further demonstration of Wishbone methods and visualization please follow the\n    notebooks in the package `Wishbone_for_single_cell_RNAseq.ipynb\n    <https://github.com/dpeerlab/wishbone/tree/master/notebooks>`_.\\\n    \"\"\"\n    try:\n        from wishbone.core import wishbone as c_wishbone\n    except ImportError:\n        raise ImportError(\n            \"\\nplease install wishbone:\\n\\n\\thttps://github.com/dpeerlab/wishbone\"\n        )\n\n    # Start cell index\n    s = np.where(adata.obs_names == start_cell)[0]\n    if len(s) == 0:\n        raise RuntimeError(\n            f\"Start cell {start_cell} not found in data. \"\n            \"Please rerun with correct start cell.\"\n        )\n    if isinstance(num_waypoints, cabc.Collection):\n        diff = np.setdiff1d(num_waypoints, adata.obs.index)\n        if diff.size > 0:\n            logging.warning(\n                \"Some of the specified waypoints are not in the data. \"\n                \"These will be removed\"\n            )\n            num_waypoints = diff.tolist()\n    elif num_waypoints > adata.shape[0]:\n        raise RuntimeError(\n            \"num_waypoints parameter is higher than the number of cells in the \"\n            \"dataset. Please select a smaller number\"\n        )\n    s = s[0]\n\n    # Run the algorithm\n    components = list(components)\n    res = c_wishbone(\n        adata.obsm['X_diffmap'][:, components],\n        s=s,\n        k=k,\n        l=k,\n        num_waypoints=num_waypoints,\n        branch=branch,\n    )\n\n    # Assign results\n    trajectory = res[\"Trajectory\"]\n    trajectory = (trajectory - np.min(trajectory)) / (\n        np.max(trajectory) - np.min(trajectory)\n    )\n    adata.obs['trajectory_wishbone'] = np.asarray(trajectory)\n\n    # branch_ = None\n    if branch:\n        branches = res[\"Branches\"].astype(int)\n        adata.obs['branch_wishbone'] = np.asarray(branches)", "idx": 85}
{"project": "Scanpy", "commit_id": "179_scanpy_1.9.0__wishbone.py__anndata_to_wishbone.py", "target": 0, "func": "def _anndata_to_wishbone(adata: AnnData):\n    from wishbone.wb import SCData, Wishbone\n\n    scdata = SCData(adata.to_df())\n    scdata.diffusion_eigenvectors = pd.DataFrame(\n        adata.obsm['X_diffmap'], index=adata.obs_names\n    )\n    wb = Wishbone(scdata)\n    wb.trajectory = adata.obs[\"trajectory_wishbone\"]\n    wb.branch = adata.obs[\"branch_wishbone\"]\n    return wb", "idx": 86}
{"project": "Scanpy", "commit_id": "17_scanpy_1.9.0_param_police.py_scanpy_log_param_types.py", "target": 0, "func": "def scanpy_log_param_types(self, fields, field_role='param', type_role='type'):\n    for _name, _type, _desc in fields:\n        if not _type or not self._obj.__module__.startswith('scanpy'):\n            continue\n        w_list = param_warnings.setdefault((self._name, self._obj), [])\n        if (_name, _type) not in w_list:\n            w_list.append((_name, _type))\n    return _format_docutils_params_orig(self, fields, field_role, type_role)", "idx": 87}
{"project": "Scanpy", "commit_id": "180_scanpy_1.9.0_get.py_rank_genes_groups_df.py", "target": 1, "func": "def rank_genes_groups_df(\n    adata: AnnData,\n    group: Union[str, Iterable[str]],\n    *,\n    key: str = \"rank_genes_groups\",\n    pval_cutoff: Optional[float] = None,\n    log2fc_min: Optional[float] = None,\n    log2fc_max: Optional[float] = None,\n    gene_symbols: Optional[str] = None,\n) -> pd.DataFrame:\n    \"\"\"\\\n    :func:`scanpy.tl.rank_genes_groups` results in the form of a\n    :class:`~pandas.DataFrame`.\n\n    Params\n    ------\n    adata\n        Object to get results from.\n    group\n        Which group (as in :func:`scanpy.tl.rank_genes_groups`'s `groupby`\n        argument) to return results from. Can be a list. All groups are\n        returned if groups is `None`.\n    key\n        Key differential expression groups were stored under.\n    pval_cutoff\n        Return only adjusted p-values below the  cutoff.\n    log2fc_min\n        Minimum logfc to return.\n    log2fc_max\n        Maximum logfc to return.\n    gene_symbols\n        Column name in `.var` DataFrame that stores gene symbols. Specifying\n        this will add that column to the returned dataframe.\n\n    Example\n    -------\n    >>> import scanpy as sc\n    >>> pbmc = sc.datasets.pbmc68k_reduced()\n    >>> sc.tl.rank_genes_groups(pbmc, groupby=\"louvain\", use_raw=True)\n    >>> dedf = sc.get.rank_genes_groups_df(pbmc, group=\"0\")\n    \"\"\"\n    if isinstance(group, str):\n        group = [group]\n    if group is None:\n        group = list(adata.uns[key]['names'].dtype.names)\n    colnames = ['names', 'scores', 'logfoldchanges', 'pvals', 'pvals_adj']\n\n    d = [pd.DataFrame(adata.uns[key][c])[group] for c in colnames]\n    d = pd.concat(d, axis=1, names=[None, 'group'], keys=colnames)\n    d = d.stack(level=1).reset_index()\n    d['group'] = pd.Categorical(d['group'], categories=group)\n    d = d.sort_values(['group', 'level_0']).drop(columns='level_0')\n\n    if pval_cutoff is not None:\n        d = d[d[\"pvals_adj\"] < pval_cutoff]\n    if log2fc_min is not None:\n        d = d[d[\"logfoldchanges\"] > log2fc_min]\n    if log2fc_max is not None:\n        d = d[d[\"logfoldchanges\"] < log2fc_max]\n    if gene_symbols is not None:\n        d = d.join(adata.var[gene_symbols], on=\"names\")\n\n    for pts, name in {'pts': 'pct_nz_group', 'pts_rest': 'pct_nz_reference'}.items():\n        if pts in adata.uns[key]:\n            pts_df = (\n                adata.uns[key][pts][group]\n                .rename_axis(index='names')\n                .reset_index()\n                .melt(id_vars='names', var_name='group', value_name=name)\n            )\n            d = d.merge(pts_df)\n\n    # remove group column for backward compat if len(group) == 1\n    if len(group) == 1:\n        d.drop(columns='group', inplace=True)\n\n    return d.reset_index(drop=True)", "idx": 88}
{"project": "Scanpy", "commit_id": "181_scanpy_1.9.0_get.py__check_indices.py", "target": 0, "func": "def _check_indices(\n    dim_df: pd.DataFrame,\n    alt_index: pd.Index,\n    dim: Literal['obs', 'var'],\n    keys: List[str],\n    alias_index: Optional[pd.Index] = None,\n    use_raw: bool = False,\n) -> Tuple[List[str], List[str], List[str]]:\n    \"\"\"Common logic for checking indices for obs_df and var_df.\"\"\"\n    if use_raw:\n        alt_repr = \"adata.raw\"\n    else:\n        alt_repr = \"adata\"\n\n    alt_dim = (\"obs\", \"var\")[dim == \"obs\"]\n\n    alias_name = None\n    if alias_index is not None:\n        alt_names = pd.Series(alt_index, index=alias_index)\n        alias_name = alias_index.name\n        alt_search_repr = f\"{alt_dim}['{alias_name}']\"\n    else:\n        alt_names = pd.Series(alt_index, index=alt_index)\n        alt_search_repr = f\"{alt_dim}_names\"\n\n    col_keys = []\n    index_keys = []\n    index_aliases = []\n    not_found = []\n\n    # check that adata.obs does not contain duplicated columns\n    # if duplicated columns names are present, they will\n    # be further duplicated when selecting them.\n    if not dim_df.columns.is_unique:\n        dup_cols = dim_df.columns[dim_df.columns.duplicated()].tolist()\n        raise ValueError(\n            f\"adata.{dim} contains duplicated columns. Please rename or remove \"\n            \"these columns first.\\n`\"\n            f\"Duplicated columns {dup_cols}\"\n        )\n\n    if not alt_index.is_unique:\n        raise ValueError(\n            f\"{alt_repr}.{alt_dim}_names contains duplicated items\\n\"\n            f\"Please rename these {alt_dim} names first for example using \"\n            f\"`adata.{alt_dim}_names_make_unique()`\"\n        )\n\n    # use only unique keys, otherwise duplicated keys will\n    # further duplicate when reordering the keys later in the function\n    for key in np.unique(keys):\n        if key in dim_df.columns:\n            col_keys.append(key)\n            if key in alt_names.index:\n                raise KeyError(\n                    f\"The key '{key}' is found in both adata.{dim} and {alt_repr}.{alt_search_repr}.\"\n                )\n        elif key in alt_names.index:\n            val = alt_names[key]\n            if isinstance(val, pd.Series):\n                # while var_names must be unique, adata.var[gene_symbols] does not\n                # It's still ambiguous to refer to a duplicated entry though.\n                assert alias_index is not None\n                raise KeyError(\n                    f\"Found duplicate entries for '{key}' in {alt_repr}.{alt_search_repr}.\"\n                )\n            index_keys.append(val)\n            index_aliases.append(key)\n        else:\n            not_found.append(key)\n    if len(not_found) > 0:\n        raise KeyError(\n            f\"Could not find keys '{not_found}' in columns of `adata.{dim}` or in\"\n            f\" {alt_repr}.{alt_search_repr}.\"\n        )\n\n    return col_keys, index_keys, index_aliases", "idx": 89}
{"project": "Scanpy", "commit_id": "182_scanpy_1.9.0_get.py__get_array_values.py", "target": 0, "func": "def _get_array_values(\n    X,\n    dim_names: pd.Index,\n    keys: List[str],\n    axis: Literal[0, 1],\n    backed: bool,\n):\n    # TODO: This should be made easier on the anndata side\n    mutable_idxer = [slice(None), slice(None)]\n    idx = dim_names.get_indexer(keys)\n\n    # for backed AnnData is important that the indices are ordered\n    if backed:\n        idx_order = np.argsort(idx)\n        rev_idxer = mutable_idxer.copy()\n        mutable_idxer[axis] = idx[idx_order]\n        rev_idxer[axis] = np.argsort(idx_order)\n        matrix = X[tuple(mutable_idxer)][tuple(rev_idxer)]\n    else:\n        mutable_idxer[axis] = idx\n        matrix = X[tuple(mutable_idxer)]\n\n    from scipy.sparse import issparse\n\n    if issparse(matrix):\n        matrix = matrix.toarray()\n\n    return matrix", "idx": 90}
{"project": "Scanpy", "commit_id": "183_scanpy_1.9.0_get.py_obs_df.py", "target": 0, "func": "def obs_df(\n    adata: AnnData,\n    keys: Iterable[str] = (),\n    obsm_keys: Iterable[Tuple[str, int]] = (),\n    *,\n    layer: str = None,\n    gene_symbols: str = None,\n    use_raw: bool = False,\n) -> pd.DataFrame:\n    \"\"\"\\\n    Return values for observations in adata.\n\n    Params\n    ------\n    adata\n        AnnData object to get values from.\n    keys\n        Keys from either `.var_names`, `.var[gene_symbols]`, or `.obs.columns`.\n    obsm_keys\n        Tuple of `(key from obsm, column index of obsm[key])`.\n    layer\n        Layer of `adata` to use as expression values.\n    gene_symbols\n        Column of `adata.var` to search for `keys` in.\n    use_raw\n        Whether to get expression values from `adata.raw`.\n\n    Returns\n    -------\n    A dataframe with `adata.obs_names` as index, and values specified by `keys`\n    and `obsm_keys`.\n\n    Examples\n    --------\n    Getting value for plotting:\n\n    >>> pbmc = sc.datasets.pbmc68k_reduced()\n    >>> plotdf = sc.get.obs_df(\n            pbmc,\n            keys=[\"CD8B\", \"n_genes\"],\n            obsm_keys=[(\"X_umap\", 0), (\"X_umap\", 1)]\n        )\n    >>> plotdf.plot.scatter(\"X_umap0\", \"X_umap1\", c=\"CD8B\")\n\n    Calculating mean expression for marker genes by cluster:\n\n    >>> pbmc = sc.datasets.pbmc68k_reduced()\n    >>> marker_genes = ['CD79A', 'MS4A1', 'CD8A', 'CD8B', 'LYZ']\n    >>> genedf = sc.get.obs_df(\n            pbmc,\n            keys=[\"louvain\", *marker_genes]\n        )\n    >>> grouped = genedf.groupby(\"louvain\")\n    >>> mean, var = grouped.mean(), grouped.var()\n    \"\"\"\n    if use_raw:\n        assert (\n            layer is None\n        ), \"Cannot specify use_raw=True and a layer at the same time.\"\n        var = adata.raw.var\n    else:\n        var = adata.var\n    if gene_symbols is not None:\n        alias_index = pd.Index(var[gene_symbols])\n    else:\n        alias_index = None\n\n    obs_cols, var_idx_keys, var_symbols = _check_indices(\n        adata.obs,\n        var.index,\n        \"obs\",\n        keys,\n        alias_index=alias_index,\n        use_raw=use_raw,\n    )\n\n    # Make df\n    df = pd.DataFrame(index=adata.obs_names)\n\n    # add var values\n    if len(var_idx_keys) > 0:\n        matrix = _get_array_values(\n            _get_obs_rep(adata, layer=layer, use_raw=use_raw),\n            var.index,\n            var_idx_keys,\n            axis=1,\n            backed=adata.isbacked,\n        )\n        df = pd.concat(\n            [df, pd.DataFrame(matrix, columns=var_symbols, index=adata.obs_names)],\n            axis=1,\n        )\n\n    # add obs values\n    if len(obs_cols) > 0:\n        df = pd.concat([df, adata.obs[obs_cols]], axis=1)\n\n    # reorder columns to given order (including duplicates keys if present)\n    if keys:\n        df = df[keys]\n\n    for k, idx in obsm_keys:\n        added_k = f\"{k}-{idx}\"\n        val = adata.obsm[k]\n        if isinstance(val, np.ndarray):\n            df[added_k] = np.ravel(val[:, idx])\n        elif isinstance(val, spmatrix):\n            df[added_k] = np.ravel(val[:, idx].toarray())\n        elif isinstance(val, pd.DataFrame):\n            df[added_k] = val.loc[:, idx]\n\n    return df", "idx": 91}
{"project": "Scanpy", "commit_id": "184_scanpy_1.9.0_get.py_var_df.py", "target": 0, "func": "def var_df(\n    adata: AnnData,\n    keys: Iterable[str] = (),\n    varm_keys: Iterable[Tuple[str, int]] = (),\n    *,\n    layer: str = None,\n) -> pd.DataFrame:\n    \"\"\"\\\n    Return values for observations in adata.\n\n    Params\n    ------\n    adata\n        AnnData object to get values from.\n    keys\n        Keys from either `.obs_names`, or `.var.columns`.\n    varm_keys\n        Tuple of `(key from varm, column index of varm[key])`.\n    layer\n        Layer of `adata` to use as expression values.\n\n    Returns\n    -------\n    A dataframe with `adata.var_names` as index, and values specified by `keys`\n    and `varm_keys`.\n    \"\"\"\n    # Argument handling\n    var_cols, obs_idx_keys, _ = _check_indices(adata.var, adata.obs_names, \"var\", keys)\n\n    # initialize df\n    df = pd.DataFrame(index=adata.var.index)\n\n    if len(obs_idx_keys) > 0:\n        matrix = _get_array_values(\n            _get_obs_rep(adata, layer=layer),\n            adata.obs_names,\n            obs_idx_keys,\n            axis=0,\n            backed=adata.isbacked,\n        ).T\n        df = pd.concat(\n            [df, pd.DataFrame(matrix, columns=obs_idx_keys, index=adata.var_names)],\n            axis=1,\n        )\n\n    # add obs values\n    if len(var_cols) > 0:\n        df = pd.concat([df, adata.var[var_cols]], axis=1)\n\n    # reorder columns to given order\n    if keys:\n        df = df[keys]\n\n    for k, idx in varm_keys:\n        added_k = f\"{k}-{idx}\"\n        val = adata.varm[k]\n        if isinstance(val, np.ndarray):\n            df[added_k] = np.ravel(val[:, idx])\n        elif isinstance(val, spmatrix):\n            df[added_k] = np.ravel(val[:, idx].toarray())\n        elif isinstance(val, pd.DataFrame):\n            df[added_k] = val.loc[:, idx]\n    return df", "idx": 92}
{"project": "Scanpy", "commit_id": "185_scanpy_1.9.0_get.py__get_obs_rep.py", "target": 0, "func": "def _get_obs_rep(adata, *, use_raw=False, layer=None, obsm=None, obsp=None):\n    \"\"\"\n    Choose array aligned with obs annotation.\n    \"\"\"\n    # https://github.com/theislab/scanpy/issues/1546\n    if not isinstance(use_raw, bool):\n        raise TypeError(f\"use_raw expected to be bool, was {type(use_raw)}.\")\n\n    is_layer = layer is not None\n    is_raw = use_raw is not False\n    is_obsm = obsm is not None\n    is_obsp = obsp is not None\n    choices_made = sum((is_layer, is_raw, is_obsm, is_obsp))\n    assert choices_made <= 1\n    if choices_made == 0:\n        return adata.X\n    elif is_layer:\n        return adata.layers[layer]\n    elif use_raw:\n        return adata.raw.X\n    elif is_obsm:\n        return adata.obsm[obsm]\n    elif is_obsp:\n        return adata.obsp[obsp]\n    else:\n        assert False, (\n            \"That was unexpected. Please report this bug at:\\n\\n\\t\"", "idx": 93}
{"project": "Scanpy", "commit_id": "186_scanpy_1.9.0_get.py__set_obs_rep.py", "target": 0, "func": "def _set_obs_rep(adata, val, *, use_raw=False, layer=None, obsm=None, obsp=None):\n    \"\"\"\n    Set value for observation rep.\n    \"\"\"\n    is_layer = layer is not None\n    is_raw = use_raw is not False\n    is_obsm = obsm is not None\n    is_obsp = obsp is not None\n    choices_made = sum((is_layer, is_raw, is_obsm, is_obsp))\n    assert choices_made <= 1\n    if choices_made == 0:\n        adata.X = val\n    elif is_layer:\n        adata.layers[layer] = val\n    elif use_raw:\n        adata.raw.X = val\n    elif is_obsm:\n        adata.obsm[obsm] = val\n    elif is_obsp:\n        adata.obsp[obsp] = val\n    else:\n        assert False, (\n            \"That was unexpected. Please report this bug at:\\n\\n\\t\"", "idx": 94}
{"project": "Scanpy", "commit_id": "187_scanpy_1.9.0__gearys_c.py_gearys_c.py", "target": 0, "func": "def gearys_c(\n    adata: AnnData,\n    *,\n    vals: Optional[Union[np.ndarray, sparse.spmatrix]] = None,\n    use_graph: Optional[str] = None,\n    layer: Optional[str] = None,\n    obsm: Optional[str] = None,\n    obsp: Optional[str] = None,\n    use_raw: bool = False,\n) -> Union[np.ndarray, float]:\n    r\"\"\"\n    Calculate `Geary's C <https://en.wikipedia.org/wiki/Geary's_C>`_, as used\n    by `VISION <https://doi.org/10.1038/s41467-019-12235-0>`_.\n\n    Geary's C is a measure of autocorrelation for some measure on a graph. This\n    can be to whether measures are correlated between neighboring cells. Lower\n    values indicate greater correlation.\n\n    .. math::\n\n        C =\n        \\frac{\n            (N - 1)\\sum_{i,j} w_{i,j} (x_i - x_j)^2\n        }{\n            2W \\sum_i (x_i - \\bar{x})^2\n        }\n\n    Params\n    ------\n    adata\n    vals\n        Values to calculate Geary's C for. If this is two dimensional, should\n        be of shape `(n_features, n_cells)`. Otherwise should be of shape\n        `(n_cells,)`. This matrix can be selected from elements of the anndata\n        object by using key word arguments: `layer`, `obsm`, `obsp`, or\n        `use_raw`.\n    use_graph\n        Key to use for graph in anndata object. If not provided, default\n        neighbors connectivities will be used instead.\n    layer\n        Key for `adata.layers` to choose `vals`.\n    obsm\n        Key for `adata.obsm` to choose `vals`.\n    obsp\n        Key for `adata.obsp` to choose `vals`.\n    use_raw\n        Whether to use `adata.raw.X` for `vals`.\n\n\n    This function can also be called on the graph and values directly. In this case\n    the signature looks like:\n\n    Params\n    ------\n    g\n        The graph\n    vals\n        The values\n\n\n    See the examples for more info.\n\n    Returns\n    -------\n    If vals is two dimensional, returns a 1 dimensional ndarray array. Returns\n    a scalar if `vals` is 1d.\n\n\n    Examples\n    --------\n\n    Calculate Gearys C for each components of a dimensionality reduction:\n\n    .. code:: python\n\n        import scanpy as sc, numpy as np\n\n        pbmc = sc.datasets.pbmc68k_processed()\n        pc_c = sc.metrics.gearys_c(pbmc, obsm=\"X_pca\")\n\n\n    It's equivalent to call the function directly on the underlying arrays:\n\n    .. code:: python\n\n        alt = sc.metrics.gearys_c(pbmc.obsp[\"connectivities\"], pbmc.obsm[\"X_pca\"].T)\n        np.testing.assert_array_equal(pc_c, alt)\n    \"\"\"\n    if use_graph is None:\n        # Fix for anndata<0.7\n        if hasattr(adata, \"obsp\") and \"connectivities\" in adata.obsp:\n            g = adata.obsp[\"connectivities\"]\n        elif \"neighbors\" in adata.uns:\n            g = adata.uns[\"neighbors\"][\"connectivities\"]\n        else:\n            raise ValueError(\"Must run neighbors first.\")\n    else:\n        raise NotImplementedError()\n    if vals is None:\n        vals = _get_obs_rep(adata, use_raw=use_raw, layer=layer, obsm=obsm, obsp=obsp).T\n    return gearys_c(g, vals)", "idx": 95}
{"project": "Scanpy", "commit_id": "188_scanpy_1.9.0__gearys_c.py__gearys_c_vec.py", "target": 0, "func": "def _gearys_c_vec(data, indices, indptr, x):\n    W = data.sum()\n    return _gearys_c_vec_W(data, indices, indptr, x, W)", "idx": 96}
{"project": "Scanpy", "commit_id": "189_scanpy_1.9.0__gearys_c.py__gearys_c_vec_W.py", "target": 0, "func": "def _gearys_c_vec_W(data, indices, indptr, x, W):\n    N = len(indptr) - 1\n    x = x.astype(np.float_)\n    x_bar = x.mean()\n\n    total = 0.0\n    for i in numba.prange(N):\n        s = slice(indptr[i], indptr[i + 1])\n        i_indices = indices[s]\n        i_data = data[s]\n        total += np.sum(i_data * ((x[i] - x[i_indices]) ** 2))\n\n    numer = (N - 1) * total\n    denom = 2 * W * ((x - x_bar) ** 2).sum()\n    C = numer / denom\n\n    return C", "idx": 97}
{"project": "Scanpy", "commit_id": "18_scanpy_1.9.0_param_police.py_show_param_warnings.py", "target": 0, "func": "def show_param_warnings(app, exception):\n    import inspect\n\n    for (fname, fun), params in param_warnings.items():\n        _, line = inspect.getsourcelines(fun)\n        file_name = inspect.getsourcefile(fun)\n        params_str = '\\n'.join(f'\\t{n}: {t}' for n, t in params)\n        warnings.warn_explicit(\n            f'\\nParameters in `{fname}` have types in docstring.\\n'\n            f'Replace them with type annotations.\\n{params_str}',\n            UserWarning,\n            file_name,\n            line,\n        )\n    if param_warnings:\n        raise RuntimeError('Encountered text parameter type. Use annotations.')", "idx": 98}
{"project": "Scanpy", "commit_id": "190_scanpy_1.9.0__gearys_c.py__gearys_c_inner_sparse_x_densevec.py", "target": 0, "func": "def _gearys_c_inner_sparse_x_densevec(g_data, g_indices, g_indptr, x, W):\n    x_bar = x.mean()\n    total = 0.0\n    N = len(x)\n    for i in numba.prange(N):\n        s = slice(g_indptr[i], g_indptr[i + 1])\n        i_indices = g_indices[s]\n        i_data = g_data[s]\n        total += np.sum(i_data * ((x[i] - x[i_indices]) ** 2))\n    numer = (N - 1) * total\n    denom = 2 * W * ((x - x_bar) ** 2).sum()\n    C = numer / denom\n    return C", "idx": 99}
{"project": "Scanpy", "commit_id": "191_scanpy_1.9.0__gearys_c.py__gearys_c_inner_sparse_x_sparsevec.py", "target": 0, "func": "def _gearys_c_inner_sparse_x_sparsevec(\n    g_data, g_indices, g_indptr, x_data, x_indices, N, W\n):\n    x = np.zeros(N, dtype=np.float_)\n    x[x_indices] = x_data\n    x_bar = np.sum(x_data) / N\n    total = 0.0\n    N = len(x)\n    for i in numba.prange(N):\n        s = slice(g_indptr[i], g_indptr[i + 1])\n        i_indices = g_indices[s]\n        i_data = g_data[s]\n        total += np.sum(i_data * ((x[i] - x[i_indices]) ** 2))\n    numer = (N - 1) * total\n    # Expanded from 2 * W * ((x_k - x_k_bar) ** 2).sum(), but uses sparsity\n    # to skip some calculations\n    # fmt: off\n    denom = (\n        2 * W\n        * (\n            np.sum(x_data ** 2)\n            - np.sum(x_data * x_bar * 2)\n            + (x_bar ** 2) * N\n        )\n    )\n    # fmt: on\n    C = numer / denom\n    return C", "idx": 100}
{"project": "Scanpy", "commit_id": "192_scanpy_1.9.0__gearys_c.py__gearys_c_mtx.py", "target": 0, "func": "def _gearys_c_mtx(g_data, g_indices, g_indptr, X):\n    M, N = X.shape\n    assert N == len(g_indptr) - 1\n    W = g_data.sum()\n    out = np.zeros(M, dtype=np.float_)\n    for k in numba.prange(M):\n        x = X[k, :].astype(np.float_)\n        out[k] = _gearys_c_inner_sparse_x_densevec(g_data, g_indices, g_indptr, x, W)\n    return out", "idx": 101}
{"project": "Scanpy", "commit_id": "193_scanpy_1.9.0__gearys_c.py__gearys_c_mtx_csr.py", "target": 0, "func": "def _gearys_c_mtx_csr(\n    g_data, g_indices, g_indptr, x_data, x_indices, x_indptr, x_shape\n):\n    M, N = x_shape\n    W = g_data.sum()\n    out = np.zeros(M, dtype=np.float_)\n    x_data_list = np.split(x_data, x_indptr[1:-1])\n    x_indices_list = np.split(x_indices, x_indptr[1:-1])\n    for k in numba.prange(M):\n        out[k] = _gearys_c_inner_sparse_x_sparsevec(\n            g_data,\n            g_indices,\n            g_indptr,\n            x_data_list[k],\n            x_indices_list[k],\n            N,\n            W,\n        )\n    return out", "idx": 102}
{"project": "Scanpy", "commit_id": "194_scanpy_1.9.0__gearys_c.py__resolve_vals.py", "target": 0, "func": "def _resolve_vals(val):\n    return np.asarray(val)", "idx": 103}
{"project": "Scanpy", "commit_id": "195_scanpy_1.9.0__gearys_c.py__.py", "target": 0, "func": "def _(val):\n    return val.to_numpy()", "idx": 104}
{"project": "Scanpy", "commit_id": "196_scanpy_1.9.0__gearys_c.py__check_vals.py", "target": 0, "func": "def _check_vals(vals):\n    \"\"\"\\\n    Checks that values wont cause issues in computation.\n\n    Returns new set of vals, and indexer to put values back into result.\n\n    For details on why this is neccesary, see:\n    https://github.com/theislab/scanpy/issues/1806\n    \"\"\"\n    from scanpy._utils import is_constant\n\n    full_result = np.empty(vals.shape[0], dtype=np.float64)\n    full_result.fill(np.nan)\n    idxer = ~is_constant(vals, axis=1)\n    if idxer.all():\n        idxer = slice(None)\n    else:\n        warnings.warn(\n            UserWarning(\n                f\"{len(idxer) - idxer.sum()} variables were constant, will return nan for these.\",\n            )\n        )\n    return vals[idxer], idxer, full_result", "idx": 105}
{"project": "Scanpy", "commit_id": "197_scanpy_1.9.0__gearys_c.py__gearys_c.py", "target": 0, "func": "def _gearys_c(g, vals) -> np.ndarray:\n    assert g.shape[0] == g.shape[1], \"`g` should be a square adjacency matrix\"\n    vals = _resolve_vals(vals)\n    g_data = g.data.astype(np.float_, copy=False)\n    if isinstance(vals, sparse.csr_matrix):\n        assert g.shape[0] == vals.shape[1]\n        new_vals, idxer, full_result = _check_vals(vals)\n        result = _gearys_c_mtx_csr(\n            g_data,\n            g.indices,\n            g.indptr,\n            new_vals.data.astype(np.float_, copy=False),\n            new_vals.indices,\n            new_vals.indptr,\n            new_vals.shape,\n        )\n        full_result[idxer] = result\n        return full_result\n    elif isinstance(vals, np.ndarray) and vals.ndim == 1:\n        assert g.shape[0] == vals.shape[0]\n        return _gearys_c_vec(g_data, g.indices, g.indptr, vals)\n    elif isinstance(vals, np.ndarray) and vals.ndim == 2:\n        assert g.shape[0] == vals.shape[1]\n        new_vals, idxer, full_result = _check_vals(vals)\n        result = _gearys_c_mtx(g_data, g.indices, g.indptr, new_vals)\n        full_result[idxer] = result\n        return full_result\n    else:\n        raise NotImplementedError()", "idx": 106}
{"project": "Scanpy", "commit_id": "198_scanpy_1.9.0__metrics.py_confusion_matrix.py", "target": 0, "func": "def confusion_matrix(\n    orig: Union[pd.Series, np.ndarray, Sequence],\n    new: Union[pd.Series, np.ndarray, Sequence],\n    data: Optional[pd.DataFrame] = None,\n    *,\n    normalize: bool = True,\n) -> pd.DataFrame:\n    \"\"\"\\\n    Given an original and new set of labels, create a labelled confusion matrix.\n\n    Parameters `orig` and `new` can either be entries in data or categorical arrays\n    of the same size.\n\n    Params\n    ------\n    orig\n        Original labels.\n    new\n        New labels.\n    data\n        Optional dataframe to fill entries from.\n    normalize\n        Should the confusion matrix be normalized?\n\n\n    Examples\n    --------\n\n    .. plot::\n\n        import scanpy as sc; import seaborn as sns\n        pbmc = sc.datasets.pbmc68k_reduced()\n        cmtx = sc.metrics.confusion_matrix(\"bulk_labels\", \"louvain\", pbmc.obs)\n        sns.heatmap(cmtx)\n\n    \"\"\"\n    from sklearn.metrics import confusion_matrix as _confusion_matrix\n\n    if data is not None:\n        if isinstance(orig, str):\n            orig = data[orig]\n        if isinstance(new, str):\n            new = data[new]\n\n    # Coercing so I don't have to deal with it later\n    orig, new = pd.Series(orig), pd.Series(new)\n    assert len(orig) == len(new)\n\n    unique_labels = pd.unique(np.concatenate((orig.values, new.values)))\n\n    # Compute\n    mtx = _confusion_matrix(orig, new, labels=unique_labels)\n    if normalize:\n        sums = mtx.sum(axis=1)[:, np.newaxis]\n        mtx = np.divide(mtx, sums, where=sums != 0)\n\n    # Label\n    orig_name = \"Original labels\" if orig.name is None else orig.name\n    new_name = \"New Labels\" if new.name is None else new.name\n    df = pd.DataFrame(\n        mtx,\n        index=pd.Index(unique_labels, name=orig_name),\n        columns=pd.Index(unique_labels, name=new_name),\n    )\n\n    # Filter\n    if is_categorical_dtype(orig):\n        orig_idx = pd.Series(orig).cat.categories\n    else:\n        orig_idx = natsorted(pd.unique(orig))\n    if is_categorical_dtype(new):\n        new_idx = pd.Series(new).cat.categories\n    else:\n        new_idx = natsorted(pd.unique(new))\n    df = df.loc[np.array(orig_idx), np.array(new_idx)]\n\n    return df", "idx": 107}
{"project": "Scanpy", "commit_id": "199_scanpy_1.9.0__morans_i.py_morans_i.py", "target": 0, "func": "def morans_i(\n    adata: AnnData,\n    *,\n    vals: Optional[Union[np.ndarray, sparse.spmatrix]] = None,\n    use_graph: Optional[str] = None,\n    layer: Optional[str] = None,\n    obsm: Optional[str] = None,\n    obsp: Optional[str] = None,\n    use_raw: bool = False,\n) -> Union[np.ndarray, float]:\n    r\"\"\"\n    Calculate Moran\u2019s I Global Autocorrelation Statistic.\n\n    Moran\u2019s I is a global autocorrelation statistic for some measure on a graph. It is commonly used in\n    spatial data analysis to assess autocorrelation on a 2D grid. It is closely related to Geary's C,\n    but not identical. More info can be found `here <https://en.wikipedia.org/wiki/Moran%27s_I>`_.\n\n    .. math::\n\n        I =\n            \\frac{\n                N \\sum_{i, j} w_{i, j} z_{i} z_{j}\n            }{\n                S_{0} \\sum_{i} z_{i}^{2}\n            }\n\n    Params\n    ------\n    adata\n    vals\n        Values to calculate Moran's I for. If this is two dimensional, should\n        be of shape `(n_features, n_cells)`. Otherwise should be of shape\n        `(n_cells,)`. This matrix can be selected from elements of the anndata\n        object by using key word arguments: `layer`, `obsm`, `obsp`, or\n        `use_raw`.\n    use_graph\n        Key to use for graph in anndata object. If not provided, default\n        neighbors connectivities will be used instead.\n    layer\n        Key for `adata.layers` to choose `vals`.\n    obsm\n        Key for `adata.obsm` to choose `vals`.\n    obsp\n        Key for `adata.obsp` to choose `vals`.\n    use_raw\n        Whether to use `adata.raw.X` for `vals`.\n\n\n    This function can also be called on the graph and values directly. In this case\n    the signature looks like:\n\n    Params\n    ------\n    g\n        The graph\n    vals\n        The values\n\n\n    See the examples for more info.\n\n    Returns\n    -------\n    If vals is two dimensional, returns a 1 dimensional ndarray array. Returns\n    a scalar if `vals` is 1d.\n\n\n    Examples\n    --------\n\n    Calculate Morans I for each components of a dimensionality reduction:\n\n    .. code:: python\n\n        import scanpy as sc, numpy as np\n\n        pbmc = sc.datasets.pbmc68k_processed()\n        pc_c = sc.metrics.morans_i(pbmc, obsm=\"X_pca\")\n\n\n    It's equivalent to call the function directly on the underlying arrays:\n\n    .. code:: python\n\n        alt = sc.metrics.morans_i(pbmc.obsp[\"connectivities\"], pbmc.obsm[\"X_pca\"].T)\n        np.testing.assert_array_equal(pc_c, alt)\n    \"\"\"\n    if use_graph is None:\n        # Fix for anndata<0.7\n        if hasattr(adata, \"obsp\") and \"connectivities\" in adata.obsp:\n            g = adata.obsp[\"connectivities\"]\n        elif \"neighbors\" in adata.uns:\n            g = adata.uns[\"neighbors\"][\"connectivities\"]\n        else:\n            raise ValueError(\"Must run neighbors first.\")\n    else:\n        raise NotImplementedError()\n    if vals is None:\n        vals = _get_obs_rep(adata, use_raw=use_raw, layer=layer, obsm=obsm, obsp=obsp).T\n    return morans_i(g, vals)", "idx": 108}
{"project": "Scanpy", "commit_id": "19_scanpy_1.9.0_param_police.py_setup.py", "target": 0, "func": "def setup(app: Sphinx):\n    NumpyDocstring._format_docutils_params = scanpy_log_param_types\n    app.connect('build-finished', show_param_warnings)", "idx": 109}
{"project": "Scanpy", "commit_id": "1_scanpy_1.9.0_conftest.py_pytest_addoption.py", "target": 0, "func": "def pytest_addoption(parser):\n    parser.addoption(\n        \"--internet-tests\",\n        action=\"store_true\",\n        default=False,\n        help=\"Run tests that retrieve stuff from the internet. This increases test time.\",", "idx": 110}
{"project": "Scanpy", "commit_id": "200_scanpy_1.9.0__morans_i.py__morans_i_vec_W_sparse.py", "target": 0, "func": "def _morans_i_vec_W_sparse(\n    g_data: np.ndarray,\n    g_indices: np.ndarray,\n    g_indptr: np.ndarray,\n    x_data: np.ndarray,\n    x_indices: np.ndarray,\n    N: int,\n    W: np.float_,\n) -> float:\n    x = np.zeros(N, dtype=x_data.dtype)\n    x[x_indices] = x_data\n    return _morans_i_vec_W(g_data, g_indices, g_indptr, x, W)", "idx": 111}
{"project": "Scanpy", "commit_id": "201_scanpy_1.9.0__morans_i.py__morans_i_vec_W.py", "target": 0, "func": "def _morans_i_vec_W(\n    g_data: np.ndarray,\n    g_indices: np.ndarray,\n    g_indptr: np.ndarray,\n    x: np.ndarray,\n    W: np.float_,\n) -> float:\n    z = x - x.mean()\n    z2ss = (z * z).sum()\n    N = len(x)\n    inum = 0.0\n\n    for i in prange(N):\n        s = slice(g_indptr[i], g_indptr[i + 1])\n        i_indices = g_indices[s]\n        i_data = g_data[s]\n        inum += (i_data * z[i_indices]).sum() * z[i]\n\n    return len(x) / W * inum / z2ss", "idx": 112}
{"project": "Scanpy", "commit_id": "202_scanpy_1.9.0__morans_i.py__morans_i_vec.py", "target": 0, "func": "def _morans_i_vec(\n    g_data: np.ndarray,\n    g_indices: np.ndarray,\n    g_indptr: np.ndarray,\n    x: np.ndarray,\n) -> float:\n    W = g_data.sum()\n    return _morans_i_vec_W(g_data, g_indices, g_indptr, x, W)", "idx": 113}
{"project": "Scanpy", "commit_id": "203_scanpy_1.9.0__morans_i.py__morans_i_mtx.py", "target": 0, "func": "def _morans_i_mtx(\n    g_data: np.ndarray,\n    g_indices: np.ndarray,\n    g_indptr: np.ndarray,\n    X: np.ndarray,\n) -> np.ndarray:\n    M, N = X.shape\n    assert N == len(g_indptr) - 1\n    W = g_data.sum()\n    out = np.zeros(M, dtype=np.float_)\n    for k in prange(M):\n        x = X[k, :]\n        out[k] = _morans_i_vec_W(g_data, g_indices, g_indptr, x, W)\n    return out", "idx": 114}
{"project": "Scanpy", "commit_id": "204_scanpy_1.9.0__morans_i.py__morans_i_mtx_csr.py", "target": 0, "func": "def _morans_i_mtx_csr(\n    g_data: np.ndarray,\n    g_indices: np.ndarray,\n    g_indptr: np.ndarray,\n    X_data: np.ndarray,\n    X_indices: np.ndarray,\n    X_indptr: np.ndarray,\n    X_shape: tuple,\n) -> np.ndarray:\n    M, N = X_shape\n    W = g_data.sum()\n    out = np.zeros(M, dtype=np.float_)\n    x_data_list = np.split(X_data, X_indptr[1:-1])\n    x_indices_list = np.split(X_indices, X_indptr[1:-1])\n    for k in prange(M):\n        out[k] = _morans_i_vec_W_sparse(\n            g_data,\n            g_indices,\n            g_indptr,\n            x_data_list[k],\n            x_indices_list[k],\n            N,\n            W,\n        )\n    return out", "idx": 115}
{"project": "Scanpy", "commit_id": "205_scanpy_1.9.0__morans_i.py__morans_i.py", "target": 0, "func": "def _morans_i(g, vals) -> np.ndarray:\n    assert g.shape[0] == g.shape[1], \"`g` should be a square adjacency matrix\"\n    vals = _resolve_vals(vals)\n    g_data = g.data.astype(np.float_, copy=False)\n    if isinstance(vals, sparse.csr_matrix):\n        assert g.shape[0] == vals.shape[1]\n        new_vals, idxer, full_result = _check_vals(vals)\n        result = _morans_i_mtx_csr(\n            g_data,\n            g.indices,\n            g.indptr,\n            new_vals.data.astype(np.float_, copy=False),\n            new_vals.indices,\n            new_vals.indptr,\n            new_vals.shape,\n        )\n        full_result[idxer] = result\n        return full_result\n    elif isinstance(vals, np.ndarray) and vals.ndim == 1:\n        assert g.shape[0] == vals.shape[0]\n        return _morans_i_vec(g_data, g.indices, g.indptr, vals)\n    elif isinstance(vals, np.ndarray) and vals.ndim == 2:\n        assert g.shape[0] == vals.shape[1]\n        new_vals, idxer, full_result = _check_vals(vals)\n        result = _morans_i_mtx(\n            g_data,\n            g.indices,\n            g.indptr,\n            new_vals.astype(np.float_, copy=False),\n        )\n        full_result[idxer] = result\n        return full_result\n    else:\n        raise NotImplementedError()", "idx": 116}
{"project": "Scanpy", "commit_id": "206_scanpy_1.9.0___init__.py_neighbors.py", "target": 0, "func": "def neighbors(\n    adata: AnnData,\n    n_neighbors: int = 15,\n    n_pcs: Optional[int] = None,\n    use_rep: Optional[str] = None,\n    knn: bool = True,\n    random_state: AnyRandom = 0,\n    method: Optional[_Method] = 'umap',\n    metric: Union[_Metric, _MetricFn] = 'euclidean',\n    metric_kwds: Mapping[str, Any] = MappingProxyType({}),\n    key_added: Optional[str] = None,\n    copy: bool = False,\n) -> Optional[AnnData]:\n    \"\"\"\\\n    Compute a neighborhood graph of observations [McInnes18]_.\n\n    The neighbor search efficiency of this heavily relies on UMAP [McInnes18]_,\n    which also provides a method for estimating connectivities of data points -\n    the connectivity of the manifold (`method=='umap'`). If `method=='gauss'`,\n    connectivities are computed according to [Coifman05]_, in the adaption of\n    [Haghverdi16]_.\n\n    Parameters\n    ----------\n    adata\n        Annotated data matrix.\n    n_neighbors\n        The size of local neighborhood (in terms of number of neighboring data\n        points) used for manifold approximation. Larger values result in more\n        global views of the manifold, while smaller values result in more local\n        data being preserved. In general values should be in the range 2 to 100.\n        If `knn` is `True`, number of nearest neighbors to be searched. If `knn`\n        is `False`, a Gaussian kernel width is set to the distance of the\n        `n_neighbors` neighbor.\n    {n_pcs}\n    {use_rep}\n    knn\n        If `True`, use a hard threshold to restrict the number of neighbors to\n        `n_neighbors`, that is, consider a knn graph. Otherwise, use a Gaussian\n        Kernel to assign low weights to neighbors more distant than the\n        `n_neighbors` nearest neighbor.\n    random_state\n        A numpy random seed.\n    method\n        Use 'umap' [McInnes18]_ or 'gauss' (Gauss kernel following [Coifman05]_\n        with adaptive width [Haghverdi16]_) for computing connectivities.\n        Use 'rapids' for the RAPIDS implementation of UMAP (experimental, GPU\n        only).\n    metric\n        A known metric\u2019s name or a callable that returns a distance.\n    metric_kwds\n        Options for the metric.\n    key_added\n        If not specified, the neighbors data is stored in .uns['neighbors'],\n        distances and connectivities are stored in .obsp['distances'] and\n        .obsp['connectivities'] respectively.\n        If specified, the neighbors data is added to .uns[key_added],\n        distances are stored in .obsp[key_added+'_distances'] and\n        connectivities in .obsp[key_added+'_connectivities'].\n    copy\n        Return a copy instead of writing to adata.\n\n    Returns\n    -------\n    Depending on `copy`, updates or returns `adata` with the following:\n\n    See `key_added` parameter description for the storage path of\n    connectivities and distances.\n\n    **connectivities** : sparse matrix of dtype `float32`.\n        Weighted adjacency matrix of the neighborhood graph of data\n        points. Weights should be interpreted as connectivities.\n    **distances** : sparse matrix of dtype `float32`.\n        Instead of decaying weights, this stores distances for each pair of\n        neighbors.\n\n    Notes\n    -----\n    If `method='umap'`, it's highly recommended to install pynndescent ``pip install pynndescent``.\n    Installing `pynndescent` can significantly increase performance,\n    and in later versions it will become a hard dependency.\n    \"\"\"\n    start = logg.info('computing neighbors')\n    adata = adata.copy() if copy else adata\n    if adata.is_view:  # we shouldn't need this here...\n        adata._init_as_actual(adata.copy())\n    neighbors = Neighbors(adata)\n    neighbors.compute_neighbors(\n        n_neighbors=n_neighbors,\n        knn=knn,\n        n_pcs=n_pcs,\n        use_rep=use_rep,\n        method=method,\n        metric=metric,\n        metric_kwds=metric_kwds,\n        random_state=random_state,\n    )\n\n    if key_added is None:\n        key_added = 'neighbors'\n        conns_key = 'connectivities'\n        dists_key = 'distances'\n    else:\n        conns_key = key_added + '_connectivities'\n        dists_key = key_added + '_distances'\n\n    adata.uns[key_added] = {}\n\n    neighbors_dict = adata.uns[key_added]\n\n    neighbors_dict['connectivities_key'] = conns_key\n    neighbors_dict['distances_key'] = dists_key\n\n    neighbors_dict['params'] = {'n_neighbors': neighbors.n_neighbors, 'method': method}\n    neighbors_dict['params']['random_state'] = random_state\n    neighbors_dict['params']['metric'] = metric\n    if metric_kwds:\n        neighbors_dict['params']['metric_kwds'] = metric_kwds\n    if use_rep is not None:\n        neighbors_dict['params']['use_rep'] = use_rep\n    if n_pcs is not None:\n        neighbors_dict['params']['n_pcs'] = n_pcs\n\n    adata.obsp[dists_key] = neighbors.distances\n    adata.obsp[conns_key] = neighbors.connectivities\n\n    if neighbors.rp_forest is not None:\n        neighbors_dict['rp_forest'] = neighbors.rp_forest\n    logg.info(\n        '    finished',\n        time=start,\n        deep=(\n            f'added to `.uns[{key_added!r}]`\\n'\n            f'    `.obsp[{dists_key!r}]`, distances for each pair of neighbors\\n'\n            f'    `.obsp[{conns_key!r}]`, weighted adjacency matrix'\n        ),\n    )\n    return adata if copy else None", "idx": 117}
{"project": "Scanpy", "commit_id": "207_scanpy_1.9.0___init__.py__rp_forest_generate.py", "target": 0, "func": "def _rp_forest_generate(\n    rp_forest_dict: RPForestDict,\n) -> Generator[FlatTree, None, None]:\n    props = FlatTree._fields\n    num_trees = len(rp_forest_dict[props[0]]['start']) - 1\n\n    for i in range(num_trees):\n        tree = []\n        for prop in props:\n            start = rp_forest_dict[prop]['start'][i]\n            end = rp_forest_dict[prop]['start'][i + 1]\n            tree.append(rp_forest_dict[prop]['data'][start:end])\n        yield FlatTree(*tree)\n\n    tree = []\n    for prop in props:\n        start = rp_forest_dict[prop]['start'][num_trees]\n        tree.append(rp_forest_dict[prop]['data'][start:])\n    yield FlatTree(*tree)", "idx": 118}
{"project": "Scanpy", "commit_id": "208_scanpy_1.9.0___init__.py_compute_neighbors_umap.py", "target": 0, "func": "def compute_neighbors_umap(\n    X: Union[np.ndarray, csr_matrix],\n    n_neighbors: int,\n    random_state: AnyRandom = None,\n    metric: Union[_Metric, _MetricFn] = 'euclidean',\n    metric_kwds: Mapping[str, Any] = MappingProxyType({}),\n    angular: bool = False,\n    verbose: bool = False,\n):\n    \"\"\"This is from umap.fuzzy_simplicial_set [McInnes18]_.\n\n    Given a set of data X, a neighborhood size, and a measure of distance\n    compute the fuzzy simplicial set (here represented as a fuzzy graph in\n    the form of a sparse matrix) associated to the data. This is done by\n    locally approximating geodesic distance at each point, creating a fuzzy\n    simplicial set for each such point, and then combining all the local\n    fuzzy simplicial sets into a global one via a fuzzy union.\n\n    Parameters\n    ----------\n    X: array of shape (n_samples, n_features)\n        The data to be modelled as a fuzzy simplicial set.\n    n_neighbors\n        The number of neighbors to use to approximate geodesic distance.\n        Larger numbers induce more global estimates of the manifold that can\n        miss finer detail, while smaller values will focus on fine manifold\n        structure to the detriment of the larger picture.\n    random_state\n        A state capable being used as a numpy random state.\n    metric\n        The metric to use to compute distances in high dimensional space.\n        If a string is passed it must match a valid predefined metric. If\n        a general metric is required a function that takes two 1d arrays and\n        returns a float can be provided. For performance purposes it is\n        required that this be a numba jit'd function. Valid string metrics\n        include:\n            * euclidean\n            * manhattan\n            * chebyshev\n            * minkowski\n            * canberra\n            * braycurtis\n            * mahalanobis\n            * wminkowski\n            * seuclidean\n            * cosine\n            * correlation\n            * haversine\n            * hamming\n            * jaccard\n            * dice\n            * russelrao\n            * kulsinski\n            * rogerstanimoto\n            * sokalmichener\n            * sokalsneath\n            * yule\n        Metrics that take arguments (such as minkowski, mahalanobis etc.)\n        can have arguments passed via the metric_kwds dictionary. At this\n        time care must be taken and dictionary elements must be ordered\n        appropriately; this will hopefully be fixed in the future.\n    metric_kwds\n        Arguments to pass on to the metric, such as the ``p`` value for\n        Minkowski distance.\n    angular\n        Whether to use angular/cosine distance for the random projection\n        forest for seeding NN-descent to determine approximate nearest\n        neighbors.\n    verbose\n        Whether to report information on the current progress of the algorithm.\n\n    Returns\n    -------\n    **knn_indices**, **knn_dists** : np.arrays of shape (n_observations, n_neighbors)\n    \"\"\"\n    with warnings.catch_warnings():\n        # umap 0.5.0\n        warnings.filterwarnings(\"ignore\", message=r\"Tensorflow not installed\")\n        from umap.umap_ import nearest_neighbors\n\n    random_state = check_random_state(random_state)\n\n    knn_indices, knn_dists, forest = nearest_neighbors(\n        X,\n        n_neighbors,\n        random_state=random_state,\n        metric=metric,\n        metric_kwds=metric_kwds,\n        angular=angular,\n        verbose=verbose,\n    )\n\n    return knn_indices, knn_dists, forest", "idx": 119}
{"project": "Scanpy", "commit_id": "209_scanpy_1.9.0___init__.py_compute_neighbors_rapids.py", "target": 0, "func": "def compute_neighbors_rapids(\n    X: np.ndarray, n_neighbors: int, metric: _Metric = 'euclidean'\n):\n    \"\"\"Compute nearest neighbors using RAPIDS cuml.\n\n    Parameters\n    ----------\n    X: array of shape (n_samples, n_features)\n        The data to compute nearest neighbors for.\n    n_neighbors\n        The number of neighbors to use.\n    metric\n        The metric to use to compute distances in high dimensional space.\n        This string must match a valid predefined metric in RAPIDS cuml.\n\n        Returns\n    -------\n    **knn_indices**, **knn_dists** : np.arrays of shape (n_observations, n_neighbors)\n    \"\"\"\n    from cuml.neighbors import NearestNeighbors\n\n    nn = NearestNeighbors(n_neighbors=n_neighbors, metric=metric)\n    X_contiguous = np.ascontiguousarray(X, dtype=np.float32)\n    nn.fit(X_contiguous)\n    knn_dist, knn_indices = nn.kneighbors(X_contiguous)\n    return knn_indices, knn_dist", "idx": 120}
{"project": "Scanpy", "commit_id": "20_scanpy_1.9.0_typed_returns.py_process_return.py", "target": 0, "func": "def process_return(lines):\n    for line in lines:\n        m = re.fullmatch(r'(?P<param>\\w+)\\s+:\\s+(?P<type>[\\w.]+)', line)\n        if m:\n            # Once this is in scanpydoc, we can use the fancy hover stuff\n            yield f'**{m[\"param\"]}** : :class:`~{m[\"type\"]}`'\n        else:\n            yield line", "idx": 121}
{"project": "Scanpy", "commit_id": "210_scanpy_1.9.0___init__.py__get_sparse_matrix_from_indices_distances_umap.py", "target": 0, "func": "def _get_sparse_matrix_from_indices_distances_umap(\n    knn_indices, knn_dists, n_obs, n_neighbors\n):\n    rows = np.zeros((n_obs * n_neighbors), dtype=np.int64)\n    cols = np.zeros((n_obs * n_neighbors), dtype=np.int64)\n    vals = np.zeros((n_obs * n_neighbors), dtype=np.float64)\n\n    for i in range(knn_indices.shape[0]):\n        for j in range(n_neighbors):\n            if knn_indices[i, j] == -1:\n                continue  # We didn't get the full knn for i\n            if knn_indices[i, j] == i:\n                val = 0.0\n            else:\n                val = knn_dists[i, j]\n\n            rows[i * n_neighbors + j] = i\n            cols[i * n_neighbors + j] = knn_indices[i, j]\n            vals[i * n_neighbors + j] = val\n\n    result = coo_matrix((vals, (rows, cols)), shape=(n_obs, n_obs))\n    result.eliminate_zeros()\n    return result.tocsr()", "idx": 122}
{"project": "Scanpy", "commit_id": "211_scanpy_1.9.0___init__.py__compute_connectivities_umap.py", "target": 0, "func": "def _compute_connectivities_umap(\n    knn_indices,\n    knn_dists,\n    n_obs,\n    n_neighbors,\n    set_op_mix_ratio=1.0,\n    local_connectivity=1.0,\n):\n    \"\"\"\\\n    This is from umap.fuzzy_simplicial_set [McInnes18]_.\n\n    Given a set of data X, a neighborhood size, and a measure of distance\n    compute the fuzzy simplicial set (here represented as a fuzzy graph in\n    the form of a sparse matrix) associated to the data. This is done by\n    locally approximating geodesic distance at each point, creating a fuzzy\n    simplicial set for each such point, and then combining all the local\n    fuzzy simplicial sets into a global one via a fuzzy union.\n    \"\"\"\n    with warnings.catch_warnings():\n        # umap 0.5.0\n        warnings.filterwarnings(\"ignore\", message=r\"Tensorflow not installed\")\n        from umap.umap_ import fuzzy_simplicial_set\n\n    X = coo_matrix(([], ([], [])), shape=(n_obs, 1))\n    connectivities = fuzzy_simplicial_set(\n        X,\n        n_neighbors,\n        None,\n        None,\n        knn_indices=knn_indices,\n        knn_dists=knn_dists,\n        set_op_mix_ratio=set_op_mix_ratio,\n        local_connectivity=local_connectivity,\n    )\n\n    if isinstance(connectivities, tuple):\n        # In umap-learn 0.4, this returns (result, sigmas, rhos)\n        connectivities = connectivities[0]\n\n    distances = _get_sparse_matrix_from_indices_distances_umap(\n        knn_indices, knn_dists, n_obs, n_neighbors\n    )\n\n    return distances, connectivities.tocsr()", "idx": 123}
{"project": "Scanpy", "commit_id": "212_scanpy_1.9.0___init__.py__get_sparse_matrix_from_indices_distances_numpy.py", "target": 0, "func": "def _get_sparse_matrix_from_indices_distances_numpy(\n    indices, distances, n_obs, n_neighbors\n):\n    n_nonzero = n_obs * n_neighbors\n    indptr = np.arange(0, n_nonzero + 1, n_neighbors)\n    D = csr_matrix(\n        (\n            distances.copy().ravel(),  # copy the data, otherwise strange behavior here\n            indices.copy().ravel(),\n            indptr,\n        ),\n        shape=(n_obs, n_obs),\n    )\n    D.eliminate_zeros()\n    return D", "idx": 124}
{"project": "Scanpy", "commit_id": "213_scanpy_1.9.0___init__.py__get_indices_distances_from_sparse_matrix.py", "target": 0, "func": "def _get_indices_distances_from_sparse_matrix(D, n_neighbors: int):\n    indices = np.zeros((D.shape[0], n_neighbors), dtype=int)\n    distances = np.zeros((D.shape[0], n_neighbors), dtype=D.dtype)\n    n_neighbors_m1 = n_neighbors - 1\n    for i in range(indices.shape[0]):\n        neighbors = D[i].nonzero()  # 'true' and 'spurious' zeros\n        indices[i, 0] = i\n        distances[i, 0] = 0\n        # account for the fact that there might be more than n_neighbors\n        # due to an approximate search\n        # [the point itself was not detected as its own neighbor during the search]\n        if len(neighbors[1]) > n_neighbors_m1:\n            sorted_indices = np.argsort(D[i][neighbors].A1)[:n_neighbors_m1]\n            indices[i, 1:] = neighbors[1][sorted_indices]\n            distances[i, 1:] = D[i][\n                neighbors[0][sorted_indices], neighbors[1][sorted_indices]\n            ]\n        else:\n            indices[i, 1:] = neighbors[1]\n            distances[i, 1:] = D[i][neighbors]\n    return indices, distances", "idx": 125}
{"project": "Scanpy", "commit_id": "214_scanpy_1.9.0___init__.py__get_indices_distances_from_dense_matrix.py", "target": 0, "func": "def _get_indices_distances_from_dense_matrix(D, n_neighbors: int):\n    sample_range = np.arange(D.shape[0])[:, None]\n    indices = np.argpartition(D, n_neighbors - 1, axis=1)[:, :n_neighbors]\n    indices = indices[sample_range, np.argsort(D[sample_range, indices])]\n    distances = D[sample_range, indices]\n    return indices, distances", "idx": 126}
{"project": "Scanpy", "commit_id": "215_scanpy_1.9.0___init__.py__backwards_compat_get_full_X_diffmap.py", "target": 0, "func": "def _backwards_compat_get_full_X_diffmap(adata: AnnData) -> np.ndarray:\n    if 'X_diffmap0' in adata.obs:\n        return np.c_[adata.obs['X_diffmap0'].values[:, None], adata.obsm['X_diffmap']]\n    else:\n        return adata.obsm['X_diffmap']", "idx": 127}
{"project": "Scanpy", "commit_id": "216_scanpy_1.9.0___init__.py__backwards_compat_get_full_eval.py", "target": 0, "func": "def _backwards_compat_get_full_eval(adata: AnnData):\n    if 'X_diffmap0' in adata.obs:\n        return np.r_[1, adata.uns['diffmap_evals']]\n    else:\n        return adata.uns['diffmap_evals']", "idx": 128}
{"project": "Scanpy", "commit_id": "217_scanpy_1.9.0___init__.py__make_forest_dict.py", "target": 0, "func": "def _make_forest_dict(forest):\n    d = {}\n    props = ('hyperplanes', 'offsets', 'children', 'indices')\n    for prop in props:\n        d[prop] = {}\n        sizes = np.fromiter(\n            (getattr(tree, prop).shape[0] for tree in forest), dtype=int\n        )\n        d[prop]['start'] = np.zeros_like(sizes)\n        if prop == 'offsets':\n            dims = sizes.sum()\n        else:\n            dims = (sizes.sum(), getattr(forest[0], prop).shape[1])\n        dtype = getattr(forest[0], prop).dtype\n        dat = np.empty(dims, dtype=dtype)\n        start = 0\n        for i, size in enumerate(sizes):\n            d[prop]['start'][i] = start\n            end = start + size\n            dat[start:end] = getattr(forest[i], prop)\n            start = end\n        d[prop]['data'] = dat\n    return d", "idx": 129}
{"project": "Scanpy", "commit_id": "218_scanpy_1.9.0___init__.py___init__.py", "target": 0, "func": "def __init__(\n        self,\n        adata: AnnData,\n        n_dcs: Optional[int] = None,\n        neighbors_key: Optional[str] = None,\n    ):\n        self._adata = adata\n        self._init_iroot()\n        # use the graph in adata\n        info_str = ''\n        self.knn: Optional[bool] = None\n        self._distances: Union[np.ndarray, csr_matrix, None] = None\n        self._connectivities: Union[np.ndarray, csr_matrix, None] = None\n        self._transitions_sym: Union[np.ndarray, csr_matrix, None] = None\n        self._number_connected_components: Optional[int] = None\n        self._rp_forest: Optional[RPForestDict] = None\n        if neighbors_key is None:\n            neighbors_key = 'neighbors'\n        if neighbors_key in adata.uns:\n            neighbors = NeighborsView(adata, neighbors_key)\n            if 'distances' in neighbors:\n                self.knn = issparse(neighbors['distances'])\n                self._distances = neighbors['distances']\n            if 'connectivities' in neighbors:\n                self.knn = issparse(neighbors['connectivities'])\n                self._connectivities = neighbors['connectivities']\n            if 'rp_forest' in neighbors:\n                self._rp_forest = neighbors['rp_forest']\n            if 'params' in neighbors:\n                self.n_neighbors = neighbors['params']['n_neighbors']\n            else:\n\n                def count_nonzero(a: Union[np.ndarray, csr_matrix]) -> int:\n                    return a.count_nonzero() if issparse(a) else np.count_nonzero(a)\n\n                # estimating n_neighbors\n                if self._connectivities is None:\n                    self.n_neighbors = int(\n                        count_nonzero(self._distances) / self._distances.shape[0]\n                    )\n                else:\n                    self.n_neighbors = int(\n                        count_nonzero(self._connectivities)\n                        / self._connectivities.shape[0]\n                        / 2\n                    )\n            info_str += '`.distances` `.connectivities` '\n            self._number_connected_components = 1\n            if issparse(self._connectivities):\n                from scipy.sparse.csgraph import connected_components\n\n                self._connected_components = connected_components(self._connectivities)\n                self._number_connected_components = self._connected_components[0]\n        if 'X_diffmap' in adata.obsm_keys():\n            self._eigen_values = _backwards_compat_get_full_eval(adata)\n            self._eigen_basis = _backwards_compat_get_full_X_diffmap(adata)\n            if n_dcs is not None:\n                if n_dcs > len(self._eigen_values):\n                    raise ValueError(\n                        'Cannot instantiate using `n_dcs`={}. '\n                        'Compute diffmap/spectrum with more components first.'.format(\n                            n_dcs\n                        )\n                    )\n                self._eigen_values = self._eigen_values[:n_dcs]\n                self._eigen_basis = self._eigen_basis[:, :n_dcs]\n            self.n_dcs = len(self._eigen_values)\n            info_str += '`.eigen_values` `.eigen_basis` `.distances_dpt`'\n        else:\n            self._eigen_values = None\n            self._eigen_basis = None\n            self.n_dcs = None\n        if info_str != '':\n            logg.debug(f'    initialized {info_str}')", "idx": 130}
{"project": "Scanpy", "commit_id": "219_scanpy_1.9.0___init__.py___getitem__.py", "target": 0, "func": "def __getitem__(self, index):\n        if isinstance(index, (int, np.integer)):\n            if self.restrict_array is None:\n                glob_index = index\n            else:\n                # map the index back to the global index\n                glob_index = self.restrict_array[index]\n            if glob_index not in self.rows:\n                self.rows[glob_index] = self.get_row(glob_index)\n            row = self.rows[glob_index]\n            if self.restrict_array is None:\n                return row\n            else:\n                return row[self.restrict_array]\n        else:\n            if self.restrict_array is None:\n                glob_index_0, glob_index_1 = index\n            else:\n                glob_index_0 = self.restrict_array[index[0]]\n                glob_index_1 = self.restrict_array[index[1]]\n            if glob_index_0 not in self.rows:\n                self.rows[glob_index_0] = self.get_row(glob_index_0)\n            return self.rows[glob_index_0][glob_index_1]", "idx": 131}
{"project": "Scanpy", "commit_id": "21_scanpy_1.9.0_typed_returns.py_scanpy_parse_returns_section.py", "target": 1, "func": "def scanpy_parse_returns_section(self, section):\n    lines_raw = list(process_return(self._dedent(self._consume_to_next_section())))\n    lines = self._format_block(':returns: ', lines_raw)\n    if lines and lines[-1]:\n        lines.append('')\n    return lines", "idx": 132}
{"project": "Scanpy", "commit_id": "220_scanpy_1.9.0___init__.py_restrict.py", "target": 0, "func": "def restrict(self, index_array):\n        \"\"\"Generate a view restricted to a subset of indices.\"\"\"\n        new_shape = index_array.shape[0], index_array.shape[0]\n        return OnFlySymMatrix(\n            self.get_row,\n            new_shape,\n            DC_start=self.DC_start,\n            DC_end=self.DC_end,\n            rows=self.rows,\n            restrict_array=index_array,", "idx": 133}
{"project": "Scanpy", "commit_id": "221_scanpy_1.9.0___init__.py_rp_forest.py", "target": 0, "func": "def rp_forest(self) -> Optional[RPForestDict]:\n        return self._rp_forest", "idx": 134}
{"project": "Scanpy", "commit_id": "222_scanpy_1.9.0___init__.py_distances.py", "target": 0, "func": "def distances(self) -> Union[np.ndarray, csr_matrix, None]:\n        \"\"\"Distances between data points (sparse matrix).\"\"\"\n        return self._distances", "idx": 135}
{"project": "Scanpy", "commit_id": "223_scanpy_1.9.0___init__.py_connectivities.py", "target": 0, "func": "def connectivities(self) -> Union[np.ndarray, csr_matrix, None]:\n        \"\"\"Connectivities between data points (sparse matrix).\"\"\"\n        return self._connectivities", "idx": 136}
{"project": "Scanpy", "commit_id": "224_scanpy_1.9.0___init__.py_transitions.py", "target": 0, "func": "def transitions(self) -> Union[np.ndarray, csr_matrix]:\n        \"\"\"Transition matrix (sparse matrix).\n\n        Is conjugate to the symmetrized transition matrix via::\n\n            self.transitions = self.Z *  self.transitions_sym / self.Z\n\n        where ``self.Z`` is the diagonal matrix storing the normalization of the\n        underlying kernel matrix.\n\n        Notes\n        -----\n        This has not been tested, in contrast to `transitions_sym`.\n        \"\"\"\n        if issparse(self.Z):\n            Zinv = self.Z.power(-1)\n        else:\n            Zinv = np.diag(1.0 / np.diag(self.Z))\n        return self.Z @ self.transitions_sym @ Zinv", "idx": 137}
{"project": "Scanpy", "commit_id": "225_scanpy_1.9.0___init__.py_transitions_sym.py", "target": 0, "func": "def transitions_sym(self) -> Union[np.ndarray, csr_matrix, None]:\n        \"\"\"Symmetrized transition matrix (sparse matrix).\n\n        Is conjugate to the transition matrix via::\n\n            self.transitions_sym = self.Z /  self.transitions * self.Z\n\n        where ``self.Z`` is the diagonal matrix storing the normalization of the\n        underlying kernel matrix.\n        \"\"\"\n        return self._transitions_sym", "idx": 138}
{"project": "Scanpy", "commit_id": "226_scanpy_1.9.0___init__.py_eigen_values.py", "target": 0, "func": "def eigen_values(self):\n        \"\"\"Eigen values of transition matrix (numpy array).\"\"\"\n        return self._eigen_values", "idx": 139}
{"project": "Scanpy", "commit_id": "227_scanpy_1.9.0___init__.py_eigen_basis.py", "target": 0, "func": "def eigen_basis(self):\n        \"\"\"Eigen basis of transition matrix (numpy array).\"\"\"\n        return self._eigen_basis", "idx": 140}
{"project": "Scanpy", "commit_id": "228_scanpy_1.9.0___init__.py_distances_dpt.py", "target": 0, "func": "def distances_dpt(self):\n        \"\"\"DPT distances (on-fly matrix).\n\n        This is yields [Haghverdi16]_, Eq. 15 from the supplement with the\n        extensions of [Wolf19]_, supplement on random-walk based distance\n        measures.\n        \"\"\"\n        return OnFlySymMatrix(self._get_dpt_row, shape=self._adata.shape)", "idx": 141}
{"project": "Scanpy", "commit_id": "229_scanpy_1.9.0___init__.py_to_igraph.py", "target": 0, "func": "def to_igraph(self):\n        \"\"\"Generate igraph from connectiviies.\"\"\"\n        return _utils.get_igraph_from_adjacency(self.connectivities)", "idx": 142}
{"project": "Scanpy", "commit_id": "22_scanpy_1.9.0_typed_returns.py_setup.py", "target": 0, "func": "def setup(app: Sphinx):\n    NumpyDocstring._parse_returns_section = scanpy_parse_returns_section", "idx": 143}
{"project": "Scanpy", "commit_id": "230_scanpy_1.9.0___init__.py_compute_neighbors.py", "target": 1, "func": "def compute_neighbors(\n        self,\n        n_neighbors: int = 30,\n        knn: bool = True,\n        n_pcs: Optional[int] = None,\n        use_rep: Optional[str] = None,\n        method: _Method = 'umap',\n        random_state: AnyRandom = 0,\n        write_knn_indices: bool = False,\n        metric: _Metric = 'euclidean',\n        metric_kwds: Mapping[str, Any] = MappingProxyType({}),\n    ) -> None:\n        \"\"\"\\\n        Compute distances and connectivities of neighbors.\n\n        Parameters\n        ----------\n        n_neighbors\n             Use this number of nearest neighbors.\n        knn\n             Restrict result to `n_neighbors` nearest neighbors.\n        {n_pcs}\n        {use_rep}\n\n        Returns\n        -------\n        Writes sparse graph attributes `.distances` and `.connectivities`.\n        Also writes `.knn_indices` and `.knn_distances` if\n        `write_knn_indices==True`.\n        \"\"\"\n        from sklearn.metrics import pairwise_distances\n\n        start_neighbors = logg.debug('computing neighbors')\n        if n_neighbors > self._adata.shape[0]:  # very small datasets\n            n_neighbors = 1 + int(0.5 * self._adata.shape[0])\n            logg.warning(f'n_obs too small: adjusting to `n_neighbors = {n_neighbors}`')\n        if method == 'umap' and not knn:\n            raise ValueError('`method = \\'umap\\' only with `knn = True`.')\n        if method not in {'umap', 'gauss', 'rapids'}:\n            raise ValueError(\"`method` needs to be 'umap', 'gauss', or 'rapids'.\")\n        if self._adata.shape[0] >= 10000 and not knn:\n            logg.warning('Using high n_obs without `knn=True` takes a lot of memory...')\n        # do not use the cached rp_forest\n        self._rp_forest = None\n        self.n_neighbors = n_neighbors\n        self.knn = knn\n        X = _choose_representation(self._adata, use_rep=use_rep, n_pcs=n_pcs)\n        # neighbor search\n        use_dense_distances = (metric == 'euclidean' and X.shape[0] < 8192) or not knn\n        if use_dense_distances:\n            _distances = pairwise_distances(X, metric=metric, **metric_kwds)\n            knn_indices, knn_distances = _get_indices_distances_from_dense_matrix(\n                _distances, n_neighbors\n            )\n            if knn:\n                self._distances = _get_sparse_matrix_from_indices_distances_numpy(\n                    knn_indices, knn_distances, X.shape[0], n_neighbors\n                )\n            else:\n                self._distances = _distances\n        elif method == 'rapids':\n            knn_indices, knn_distances = compute_neighbors_rapids(\n                X, n_neighbors, metric=metric\n            )\n        else:\n            # non-euclidean case and approx nearest neighbors\n            if X.shape[0] < 4096:\n                X = pairwise_distances(X, metric=metric, **metric_kwds)\n                metric = 'precomputed'\n            knn_indices, knn_distances, forest = compute_neighbors_umap(\n                X, n_neighbors, random_state, metric=metric, metric_kwds=metric_kwds\n            )\n            # very cautious here\n            try:\n                if forest:\n                    self._rp_forest = _make_forest_dict(forest)\n            except Exception:  # TODO catch the correct exception\n                pass\n        # write indices as attributes\n        if write_knn_indices:\n            self.knn_indices = knn_indices\n            self.knn_distances = knn_distances\n        start_connect = logg.debug('computed neighbors', time=start_neighbors)\n        if not use_dense_distances or method in {'umap', 'rapids'}:\n            # we need self._distances also for method == 'gauss' if we didn't\n            # use dense distances\n            self._distances, self._connectivities = _compute_connectivities_umap(\n                knn_indices,\n                knn_distances,\n                self._adata.shape[0],\n                self.n_neighbors,\n            )\n        # overwrite the umap connectivities if method is 'gauss'\n        # self._distances is unaffected by this\n        if method == 'gauss':\n            self._compute_connectivities_diffmap()\n        logg.debug('computed connectivities', time=start_connect)\n        self._number_connected_components = 1\n        if issparse(self._connectivities):\n            from scipy.sparse.csgraph import connected_components\n\n            self._connected_components = connected_components(self._connectivities)\n            self._number_connected_components = self._connected_components[0]", "idx": 144}
{"project": "Scanpy", "commit_id": "231_scanpy_1.9.0___init__.py__compute_connectivities_diffmap.py", "target": 0, "func": "def _compute_connectivities_diffmap(self, density_normalize=True):\n        # init distances\n        if self.knn:\n            Dsq = self._distances.power(2)\n            indices, distances_sq = _get_indices_distances_from_sparse_matrix(\n                Dsq, self.n_neighbors\n            )\n        else:\n            Dsq = np.power(self._distances, 2)\n            indices, distances_sq = _get_indices_distances_from_dense_matrix(\n                Dsq, self.n_neighbors\n            )\n\n        # exclude the first point, the 0th neighbor\n        indices = indices[:, 1:]\n        distances_sq = distances_sq[:, 1:]\n\n        # choose sigma, the heuristic here doesn't seem to make much of a difference,\n        # but is used to reproduce the figures of Haghverdi et al. (2016)\n        if self.knn:\n            # as the distances are not sorted\n            # we have decay within the n_neighbors first neighbors\n            sigmas_sq = np.median(distances_sq, axis=1)\n        else:\n            # the last item is already in its sorted position through argpartition\n            # we have decay beyond the n_neighbors neighbors\n            sigmas_sq = distances_sq[:, -1] / 4\n        sigmas = np.sqrt(sigmas_sq)\n\n        # compute the symmetric weight matrix\n        if not issparse(self._distances):\n            Num = 2 * np.multiply.outer(sigmas, sigmas)\n            Den = np.add.outer(sigmas_sq, sigmas_sq)\n            W = np.sqrt(Num / Den) * np.exp(-Dsq / Den)\n            # make the weight matrix sparse\n            if not self.knn:\n                mask = W > 1e-14\n                W[~mask] = 0\n            else:\n                # restrict number of neighbors to ~k\n                # build a symmetric mask\n                mask = np.zeros(Dsq.shape, dtype=bool)\n                for i, row in enumerate(indices):\n                    mask[i, row] = True\n                    for j in row:\n                        if i not in set(indices[j]):\n                            W[j, i] = W[i, j]\n                            mask[j, i] = True\n                # set all entries that are not nearest neighbors to zero\n                W[~mask] = 0\n        else:\n            W = (\n                Dsq.copy()\n            )  # need to copy the distance matrix here; what follows is inplace\n            for i in range(len(Dsq.indptr[:-1])):\n                row = Dsq.indices[Dsq.indptr[i] : Dsq.indptr[i + 1]]\n                num = 2 * sigmas[i] * sigmas[row]\n                den = sigmas_sq[i] + sigmas_sq[row]\n                W.data[Dsq.indptr[i] : Dsq.indptr[i + 1]] = np.sqrt(num / den) * np.exp(\n                    -Dsq.data[Dsq.indptr[i] : Dsq.indptr[i + 1]] / den\n                )\n            W = W.tolil()\n            for i, row in enumerate(indices):\n                for j in row:\n                    if i not in set(indices[j]):\n                        W[j, i] = W[i, j]\n            W = W.tocsr()\n\n        self._connectivities = W", "idx": 145}
{"project": "Scanpy", "commit_id": "232_scanpy_1.9.0___init__.py_compute_transitions.py", "target": 0, "func": "def compute_transitions(self, density_normalize: bool = True):\n        \"\"\"\\\n        Compute transition matrix.\n\n        Parameters\n        ----------\n        density_normalize\n            The density rescaling of Coifman and Lafon (2006): Then only the\n            geometry of the data matters, not the sampled density.\n\n        Returns\n        -------\n        Makes attributes `.transitions_sym` and `.transitions` available.\n        \"\"\"\n        start = logg.info('computing transitions')\n        W = self._connectivities\n        # density normalization as of Coifman et al. (2005)\n        # ensures that kernel matrix is independent of sampling density\n        if density_normalize:\n            # q[i] is an estimate for the sampling density at point i\n            # it's also the degree of the underlying graph\n            q = np.asarray(W.sum(axis=0))\n            if not issparse(W):\n                Q = np.diag(1.0 / q)\n            else:\n                Q = scipy.sparse.spdiags(1.0 / q, 0, W.shape[0], W.shape[0])\n            K = Q @ W @ Q\n        else:\n            K = W\n\n        # z[i] is the square root of the row sum of K\n        z = np.sqrt(np.asarray(K.sum(axis=0)))\n        if not issparse(K):\n            self.Z = np.diag(1.0 / z)\n        else:\n            self.Z = scipy.sparse.spdiags(1.0 / z, 0, K.shape[0], K.shape[0])\n        self._transitions_sym = self.Z @ K @ self.Z\n        logg.info('    finished', time=start)", "idx": 146}
{"project": "Scanpy", "commit_id": "233_scanpy_1.9.0___init__.py_compute_eigen.py", "target": 0, "func": "def compute_eigen(\n        self,\n        n_comps: int = 15,\n        sym: Optional[bool] = None,\n        sort: Literal['decrease', 'increase'] = 'decrease',\n        random_state: AnyRandom = 0,\n    ):\n        \"\"\"\\\n        Compute eigen decomposition of transition matrix.\n\n        Parameters\n        ----------\n        n_comps\n            Number of eigenvalues/vectors to be computed, set `n_comps = 0` if\n            you need all eigenvectors.\n        sym\n            Instead of computing the eigendecomposition of the assymetric\n            transition matrix, computed the eigendecomposition of the symmetric\n            Ktilde matrix.\n        random_state\n            A numpy random seed\n\n        Returns\n        -------\n        Writes the following attributes.\n\n        eigen_values : numpy.ndarray\n            Eigenvalues of transition matrix.\n        eigen_basis : numpy.ndarray\n             Matrix of eigenvectors (stored in columns).  `.eigen_basis` is\n             projection of data matrix on right eigenvectors, that is, the\n             projection on the diffusion components.  these are simply the\n             components of the right eigenvectors and can directly be used for\n             plotting.\n        \"\"\"\n        np.set_printoptions(precision=10)\n        if self._transitions_sym is None:\n            raise ValueError('Run `.compute_transitions` first.')\n        matrix = self._transitions_sym\n        # compute the spectrum\n        if n_comps == 0:\n            evals, evecs = scipy.linalg.eigh(matrix)\n        else:\n            n_comps = min(matrix.shape[0] - 1, n_comps)\n            # ncv = max(2 * n_comps + 1, int(np.sqrt(matrix.shape[0])))\n            ncv = None\n            which = 'LM' if sort == 'decrease' else 'SM'\n            # it pays off to increase the stability with a bit more precision\n            matrix = matrix.astype(np.float64)\n\n            # Setting the random initial vector\n            random_state = check_random_state(random_state)\n            v0 = random_state.standard_normal((matrix.shape[0]))\n            evals, evecs = scipy.sparse.linalg.eigsh(\n                matrix, k=n_comps, which=which, ncv=ncv, v0=v0\n            )\n            evals, evecs = evals.astype(np.float32), evecs.astype(np.float32)\n        if sort == 'decrease':\n            evals = evals[::-1]\n            evecs = evecs[:, ::-1]\n        logg.info(\n            '    eigenvalues of transition matrix\\n'\n            '    {}'.format(str(evals).replace('\\n', '\\n    '))\n        )\n        if self._number_connected_components > len(evals) / 2:\n            logg.warning('Transition matrix has many disconnected components!')\n        self._eigen_values = evals\n        self._eigen_basis = evecs", "idx": 147}
{"project": "Scanpy", "commit_id": "234_scanpy_1.9.0___init__.py__init_iroot.py", "target": 0, "func": "def _init_iroot(self):\n        self.iroot = None\n        # set iroot directly\n        if 'iroot' in self._adata.uns:\n            if self._adata.uns['iroot'] >= self._adata.n_obs:\n                logg.warning(\n                    f'Root cell index {self._adata.uns[\"iroot\"]} does not '\n                    f'exist for {self._adata.n_obs} samples. It\u2019s ignored.'\n                )\n            else:\n                self.iroot = self._adata.uns['iroot']\n            return\n        # set iroot via xroot\n        xroot = None\n        if 'xroot' in self._adata.uns:\n            xroot = self._adata.uns['xroot']\n        elif 'xroot' in self._adata.var:\n            xroot = self._adata.var['xroot']\n        # see whether we can set self.iroot using the full data matrix\n        if xroot is not None and xroot.size == self._adata.shape[1]:\n            self._set_iroot_via_xroot(xroot)", "idx": 148}
{"project": "Scanpy", "commit_id": "235_scanpy_1.9.0___init__.py__get_dpt_row.py", "target": 0, "func": "def _get_dpt_row(self, i):\n        mask = None\n        if self._number_connected_components > 1:\n            label = self._connected_components[1][i]\n            mask = self._connected_components[1] == label\n        row = sum(\n            (\n                self.eigen_values[j]\n                / (1 - self.eigen_values[j])\n                * (self.eigen_basis[i, j] - self.eigen_basis[:, j])\n            )\n            ** 2\n            # account for float32 precision\n            for j in range(0, self.eigen_values.size)\n            if self.eigen_values[j] < 0.9994\n        )\n        # thanks to Marius Lange for pointing Alex to this:\n        # we will likely remove the contributions from the stationary state below when making\n        # backwards compat breaking changes, they originate from an early implementation in 2015\n        # they never seem to have deteriorated results, but also other distance measures (see e.g.\n        # PAGA paper) don't have it, which makes sense\n        row += sum(\n            (self.eigen_basis[i, k] - self.eigen_basis[:, k]) ** 2\n            for k in range(0, self.eigen_values.size)\n            if self.eigen_values[k] >= 0.9994\n        )\n        if mask is not None:\n            row[~mask] = np.inf\n        return np.sqrt(row)", "idx": 149}
{"project": "Scanpy", "commit_id": "236_scanpy_1.9.0___init__.py__set_pseudotime.py", "target": 0, "func": "def _set_pseudotime(self):\n        \"\"\"Return pseudotime with respect to root point.\"\"\"\n        self.pseudotime = self.distances_dpt[self.iroot].copy()\n        self.pseudotime /= np.max(self.pseudotime[self.pseudotime < np.inf])", "idx": 150}
{"project": "Scanpy", "commit_id": "237_scanpy_1.9.0___init__.py__set_iroot_via_xroot.py", "target": 0, "func": "def _set_iroot_via_xroot(self, xroot):\n        \"\"\"Determine the index of the root cell.\n\n        Given an expression vector, find the observation index that is closest\n        to this vector.\n\n        Parameters\n        ----------\n        xroot : np.ndarray\n            Vector that marks the root cell, the vector storing the initial\n            condition, only relevant for computing pseudotime.\n        \"\"\"\n        if self._adata.shape[1] != xroot.size:\n            raise ValueError(\n                'The root vector you provided does not have the ' 'correct dimension.'\n            )\n        # this is the squared distance\n        dsqroot = 1e10\n        iroot = 0\n        for i in range(self._adata.shape[0]):\n            diff = self._adata.X[i, :] - xroot\n            dsq = diff @ diff\n            if dsq < dsqroot:\n                dsqroot = dsq\n                iroot = i\n                if np.sqrt(dsqroot) < 1e-10:\n                    break\n        logg.debug(f'setting root index to {iroot}')\n        if self.iroot is not None and iroot != self.iroot:\n            logg.warning(f'Changing index of iroot from {self.iroot} to {iroot}.')\n        self.iroot = iroot", "idx": 151}
{"project": "Scanpy", "commit_id": "238_scanpy_1.9.0___init__.py_count_nonzero.py", "target": 0, "func": "def count_nonzero(a: Union[np.ndarray, csr_matrix]) -> int:\n                    return a.count_nonzero() if issparse(a) else np.count_nonzero(a)", "idx": 152}
{"project": "Scanpy", "commit_id": "239_scanpy_1.9.0_palettes.py__plot_color_cycle.py", "target": 0, "func": "def _plot_color_cycle(clists: Mapping[str, Sequence[str]]):\n    import numpy as np\n    import matplotlib.pyplot as plt\n    from matplotlib.colors import ListedColormap, BoundaryNorm\n\n    fig, axes = plt.subplots(nrows=len(clists))  # type: plt.Figure, plt.Axes\n    fig.subplots_adjust(top=0.95, bottom=0.01, left=0.3, right=0.99)\n    axes[0].set_title('Color Maps/Cycles', fontsize=14)\n\n    for ax, (name, clist) in zip(axes, clists.items()):\n        n = len(clist)\n        ax.imshow(\n            np.arange(n)[None, :].repeat(2, 0),\n            aspect='auto',\n            cmap=ListedColormap(clist),\n            norm=BoundaryNorm(np.arange(n + 1) - 0.5, n),\n        )\n        pos = list(ax.get_position().bounds)\n        x_text = pos[0] - 0.01\n        y_text = pos[1] + pos[3] / 2.0\n        fig.text(x_text, y_text, name, va='center', ha='right', fontsize=10)\n\n    # Turn off all ticks & spines\n    for ax in axes:\n        ax.set_axis_off()\n    fig.show()", "idx": 153}
{"project": "Scanpy", "commit_id": "23_scanpy_1.9.0_cli.py__cmd_settings.py", "target": 0, "func": "def _cmd_settings() -> None:\n    from . import settings\n\n    print(settings)", "idx": 154}
{"project": "Scanpy", "commit_id": "240_scanpy_1.9.0__anndata.py_scatter.py", "target": 0, "func": "def scatter(\n    adata: AnnData,\n    x: Optional[str] = None,\n    y: Optional[str] = None,\n    color: Union[str, Collection[str]] = None,\n    use_raw: Optional[bool] = None,\n    layers: Union[str, Collection[str]] = None,\n    sort_order: bool = True,\n    alpha: Optional[float] = None,\n    basis: Optional[_Basis] = None,\n    groups: Union[str, Iterable[str]] = None,\n    components: Union[str, Collection[str]] = None,\n    projection: Literal['2d', '3d'] = '2d',\n    legend_loc: str = 'right margin',\n    legend_fontsize: Union[int, float, _FontSize, None] = None,\n    legend_fontweight: Union[int, _FontWeight, None] = None,\n    legend_fontoutline: float = None,\n    color_map: Union[str, Colormap] = None,\n    palette: Union[Cycler, ListedColormap, ColorLike, Sequence[ColorLike]] = None,\n    frameon: Optional[bool] = None,\n    right_margin: Optional[float] = None,\n    left_margin: Optional[float] = None,\n    size: Union[int, float, None] = None,\n    title: Optional[str] = None,\n    show: Optional[bool] = None,\n    save: Union[str, bool, None] = None,\n    ax: Optional[Axes] = None,\n):\n    \"\"\"\\\n    Scatter plot along observations or variables axes.\n\n    Color the plot using annotations of observations (`.obs`), variables\n    (`.var`) or expression of genes (`.var_names`).\n\n    Parameters\n    ----------\n    adata\n        Annotated data matrix.\n    x\n        x coordinate.\n    y\n        y coordinate.\n    color\n        Keys for annotations of observations/cells or variables/genes,\n        or a hex color specification, e.g.,\n        `'ann1'`, `'#fe57a1'`, or `['ann1', 'ann2']`.\n    use_raw\n        Whether to use `raw` attribute of `adata`. Defaults to `True` if `.raw` is present.\n    layers\n        Use the `layers` attribute of `adata` if present: specify the layer for\n        `x`, `y` and `color`. If `layers` is a string, then it is expanded to\n        `(layers, layers, layers)`.\n    basis\n        String that denotes a plotting tool that computed coordinates.\n    {scatter_temp}\n    {show_save_ax}\n\n    Returns\n    -------\n    If `show==False` a :class:`~matplotlib.axes.Axes` or a list of it.\n    \"\"\"\n    args = locals()\n    if _check_use_raw(adata, use_raw):\n        var_index = adata.raw.var.index\n    else:\n        var_index = adata.var.index\n    if basis is not None:\n        return _scatter_obs(**args)\n    if x is None or y is None:\n        raise ValueError('Either provide a `basis` or `x` and `y`.')\n    if (\n        (x in adata.obs.keys() or x in var_index)\n        and (y in adata.obs.keys() or y in var_index)\n        and (color is None or color in adata.obs.keys() or color in var_index)\n    ):\n        return _scatter_obs(**args)\n    if (\n        (x in adata.var.keys() or x in adata.obs.index)\n        and (y in adata.var.keys() or y in adata.obs.index)\n        and (color is None or color in adata.var.keys() or color in adata.obs.index)\n    ):\n        adata_T = adata.T\n        axs = _scatter_obs(\n            adata=adata_T,\n            **{name: val for name, val in args.items() if name != 'adata'},\n        )\n        # store .uns annotations that were added to the new adata object\n        adata.uns = adata_T.uns\n        return axs\n    raise ValueError(\n        '`x`, `y`, and potential `color` inputs must all '", "idx": 155}
{"project": "Scanpy", "commit_id": "241_scanpy_1.9.0__anndata.py__scatter_obs.py", "target": 1, "func": "def _scatter_obs(\n    adata: AnnData,\n    x=None,\n    y=None,\n    color=None,\n    use_raw=None,\n    layers=None,\n    sort_order=True,\n    alpha=None,\n    basis=None,\n    groups=None,\n    components=None,\n    projection: Literal['2d', '3d'] = '2d',\n    legend_loc='right margin',\n    legend_fontsize=None,\n    legend_fontweight=None,\n    legend_fontoutline=None,\n    color_map=None,\n    palette=None,\n    frameon=None,\n    right_margin=None,\n    left_margin=None,\n    size=None,\n    title=None,\n    show=None,\n    save=None,\n    ax=None,\n):\n    \"\"\"See docstring of scatter.\"\"\"\n    sanitize_anndata(adata)\n    from scipy.sparse import issparse\n\n    use_raw = _check_use_raw(adata, use_raw)\n\n    # Process layers\n    if layers in ['X', None] or (\n        isinstance(layers, str) and layers in adata.layers.keys()\n    ):\n        layers = (layers, layers, layers)\n    elif isinstance(layers, cabc.Collection) and len(layers) == 3:\n        layers = tuple(layers)\n        for layer in layers:\n            if layer not in adata.layers.keys() and layer not in ['X', None]:\n                raise ValueError(\n                    '`layers` should have elements that are '\n                    'either None or in adata.layers.keys().'\n                )\n    else:\n        raise ValueError(\n            \"`layers` should be a string or a collection of strings \"\n            f\"with length 3, had value '{layers}'\"\n        )\n    if use_raw and layers not in [('X', 'X', 'X'), (None, None, None)]:\n        ValueError('`use_raw` must be `False` if layers are used.')\n\n    if legend_loc not in VALID_LEGENDLOCS:\n        raise ValueError(\n            f'Invalid `legend_loc`, need to be one of: {VALID_LEGENDLOCS}.'\n        )\n    if components is None:\n        components = '1,2' if '2d' in projection else '1,2,3'\n    if isinstance(components, str):\n        components = components.split(',')\n    components = np.array(components).astype(int) - 1\n    # color can be a obs column name or a matplotlib color specification\n    keys = (\n        ['grey']\n        if color is None\n        else [color]\n        if isinstance(color, str) or is_color_like(color)\n        else color\n    )\n    if title is not None and isinstance(title, str):\n        title = [title]\n    highlights = adata.uns['highlights'] if 'highlights' in adata.uns else []\n    if basis is not None:\n        try:\n            # ignore the '0th' diffusion component\n            if basis == 'diffmap':\n                components += 1\n            Y = adata.obsm['X_' + basis][:, components]\n            # correct the component vector for use in labeling etc.\n            if basis == 'diffmap':\n                components -= 1\n        except KeyError:\n            raise KeyError(\n                f'compute coordinates using visualization tool {basis} first'\n            )\n    elif x is not None and y is not None:\n        if use_raw:\n            if x in adata.obs.columns:\n                x_arr = adata.obs_vector(x)\n            else:\n                x_arr = adata.raw.obs_vector(x)\n            if y in adata.obs.columns:\n                y_arr = adata.obs_vector(y)\n            else:\n                y_arr = adata.raw.obs_vector(y)\n        else:\n            x_arr = adata.obs_vector(x, layer=layers[0])\n            y_arr = adata.obs_vector(y, layer=layers[1])\n\n        Y = np.c_[x_arr, y_arr]\n    else:\n        raise ValueError('Either provide a `basis` or `x` and `y`.')\n\n    if size is None:\n        n = Y.shape[0]\n        size = 120000 / n\n\n    if legend_loc.startswith('on data') and legend_fontsize is None:\n        legend_fontsize = rcParams['legend.fontsize']\n    elif legend_fontsize is None:\n        legend_fontsize = rcParams['legend.fontsize']\n\n    palette_was_none = False\n    if palette is None:\n        palette_was_none = True\n    if isinstance(palette, cabc.Sequence):\n        if not is_color_like(palette[0]):\n            palettes = palette\n        else:\n            palettes = [palette]\n    else:\n        palettes = [palette for _ in range(len(keys))]\n    for i, palette in enumerate(palettes):\n        palettes[i] = _utils.default_palette(palette)\n\n    if basis is not None:\n        component_name = (\n            'DC'\n            if basis == 'diffmap'\n            else 'tSNE'\n            if basis == 'tsne'\n            else 'UMAP'\n            if basis == 'umap'\n            else 'PC'\n            if basis == 'pca'\n            else 'TriMap'\n            if basis == 'trimap'\n            else basis.replace('draw_graph_', '').upper()\n            if 'draw_graph' in basis\n            else basis\n        )\n    else:\n        component_name = None\n    axis_labels = (x, y) if component_name is None else None\n    show_ticks = True if component_name is None else False\n\n    # generate the colors\n    color_ids = []\n    categoricals = []\n    colorbars = []\n    for ikey, key in enumerate(keys):\n        c = 'white'\n        categorical = False  # by default, assume continuous or flat color\n        colorbar = None\n        # test whether we have categorial or continuous annotation\n        if key in adata.obs_keys():\n            if is_categorical_dtype(adata.obs[key]):\n                categorical = True\n            else:\n                c = adata.obs[key]\n        # coloring according to gene expression\n        elif use_raw and adata.raw is not None and key in adata.raw.var_names:\n            c = adata.raw.obs_vector(key)\n        elif key in adata.var_names:\n            c = adata.obs_vector(key, layer=layers[2])\n        elif is_color_like(key):  # a flat color\n            c = key\n            colorbar = False\n        else:\n            raise ValueError(\n                f'key {key!r} is invalid! pass valid observation annotation, '\n                f'one of {adata.obs_keys()} or a gene name {adata.var_names}'\n            )\n        if colorbar is None:\n            colorbar = not categorical\n        colorbars.append(colorbar)\n        if categorical:\n            categoricals.append(ikey)\n        color_ids.append(c)\n\n    if right_margin is None and len(categoricals) > 0:\n        if legend_loc == 'right margin':\n            right_margin = 0.5\n    if title is None and keys[0] is not None:\n        title = [\n            key.replace('_', ' ') if not is_color_like(key) else '' for key in keys\n        ]\n\n    axs = scatter_base(\n        Y,\n        title=title,\n        alpha=alpha,\n        component_name=component_name,\n        axis_labels=axis_labels,\n        component_indexnames=components + 1,\n        projection=projection,\n        colors=color_ids,\n        highlights=highlights,\n        colorbars=colorbars,\n        right_margin=right_margin,\n        left_margin=left_margin,\n        sizes=[size for _ in keys],\n        color_map=color_map,\n        show_ticks=show_ticks,\n        ax=ax,\n    )\n\n    def add_centroid(centroids, name, Y, mask):\n        Y_mask = Y[mask]\n        if Y_mask.shape[0] == 0:\n            return\n        median = np.median(Y_mask, axis=0)\n        i = np.argmin(np.sum(np.abs(Y_mask - median), axis=1))\n        centroids[name] = Y_mask[i]\n\n    # loop over all categorical annotation and plot it\n    for i, ikey in enumerate(categoricals):\n        palette = palettes[i]\n        key = keys[ikey]\n        _utils.add_colors_for_categorical_sample_annotation(\n            adata, key, palette, force_update_colors=not palette_was_none\n        )\n        # actually plot the groups\n        mask_remaining = np.ones(Y.shape[0], dtype=bool)\n        centroids = {}\n        if groups is None:\n            for iname, name in enumerate(adata.obs[key].cat.categories):\n                if name not in settings.categories_to_ignore:\n                    mask = scatter_group(\n                        axs[ikey],\n                        key,\n                        iname,\n                        adata,\n                        Y,\n                        projection,\n                        size=size,\n                        alpha=alpha,\n                    )\n                    mask_remaining[mask] = False\n                    if legend_loc.startswith('on data'):\n                        add_centroid(centroids, name, Y, mask)\n        else:\n            groups = [groups] if isinstance(groups, str) else groups\n            for name in groups:\n                if name not in set(adata.obs[key].cat.categories):\n                    raise ValueError(\n                        f'{name!r} is invalid! specify valid name, '\n                        f'one of {adata.obs[key].cat.categories}'\n                    )\n                else:\n                    iname = np.flatnonzero(\n                        adata.obs[key].cat.categories.values == name\n                    )[0]\n                    mask = scatter_group(\n                        axs[ikey],\n                        key,\n                        iname,\n                        adata,\n                        Y,\n                        projection,\n                        size=size,\n                        alpha=alpha,\n                    )\n                    if legend_loc.startswith('on data'):\n                        add_centroid(centroids, name, Y, mask)\n                    mask_remaining[mask] = False\n        if mask_remaining.sum() > 0:\n            data = [Y[mask_remaining, 0], Y[mask_remaining, 1]]\n            if projection == '3d':\n                data.append(Y[mask_remaining, 2])\n            axs[ikey].scatter(\n                *data,\n                marker='.',\n                c='lightgrey',\n                s=size,\n                edgecolors='none',\n                zorder=-1,\n            )\n        legend = None\n        if legend_loc.startswith('on data'):\n            if legend_fontweight is None:\n                legend_fontweight = 'bold'\n            if legend_fontoutline is not None:\n                path_effect = [\n                    patheffects.withStroke(linewidth=legend_fontoutline, foreground='w')\n                ]\n            else:\n                path_effect = None\n            for name, pos in centroids.items():\n                axs[ikey].text(\n                    pos[0],\n                    pos[1],\n                    name,\n                    weight=legend_fontweight,\n                    verticalalignment='center',\n                    horizontalalignment='center',\n                    fontsize=legend_fontsize,\n                    path_effects=path_effect,\n                )\n\n            all_pos = np.zeros((len(adata.obs[key].cat.categories), 2))\n            for iname, name in enumerate(adata.obs[key].cat.categories):\n                if name in centroids:\n                    all_pos[iname] = centroids[name]\n                else:\n                    all_pos[iname] = [np.nan, np.nan]\n            if legend_loc == 'on data export':\n                filename = settings.writedir / 'pos.csv'\n                logg.warning(f'exporting label positions to {filename}')\n                settings.writedir.mkdir(parents=True, exist_ok=True)\n                np.savetxt(filename, all_pos, delimiter=',')\n        elif legend_loc == 'right margin':\n            legend = axs[ikey].legend(\n                frameon=False,\n                loc='center left',\n                bbox_to_anchor=(1, 0.5),\n                ncol=(\n                    1\n                    if len(adata.obs[key].cat.categories) <= 14\n                    else 2\n                    if len(adata.obs[key].cat.categories) <= 30\n                    else 3\n                ),\n                fontsize=legend_fontsize,\n            )\n        elif legend_loc != 'none':\n            legend = axs[ikey].legend(\n                frameon=False, loc=legend_loc, fontsize=legend_fontsize\n            )\n        if legend is not None:\n            for handle in legend.legendHandles:\n                handle.set_sizes([300.0])\n\n    # draw a frame around the scatter\n    frameon = settings._frameon if frameon is None else frameon\n    if not frameon and x is None and y is None:\n        for ax in axs:\n            ax.set_xlabel('')\n            ax.set_ylabel('')\n            ax.set_frame_on(False)\n\n    show = settings.autoshow if show is None else show\n    _utils.savefig_or_show('scatter' if basis is None else basis, show=show, save=save)\n    if not show:\n        return axs if len(keys) > 1 else axs[0]", "idx": 156}
{"project": "Scanpy", "commit_id": "242_scanpy_1.9.0__anndata.py_ranking.py", "target": 0, "func": "def ranking(\n    adata: AnnData,\n    attr: Literal['var', 'obs', 'uns', 'varm', 'obsm'],\n    keys: Union[str, Sequence[str]],\n    dictionary=None,\n    indices=None,\n    labels=None,\n    color='black',\n    n_points=30,\n    log=False,\n    include_lowest=False,\n    show=None,\n):\n    \"\"\"\\\n    Plot rankings.\n\n    See, for example, how this is used in pl.pca_ranking.\n\n    Parameters\n    ----------\n    adata\n        The data.\n    attr\n        The attribute of AnnData that contains the score.\n    keys\n        The scores to look up an array from the attribute of adata.\n\n    Returns\n    -------\n    Returns matplotlib gridspec with access to the axes.\n    \"\"\"\n    if isinstance(keys, str) and indices is not None:\n        scores = getattr(adata, attr)[keys][:, indices]\n        keys = [f'{keys[:-1]}{i + 1}' for i in indices]\n    else:\n        if dictionary is None:\n            scores = getattr(adata, attr)[keys]\n        else:\n            scores = getattr(adata, attr)[dictionary][keys]\n    n_panels = len(keys) if isinstance(keys, list) else 1\n    if n_panels == 1:\n        scores, keys = scores[:, None], [keys]\n    if log:\n        scores = np.log(scores)\n    if labels is None:\n        labels = (\n            adata.var_names\n            if attr in {'var', 'varm'}\n            else np.arange(scores.shape[0]).astype(str)\n        )\n    if isinstance(labels, str):\n        labels = [labels + str(i + 1) for i in range(scores.shape[0])]\n    if n_panels <= 5:\n        n_rows, n_cols = 1, n_panels\n    else:\n        n_rows, n_cols = 2, int(n_panels / 2 + 0.5)\n    _ = pl.figure(\n        figsize=(\n            n_cols * rcParams['figure.figsize'][0],\n            n_rows * rcParams['figure.figsize'][1],\n        )\n    )\n    left, bottom = 0.2 / n_cols, 0.13 / n_rows\n    gs = gridspec.GridSpec(\n        wspace=0.2,\n        nrows=n_rows,\n        ncols=n_cols,\n        left=left,\n        bottom=bottom,\n        right=1 - (n_cols - 1) * left - 0.01 / n_cols,\n        top=1 - (n_rows - 1) * bottom - 0.1 / n_rows,\n    )\n    for iscore, score in enumerate(scores.T):\n        pl.subplot(gs[iscore])\n        order_scores = np.argsort(score)[::-1]\n        if not include_lowest:\n            indices = order_scores[: n_points + 1]\n        else:\n            indices = order_scores[: n_points // 2]\n            neg_indices = order_scores[-(n_points - (n_points // 2)) :]\n        txt_args = dict(\n            color=color,\n            rotation='vertical',\n            verticalalignment='bottom',\n            horizontalalignment='center',\n            fontsize=8,\n        )\n        for ig, g in enumerate(indices):\n            pl.text(ig, score[g], labels[g], **txt_args)\n        if include_lowest:\n            score_mid = (score[g] + score[neg_indices[0]]) / 2\n            if (len(indices) + len(neg_indices)) < len(order_scores):\n                pl.text(len(indices), score_mid, '\u22ee', **txt_args)\n                for ig, g in enumerate(neg_indices):\n                    pl.text(ig + len(indices) + 2, score[g], labels[g], **txt_args)\n            else:\n                for ig, g in enumerate(neg_indices):\n                    pl.text(ig + len(indices), score[g], labels[g], **txt_args)\n            pl.xticks([])\n        pl.title(keys[iscore].replace('_', ' '))\n        if n_panels <= 5 or iscore > n_cols:\n            pl.xlabel('ranking')\n        pl.xlim(-0.9, n_points + 0.9 + (1 if include_lowest else 0))\n        score_min, score_max = (\n            np.min(score[neg_indices if include_lowest else indices]),\n            np.max(score[indices]),\n        )\n        pl.ylim(\n            (0.95 if score_min > 0 else 1.05) * score_min,\n            (1.05 if score_max > 0 else 0.95) * score_max,\n        )\n    show = settings.autoshow if show is None else show\n    if not show:\n        return gs", "idx": 157}
{"project": "Scanpy", "commit_id": "243_scanpy_1.9.0__anndata.py_violin.py", "target": 0, "func": "def violin(\n    adata: AnnData,\n    keys: Union[str, Sequence[str]],\n    groupby: Optional[str] = None,\n    log: bool = False,\n    use_raw: Optional[bool] = None,\n    stripplot: bool = True,\n    jitter: Union[float, bool] = True,\n    size: int = 1,\n    layer: Optional[str] = None,\n    scale: Literal['area', 'count', 'width'] = 'width',\n    order: Optional[Sequence[str]] = None,\n    multi_panel: Optional[bool] = None,\n    xlabel: str = '',\n    ylabel: Optional[Union[str, Sequence[str]]] = None,\n    rotation: Optional[float] = None,\n    show: Optional[bool] = None,\n    save: Union[bool, str, None] = None,\n    ax: Optional[Axes] = None,\n    **kwds,\n):\n    \"\"\"\\\n    Violin plot.\n\n    Wraps :func:`seaborn.violinplot` for :class:`~anndata.AnnData`.\n\n    Parameters\n    ----------\n    adata\n        Annotated data matrix.\n    keys\n        Keys for accessing variables of `.var_names` or fields of `.obs`.\n    groupby\n        The key of the observation grouping to consider.\n    log\n        Plot on logarithmic axis.\n    use_raw\n        Whether to use `raw` attribute of `adata`. Defaults to `True` if `.raw` is present.\n    stripplot\n        Add a stripplot on top of the violin plot.\n        See :func:`~seaborn.stripplot`.\n    jitter\n        Add jitter to the stripplot (only when stripplot is True)\n        See :func:`~seaborn.stripplot`.\n    size\n        Size of the jitter points.\n    layer\n        Name of the AnnData object layer that wants to be plotted. By\n        default adata.raw.X is plotted. If `use_raw=False` is set,\n        then `adata.X` is plotted. If `layer` is set to a valid layer name,\n        then the layer is plotted. `layer` takes precedence over `use_raw`.\n    scale\n        The method used to scale the width of each violin.\n        If 'width' (the default), each violin will have the same width.\n        If 'area', each violin will have the same area.\n        If 'count', a violin\u2019s width corresponds to the number of observations.\n    order\n        Order in which to show the categories.\n    multi_panel\n        Display keys in multiple panels also when `groupby is not None`.\n    xlabel\n        Label of the x axis. Defaults to `groupby` if `rotation` is `None`,\n        otherwise, no label is shown.\n    ylabel\n        Label of the y axis. If `None` and `groupby` is `None`, defaults\n        to `'value'`. If `None` and `groubpy` is not `None`, defaults to `keys`.\n    rotation\n        Rotation of xtick labels.\n    {show_save_ax}\n    **kwds\n        Are passed to :func:`~seaborn.violinplot`.\n\n    Returns\n    -------\n    A :class:`~matplotlib.axes.Axes` object if `ax` is `None` else `None`.\n\n    Examples\n    --------\n\n    .. plot::\n        :context: close-figs\n\n        import scanpy as sc\n        adata = sc.datasets.pbmc68k_reduced()\n        sc.pl.violin(adata, keys='S_score')\n\n    Plot by category. Rotate x-axis labels so that they do not overlap.\n\n    .. plot::\n        :context: close-figs\n\n        sc.pl.violin(adata, keys='S_score', groupby='bulk_labels', rotation=90)\n\n    Set order of categories to be plotted or select specific categories to be plotted.\n\n    .. plot::\n        :context: close-figs\n\n        groupby_order = ['CD34+', 'CD19+ B']\n        sc.pl.violin(adata, keys='S_score', groupby='bulk_labels', rotation=90,\n            order=groupby_order)\n\n    Plot multiple keys.\n\n    .. plot::\n        :context: close-figs\n\n        sc.pl.violin(adata, keys=['S_score', 'G2M_score'], groupby='bulk_labels',\n            rotation=90)\n\n    For large datasets consider omitting the overlaid scatter plot.\n\n    .. plot::\n        :context: close-figs\n\n        sc.pl.violin(adata, keys='S_score', stripplot=False)\n\n    .. currentmodule:: scanpy\n\n    See also\n    --------\n    pl.stacked_violin\n    \"\"\"\n    import seaborn as sns  # Slow import, only import if called\n\n    sanitize_anndata(adata)\n    use_raw = _check_use_raw(adata, use_raw)\n    if isinstance(keys, str):\n        keys = [keys]\n    keys = list(OrderedDict.fromkeys(keys))  # remove duplicates, preserving the order\n\n    if isinstance(ylabel, (str, type(None))):\n        ylabel = [ylabel] * (1 if groupby is None else len(keys))\n    if groupby is None:\n        if len(ylabel) != 1:\n            raise ValueError(\n                f'Expected number of y-labels to be `1`, found `{len(ylabel)}`.'\n            )\n    elif len(ylabel) != len(keys):\n        raise ValueError(\n            f'Expected number of y-labels to be `{len(keys)}`, '\n            f'found `{len(ylabel)}`.'\n        )\n\n    if groupby is not None:\n        obs_df = get.obs_df(adata, keys=[groupby] + keys, layer=layer, use_raw=use_raw)\n        if kwds.get('palette', None) is None:\n            if not is_categorical_dtype(adata.obs[groupby]):\n                raise ValueError(\n                    f'The column `adata.obs[{groupby!r}]` needs to be categorical, '\n                    f'but is of dtype {adata.obs[groupby].dtype}.'\n                )\n            _utils.add_colors_for_categorical_sample_annotation(adata, groupby)\n            kwds['palette'] = dict(\n                zip(obs_df[groupby].cat.categories, adata.uns[f'{groupby}_colors'])\n            )\n    else:\n        obs_df = get.obs_df(adata, keys=keys, layer=layer, use_raw=use_raw)\n    if groupby is None:\n        obs_tidy = pd.melt(obs_df, value_vars=keys)\n        x = 'variable'\n        ys = ['value']\n    else:\n        obs_tidy = obs_df\n        x = groupby\n        ys = keys\n\n    if multi_panel and groupby is None and len(ys) == 1:\n        # This is a quick and dirty way for adapting scales across several\n        # keys if groupby is None.\n        y = ys[0]\n\n        g = sns.catplot(\n            y=y,\n            data=obs_tidy,\n            kind=\"violin\",\n            scale=scale,\n            col=x,\n            col_order=keys,\n            sharey=False,\n            order=keys,\n            cut=0,\n            inner=None,\n            **kwds,\n        )\n\n        if stripplot:\n            grouped_df = obs_tidy.groupby(x)\n            for ax_id, key in zip(range(g.axes.shape[1]), keys):\n                sns.stripplot(\n                    y=y,\n                    data=grouped_df.get_group(key),\n                    jitter=jitter,\n                    size=size,\n                    color=\"black\",\n                    ax=g.axes[0, ax_id],\n                )\n        if log:\n            g.set(yscale='log')\n        g.set_titles(col_template='{col_name}').set_xlabels('')\n        if rotation is not None:\n            for ax in g.axes[0]:\n                ax.tick_params(axis='x', labelrotation=rotation)\n    else:\n        # set by default the violin plot cut=0 to limit the extend\n        # of the violin plot (see stacked_violin code) for more info.\n        kwds.setdefault('cut', 0)\n        kwds.setdefault('inner')\n\n        if ax is None:\n            axs, _, _, _ = setup_axes(\n                ax=ax,\n                panels=['x'] if groupby is None else keys,\n                show_ticks=True,\n                right_margin=0.3,\n            )\n        else:\n            axs = [ax]\n        for ax, y, ylab in zip(axs, ys, ylabel):\n            ax = sns.violinplot(\n                x=x,\n                y=y,\n                data=obs_tidy,\n                order=order,\n                orient='vertical',\n                scale=scale,\n                ax=ax,\n                **kwds,\n            )\n            if stripplot:\n                ax = sns.stripplot(\n                    x=x,\n                    y=y,\n                    data=obs_tidy,\n                    order=order,\n                    jitter=jitter,\n                    color='black',\n                    size=size,\n                    ax=ax,\n                )\n            if xlabel == '' and groupby is not None and rotation is None:\n                xlabel = groupby.replace('_', ' ')\n            ax.set_xlabel(xlabel)\n            if ylab is not None:\n                ax.set_ylabel(ylab)\n\n            if log:\n                ax.set_yscale('log')\n            if rotation is not None:\n                ax.tick_params(axis='x', labelrotation=rotation)\n    show = settings.autoshow if show is None else show\n    _utils.savefig_or_show('violin', show=show, save=save)\n    if not show:\n        if multi_panel and groupby is None and len(ys) == 1:\n            return g\n        elif len(axs) == 1:\n            return axs[0]\n        else:\n            return axs", "idx": 158}
{"project": "Scanpy", "commit_id": "244_scanpy_1.9.0__anndata.py_clustermap.py", "target": 0, "func": "def clustermap(\n    adata: AnnData,\n    obs_keys: str = None,\n    use_raw: Optional[bool] = None,\n    show: Optional[bool] = None,\n    save: Union[bool, str, None] = None,\n    **kwds,\n):\n    \"\"\"\\\n    Hierarchically-clustered heatmap.\n\n    Wraps :func:`seaborn.clustermap` for :class:`~anndata.AnnData`.\n\n    Parameters\n    ----------\n    adata\n        Annotated data matrix.\n    obs_keys\n        Categorical annotation to plot with a different color map.\n        Currently, only a single key is supported.\n    use_raw\n        Whether to use `raw` attribute of `adata`. Defaults to `True` if `.raw` is present.\n    {show_save_ax}\n    **kwds\n        Keyword arguments passed to :func:`~seaborn.clustermap`.\n\n    Returns\n    -------\n    If `show` is `False`, a :class:`~seaborn.ClusterGrid` object\n    (see :func:`~seaborn.clustermap`).\n\n    Examples\n    --------\n    Soon to come with figures. In the meanwile, see :func:`~seaborn.clustermap`.\n\n    >>> import scanpy as sc\n    >>> adata = sc.datasets.krumsiek11()\n    >>> sc.pl.clustermap(adata, obs_keys='cell_type')\n    \"\"\"\n    import seaborn as sns  # Slow import, only import if called\n\n    if not isinstance(obs_keys, (str, type(None))):\n        raise ValueError('Currently, only a single key is supported.')\n    sanitize_anndata(adata)\n    use_raw = _check_use_raw(adata, use_raw)\n    X = adata.raw.X if use_raw else adata.X\n    if issparse(X):\n        X = X.toarray()\n    df = pd.DataFrame(X, index=adata.obs_names, columns=adata.var_names)\n    if obs_keys is not None:\n        row_colors = adata.obs[obs_keys]\n        _utils.add_colors_for_categorical_sample_annotation(adata, obs_keys)\n        # do this more efficiently... just a quick solution\n        lut = dict(zip(row_colors.cat.categories, adata.uns[obs_keys + '_colors']))\n        row_colors = adata.obs[obs_keys].map(lut)\n        g = sns.clustermap(df, row_colors=row_colors.values, **kwds)\n    else:\n        g = sns.clustermap(df, **kwds)\n    show = settings.autoshow if show is None else show\n    _utils.savefig_or_show('clustermap', show=show, save=save)\n    if show:\n        pl.show()\n    else:\n        return g", "idx": 159}
{"project": "Scanpy", "commit_id": "245_scanpy_1.9.0__anndata.py_heatmap.py", "target": 0, "func": "def heatmap(\n    adata: AnnData,\n    var_names: Union[_VarNames, Mapping[str, _VarNames]],\n    groupby: Union[str, Sequence[str]],\n    use_raw: Optional[bool] = None,\n    log: bool = False,\n    num_categories: int = 7,\n    dendrogram: Union[bool, str] = False,\n    gene_symbols: Optional[str] = None,\n    var_group_positions: Optional[Sequence[Tuple[int, int]]] = None,\n    var_group_labels: Optional[Sequence[str]] = None,\n    var_group_rotation: Optional[float] = None,\n    layer: Optional[str] = None,\n    standard_scale: Optional[Literal['var', 'obs']] = None,\n    swap_axes: bool = False,\n    show_gene_labels: Optional[bool] = None,\n    show: Optional[bool] = None,\n    save: Union[str, bool, None] = None,\n    figsize: Optional[Tuple[float, float]] = None,\n    vmin: Optional[float] = None,\n    vmax: Optional[float] = None,\n    vcenter: Optional[float] = None,\n    norm: Optional[Normalize] = None,\n    **kwds,\n):\n    \"\"\"\\\n    Heatmap of the expression values of genes.\n\n    If `groupby` is given, the heatmap is ordered by the respective group. For\n    example, a list of marker genes can be plotted, ordered by clustering. If\n    the `groupby` observation annotation is not categorical the observation\n    annotation is turned into a categorical by binning the data into the number\n    specified in `num_categories`.\n\n    Parameters\n    ----------\n    {common_plot_args}\n    standard_scale\n        Whether or not to standardize that dimension between 0 and 1, meaning for each variable or observation,\n        subtract the minimum and divide each by its maximum.\n    swap_axes\n         By default, the x axis contains `var_names` (e.g. genes) and the y axis the `groupby`\n         categories (if any). By setting `swap_axes` then x are the `groupby` categories and y the `var_names`.\n    show_gene_labels\n         By default gene labels are shown when there are 50 or less genes. Otherwise the labels are removed.\n    {show_save_ax}\n    {vminmax}\n    **kwds\n        Are passed to :func:`matplotlib.pyplot.imshow`.\n\n    Returns\n    -------\n    List of :class:`~matplotlib.axes.Axes`\n\n    Examples\n    -------\n    .. plot::\n        :context: close-figs\n\n        import scanpy as sc\n        adata = sc.datasets.pbmc68k_reduced()\n        markers = ['C1QA', 'PSAP', 'CD79A', 'CD79B', 'CST3', 'LYZ']\n        sc.pl.heatmap(adata, markers, groupby='bulk_labels', swap_axes=True)\n\n    .. currentmodule:: scanpy\n\n    See also\n    --------\n    pl.rank_genes_groups_heatmap\n    tl.rank_genes_groups\n    \"\"\"\n    var_names, var_group_labels, var_group_positions = _check_var_names_type(\n        var_names, var_group_labels, var_group_positions\n    )\n\n    categories, obs_tidy = _prepare_dataframe(\n        adata,\n        var_names,\n        groupby,\n        use_raw,\n        log,\n        num_categories,\n        gene_symbols=gene_symbols,\n        layer=layer,\n    )\n\n    # check if var_group_labels are a subset of categories:\n    if var_group_labels is not None:\n        if set(var_group_labels).issubset(categories):\n            var_groups_subset_of_groupby = True\n        else:\n            var_groups_subset_of_groupby = False\n\n    if standard_scale == 'obs':\n        obs_tidy = obs_tidy.sub(obs_tidy.min(1), axis=0)\n        obs_tidy = obs_tidy.div(obs_tidy.max(1), axis=0).fillna(0)\n    elif standard_scale == 'var':\n        obs_tidy -= obs_tidy.min(0)\n        obs_tidy = (obs_tidy / obs_tidy.max(0)).fillna(0)\n    elif standard_scale is None:\n        pass\n    else:\n        logg.warning('Unknown type for standard_scale, ignored')\n\n    if groupby is None or len(categories) <= 1:\n        categorical = False\n        # dendrogram can only be computed  between groupby categories\n        dendrogram = False\n    else:\n        categorical = True\n        # get categories colors\n        if isinstance(groupby, str) and is_categorical_dtype(adata.obs[groupby]):\n            # saved category colors only work when groupby is valid adata.obs\n            # categorical column. When groupby is a numerical column\n            # or when groupby is a list of columns the colors are assigned on the fly,\n            # which may create inconsistencies in multiple runs that require sorting\n            # of the categories (eg. when dendrogram is plotted).\n            if groupby + \"_colors\" not in adata.uns:\n                # if colors are not found, assign a new palette\n                # and save it using the same code for embeddings\n                from ._tools.scatterplots import _get_palette\n\n                _get_palette(adata, groupby)\n            groupby_colors = adata.uns[groupby + \"_colors\"]\n        else:\n            # this case happen when adata.obs[groupby] is numeric\n            # the values are converted into a category on the fly\n            groupby_colors = None\n\n    if dendrogram:\n        dendro_data = _reorder_categories_after_dendrogram(\n            adata,\n            groupby,\n            dendrogram,\n            var_names=var_names,\n            var_group_labels=var_group_labels,\n            var_group_positions=var_group_positions,\n            categories=categories,\n        )\n\n        var_group_labels = dendro_data['var_group_labels']\n        var_group_positions = dendro_data['var_group_positions']\n\n        # reorder obs_tidy\n        if dendro_data['var_names_idx_ordered'] is not None:\n            obs_tidy = obs_tidy.iloc[:, dendro_data['var_names_idx_ordered']]\n            var_names = [var_names[x] for x in dendro_data['var_names_idx_ordered']]\n\n        obs_tidy.index = obs_tidy.index.reorder_categories(\n            [categories[x] for x in dendro_data['categories_idx_ordered']],\n            ordered=True,\n        )\n\n        # reorder groupby colors\n        if groupby_colors is not None:\n            groupby_colors = [\n                groupby_colors[x] for x in dendro_data['categories_idx_ordered']\n            ]\n\n    if show_gene_labels is None:\n        if len(var_names) <= 50:\n            show_gene_labels = True\n        else:\n            show_gene_labels = False\n            logg.warning(\n                'Gene labels are not shown when more than 50 genes are visualized. '\n                'To show gene labels set `show_gene_labels=True`'\n            )\n    if categorical:\n        obs_tidy = obs_tidy.sort_index()\n\n    colorbar_width = 0.2\n    norm = check_colornorm(vmin, vmax, vcenter, norm)\n\n    if not swap_axes:\n        # define a layout of 2 rows x 4 columns\n        # first row is for 'brackets' (if no brackets needed, the height of this row\n        # is zero) second row is for main content. This second row is divided into\n        # three axes:\n        #   first ax is for the categories defined by `groupby`\n        #   second ax is for the heatmap\n        #   third ax is for the dendrogram\n        #   fourth ax is for colorbar\n\n        dendro_width = 1 if dendrogram else 0\n        groupby_width = 0.2 if categorical else 0\n        if figsize is None:\n            height = 6\n            if show_gene_labels:\n                heatmap_width = len(var_names) * 0.3\n            else:\n                heatmap_width = 8\n            width = heatmap_width + dendro_width + groupby_width\n        else:\n            width, height = figsize\n            heatmap_width = width - (dendro_width + groupby_width)\n\n        if var_group_positions is not None and len(var_group_positions) > 0:\n            # add some space in case 'brackets' want to be plotted on top of the image\n            height_ratios = [0.15, height]\n        else:\n            height_ratios = [0, height]\n\n        width_ratios = [\n            groupby_width,\n            heatmap_width,\n            dendro_width,\n            colorbar_width,\n        ]\n        fig = pl.figure(figsize=(width, height))\n\n        axs = gridspec.GridSpec(\n            nrows=2,\n            ncols=4,\n            width_ratios=width_ratios,\n            wspace=0.15 / width,\n            hspace=0.13 / height,\n            height_ratios=height_ratios,\n        )\n\n        heatmap_ax = fig.add_subplot(axs[1, 1])\n        kwds.setdefault('interpolation', 'nearest')\n        im = heatmap_ax.imshow(obs_tidy.values, aspect='auto', norm=norm, **kwds)\n\n        heatmap_ax.set_ylim(obs_tidy.shape[0] - 0.5, -0.5)\n        heatmap_ax.set_xlim(-0.5, obs_tidy.shape[1] - 0.5)\n        heatmap_ax.tick_params(axis='y', left=False, labelleft=False)\n        heatmap_ax.set_ylabel('')\n        heatmap_ax.grid(False)\n\n        if show_gene_labels:\n            heatmap_ax.tick_params(axis='x', labelsize='small')\n            heatmap_ax.set_xticks(np.arange(len(var_names)))\n            heatmap_ax.set_xticklabels(var_names, rotation=90)\n        else:\n            heatmap_ax.tick_params(axis='x', labelbottom=False, bottom=False)\n        # plot colorbar\n        _plot_colorbar(im, fig, axs[1, 3])\n\n        if categorical:\n            groupby_ax = fig.add_subplot(axs[1, 0])\n            (\n                label2code,\n                ticks,\n                labels,\n                groupby_cmap,\n                norm,\n            ) = _plot_categories_as_colorblocks(\n                groupby_ax, obs_tidy, colors=groupby_colors, orientation='left'\n            )\n\n            # add lines to main heatmap\n            line_positions = (\n                np.cumsum(obs_tidy.index.value_counts(sort=False))[:-1] - 0.5\n            )\n            heatmap_ax.hlines(\n                line_positions,\n                -0.5,\n                len(var_names) - 0.5,\n                lw=1,\n                color='black',\n                zorder=10,\n                clip_on=False,\n            )\n\n        if dendrogram:\n            dendro_ax = fig.add_subplot(axs[1, 2], sharey=heatmap_ax)\n            _plot_dendrogram(\n                dendro_ax, adata, groupby, ticks=ticks, dendrogram_key=dendrogram\n            )\n\n        # plot group legends on top of heatmap_ax (if given)\n        if var_group_positions is not None and len(var_group_positions) > 0:\n            gene_groups_ax = fig.add_subplot(axs[0, 1], sharex=heatmap_ax)\n            _plot_gene_groups_brackets(\n                gene_groups_ax,\n                group_positions=var_group_positions,\n                group_labels=var_group_labels,\n                rotation=var_group_rotation,\n                left_adjustment=-0.3,\n                right_adjustment=0.3,\n            )\n\n    # swap axes case\n    else:\n        # define a layout of 3 rows x 3 columns\n        # The first row is for the dendrogram (if not dendrogram height is zero)\n        # second row is for main content. This col is divided into three axes:\n        #   first ax is for the heatmap\n        #   second ax is for 'brackets' if any (othwerise width is zero)\n        #   third ax is for colorbar\n\n        dendro_height = 0.8 if dendrogram else 0\n        groupby_height = 0.13 if categorical else 0\n        if figsize is None:\n            if show_gene_labels:\n                heatmap_height = len(var_names) * 0.18\n            else:\n                heatmap_height = 4\n            width = 10\n            height = heatmap_height + dendro_height + groupby_height\n        else:\n            width, height = figsize\n            heatmap_height = height - (dendro_height + groupby_height)\n\n        height_ratios = [dendro_height, heatmap_height, groupby_height]\n\n        if var_group_positions is not None and len(var_group_positions) > 0:\n            # add some space in case 'brackets' want to be plotted on top of the image\n            width_ratios = [width, 0.14, colorbar_width]\n        else:\n            width_ratios = [width, 0, colorbar_width]\n\n        fig = pl.figure(figsize=(width, height))\n        axs = gridspec.GridSpec(\n            nrows=3,\n            ncols=3,\n            wspace=0.25 / width,\n            hspace=0.3 / height,\n            width_ratios=width_ratios,\n            height_ratios=height_ratios,\n        )\n\n        # plot heatmap\n        heatmap_ax = fig.add_subplot(axs[1, 0])\n\n        kwds.setdefault('interpolation', 'nearest')\n        im = heatmap_ax.imshow(obs_tidy.T.values, aspect='auto', norm=norm, **kwds)\n        heatmap_ax.set_xlim(0 - 0.5, obs_tidy.shape[0] - 0.5)\n        heatmap_ax.set_ylim(obs_tidy.shape[1] - 0.5, -0.5)\n        heatmap_ax.tick_params(axis='x', bottom=False, labelbottom=False)\n        heatmap_ax.set_xlabel('')\n        heatmap_ax.grid(False)\n        if show_gene_labels:\n            heatmap_ax.tick_params(axis='y', labelsize='small', length=1)\n            heatmap_ax.set_yticks(np.arange(len(var_names)))\n            heatmap_ax.set_yticklabels(var_names, rotation=0)\n        else:\n            heatmap_ax.tick_params(axis='y', labelleft=False, left=False)\n\n        if categorical:\n            groupby_ax = fig.add_subplot(axs[2, 0])\n            (\n                label2code,\n                ticks,\n                labels,\n                groupby_cmap,\n                norm,\n            ) = _plot_categories_as_colorblocks(\n                groupby_ax, obs_tidy, colors=groupby_colors, orientation='bottom'\n            )\n            # add lines to main heatmap\n            line_positions = (\n                np.cumsum(obs_tidy.index.value_counts(sort=False))[:-1] - 0.5\n            )\n            heatmap_ax.vlines(\n                line_positions,\n                -0.5,\n                len(var_names) - 0.5,\n                lw=1,\n                color='black',\n                zorder=10,\n                clip_on=False,\n            )\n\n        if dendrogram:\n            dendro_ax = fig.add_subplot(axs[0, 0], sharex=heatmap_ax)\n            _plot_dendrogram(\n                dendro_ax,\n                adata,\n                groupby,\n                dendrogram_key=dendrogram,\n                ticks=ticks,\n                orientation='top',\n            )\n\n        # plot group legends next to the heatmap_ax (if given)\n        if var_group_positions is not None and len(var_group_positions) > 0:\n            gene_groups_ax = fig.add_subplot(axs[1, 1])\n            arr = []\n            for idx, (label, pos) in enumerate(\n                zip(var_group_labels, var_group_positions)\n            ):\n                if var_groups_subset_of_groupby:\n                    label_code = label2code[label]\n                else:\n                    label_code = idx\n                arr += [label_code] * (pos[1] + 1 - pos[0])\n            gene_groups_ax.imshow(\n                np.array([arr]).T, aspect='auto', cmap=groupby_cmap, norm=norm\n            )\n            gene_groups_ax.axis('off')\n\n        # plot colorbar\n        _plot_colorbar(im, fig, axs[1, 2])\n\n    return_ax_dict = {'heatmap_ax': heatmap_ax}\n    if categorical:\n        return_ax_dict['groupby_ax'] = groupby_ax\n    if dendrogram:\n        return_ax_dict['dendrogram_ax'] = dendro_ax\n    if var_group_positions is not None and len(var_group_positions) > 0:\n        return_ax_dict['gene_groups_ax'] = gene_groups_ax\n\n    _utils.savefig_or_show('heatmap', show=show, save=save)\n    show = settings.autoshow if show is None else show\n    if not show:\n        return return_ax_dict", "idx": 160}
{"project": "Scanpy", "commit_id": "246_scanpy_1.9.0__anndata.py_tracksplot.py", "target": 0, "func": "def tracksplot(\n    adata: AnnData,\n    var_names: Union[_VarNames, Mapping[str, _VarNames]],\n    groupby: Union[str, Sequence[str]],\n    use_raw: Optional[bool] = None,\n    log: bool = False,\n    dendrogram: Union[bool, str] = False,\n    gene_symbols: Optional[str] = None,\n    var_group_positions: Optional[Sequence[Tuple[int, int]]] = None,\n    var_group_labels: Optional[Sequence[str]] = None,\n    layer: Optional[str] = None,\n    show: Optional[bool] = None,\n    save: Union[str, bool, None] = None,\n    figsize: Optional[Tuple[float, float]] = None,\n    **kwds,\n):\n    \"\"\"\\\n    In this type of plot each var_name is plotted as a filled line plot where the\n    y values correspond to the var_name values and x is each of the cells. Best results\n    are obtained when using raw counts that are not log.\n\n    `groupby` is required to sort and order the values using the respective group\n    and should be a categorical value.\n\n    Parameters\n    ----------\n    {common_plot_args}\n    {show_save_ax}\n    **kwds\n        Are passed to :func:`~seaborn.heatmap`.\n\n    Returns\n    -------\n    A list of :class:`~matplotlib.axes.Axes`.\n\n    Examples\n    --------\n    >>> import scanpy as sc\n    >>> adata = sc.datasets.pbmc68k_reduced()\n    >>> markers = ['C1QA', 'PSAP', 'CD79A', 'CD79B', 'CST3', 'LYZ']\n    >>> sc.pl.tracksplot(adata, markers, 'bulk_labels', dendrogram=True)\n\n    Using var_names as dict:\n\n    >>> markers = {{'T-cell': 'CD3D', 'B-cell': 'CD79A', 'myeloid': 'CST3'}}\n    >>> sc.pl.heatmap(adata, markers, groupby='bulk_labels', dendrogram=True)\n\n    .. currentmodule:: scanpy\n\n    See also\n    --------\n    pl.rank_genes_groups_tracksplot: to plot marker genes identified using the :func:`~scanpy.tl.rank_genes_groups` function.\n    \"\"\"\n\n    if groupby not in adata.obs_keys() or adata.obs[groupby].dtype.name != 'category':\n        raise ValueError(\n            'groupby has to be a valid categorical observation. '\n            f'Given value: {groupby}, valid categorical observations: '\n            f'{[x for x in adata.obs_keys() if adata.obs[x].dtype.name == \"category\"]}'\n        )\n\n    var_names, var_group_labels, var_group_positions = _check_var_names_type(\n        var_names, var_group_labels, var_group_positions\n    )\n\n    categories, obs_tidy = _prepare_dataframe(\n        adata,\n        var_names,\n        groupby,\n        use_raw,\n        log,\n        None,\n        gene_symbols=gene_symbols,\n        layer=layer,\n    )\n\n    # get categories colors:\n    if groupby + \"_colors\" not in adata.uns:\n        from ._utils import _set_default_colors_for_categorical_obs\n\n        _set_default_colors_for_categorical_obs(adata, groupby)\n    groupby_colors = adata.uns[groupby + \"_colors\"]\n\n    if dendrogram:\n        # compute dendrogram if needed and reorder\n        # rows and columns to match leaves order.\n        dendro_data = _reorder_categories_after_dendrogram(\n            adata,\n            groupby,\n            dendrogram,\n            var_names=var_names,\n            var_group_labels=var_group_labels,\n            var_group_positions=var_group_positions,\n            categories=categories,\n        )\n        # reorder obs_tidy\n        if dendro_data['var_names_idx_ordered'] is not None:\n            obs_tidy = obs_tidy.iloc[:, dendro_data['var_names_idx_ordered']]\n            var_names = [var_names[x] for x in dendro_data['var_names_idx_ordered']]\n\n        obs_tidy.index = obs_tidy.index.reorder_categories(\n            [categories[x] for x in dendro_data['categories_idx_ordered']],\n            ordered=True,\n        )\n        categories = [categories[x] for x in dendro_data['categories_idx_ordered']]\n\n        groupby_colors = [\n            groupby_colors[x] for x in dendro_data['categories_idx_ordered']\n        ]\n\n    obs_tidy = obs_tidy.sort_index()\n\n    # obtain the start and end of each category and make\n    # a list of ranges that will be used to plot a different\n    # color\n    cumsum = [0] + list(np.cumsum(obs_tidy.index.value_counts(sort=False)))\n    x_values = [(x, y) for x, y in zip(cumsum[:-1], cumsum[1:])]\n\n    dendro_height = 1 if dendrogram else 0\n\n    groupby_height = 0.24\n    # +2 because of dendrogram on top and categories at bottom\n    num_rows = len(var_names) + 2\n    if figsize is None:\n        width = 12\n        track_height = 0.25\n    else:\n        width, height = figsize\n        track_height = (height - (dendro_height + groupby_height)) / len(var_names)\n\n    height_ratios = [dendro_height] + [track_height] * len(var_names) + [groupby_height]\n    height = sum(height_ratios)\n\n    obs_tidy = obs_tidy.T\n\n    fig = pl.figure(figsize=(width, height))\n    axs = gridspec.GridSpec(\n        ncols=2,\n        nrows=num_rows,\n        wspace=1.0 / width,\n        hspace=0,\n        height_ratios=height_ratios,\n        width_ratios=[width, 0.14],\n    )\n    axs_list = []\n    first_ax = None\n    for idx, var in enumerate(var_names):\n        ax_idx = idx + 1  # this is because of the dendrogram\n        if first_ax is None:\n            ax = fig.add_subplot(axs[ax_idx, 0])\n            first_ax = ax\n        else:\n            ax = fig.add_subplot(axs[ax_idx, 0], sharex=first_ax)\n        axs_list.append(ax)\n        for cat_idx, category in enumerate(categories):\n            x_start, x_end = x_values[cat_idx]\n            ax.fill_between(\n                range(x_start, x_end),\n                0,\n                obs_tidy.iloc[idx, x_start:x_end],\n                lw=0.1,\n                color=groupby_colors[cat_idx],\n            )\n\n        # remove the xticks labels except for the last processed plot.\n        # Because the plots share the x axis it is redundant and less compact\n        # to plot the axis for each plot\n        if idx < len(var_names) - 1:\n            ax.tick_params(labelbottom=False, labeltop=False, bottom=False, top=False)\n            ax.set_xlabel('')\n        if log:\n            ax.set_yscale('log')\n        ax.spines['left'].set_visible(False)\n        ax.spines['top'].set_visible(False)\n        ax.spines['bottom'].set_visible(False)\n        ax.grid(False)\n        ymin, ymax = ax.get_ylim()\n        ymax = int(ymax)\n        ax.set_yticks([ymax])\n        ax.set_yticklabels([str(ymax)], ha='left', va='top')\n        ax.spines['right'].set_position(('axes', 1.01))\n        ax.tick_params(\n            axis='y',\n            labelsize='x-small',\n            right=True,\n            left=False,\n            length=2,\n            which='both',\n            labelright=True,\n            labelleft=False,\n            direction='in',\n        )\n        ax.set_ylabel(var, rotation=0, fontsize='small', ha='right', va='bottom')\n        ax.yaxis.set_label_coords(-0.005, 0.1)\n    ax.set_xlim(0, x_end)\n    ax.tick_params(axis='x', bottom=False, labelbottom=False)\n\n    # the ax to plot the groupby categories is split to add a small space\n    # between the rest of the plot and the categories\n    axs2 = gridspec.GridSpecFromSubplotSpec(\n        2, 1, subplot_spec=axs[num_rows - 1, 0], height_ratios=[1, 1]\n    )\n\n    groupby_ax = fig.add_subplot(axs2[1])\n\n    label2code, ticks, labels, groupby_cmap, norm = _plot_categories_as_colorblocks(\n        groupby_ax, obs_tidy.T, colors=groupby_colors, orientation='bottom'\n    )\n    # add lines to plot\n    overlay_ax = fig.add_subplot(axs[1:-1, 0], sharex=first_ax)\n    line_positions = np.cumsum(obs_tidy.T.index.value_counts(sort=False))[:-1]\n    overlay_ax.vlines(line_positions, 0, 1, lw=0.5, linestyle=\"--\")\n    overlay_ax.axis('off')\n    overlay_ax.set_ylim(0, 1)\n\n    if dendrogram:\n        dendro_ax = fig.add_subplot(axs[0], sharex=first_ax)\n        _plot_dendrogram(\n            dendro_ax,\n            adata,\n            groupby,\n            dendrogram_key=dendrogram,\n            orientation='top',\n            ticks=ticks,\n        )\n\n    if var_group_positions is not None and len(var_group_positions) > 0:\n        gene_groups_ax = fig.add_subplot(axs[1:-1, 1])\n        arr = []\n        for idx, pos in enumerate(var_group_positions):\n            arr += [idx] * (pos[1] + 1 - pos[0])\n\n        gene_groups_ax.imshow(\n            np.array([arr]).T, aspect='auto', cmap=groupby_cmap, norm=norm\n        )\n        gene_groups_ax.axis('off')\n\n    return_ax_dict = {'track_axes': axs_list, 'groupby_ax': groupby_ax}\n    if dendrogram:\n        return_ax_dict['dendrogram_ax'] = dendro_ax\n    if var_group_positions is not None and len(var_group_positions) > 0:\n        return_ax_dict['gene_groups_ax'] = gene_groups_ax\n\n    _utils.savefig_or_show('tracksplot', show=show, save=save)\n    show = settings.autoshow if show is None else show\n    if not show:\n        return return_ax_dict", "idx": 161}
{"project": "Scanpy", "commit_id": "247_scanpy_1.9.0__anndata.py_dendrogram.py", "target": 0, "func": "def dendrogram(\n    adata: AnnData,\n    groupby: str,\n    *,\n    dendrogram_key: Optional[str] = None,\n    orientation: Literal['top', 'bottom', 'left', 'right'] = 'top',\n    remove_labels: bool = False,\n    show: Optional[bool] = None,\n    save: Union[str, bool, None] = None,\n    ax: Optional[Axes] = None,\n):\n    \"\"\"\\\n    Plots a dendrogram of the categories defined in `groupby`.\n\n    See :func:`~scanpy.tl.dendrogram`.\n\n    Parameters\n    ----------\n    adata\n        Annotated data matrix.\n    groupby\n        Categorical data column used to create the dendrogram\n    dendrogram_key\n        Key under with the dendrogram information was stored.\n        By default the dendrogram information is stored under\n        `.uns[f'dendrogram_{{groupby}}']`.\n    orientation\n        Origin of the tree. Will grow into the opposite direction.\n    remove_labels\n        Don\u2019t draw labels. Used e.g. by :func:`scanpy.pl.matrixplot`\n        to annotate matrix columns/rows.\n    {show_save_ax}\n\n    Returns\n    -------\n    :class:`matplotlib.axes.Axes`\n\n    Examples\n    --------\n    .. plot::\n        :context: close-figs\n\n        import scanpy as sc\n        adata = sc.datasets.pbmc68k_reduced()\n        sc.tl.dendrogram(adata, 'bulk_labels')\n        sc.pl.dendrogram(adata, 'bulk_labels')\n\n    .. currentmodule:: scanpy\n\n    \"\"\"\n    if ax is None:\n        _, ax = pl.subplots()\n    _plot_dendrogram(\n        ax,\n        adata,\n        groupby,\n        dendrogram_key=dendrogram_key,\n        remove_labels=remove_labels,\n        orientation=orientation,\n    )\n    _utils.savefig_or_show('dendrogram', show=show, save=save)\n    return ax", "idx": 162}
{"project": "Scanpy", "commit_id": "248_scanpy_1.9.0__anndata.py_correlation_matrix.py", "target": 0, "func": "def correlation_matrix(\n    adata: AnnData,\n    groupby: str,\n    show_correlation_numbers: bool = False,\n    dendrogram: Union[bool, str, None] = None,\n    figsize: Optional[Tuple[float, float]] = None,\n    show: Optional[bool] = None,\n    save: Union[str, bool, None] = None,\n    ax: Optional[Axes] = None,\n    vmin: Optional[float] = None,\n    vmax: Optional[float] = None,\n    vcenter: Optional[float] = None,\n    norm: Optional[Normalize] = None,\n    **kwds,\n) -> Union[Axes, List[Axes]]:\n    \"\"\"\\\n    Plots the correlation matrix computed as part of `sc.tl.dendrogram`.\n\n    Parameters\n    ----------\n    adata\n    groupby\n        Categorical data column used to create the dendrogram\n    show_correlation_numbers\n        If `show_correlation=True`, plot the correlation on top of each cell.\n    dendrogram\n        If True or a valid dendrogram key, a dendrogram based on the\n        hierarchical clustering between the `groupby` categories is added.\n        The dendrogram is computed using :func:`scanpy.tl.dendrogram`.\n        If `tl.dendrogram` has not been called previously,\n        the function is called with default parameters.\n    figsize\n        By default a figure size that aims to produce a squared correlation\n        matrix plot is used. Format is (width, height)\n    {show_save_ax}\n    {vminmax}\n    **kwds\n        Only if `show_correlation` is True:\n        Are passed to :func:`matplotlib.pyplot.pcolormesh` when plotting the\n        correlation heatmap. `cmap` can be used to change the color palette.\n\n    Returns\n    -------\n\n    Examples\n    --------\n    >>> import scanpy as sc\n    >>> adata = sc.datasets.pbmc68k_reduced()\n    >>> sc.tl.dendrogram(adata, 'bulk_labels')\n    >>> sc.pl.correlation_matrix(adata, 'bulk_labels')\n    \"\"\"\n\n    dendrogram_key = _get_dendrogram_key(adata, dendrogram, groupby)\n\n    index = adata.uns[dendrogram_key]['categories_idx_ordered']\n    corr_matrix = adata.uns[dendrogram_key]['correlation_matrix']\n    # reorder matrix columns according to the dendrogram\n    if dendrogram is None:\n        dendrogram = ax is None\n    if dendrogram:\n        if ax is not None:\n            raise ValueError('Can only plot dendrogram when not plotting to an axis')\n        assert (len(index)) == corr_matrix.shape[0]\n        corr_matrix = corr_matrix[index, :]\n        corr_matrix = corr_matrix[:, index]\n        labels = list(adata.obs[groupby].cat.categories)\n        labels = np.array(labels).astype('str')[index]\n    else:\n        labels = adata.obs[groupby].cat.categories\n    num_rows = corr_matrix.shape[0]\n    colorbar_height = 0.2\n    if dendrogram:\n        dendrogram_width = 1.8\n    else:\n        dendrogram_width = 0\n    if figsize is None:\n        corr_matrix_height = num_rows * 0.6\n        height = corr_matrix_height + colorbar_height\n        width = corr_matrix_height + dendrogram_width\n    else:\n        width, height = figsize\n        corr_matrix_height = height - colorbar_height\n\n    fig = pl.figure(figsize=(width, height)) if ax is None else None\n    # layout with 2 rows and 2  columns:\n    # row 1: dendrogram + correlation matrix\n    # row 2: nothing + colormap bar (horizontal)\n    gs = gridspec.GridSpec(\n        nrows=2,\n        ncols=2,\n        width_ratios=[dendrogram_width, corr_matrix_height],\n        height_ratios=[corr_matrix_height, colorbar_height],\n        wspace=0.01,\n        hspace=0.05,\n    )\n\n    axs = []\n    corr_matrix_ax = fig.add_subplot(gs[1]) if ax is None else ax\n    if dendrogram:\n        dendro_ax = fig.add_subplot(gs[0], sharey=corr_matrix_ax)\n        _plot_dendrogram(\n            dendro_ax,\n            adata,\n            groupby,\n            dendrogram_key=dendrogram_key,\n            remove_labels=True,\n            orientation='left',\n            ticks=np.arange(corr_matrix.shape[0]) + 0.5,\n        )\n        axs.append(dendro_ax)\n    # define some default pcolormesh parameters\n    if 'edgecolors' not in kwds:\n        if corr_matrix.shape[0] > 30:\n            # when there are too many rows it is better to remove\n            # the black lines surrounding the boxes in the heatmap\n            kwds['edgecolors'] = 'none'\n        else:\n            kwds['edgecolors'] = 'black'\n            kwds.setdefault('linewidth', 0.01)\n    if vmax is None and vmin is None and norm is None:\n        vmax = 1\n        vmin = -1\n    norm = check_colornorm(vmin, vmax, vcenter, norm)\n    if 'cmap' not in kwds:\n        # by default use a divergent color map\n        kwds['cmap'] = 'bwr'\n\n    img_mat = corr_matrix_ax.pcolormesh(corr_matrix, norm=norm, **kwds)\n    corr_matrix_ax.set_xlim(0, num_rows)\n    corr_matrix_ax.set_ylim(0, num_rows)\n\n    corr_matrix_ax.yaxis.tick_right()\n    corr_matrix_ax.set_yticks(np.arange(corr_matrix.shape[0]) + 0.5)\n    corr_matrix_ax.set_yticklabels(labels)\n\n    corr_matrix_ax.xaxis.set_tick_params(labeltop=True)\n    corr_matrix_ax.xaxis.set_tick_params(labelbottom=False)\n    corr_matrix_ax.set_xticks(np.arange(corr_matrix.shape[0]) + 0.5)\n    corr_matrix_ax.set_xticklabels(labels, rotation=45, ha='left')\n\n    for ax_name in 'xy':\n        corr_matrix_ax.tick_params(axis=ax_name, which='both', bottom=False, top=False)\n\n    if show_correlation_numbers:\n        for row, col in product(range(num_rows), repeat=2):\n            corr_matrix_ax.text(\n                row + 0.5,\n                col + 0.5,\n                f\"{corr_matrix[row, col]:.2f}\",\n                ha='center',\n                va='center',\n            )\n\n    axs.append(corr_matrix_ax)\n\n    if ax is None:  # Plot colorbar\n        colormap_ax = fig.add_subplot(gs[3])\n        cobar = pl.colorbar(img_mat, cax=colormap_ax, orientation='horizontal')\n        cobar.solids.set_edgecolor(\"face\")\n        axs.append(colormap_ax)\n\n    show = settings.autoshow if show is None else show\n    _utils.savefig_or_show('correlation_matrix', show=show, save=save)\n    if ax is None and not show:\n        return axs", "idx": 163}
{"project": "Scanpy", "commit_id": "249_scanpy_1.9.0__anndata.py__prepare_dataframe.py", "target": 0, "func": "def _prepare_dataframe(\n    adata: AnnData,\n    var_names: Union[_VarNames, Mapping[str, _VarNames]],\n    groupby: Optional[Union[str, Sequence[str]]] = None,\n    use_raw: Optional[bool] = None,\n    log: bool = False,\n    num_categories: int = 7,\n    layer=None,\n    gene_symbols: Optional[str] = None,\n):\n    \"\"\"\n    Given the anndata object, prepares a data frame in which the row index are the categories\n    defined by group by and the columns correspond to var_names.\n\n    Parameters\n    ----------\n    adata\n        Annotated data matrix.\n    var_names\n        `var_names` should be a valid subset of  `adata.var_names`.\n    groupby\n        The key of the observation grouping to consider. It is expected that\n        groupby is a categorical. If groupby is not a categorical observation,\n        it would be subdivided into `num_categories`.\n    use_raw\n        Whether to use `raw` attribute of `adata`. Defaults to `True` if `.raw` is present.\n    log\n        Use the log of the values.\n    layer\n        AnnData layer to use. Takes precedence over `use_raw`\n    num_categories\n        Only used if groupby observation is not categorical. This value\n        determines the number of groups into which the groupby observation\n        should be subdivided.\n    gene_symbols\n        Key for field in .var that stores gene symbols.\n\n    Returns\n    -------\n    Tuple of `pandas.DataFrame` and list of categories.\n    \"\"\"\n\n    sanitize_anndata(adata)\n    use_raw = _check_use_raw(adata, use_raw)\n    if layer is not None:\n        use_raw = False\n    if isinstance(var_names, str):\n        var_names = [var_names]\n\n    groupby_index = None\n    if groupby is not None:\n        if isinstance(groupby, str):\n            # if not a list, turn into a list\n            groupby = [groupby]\n        for group in groupby:\n            if group not in list(adata.obs_keys()) + [adata.obs.index.name]:\n                if adata.obs.index.name is not None:\n                    msg = f' or index name \"{adata.obs.index.name}\"'\n                else:\n                    msg = ''\n                raise ValueError(\n                    'groupby has to be a valid observation. '\n                    f'Given {group}, is not in observations: {adata.obs_keys()}' + msg\n                )\n            if group in adata.obs.keys() and group == adata.obs.index.name:\n                raise ValueError(\n                    f'Given group {group} is both and index and a column level, '\n                    'which is ambiguous.'\n                )\n            if group == adata.obs.index.name:\n                groupby_index = group\n    if groupby_index is not None:\n        # obs_tidy contains adata.obs.index\n        # and does not need to be given\n        groupby = groupby.copy()  # copy to not modify user passed parameter\n        groupby.remove(groupby_index)\n    keys = list(groupby) + list(np.unique(var_names))\n    obs_tidy = get.obs_df(\n        adata, keys=keys, layer=layer, use_raw=use_raw, gene_symbols=gene_symbols\n    )\n    assert np.all(np.array(keys) == np.array(obs_tidy.columns))\n\n    if groupby_index is not None:\n        # reset index to treat all columns the same way.\n        obs_tidy.reset_index(inplace=True)\n        groupby.append(groupby_index)\n\n    if groupby is None:\n        categorical = pd.Series(np.repeat('', len(obs_tidy))).astype('category')\n    elif len(groupby) == 1 and is_numeric_dtype(obs_tidy[groupby[0]]):\n        # if the groupby column is not categorical, turn it into one\n        # by subdividing into  `num_categories` categories\n        categorical = pd.cut(obs_tidy[groupby[0]], num_categories)\n    elif len(groupby) == 1:\n        categorical = obs_tidy[groupby[0]].astype('category')\n        categorical.name = groupby[0]\n    else:\n        # join the groupby values  using \"_\" to make a new 'category'\n        categorical = obs_tidy[groupby].apply('_'.join, axis=1).astype('category')\n        categorical.name = \"_\".join(groupby)\n\n        # preserve category order\n        from itertools import product\n\n        order = {\n            \"_\".join(k): idx\n            for idx, k in enumerate(\n                product(*(obs_tidy[g].cat.categories for g in groupby))\n            )\n        }\n        categorical = categorical.cat.reorder_categories(\n            sorted(categorical.cat.categories, key=lambda x: order[x])\n        )\n    obs_tidy = obs_tidy[var_names].set_index(categorical)\n    categories = obs_tidy.index.categories\n\n    if log:\n        obs_tidy = np.log1p(obs_tidy)\n\n    return categories, obs_tidy", "idx": 164}
{"project": "Scanpy", "commit_id": "24_scanpy_1.9.0_cli.py_main.py", "target": 0, "func": "def main(\n    argv: Optional[Sequence[str]] = None, *, check: bool = True, **runargs\n) -> Optional[CompletedProcess]:\n    \"\"\"\\\n    Run a builtin scanpy command or a scanpy-* subcommand.\n\n    Uses :func:`subcommand.run` for the latter:\n    `~run(['scanpy', *argv], **runargs)`\n    \"\"\"\n    parser = ArgumentParser(\n        description=(\n            \"There are a few packages providing commands. \"\n            \"Try e.g. `pip install scanpy-scripts`!\"\n        )\n    )\n    parser.set_defaults(func=parser.print_help)\n\n    subparsers: _DelegatingSubparsersAction = parser.add_subparsers(\n        action=_DelegatingSubparsersAction,\n        _command='scanpy',\n        _runargs={**runargs, 'check': check},\n    )\n\n    parser_settings = subparsers.add_parser('settings')\n    parser_settings.set_defaults(func=_cmd_settings)\n\n    args = parser.parse_args(argv)\n    return args.func()", "idx": 165}
{"project": "Scanpy", "commit_id": "250_scanpy_1.9.0__anndata.py__plot_gene_groups_brackets.py", "target": 0, "func": "def _plot_gene_groups_brackets(\n    gene_groups_ax: Axes,\n    group_positions: Iterable[Tuple[int, int]],\n    group_labels: Sequence[str],\n    left_adjustment: float = -0.3,\n    right_adjustment: float = 0.3,\n    rotation: Optional[float] = None,\n    orientation: Literal['top', 'right'] = 'top',\n):\n    \"\"\"\\\n    Draws brackets that represent groups of genes on the give axis.\n    For best results, this axis is located on top of an image whose\n    x axis contains gene names.\n\n    The gene_groups_ax should share the x axis with the main ax.\n\n    Eg: gene_groups_ax = fig.add_subplot(axs[0, 0], sharex=dot_ax)\n\n    This function is used by dotplot, heatmap etc.\n\n    Parameters\n    ----------\n    gene_groups_ax\n        In this axis the gene marks are drawn\n    group_positions\n        Each item in the list, should contain the start and end position that the\n        bracket should cover.\n        Eg. [(0, 4), (5, 8)] means that there are two brackets, one for the var_names (eg genes)\n        in positions 0-4 and other for positions 5-8\n    group_labels\n        List of group labels\n    left_adjustment\n        adjustment to plot the bracket start slightly before or after the first gene position.\n        If the value is negative the start is moved before.\n    right_adjustment\n        adjustment to plot the bracket end slightly before or after the last gene position\n        If the value is negative the start is moved before.\n    rotation\n        rotation degrees for the labels. If not given, small labels (<4 characters) are not\n        rotated, otherwise, they are rotated 90 degrees\n    orientation\n        location of the brackets. Either `top` or `right`\n    Returns\n    -------\n    None\n    \"\"\"\n    import matplotlib.patches as patches\n    from matplotlib.path import Path\n\n    # get the 'brackets' coordinates as lists of start and end positions\n\n    left = [x[0] + left_adjustment for x in group_positions]\n    right = [x[1] + right_adjustment for x in group_positions]\n\n    # verts and codes are used by PathPatch to make the brackets\n    verts = []\n    codes = []\n    if orientation == 'top':\n        # rotate labels if any of them is longer than 4 characters\n        if rotation is None and group_labels:\n            if max([len(x) for x in group_labels]) > 4:\n                rotation = 90\n            else:\n                rotation = 0\n        for idx in range(len(left)):\n            verts.append((left[idx], 0))  # lower-left\n            verts.append((left[idx], 0.6))  # upper-left\n            verts.append((right[idx], 0.6))  # upper-right\n            verts.append((right[idx], 0))  # lower-right\n\n            codes.append(Path.MOVETO)\n            codes.append(Path.LINETO)\n            codes.append(Path.LINETO)\n            codes.append(Path.LINETO)\n\n            try:\n                group_x_center = left[idx] + float(right[idx] - left[idx]) / 2\n                gene_groups_ax.text(\n                    group_x_center,\n                    1.1,\n                    group_labels[idx],\n                    ha='center',\n                    va='bottom',\n                    rotation=rotation,\n                )\n            except Exception:  # TODO catch the correct exception\n                pass\n    else:\n        top = left\n        bottom = right\n        for idx in range(len(top)):\n            verts.append((0, top[idx]))  # upper-left\n            verts.append((0.15, top[idx]))  # upper-right\n            verts.append((0.15, bottom[idx]))  # lower-right\n            verts.append((0, bottom[idx]))  # lower-left\n\n            codes.append(Path.MOVETO)\n            codes.append(Path.LINETO)\n            codes.append(Path.LINETO)\n            codes.append(Path.LINETO)\n\n            try:\n                diff = bottom[idx] - top[idx]\n                group_y_center = top[idx] + float(diff) / 2\n                if diff * 2 < len(group_labels[idx]):\n                    # cut label to fit available space\n                    group_labels[idx] = group_labels[idx][: int(diff * 2)] + \".\"\n                gene_groups_ax.text(\n                    0.6,\n                    group_y_center,\n                    group_labels[idx],\n                    ha='right',\n                    va='center',\n                    rotation=270,\n                    fontsize='small',\n                )\n            except Exception as e:\n                print('problems {}'.format(e))\n                pass\n\n    path = Path(verts, codes)\n\n    patch = patches.PathPatch(path, facecolor='none', lw=1.5)\n\n    gene_groups_ax.add_patch(patch)\n    gene_groups_ax.grid(False)\n    gene_groups_ax.axis('off')\n    # remove y ticks\n    gene_groups_ax.tick_params(axis='y', left=False, labelleft=False)\n    # remove x ticks and labels\n    gene_groups_ax.tick_params(\n        axis='x', bottom=False, labelbottom=False, labeltop=False", "idx": 166}
{"project": "Scanpy", "commit_id": "251_scanpy_1.9.0__anndata.py__reorder_categories_after_dendrogram.py", "target": 0, "func": "def _reorder_categories_after_dendrogram(\n    adata: AnnData,\n    groupby,\n    dendrogram,\n    var_names=None,\n    var_group_labels=None,\n    var_group_positions=None,\n    categories=None,\n):\n    \"\"\"\\\n    Function used by plotting functions that need to reorder the the groupby\n    observations based on the dendrogram results.\n\n    The function checks if a dendrogram has already been precomputed.\n    If not, `sc.tl.dendrogram` is run with default parameters.\n\n    The results found in `.uns[dendrogram_key]` are used to reorder\n    `var_group_labels` and `var_group_positions`.\n\n\n    Returns\n    -------\n    dictionary with keys:\n    'categories_idx_ordered', 'var_group_names_idx_ordered',\n    'var_group_labels', and 'var_group_positions'\n    \"\"\"\n\n    key = _get_dendrogram_key(adata, dendrogram, groupby)\n\n    if isinstance(groupby, str):\n        groupby = [groupby]\n\n    dendro_info = adata.uns[key]\n    if groupby != dendro_info['groupby']:\n        raise ValueError(\n            \"Incompatible observations. The precomputed dendrogram contains \"\n            f\"information for the observation: '{groupby}' while the plot is \"\n            f\"made for the observation: '{dendro_info['groupby']}. \"\n            \"Please run `sc.tl.dendrogram` using the right observation.'\"\n        )\n\n    if categories is None:\n        categories = adata.obs[dendro_info['groupby']].cat.categories\n\n    # order of groupby categories\n    categories_idx_ordered = dendro_info['categories_idx_ordered']\n    categories_ordered = dendro_info['categories_ordered']\n\n    if len(categories) != len(categories_idx_ordered):\n        raise ValueError(\n            \"Incompatible observations. Dendrogram data has \"\n            f\"{len(categories_idx_ordered)} categories but current groupby \"\n            f\"observation {groupby!r} contains {len(categories)} categories. \"\n            \"Most likely the underlying groupby observation changed after the \"\n            \"initial computation of `sc.tl.dendrogram`. \"\n            \"Please run `sc.tl.dendrogram` again.'\"\n        )\n\n    # reorder var_groups (if any)\n    if var_names is not None:\n        var_names_idx_ordered = list(range(len(var_names)))\n\n    if var_group_positions:\n        if set(var_group_labels) == set(categories):\n            positions_ordered = []\n            labels_ordered = []\n            position_start = 0\n            var_names_idx_ordered = []\n            for cat_name in categories_ordered:\n                idx = var_group_labels.index(cat_name)\n                position = var_group_positions[idx]\n                _var_names = var_names[position[0] : position[1] + 1]\n                var_names_idx_ordered.extend(range(position[0], position[1] + 1))\n                positions_ordered.append(\n                    (position_start, position_start + len(_var_names) - 1)\n                )\n                position_start += len(_var_names)\n                labels_ordered.append(var_group_labels[idx])\n            var_group_labels = labels_ordered\n            var_group_positions = positions_ordered\n        else:\n            logg.warning(\n                \"Groups are not reordered because the `groupby` categories \"\n                \"and the `var_group_labels` are different.\\n\"\n                f\"categories: {_format_first_three_categories(categories)}\\n\"\n                f\"var_group_labels: {_format_first_three_categories(var_group_labels)}\"\n            )\n    else:\n        var_names_idx_ordered = None\n\n    if var_names_idx_ordered is not None:\n        var_names_ordered = [var_names[x] for x in var_names_idx_ordered]\n    else:\n        var_names_ordered = None\n\n    return dict(\n        categories_idx_ordered=categories_idx_ordered,\n        categories_ordered=dendro_info['categories_ordered'],\n        var_names_idx_ordered=var_names_idx_ordered,\n        var_names_ordered=var_names_ordered,\n        var_group_labels=var_group_labels,\n        var_group_positions=var_group_positions,", "idx": 167}
{"project": "Scanpy", "commit_id": "252_scanpy_1.9.0__anndata.py__format_first_three_categories.py", "target": 0, "func": "def _format_first_three_categories(categories):\n    categories = list(categories)\n    if len(categories) > 3:\n        categories = categories[:3] + ['etc.']\n    return ', '.join(categories)", "idx": 168}
{"project": "Scanpy", "commit_id": "253_scanpy_1.9.0__anndata.py__get_dendrogram_key.py", "target": 0, "func": "def _get_dendrogram_key(adata, dendrogram_key, groupby):\n    # the `dendrogram_key` can be a bool an NoneType or the name of the\n    # dendrogram key. By default the name of the dendrogram key is 'dendrogram'\n    if not isinstance(dendrogram_key, str):\n        if isinstance(groupby, str):\n            dendrogram_key = f'dendrogram_{groupby}'\n        elif isinstance(groupby, list):\n            dendrogram_key = f'dendrogram_{\"_\".join(groupby)}'\n\n    if dendrogram_key not in adata.uns:\n        from ..tools._dendrogram import dendrogram\n\n        logg.warning(\n            f\"dendrogram data not found (using key={dendrogram_key}). \"\n            \"Running `sc.tl.dendrogram` with default parameters. For fine \"\n            \"tuning it is recommended to run `sc.tl.dendrogram` independently.\"\n        )\n        dendrogram(adata, groupby, key_added=dendrogram_key)\n\n    if 'dendrogram_info' not in adata.uns[dendrogram_key]:\n        raise ValueError(\n            f\"The given dendrogram key ({dendrogram_key!r}) does not contain \"\n            \"valid dendrogram information.\"\n        )\n\n    return dendrogram_key", "idx": 169}
{"project": "Scanpy", "commit_id": "254_scanpy_1.9.0__anndata.py__plot_dendrogram.py", "target": 0, "func": "def _plot_dendrogram(\n    dendro_ax: Axes,\n    adata: AnnData,\n    groupby: str,\n    dendrogram_key: Optional[str] = None,\n    orientation: Literal['top', 'bottom', 'left', 'right'] = 'right',\n    remove_labels: bool = True,\n    ticks: Optional[Collection[float]] = None,\n):\n    \"\"\"\\\n    Plots a dendrogram on the given ax using the precomputed dendrogram\n    information stored in `.uns[dendrogram_key]`\n    \"\"\"\n\n    dendrogram_key = _get_dendrogram_key(adata, dendrogram_key, groupby)\n\n    def translate_pos(pos_list, new_ticks, old_ticks):\n        \"\"\"\\\n        transforms the dendrogram coordinates to a given new position.\n        The xlabel_pos and orig_ticks should be of the same\n        length.\n\n        This is mostly done for the heatmap case, where the position of the\n        dendrogram leaves needs to be adjusted depending on the category size.\n\n        Parameters\n        ----------\n        pos_list\n            list of dendrogram positions that should be translated\n        new_ticks\n            sorted list of goal tick positions (e.g. [0,1,2,3] )\n        old_ticks\n            sorted list of original tick positions (e.g. [5, 15, 25, 35]),\n            This list is usually the default position used by\n            `scipy.cluster.hierarchy.dendrogram`.\n\n        Returns\n        -------\n        translated list of positions\n\n        Examples\n        --------\n        >>> translate_pos(\n        ...     [5, 15, 20, 21],\n        ...     [0,  1,  2, 3 ],\n        ...     [5, 15, 25, 35],\n        ... )\n        [0, 1, 1.5, 1.6]\n        \"\"\"\n        # of given coordinates.\n\n        if not isinstance(old_ticks, list):\n            # assume that the list is a numpy array\n            old_ticks = old_ticks.tolist()\n        new_xs = []\n        for x_val in pos_list:\n            if x_val in old_ticks:\n                new_x_val = new_ticks[old_ticks.index(x_val)]\n            else:\n                # find smaller and bigger indices\n                idx_next = np.searchsorted(old_ticks, x_val, side=\"left\")\n                idx_prev = idx_next - 1\n                old_min = old_ticks[idx_prev]\n                old_max = old_ticks[idx_next]\n                new_min = new_ticks[idx_prev]\n                new_max = new_ticks[idx_next]\n                new_x_val = ((x_val - old_min) / (old_max - old_min)) * (\n                    new_max - new_min\n                ) + new_min\n            new_xs.append(new_x_val)\n        return new_xs\n\n    dendro_info = adata.uns[dendrogram_key]['dendrogram_info']\n    leaves = dendro_info[\"ivl\"]\n    icoord = np.array(dendro_info['icoord'])\n    dcoord = np.array(dendro_info['dcoord'])\n\n    orig_ticks = np.arange(5, len(leaves) * 10 + 5, 10).astype(float)\n    # check that ticks has the same length as orig_ticks\n    if ticks is not None and len(orig_ticks) != len(ticks):\n        logg.warning(\n            \"ticks argument does not have the same size as orig_ticks. \"\n            \"The argument will be ignored\"\n        )\n        ticks = None\n\n    for xs, ys in zip(icoord, dcoord):\n        if ticks is not None:\n            xs = translate_pos(xs, ticks, orig_ticks)\n        if orientation in ['right', 'left']:\n            xs, ys = ys, xs\n        dendro_ax.plot(xs, ys, color='#555555')\n\n    dendro_ax.tick_params(bottom=False, top=False, left=False, right=False)\n    ticks = ticks if ticks is not None else orig_ticks\n    if orientation in ['right', 'left']:\n        dendro_ax.set_yticks(ticks)\n        dendro_ax.set_yticklabels(leaves, fontsize='small', rotation=0)\n        dendro_ax.tick_params(labelbottom=False, labeltop=False)\n        if orientation == 'left':\n            xmin, xmax = dendro_ax.get_xlim()\n            dendro_ax.set_xlim(xmax, xmin)\n            dendro_ax.tick_params(labelleft=False, labelright=True)\n    else:\n        dendro_ax.set_xticks(ticks)\n        dendro_ax.set_xticklabels(leaves, fontsize='small', rotation=90)\n        dendro_ax.tick_params(labelleft=False, labelright=False)\n        if orientation == 'bottom':\n            ymin, ymax = dendro_ax.get_ylim()\n            dendro_ax.set_ylim(ymax, ymin)\n            dendro_ax.tick_params(labeltop=True, labelbottom=False)\n\n    if remove_labels:\n        dendro_ax.tick_params(\n            labelbottom=False, labeltop=False, labelleft=False, labelright=False\n        )\n\n    dendro_ax.grid(False)\n\n    dendro_ax.spines['right'].set_visible(False)\n    dendro_ax.spines['top'].set_visible(False)\n    dendro_ax.spines['left'].set_visible(False)\n    dendro_ax.spines['bottom'].set_visible(False)", "idx": 170}
{"project": "Scanpy", "commit_id": "255_scanpy_1.9.0__anndata.py__plot_categories_as_colorblocks.py", "target": 0, "func": "def _plot_categories_as_colorblocks(\n    groupby_ax: Axes,\n    obs_tidy: pd.DataFrame,\n    colors=None,\n    orientation: Literal['top', 'bottom', 'left', 'right'] = 'left',\n    cmap_name: str = 'tab20',\n):\n    \"\"\"\\\n    Plots categories as colored blocks. If orientation is 'left', the categories\n    are plotted vertically, otherwise they are plotted horizontally.\n\n    Parameters\n    ----------\n    groupby_ax\n    obs_tidy\n    colors\n        Sequence of valid color names to use for each category.\n    orientation\n    cmap_name\n        Name of colormap to use, in case colors is None\n\n    Returns\n    -------\n    ticks position, labels, colormap\n    \"\"\"\n\n    groupby = obs_tidy.index.name\n    from matplotlib.colors import ListedColormap, BoundaryNorm\n\n    if colors is None:\n        groupby_cmap = pl.get_cmap(cmap_name)\n    else:\n        groupby_cmap = ListedColormap(colors, groupby + '_cmap')\n    norm = BoundaryNorm(np.arange(groupby_cmap.N + 1) - 0.5, groupby_cmap.N)\n\n    # determine groupby label positions such that they appear\n    # centered next/below to the color code rectangle assigned to the category\n    value_sum = 0\n    ticks = []  # list of centered position of the labels\n    labels = []\n    label2code = {}  # dictionary of numerical values asigned to each label\n    for code, (label, value) in enumerate(\n        obs_tidy.index.value_counts(sort=False).iteritems()\n    ):\n        ticks.append(value_sum + (value / 2))\n        labels.append(label)\n        value_sum += value\n        label2code[label] = code\n\n    groupby_ax.grid(False)\n\n    if orientation == 'left':\n        groupby_ax.imshow(\n            np.array([[label2code[lab] for lab in obs_tidy.index]]).T,\n            aspect='auto',\n            cmap=groupby_cmap,\n            norm=norm,\n        )\n        if len(labels) > 1:\n            groupby_ax.set_yticks(ticks)\n            groupby_ax.set_yticklabels(labels)\n\n        # remove y ticks\n        groupby_ax.tick_params(axis='y', left=False, labelsize='small')\n        # remove x ticks and labels\n        groupby_ax.tick_params(axis='x', bottom=False, labelbottom=False)\n\n        # remove surrounding lines\n        groupby_ax.spines['right'].set_visible(False)\n        groupby_ax.spines['top'].set_visible(False)\n        groupby_ax.spines['left'].set_visible(False)\n        groupby_ax.spines['bottom'].set_visible(False)\n\n        groupby_ax.set_ylabel(groupby)\n    else:\n        groupby_ax.imshow(\n            np.array([[label2code[lab] for lab in obs_tidy.index]]),\n            aspect='auto',\n            cmap=groupby_cmap,\n            norm=norm,\n        )\n        if len(labels) > 1:\n            groupby_ax.set_xticks(ticks)\n            if max([len(str(x)) for x in labels]) < 3:\n                # if the labels are small do not rotate them\n                rotation = 0\n            else:\n                rotation = 90\n            groupby_ax.set_xticklabels(labels, rotation=rotation)\n\n        # remove x ticks\n        groupby_ax.tick_params(axis='x', bottom=False, labelsize='small')\n        # remove y ticks and labels\n        groupby_ax.tick_params(axis='y', left=False, labelleft=False)\n\n        # remove surrounding lines\n        groupby_ax.spines['right'].set_visible(False)\n        groupby_ax.spines['top'].set_visible(False)\n        groupby_ax.spines['left'].set_visible(False)\n        groupby_ax.spines['bottom'].set_visible(False)\n\n        groupby_ax.set_xlabel(groupby)\n\n    return label2code, ticks, labels, groupby_cmap, norm", "idx": 171}
{"project": "Scanpy", "commit_id": "256_scanpy_1.9.0__anndata.py__plot_colorbar.py", "target": 0, "func": "def _plot_colorbar(mappable, fig, subplot_spec, max_cbar_height: float = 4.0):\n    \"\"\"\n    Plots a vertical color bar based on mappable.\n    The height of the colorbar is min(figure-height, max_cmap_height)\n\n    Parameters\n    ----------\n    mappable\n        The image to which the colorbar applies.\n    fig\n        The figure object\n    subplot_spec\n        The gridspec subplot. Eg. axs[1,2]\n    max_cbar_height\n        The maximum colorbar height\n\n    Returns\n    -------\n    color bar ax\n    \"\"\"\n    width, height = fig.get_size_inches()\n    if height > max_cbar_height:\n        # to make the colorbar shorter, the\n        # ax is split and the lower portion is used.\n        axs2 = gridspec.GridSpecFromSubplotSpec(\n            2,\n            1,\n            subplot_spec=subplot_spec,\n            height_ratios=[height - max_cbar_height, max_cbar_height],\n        )\n        heatmap_cbar_ax = fig.add_subplot(axs2[1])\n    else:\n        heatmap_cbar_ax = fig.add_subplot(subplot_spec)\n    pl.colorbar(mappable, cax=heatmap_cbar_ax)\n    return heatmap_cbar_ax", "idx": 172}
{"project": "Scanpy", "commit_id": "257_scanpy_1.9.0__anndata.py__check_var_names_type.py", "target": 0, "func": "def _check_var_names_type(var_names, var_group_labels, var_group_positions):\n    \"\"\"\n    checks if var_names is a dict. Is this is the cases, then set the\n    correct values for var_group_labels and var_group_positions\n\n    Returns\n    -------\n    var_names, var_group_labels, var_group_positions\n\n    \"\"\"\n    if isinstance(var_names, cabc.Mapping):\n        if var_group_labels is not None or var_group_positions is not None:\n            logg.warning(\n                \"`var_names` is a dictionary. This will reset the current \"\n                \"value of `var_group_labels` and `var_group_positions`.\"\n            )\n        var_group_labels = []\n        _var_names = []\n        var_group_positions = []\n        start = 0\n        for label, vars_list in var_names.items():\n            if isinstance(vars_list, str):\n                vars_list = [vars_list]\n            # use list() in case var_list is a numpy array or pandas series\n            _var_names.extend(list(vars_list))\n            var_group_labels.append(label)\n            var_group_positions.append((start, start + len(vars_list) - 1))\n            start += len(vars_list)\n        var_names = _var_names\n\n    elif isinstance(var_names, str):\n        var_names = [var_names]\n\n    return var_names, var_group_labels, var_group_positions", "idx": 173}
{"project": "Scanpy", "commit_id": "258_scanpy_1.9.0__anndata.py_add_centroid.py", "target": 0, "func": "def add_centroid(centroids, name, Y, mask):\n        Y_mask = Y[mask]\n        if Y_mask.shape[0] == 0:\n            return\n        median = np.median(Y_mask, axis=0)\n        i = np.argmin(np.sum(np.abs(Y_mask - median), axis=1))\n        centroids[name] = Y_mask[i]", "idx": 174}
{"project": "Scanpy", "commit_id": "259_scanpy_1.9.0__anndata.py_translate_pos.py", "target": 0, "func": "def translate_pos(pos_list, new_ticks, old_ticks):\n        \"\"\"\\\n        transforms the dendrogram coordinates to a given new position.\n        The xlabel_pos and orig_ticks should be of the same\n        length.\n\n        This is mostly done for the heatmap case, where the position of the\n        dendrogram leaves needs to be adjusted depending on the category size.\n\n        Parameters\n        ----------\n        pos_list\n            list of dendrogram positions that should be translated\n        new_ticks\n            sorted list of goal tick positions (e.g. [0,1,2,3] )\n        old_ticks\n            sorted list of original tick positions (e.g. [5, 15, 25, 35]),\n            This list is usually the default position used by\n            `scipy.cluster.hierarchy.dendrogram`.\n\n        Returns\n        -------\n        translated list of positions\n\n        Examples\n        --------\n        >>> translate_pos(\n        ...     [5, 15, 20, 21],\n        ...     [0,  1,  2, 3 ],\n        ...     [5, 15, 25, 35],\n        ... )\n        [0, 1, 1.5, 1.6]\n        \"\"\"\n        # of given coordinates.\n\n        if not isinstance(old_ticks, list):\n            # assume that the list is a numpy array\n            old_ticks = old_ticks.tolist()\n        new_xs = []\n        for x_val in pos_list:\n            if x_val in old_ticks:\n                new_x_val = new_ticks[old_ticks.index(x_val)]\n            else:\n                # find smaller and bigger indices\n                idx_next = np.searchsorted(old_ticks, x_val, side=\"left\")\n                idx_prev = idx_next - 1\n                old_min = old_ticks[idx_prev]\n                old_max = old_ticks[idx_next]\n                new_min = new_ticks[idx_prev]\n                new_max = new_ticks[idx_next]\n                new_x_val = ((x_val - old_min) / (old_max - old_min)) * (\n                    new_max - new_min\n                ) + new_min\n            new_xs.append(new_x_val)\n        return new_xs", "idx": 175}
{"project": "Scanpy", "commit_id": "25_scanpy_1.9.0_cli.py_console_main.py", "target": 0, "func": "def console_main():\n    \"\"\"\\\n    This serves as CLI entry point and will not show a Python traceback\n    if a called command fails\n    \"\"\"\n    cmd = main(check=False)\n    if cmd is not None:\n        sys.exit(cmd.returncode)", "idx": 176}
{"project": "Scanpy", "commit_id": "260_scanpy_1.9.0__baseplot_class.py___init__.py", "target": 0, "func": "def __init__(\n        self,\n        adata: AnnData,\n        var_names: Union[_VarNames, Mapping[str, _VarNames]],\n        groupby: Union[str, Sequence[str]],\n        use_raw: Optional[bool] = None,\n        log: bool = False,\n        num_categories: int = 7,\n        categories_order: Optional[Sequence[str]] = None,\n        title: Optional['str'] = None,\n        figsize: Optional[Tuple[float, float]] = None,\n        gene_symbols: Optional[str] = None,\n        var_group_positions: Optional[Sequence[Tuple[int, int]]] = None,\n        var_group_labels: Optional[Sequence[str]] = None,\n        var_group_rotation: Optional[float] = None,\n        layer: Optional[str] = None,\n        ax: Optional[_AxesSubplot] = None,\n        vmin: Optional[float] = None,\n        vmax: Optional[float] = None,\n        vcenter: Optional[float] = None,\n        norm: Optional[Normalize] = None,\n        **kwds,\n    ):\n        self.var_names = var_names\n        self.var_group_labels = var_group_labels\n        self.var_group_positions = var_group_positions\n        self.var_group_rotation = var_group_rotation\n        self.width, self.height = figsize if figsize is not None else (None, None)\n\n        self.has_var_groups = (\n            True\n            if var_group_positions is not None and len(var_group_positions) > 0\n            else False\n        )\n\n        self._update_var_groups()\n\n        self.categories, self.obs_tidy = _prepare_dataframe(\n            adata,\n            self.var_names,\n            groupby,\n            use_raw,\n            log,\n            num_categories,\n            layer=layer,\n            gene_symbols=gene_symbols,\n        )\n        if len(self.categories) > self.MAX_NUM_CATEGORIES:\n            warn(\n                f\"Over {self.MAX_NUM_CATEGORIES} categories found. \"\n                \"Plot would be very large.\"\n            )\n\n        if categories_order is not None:\n            if set(self.obs_tidy.index.categories) != set(categories_order):\n                logg.error(\n                    \"Please check that the categories given by \"\n                    \"the `order` parameter match the categories that \"\n                    \"want to be reordered.\\n\\n\"\n                    \"Mismatch: \"\n                    f\"{set(self.obs_tidy.index.categories).difference(categories_order)}\\n\\n\"\n                    f\"Given order categories: {categories_order}\\n\\n\"\n                    f\"{groupby} categories: {list(self.obs_tidy.index.categories)}\\n\"\n                )\n                return\n\n        self.adata = adata\n        self.groupby = [groupby] if isinstance(groupby, str) else groupby\n        self.log = log\n        self.kwds = kwds\n\n        VBoundNorm = namedtuple('VBoundNorm', ['vmin', 'vmax', 'vcenter', 'norm'])\n        self.vboundnorm = VBoundNorm(vmin=vmin, vmax=vmax, vcenter=vcenter, norm=norm)\n\n        # set default values for legend\n        self.color_legend_title = self.DEFAULT_COLOR_LEGEND_TITLE\n        self.legends_width = self.DEFAULT_LEGENDS_WIDTH\n\n        # set style defaults\n        self.cmap = self.DEFAULT_COLORMAP\n\n        # style default parameters\n        self.are_axes_swapped = False\n        self.categories_order = categories_order\n        self.var_names_idx_order = None\n\n        self.wspace = self.DEFAULT_WSPACE\n\n        # minimum height required for legends to plot properly\n        self.min_figure_height = self.MIN_FIGURE_HEIGHT\n\n        self.fig_title = title\n\n        self.group_extra_size = 0\n        self.plot_group_extra = None\n        # after .render() is called the fig value is assigned and ax_dict\n        # contains a dictionary of the axes used in the plot\n        self.fig = None\n        self.ax_dict = None\n        self.ax = ax", "idx": 177}
{"project": "Scanpy", "commit_id": "261_scanpy_1.9.0__baseplot_class.py_swap_axes.py", "target": 0, "func": "def swap_axes(self, swap_axes: Optional[bool] = True):\n        \"\"\"\n        Plots a transposed image.\n\n        By default, the x axis contains `var_names` (e.g. genes) and the y\n        axis the `groupby` categories. By setting `swap_axes` then x are\n        the `groupby` categories and y the `var_names`.\n\n        Parameters\n        ----------\n        swap_axes\n            Boolean to turn on (True) or off (False) 'swap_axes'. Default True\n\n\n        Returns\n        -------\n        BasePlot\n\n        \"\"\"\n        self.DEFAULT_CATEGORY_HEIGHT, self.DEFAULT_CATEGORY_WIDTH = (\n            self.DEFAULT_CATEGORY_WIDTH,\n            self.DEFAULT_CATEGORY_HEIGHT,\n        )\n\n        self.are_axes_swapped = swap_axes\n        return self", "idx": 178}
{"project": "Scanpy", "commit_id": "262_scanpy_1.9.0__baseplot_class.py_add_dendrogram.py", "target": 0, "func": "def add_dendrogram(\n        self,\n        show: Optional[bool] = True,\n        dendrogram_key: Optional[str] = None,\n        size: Optional[float] = 0.8,\n    ):\n        \"\"\"\\\n        Show dendrogram based on the hierarchical clustering between the `groupby`\n        categories. Categories are reordered to match the dendrogram order.\n\n        The dendrogram information is computed using :func:`scanpy.tl.dendrogram`.\n        If `sc.tl.dendrogram` has not been called previously the function is called\n        with default parameters.\n\n        The dendrogram is by default shown on the right side of the plot or on top\n        if the axes are swapped.\n\n        `var_names` are reordered to produce a more pleasing output if:\n            * The data contains `var_groups`\n            * the `var_groups` match the categories.\n\n        The previous conditions happen by default when using Plot\n        to show the results from :func:`~scanpy.tl.rank_genes_groups` (aka gene markers), by\n        calling `scanpy.tl.rank_genes_groups_(plot_name)`.\n\n\n        Parameters\n        ----------\n        show\n            Boolean to turn on (True) or off (False) 'add_dendrogram'\n        dendrogram_key\n            Needed if `sc.tl.dendrogram` saved the dendrogram using a key different\n            than the default name.\n        size\n            size of the dendrogram. Corresponds to width when dendrogram shown on\n            the right of the plot, or height when shown on top. The unit is the same\n            as in matplotlib (inches).\n\n        Returns\n        -------\n        BasePlot\n\n\n        Examples\n        --------\n        >>> adata = sc.datasets.pbmc68k_reduced()\n        >>> markers = {'T-cell': 'CD3D', 'B-cell': 'CD79A', 'myeloid': 'CST3'}\n        >>> sc.pl.BasePlot(adata, markers, groupby='bulk_labels').add_dendrogram().show()\n\n        \"\"\"\n\n        if not show:\n            self.plot_group_extra = None\n            return self\n\n        if self.groupby is None or len(self.categories) <= 2:\n            # dendrogram can only be computed  between groupby categories\n            logg.warning(\n                \"Dendrogram not added. Dendrogram is added only \"\n                \"when the number of categories to plot > 2\"\n            )\n            return self\n\n        self.group_extra_size = size\n\n        # to correctly plot the dendrogram the categories need to be ordered\n        # according to the dendrogram ordering.\n        self._reorder_categories_after_dendrogram(dendrogram_key)\n\n        dendro_ticks = np.arange(len(self.categories)) + 0.5\n\n        self.group_extra_size = size\n        self.plot_group_extra = {\n            'kind': 'dendrogram',\n            'width': size,\n            'dendrogram_key': dendrogram_key,\n            'dendrogram_ticks': dendro_ticks,\n        }\n        return self", "idx": 179}
{"project": "Scanpy", "commit_id": "263_scanpy_1.9.0__baseplot_class.py_add_totals.py", "target": 0, "func": "def add_totals(\n        self,\n        show: Optional[bool] = True,\n        sort: Literal['ascending', 'descending'] = None,\n        size: Optional[float] = 0.8,\n        color: Optional[Union[ColorLike, Sequence[ColorLike]]] = None,\n    ):\n        \"\"\"\\\n        Show barplot for the number of cells in in `groupby` category.\n\n        The barplot is by default shown on the right side of the plot or on top\n        if the axes are swapped.\n\n\n        Parameters\n        ----------\n        show\n            Boolean to turn on (True) or off (False) 'add_totals'\n        sort\n            Set to either 'ascending' or 'descending' to reorder the categories\n            by cell number\n        size\n            size of the barplot. Corresponds to width when shown on\n            the right of the plot, or height when shown on top. The unit is the same\n            as in matplotlib (inches).\n        color\n            Color for the bar plots or list of colors for each of the bar plots.\n            By default, each bar plot uses the colors assigned in\n            `adata.uns[{groupby}_colors]`.\n\n\n        Returns\n        -------\n        BasePlot\n\n\n        Examples\n        --------\n        >>> adata = sc.datasets.pbmc68k_reduced()\n        >>> markers = {'T-cell': 'CD3D', 'B-cell': 'CD79A', 'myeloid': 'CST3'}\n        >>> sc.pl.BasePlot(adata, markers, groupby='bulk_labels').add_totals().show()\n        \"\"\"\n        self.group_extra_size = size\n\n        if not show:\n            # hide totals\n            self.plot_group_extra = None\n            self.group_extra_size = 0\n            return self\n\n        _sort = True if sort is not None else False\n        _ascending = True if sort == 'ascending' else False\n        counts_df = self.obs_tidy.index.value_counts(sort=_sort, ascending=_ascending)\n\n        if _sort:\n            self.categories_order = counts_df.index\n\n        self.plot_group_extra = {\n            'kind': 'group_totals',\n            'width': size,\n            'sort': sort,\n            'counts_df': counts_df,\n            'color': color,\n        }\n        return self", "idx": 180}
{"project": "Scanpy", "commit_id": "264_scanpy_1.9.0__baseplot_class.py_style.py", "target": 0, "func": "def style(self, cmap: Optional[str] = DEFAULT_COLORMAP):\n        \"\"\"\\\n        Set visual style parameters\n\n        Parameters\n        ----------\n        cmap\n            colormap\n\n        Returns\n        -------\n        BasePlot\n        \"\"\"\n\n        self.cmap = cmap", "idx": 181}
{"project": "Scanpy", "commit_id": "265_scanpy_1.9.0__baseplot_class.py_legend.py", "target": 0, "func": "def legend(\n        self,\n        show: Optional[bool] = True,\n        title: Optional[str] = DEFAULT_COLOR_LEGEND_TITLE,\n        width: Optional[float] = DEFAULT_LEGENDS_WIDTH,\n    ):\n        \"\"\"\\\n        Configure legend parameters\n\n        Parameters\n        ----------\n        show\n            Set to 'False' to hide the default plot of the legend. This sets the\n            legend width to zero which will result in a wider main plot.\n        title\n            Legend title. Appears on top of the color bar. Use '\\\\n' to add line breaks.\n        width\n            Width of the legend. The unit is the same as in matplotlib (inches)\n\n        Returns\n        -------\n        BasePlot\n\n\n        Examples\n        --------\n\n        Set legend title:\n\n        >>> adata = sc.datasets.pbmc68k_reduced()\n        >>> markers = {'T-cell': 'CD3D', 'B-cell': 'CD79A', 'myeloid': 'CST3'}\n        >>> dp = sc.pl.BasePlot(adata, markers, groupby='bulk_labels')\n        >>> dp.legend(colorbar_title='log(UMI counts + 1)').show()\n        \"\"\"\n\n        if not show:\n            # turn of legends by setting width to 0\n            self.legends_width = 0\n        else:\n            self.color_legend_title = title\n            self.legends_width = width\n\n        return self", "idx": 182}
{"project": "Scanpy", "commit_id": "266_scanpy_1.9.0__baseplot_class.py_get_axes.py", "target": 0, "func": "def get_axes(self):\n        if self.ax_dict is None:\n            self.make_figure()\n        return self.ax_dict", "idx": 183}
{"project": "Scanpy", "commit_id": "267_scanpy_1.9.0__baseplot_class.py__plot_totals.py", "target": 0, "func": "def _plot_totals(\n        self, total_barplot_ax: Axes, orientation: Literal['top', 'right']\n    ):\n        \"\"\"\n        Makes the bar plot for totals\n        \"\"\"\n        params = self.plot_group_extra\n        counts_df = params['counts_df']\n        if self.categories_order is not None:\n            counts_df = counts_df.loc[self.categories_order]\n        if params['color'] is None:\n            if f'{self.groupby}_colors' in self.adata.uns:\n                color = self.adata.uns[f'{self.groupby}_colors']\n            else:\n                color = 'salmon'\n        else:\n            color = params['color']\n\n        if orientation == 'top':\n            counts_df.plot(\n                kind=\"bar\",\n                color=color,\n                position=0.5,\n                ax=total_barplot_ax,\n                edgecolor=\"black\",\n                width=0.65,\n            )\n            # add numbers to the top of the bars\n            max_y = max([p.get_height() for p in total_barplot_ax.patches])\n\n            for p in total_barplot_ax.patches:\n                p.set_x(p.get_x() + 0.5)\n                if p.get_height() >= 1000:\n                    display_number = f'{np.round(p.get_height()/1000, decimals=1)}k'\n                else:\n                    display_number = np.round(p.get_height(), decimals=1)\n                total_barplot_ax.annotate(\n                    display_number,\n                    (p.get_x() + p.get_width() / 2.0, (p.get_height() + max_y * 0.05)),\n                    ha=\"center\",\n                    va=\"top\",\n                    xytext=(0, 10),\n                    fontsize=\"x-small\",\n                    textcoords=\"offset points\",\n                )\n            # for k in total_barplot_ax.spines.keys():\n            #     total_barplot_ax.spines[k].set_visible(False)\n            total_barplot_ax.set_ylim(0, max_y * 1.4)\n\n        elif orientation == 'right':\n            counts_df.plot(\n                kind=\"barh\",\n                color=color,\n                position=-0.3,\n                ax=total_barplot_ax,\n                edgecolor=\"black\",\n                width=0.65,\n            )\n\n            # add numbers to the right of the bars\n            max_x = max([p.get_width() for p in total_barplot_ax.patches])\n            for p in total_barplot_ax.patches:\n                if p.get_width() >= 1000:\n                    display_number = f'{np.round(p.get_width()/1000, decimals=1)}k'\n                else:\n                    display_number = np.round(p.get_width(), decimals=1)\n                total_barplot_ax.annotate(\n                    display_number,\n                    ((p.get_width()), p.get_y() + p.get_height()),\n                    ha=\"center\",\n                    va=\"top\",\n                    xytext=(10, 10),\n                    fontsize=\"x-small\",\n                    textcoords=\"offset points\",\n                )\n            total_barplot_ax.set_xlim(0, max_x * 1.4)\n\n        total_barplot_ax.grid(False)\n        total_barplot_ax.axis(\"off\")", "idx": 184}
{"project": "Scanpy", "commit_id": "268_scanpy_1.9.0__baseplot_class.py__plot_colorbar.py", "target": 0, "func": "def _plot_colorbar(self, color_legend_ax: Axes, normalize):\n        \"\"\"\n        Plots a horizontal colorbar given the ax an normalize values\n\n        Parameters\n        ----------\n        color_legend_ax\n        normalize\n\n        Returns\n        -------\n        None, updates color_legend_ax\n\n        \"\"\"\n        cmap = pl.get_cmap(self.cmap)\n\n        import matplotlib.colorbar\n        from matplotlib.cm import ScalarMappable\n\n        mappable = ScalarMappable(norm=normalize, cmap=cmap)\n\n        matplotlib.colorbar.Colorbar(\n            color_legend_ax, mappable=mappable, orientation='horizontal'\n        )\n\n        color_legend_ax.set_title(self.color_legend_title, fontsize='small')\n\n        color_legend_ax.xaxis.set_tick_params(labelsize='small')", "idx": 185}
{"project": "Scanpy", "commit_id": "269_scanpy_1.9.0__baseplot_class.py__plot_legend.py", "target": 0, "func": "def _plot_legend(self, legend_ax, return_ax_dict, normalize):\n\n        # to maintain the fixed height size of the legends, a\n        # spacer of variable height is added at top and bottom.\n        # The structure for the legends is:\n        # first row: variable space to keep the first rows of the same size\n        # second row: size legend\n\n        legend_height = self.min_figure_height * 0.08\n        height_ratios = [\n            self.height - legend_height,\n            legend_height,\n        ]\n        fig, legend_gs = make_grid_spec(\n            legend_ax, nrows=2, ncols=1, height_ratios=height_ratios\n        )\n\n        color_legend_ax = fig.add_subplot(legend_gs[1])\n\n        self._plot_colorbar(color_legend_ax, normalize)\n        return_ax_dict['color_legend_ax'] = color_legend_ax", "idx": 186}
{"project": "Scanpy", "commit_id": "26_scanpy_1.9.0_cli.py___init__.py", "target": 0, "func": "def __init__(self, cd: _CommandDelegator, subcmd: str):\n        super().__init__(f'{cd.command}-{subcmd}', add_help=False)\n        self.cd = cd\n        self.subcmd = subcmd", "idx": 187}
{"project": "Scanpy", "commit_id": "270_scanpy_1.9.0__baseplot_class.py__mainplot.py", "target": 0, "func": "def _mainplot(self, ax):\n        y_labels = self.categories\n        x_labels = self.var_names\n\n        if self.var_names_idx_order is not None:\n            x_labels = [x_labels[x] for x in self.var_names_idx_order]\n\n        if self.categories_order is not None:\n            y_labels = self.categories_order\n\n        if self.are_axes_swapped:\n            x_labels, y_labels = y_labels, x_labels\n            ax.set_xlabel(self.groupby)\n        else:\n            ax.set_ylabel(self.groupby)\n\n        y_ticks = np.arange(len(y_labels)) + 0.5\n        ax.set_yticks(y_ticks)\n        ax.set_yticklabels(y_labels)\n\n        x_ticks = np.arange(len(x_labels)) + 0.5\n        ax.set_xticks(x_ticks)\n        ax.set_xticklabels(x_labels, rotation=90, ha='center', minor=False)\n\n        ax.tick_params(axis='both', labelsize='small')\n        ax.grid(False)\n\n        # to be consistent with the heatmap plot, is better to\n        # invert the order of the y-axis, such that the first group is on\n        # top\n        ax.set_ylim(len(y_labels), 0)\n        ax.set_xlim(0, len(x_labels))\n\n        return check_colornorm(\n            self.vboundnorm.vmin,\n            self.vboundnorm.vmax,\n            self.vboundnorm.vcenter,\n            self.vboundnorm.norm,", "idx": 188}
{"project": "Scanpy", "commit_id": "271_scanpy_1.9.0__baseplot_class.py_make_figure.py", "target": 0, "func": "def make_figure(self):\n        \"\"\"\n        Renders the image but does not call :func:`matplotlib.pyplot.show`. Useful\n        when several plots are put together into one figure.\n\n        See also\n        --------\n        `show()`: Renders and shows the plot.\n        `savefig()`: Saves the plot.\n\n        Examples\n        --------\n\n        >>> import matplotlib.pyplot as plt\n        >>> adata = sc.datasets.pbmc68k_reduced()\n        >>> markers = ['C1QA', 'PSAP', 'CD79A', 'CD79B', 'CST3', 'LYZ']\n        >>> fig, (ax0, ax1) = plt.subplots(1, 2)\n        >>> sc.pl.MatrixPlot(adata, markers, groupby='bulk_labels', ax=ax0)\\\n        ...               .style(cmap='Blues', edge_color='none').make_figure()\n        >>> sc.pl.DotPlot(adata, markers, groupby='bulk_labels', ax=ax1).make_figure()\n        \"\"\"\n\n        category_height = self.DEFAULT_CATEGORY_HEIGHT\n        category_width = self.DEFAULT_CATEGORY_WIDTH\n\n        if self.height is None:\n            mainplot_height = len(self.categories) * category_height\n            mainplot_width = (\n                len(self.var_names) * category_width + self.group_extra_size\n            )\n            if self.are_axes_swapped:\n                mainplot_height, mainplot_width = mainplot_width, mainplot_height\n\n            height = mainplot_height + 1  # +1 for labels\n\n            # if the number of categories is small use\n            # a larger height, otherwise the legends do not fit\n            self.height = max([self.min_figure_height, height])\n            self.width = mainplot_width + self.legends_width\n        else:\n            self.min_figure_height = self.height\n            mainplot_height = self.height\n\n            mainplot_width = self.width - (self.legends_width + self.group_extra_size)\n\n        return_ax_dict = {}\n        # define a layout of 1 rows x 2 columns\n        #   first ax is for the main figure.\n        #   second ax is to plot legends\n        legends_width_spacer = 0.7 / self.width\n\n        self.fig, gs = make_grid_spec(\n            self.ax or (self.width, self.height),\n            nrows=1,\n            ncols=2,\n            wspace=legends_width_spacer,\n            width_ratios=[mainplot_width + self.group_extra_size, self.legends_width],\n        )\n\n        if self.has_var_groups:\n            # add some space in case 'brackets' want to be plotted on top of the image\n            if self.are_axes_swapped:\n                var_groups_height = category_height\n            else:\n                var_groups_height = category_height / 2\n\n        else:\n            var_groups_height = 0\n\n        mainplot_width = mainplot_width - self.group_extra_size\n        spacer_height = self.height - var_groups_height - mainplot_height\n        if not self.are_axes_swapped:\n            height_ratios = [spacer_height, var_groups_height, mainplot_height]\n            width_ratios = [mainplot_width, self.group_extra_size]\n\n        else:\n            height_ratios = [spacer_height, self.group_extra_size, mainplot_height]\n            width_ratios = [mainplot_width, var_groups_height]\n            # gridspec is the same but rows and columns are swapped\n\n        if self.fig_title is not None and self.fig_title.strip() != '':\n            # for the figure title use the ax that contains\n            # all the main graphical elements (main plot, dendrogram etc)\n            # otherwise the title may overlay with the figure.\n            # also, this puts the title centered on the main figure and not\n            # centered between the main figure and the legends\n            _ax = self.fig.add_subplot(gs[0, 0])\n            _ax.axis('off')\n            _ax.set_title(self.fig_title)\n\n        # the main plot is divided into three rows and two columns\n        # first row is an spacer that is adjusted in case the\n        #           legends need more height than the main plot\n        # second row is for brackets (if needed),\n        # third row is for mainplot and dendrogram/totals (legend goes in gs[0,1]\n        # defined earlier)\n        mainplot_gs = gridspec.GridSpecFromSubplotSpec(\n            nrows=3,\n            ncols=2,\n            wspace=self.wspace,\n            hspace=0.0,\n            subplot_spec=gs[0, 0],\n            width_ratios=width_ratios,\n            height_ratios=height_ratios,\n        )\n        main_ax = self.fig.add_subplot(mainplot_gs[2, 0])\n        return_ax_dict['mainplot_ax'] = main_ax\n        if not self.are_axes_swapped:\n            if self.plot_group_extra is not None:\n                group_extra_ax = self.fig.add_subplot(mainplot_gs[2, 1], sharey=main_ax)\n                group_extra_orientation = 'right'\n            if self.has_var_groups:\n                gene_groups_ax = self.fig.add_subplot(mainplot_gs[1, 0], sharex=main_ax)\n                var_group_orientation = 'top'\n        else:\n            if self.plot_group_extra:\n                group_extra_ax = self.fig.add_subplot(mainplot_gs[1, 0], sharex=main_ax)\n                group_extra_orientation = 'top'\n            if self.has_var_groups:\n                gene_groups_ax = self.fig.add_subplot(mainplot_gs[2, 1], sharey=main_ax)\n                var_group_orientation = 'right'\n\n        if self.plot_group_extra is not None:\n            if self.plot_group_extra['kind'] == 'dendrogram':\n                _plot_dendrogram(\n                    group_extra_ax,\n                    self.adata,\n                    self.groupby,\n                    dendrogram_key=self.plot_group_extra['dendrogram_key'],\n                    ticks=self.plot_group_extra['dendrogram_ticks'],\n                    orientation=group_extra_orientation,\n                )\n            if self.plot_group_extra['kind'] == 'group_totals':\n                self._plot_totals(group_extra_ax, group_extra_orientation)\n\n            return_ax_dict['group_extra_ax'] = group_extra_ax\n\n        # plot group legends on top or left of main_ax (if given)\n        if self.has_var_groups:\n            self._plot_var_groups_brackets(\n                gene_groups_ax,\n                group_positions=self.var_group_positions,\n                group_labels=self.var_group_labels,\n                rotation=self.var_group_rotation,\n                left_adjustment=0.2,\n                right_adjustment=0.7,\n                orientation=var_group_orientation,\n            )\n            return_ax_dict['gene_group_ax'] = gene_groups_ax\n\n        # plot the mainplot\n        normalize = self._mainplot(main_ax)\n\n        # code from pandas.plot in add_totals adds\n        # minor ticks that need to be removed\n        main_ax.yaxis.set_tick_params(which='minor', left=False, right=False)\n        main_ax.xaxis.set_tick_params(which='minor', top=False, bottom=False, length=0)\n        main_ax.set_zorder(100)\n        if self.legends_width > 0:\n            legend_ax = self.fig.add_subplot(gs[0, 1])\n            self._plot_legend(legend_ax, return_ax_dict, normalize)\n\n        self.ax_dict = return_ax_dict", "idx": 189}
{"project": "Scanpy", "commit_id": "272_scanpy_1.9.0__baseplot_class.py_show.py", "target": 0, "func": "def show(self, return_axes: Optional[bool] = None):\n        \"\"\"\n        Show the figure\n\n        Parameters\n        ----------\n        return_axes\n             If true return a dictionary with the figure axes. When return_axes is true\n             then :func:`matplotlib.pyplot.show` is not called.\n\n        Returns\n        -------\n        If `return_axes=True`: Dict of :class:`matplotlib.axes.Axes`. The dict key\n        indicates the type of ax (eg. `mainplot_ax`)\n\n        See also\n        --------\n        `render()`: Renders the plot but does not call :func:`matplotlib.pyplot.show`\n        `savefig()`: Saves the plot.\n\n        Examples\n        -------\n        >>> adata = sc.datasets.pbmc68k_reduced()\n        >>> markers = ['C1QA', 'PSAP', 'CD79A', 'CD79B', 'CST3', 'LYZ']\n        >>> sc.pl.Plot(adata, markers, groupby='bulk_labels').show()\n\n        \"\"\"\n\n        self.make_figure()\n\n        if return_axes:\n            return self.ax_dict\n        else:\n            pl.show()", "idx": 190}
{"project": "Scanpy", "commit_id": "273_scanpy_1.9.0__baseplot_class.py_savefig.py", "target": 0, "func": "def savefig(self, filename: str, bbox_inches: Optional[str] = 'tight', **kwargs):\n        \"\"\"\n        Save the current figure\n\n        Parameters\n        ----------\n        filename\n            Figure filename. Figure *format* is taken from the file ending unless\n            the parameter `format` is given.\n        bbox_inches\n            By default is set to 'tight' to avoid cropping of the legends.\n        kwargs\n            Passed to :func:`matplotlib.pyplot.savefig`\n\n        See also\n        --------\n        `render()`: Renders the plot but does not call :func:`matplotlib.pyplot.show`\n        `show()`: Renders and shows the plot\n\n        Examples\n        -------\n        >>> adata = sc.datasets.pbmc68k_reduced()\n        >>> markers = ['C1QA', 'PSAP', 'CD79A', 'CD79B', 'CST3', 'LYZ']\n        >>> sc.pl.BasePlot(adata, markers, groupby='bulk_labels').savefig('plot.pdf')\n\n        \"\"\"\n        self.make_figure()\n        pl.savefig(filename, bbox_inches=bbox_inches, **kwargs)", "idx": 191}
{"project": "Scanpy", "commit_id": "274_scanpy_1.9.0__baseplot_class.py__reorder_categories_after_dendrogram.py", "target": 0, "func": "def _reorder_categories_after_dendrogram(self, dendrogram):\n        \"\"\"\\\n        Function used by plotting functions that need to reorder the the groupby\n        observations based on the dendrogram results.\n\n        The function checks if a dendrogram has already been precomputed.\n        If not, `sc.tl.dendrogram` is run with default parameters.\n\n        The results found in `.uns[dendrogram_key]` are used to reorder\n        `var_group_labels` and `var_group_positions`.\n\n\n        Returns\n        -------\n        None internally updates\n        'categories_idx_ordered', 'var_group_names_idx_ordered',\n        'var_group_labels' and 'var_group_positions'\n        \"\"\"\n\n        def _format_first_three_categories(_categories):\n            \"\"\"used to clean up warning message\"\"\"\n            _categories = list(_categories)\n            if len(_categories) > 3:\n                _categories = _categories[:3] + ['etc.']\n            return ', '.join(_categories)\n\n        key = _get_dendrogram_key(self.adata, dendrogram, self.groupby)\n\n        dendro_info = self.adata.uns[key]\n        if self.groupby != dendro_info['groupby']:\n            raise ValueError(\n                \"Incompatible observations. The precomputed dendrogram contains \"\n                f\"information for the observation: '{self.groupby}' while the plot is \"\n                f\"made for the observation: '{dendro_info['groupby']}. \"\n                \"Please run `sc.tl.dendrogram` using the right observation.'\"\n            )\n\n        # order of groupby categories\n        categories_idx_ordered = dendro_info['categories_idx_ordered']\n        categories_ordered = dendro_info['categories_ordered']\n\n        if len(self.categories) != len(categories_idx_ordered):\n            raise ValueError(\n                \"Incompatible observations. Dendrogram data has \"\n                f\"{len(categories_idx_ordered)} categories but current groupby \"\n                f\"observation {self.groupby!r} contains {len(self.categories)} categories. \"\n                \"Most likely the underlying groupby observation changed after the \"\n                \"initial computation of `sc.tl.dendrogram`. \"\n                \"Please run `sc.tl.dendrogram` again.'\"\n            )\n\n        # reorder var_groups (if any)\n        if self.var_names is not None:\n            var_names_idx_ordered = list(range(len(self.var_names)))\n\n        if self.has_var_groups:\n            if set(self.var_group_labels) == set(self.categories):\n                positions_ordered = []\n                labels_ordered = []\n                position_start = 0\n                var_names_idx_ordered = []\n                for cat_name in categories_ordered:\n                    idx = self.var_group_labels.index(cat_name)\n                    position = self.var_group_positions[idx]\n                    _var_names = self.var_names[position[0] : position[1] + 1]\n                    var_names_idx_ordered.extend(range(position[0], position[1] + 1))\n                    positions_ordered.append(\n                        (position_start, position_start + len(_var_names) - 1)\n                    )\n                    position_start += len(_var_names)\n                    labels_ordered.append(self.var_group_labels[idx])\n                self.var_group_labels = labels_ordered\n                self.var_group_positions = positions_ordered\n            else:\n                logg.warning(\n                    \"Groups are not reordered because the `groupby` categories \"\n                    \"and the `var_group_labels` are different.\\n\"\n                    f\"categories: {_format_first_three_categories(self.categories)}\\n\"\n                    \"var_group_labels: \"\n                    f\"{_format_first_three_categories(self.var_group_labels)}\"\n                )\n\n        if var_names_idx_ordered is not None:\n            var_names_ordered = [self.var_names[x] for x in var_names_idx_ordered]\n        else:\n            var_names_ordered = None\n\n        self.categories_idx_ordered = categories_idx_ordered\n        self.categories_order = dendro_info['categories_ordered']\n        self.var_names_idx_order = var_names_idx_ordered\n        self.var_names_ordered = var_names_ordered", "idx": 192}
{"project": "Scanpy", "commit_id": "275_scanpy_1.9.0__baseplot_class.py__plot_var_groups_brackets.py", "target": 0, "func": "def _plot_var_groups_brackets(\n        gene_groups_ax: Axes,\n        group_positions: Iterable[Tuple[int, int]],\n        group_labels: Sequence[str],\n        left_adjustment: float = -0.3,\n        right_adjustment: float = 0.3,\n        rotation: Optional[float] = None,\n        orientation: Literal['top', 'right'] = 'top',\n    ):\n        \"\"\"\\\n        Draws brackets that represent groups of genes on the give axis.\n        For best results, this axis is located on top of an image whose\n        x axis contains gene names.\n\n        The gene_groups_ax should share the x axis with the main ax.\n\n        Eg: gene_groups_ax = fig.add_subplot(axs[0, 0], sharex=dot_ax)\n\n        Parameters\n        ----------\n        gene_groups_ax\n            In this axis the gene marks are drawn\n        group_positions\n            Each item in the list, should contain the start and end position that the\n            bracket should cover.\n            Eg. [(0, 4), (5, 8)] means that there are two brackets, one for the var_names (eg genes)\n            in positions 0-4 and other for positions 5-8\n        group_labels\n            List of group labels\n        left_adjustment\n            adjustment to plot the bracket start slightly before or after the first gene position.\n            If the value is negative the start is moved before.\n        right_adjustment\n            adjustment to plot the bracket end slightly before or after the last gene position\n            If the value is negative the start is moved before.\n        rotation\n            rotation degrees for the labels. If not given, small labels (<4 characters) are not\n            rotated, otherwise, they are rotated 90 degrees\n        orientation\n            location of the brackets. Either `top` or `right`\n        Returns\n        -------\n        None\n        \"\"\"\n        import matplotlib.patches as patches\n        from matplotlib.path import Path\n\n        # get the 'brackets' coordinates as lists of start and end positions\n\n        left = [x[0] + left_adjustment for x in group_positions]\n        right = [x[1] + right_adjustment for x in group_positions]\n\n        # verts and codes are used by PathPatch to make the brackets\n        verts = []\n        codes = []\n        if orientation == 'top':\n            # rotate labels if any of them is longer than 4 characters\n            if rotation is None and group_labels:\n                if max([len(x) for x in group_labels]) > 4:\n                    rotation = 90\n                else:\n                    rotation = 0\n            for idx, (left_coor, right_coor) in enumerate(zip(left, right)):\n                verts.append((left_coor, 0))  # lower-left\n                verts.append((left_coor, 0.6))  # upper-left\n                verts.append((right_coor, 0.6))  # upper-right\n                verts.append((right_coor, 0))  # lower-right\n\n                codes.append(Path.MOVETO)\n                codes.append(Path.LINETO)\n                codes.append(Path.LINETO)\n                codes.append(Path.LINETO)\n\n                group_x_center = left[idx] + float(right[idx] - left[idx]) / 2\n                gene_groups_ax.text(\n                    group_x_center,\n                    1.1,\n                    group_labels[idx],\n                    ha='center',\n                    va='bottom',\n                    rotation=rotation,\n                )\n        else:\n            top = left\n            bottom = right\n            for idx, (top_coor, bottom_coor) in enumerate(zip(top, bottom)):\n                verts.append((0, top_coor))  # upper-left\n                verts.append((0.4, top_coor))  # upper-right\n                verts.append((0.4, bottom_coor))  # lower-right\n                verts.append((0, bottom_coor))  # lower-left\n\n                codes.append(Path.MOVETO)\n                codes.append(Path.LINETO)\n                codes.append(Path.LINETO)\n                codes.append(Path.LINETO)\n\n                diff = bottom[idx] - top[idx]\n                group_y_center = top[idx] + float(diff) / 2\n                if diff * 2 < len(group_labels[idx]):\n                    # cut label to fit available space\n                    group_labels[idx] = group_labels[idx][: int(diff * 2)] + \".\"\n                gene_groups_ax.text(\n                    1.1,\n                    group_y_center,\n                    group_labels[idx],\n                    ha='right',\n                    va='center',\n                    rotation=270,\n                    fontsize='small',\n                )\n\n        path = Path(verts, codes)\n\n        patch = patches.PathPatch(path, facecolor='none', lw=1.5)\n\n        gene_groups_ax.add_patch(patch)\n        gene_groups_ax.grid(False)\n        gene_groups_ax.axis('off')\n        # remove y ticks\n        gene_groups_ax.tick_params(axis='y', left=False, labelleft=False)\n        # remove x ticks and labels\n        gene_groups_ax.tick_params(\n            axis='x', bottom=False, labelbottom=False, labeltop=False", "idx": 193}
{"project": "Scanpy", "commit_id": "276_scanpy_1.9.0__baseplot_class.py__update_var_groups.py", "target": 0, "func": "def _update_var_groups(self):\n        \"\"\"\n        checks if var_names is a dict. Is this is the cases, then set the\n        correct values for var_group_labels and var_group_positions\n\n        updates var_names, var_group_labels, var_group_positions\n\n        Returns\n        -------\n        None\n\n        \"\"\"\n        if isinstance(self.var_names, cabc.Mapping):\n            if self.has_var_groups:\n                logg.warning(\n                    \"`var_names` is a dictionary. This will reset the current \"\n                    \"values of `var_group_labels` and `var_group_positions`.\"\n                )\n            var_group_labels = []\n            _var_names = []\n            var_group_positions = []\n            start = 0\n            for label, vars_list in self.var_names.items():\n                if isinstance(vars_list, str):\n                    vars_list = [vars_list]\n                # use list() in case var_list is a numpy array or pandas series\n                _var_names.extend(list(vars_list))\n                var_group_labels.append(label)\n                var_group_positions.append((start, start + len(vars_list) - 1))\n                start += len(vars_list)\n            self.var_names = _var_names\n            self.var_group_labels = var_group_labels\n            self.var_group_positions = var_group_positions\n            self.has_var_groups = True\n\n        elif isinstance(self.var_names, str):\n            self.var_names = [self.var_names]", "idx": 194}
{"project": "Scanpy", "commit_id": "277_scanpy_1.9.0__baseplot_class.py__format_first_three_categories.py", "target": 0, "func": "def _format_first_three_categories(_categories):\n            \"\"\"used to clean up warning message\"\"\"\n            _categories = list(_categories)\n            if len(_categories) > 3:\n                _categories = _categories[:3] + ['etc.']\n            return ', '.join(_categories)", "idx": 195}
{"project": "Scanpy", "commit_id": "278_scanpy_1.9.0__dotplot.py_dotplot.py", "target": 0, "func": "def dotplot(\n    adata: AnnData,\n    var_names: Union[_VarNames, Mapping[str, _VarNames]],\n    groupby: Union[str, Sequence[str]],\n    use_raw: Optional[bool] = None,\n    log: bool = False,\n    num_categories: int = 7,\n    expression_cutoff: float = 0.0,\n    mean_only_expressed: bool = False,\n    cmap: str = 'Reds',\n    dot_max: Optional[float] = DotPlot.DEFAULT_DOT_MAX,\n    dot_min: Optional[float] = DotPlot.DEFAULT_DOT_MIN,\n    standard_scale: Optional[Literal['var', 'group']] = None,\n    smallest_dot: Optional[float] = DotPlot.DEFAULT_SMALLEST_DOT,\n    title: Optional[str] = None,\n    colorbar_title: Optional[str] = DotPlot.DEFAULT_COLOR_LEGEND_TITLE,\n    size_title: Optional[str] = DotPlot.DEFAULT_SIZE_LEGEND_TITLE,\n    figsize: Optional[Tuple[float, float]] = None,\n    dendrogram: Union[bool, str] = False,\n    gene_symbols: Optional[str] = None,\n    var_group_positions: Optional[Sequence[Tuple[int, int]]] = None,\n    var_group_labels: Optional[Sequence[str]] = None,\n    var_group_rotation: Optional[float] = None,\n    layer: Optional[str] = None,\n    swap_axes: Optional[bool] = False,\n    dot_color_df: Optional[pd.DataFrame] = None,\n    show: Optional[bool] = None,\n    save: Union[str, bool, None] = None,\n    ax: Optional[_AxesSubplot] = None,\n    return_fig: Optional[bool] = False,\n    vmin: Optional[float] = None,\n    vmax: Optional[float] = None,\n    vcenter: Optional[float] = None,\n    norm: Optional[Normalize] = None,\n    **kwds,\n) -> Union[DotPlot, dict, None]:\n    \"\"\"\\\n    Makes a *dot plot* of the expression values of `var_names`.\n\n    For each var_name and each `groupby` category a dot is plotted.\n    Each dot represents two values: mean expression within each category\n    (visualized by color) and fraction of cells expressing the `var_name` in the\n    category (visualized by the size of the dot). If `groupby` is not given,\n    the dotplot assumes that all data belongs to a single category.\n\n    .. note::\n       A gene is considered expressed if the expression value in the `adata` (or\n       `adata.raw`) is above the specified threshold which is zero by default.\n\n    An example of dotplot usage is to visualize, for multiple marker genes,\n    the mean value and the percentage of cells expressing the gene\n    across  multiple clusters.\n\n    This function provides a convenient interface to the :class:`~scanpy.pl.DotPlot`\n    class. If you need more flexibility, you should use :class:`~scanpy.pl.DotPlot`\n    directly.\n\n    Parameters\n    ----------\n    {common_plot_args}\n    {groupby_plots_args}\n    size_title\n        Title for the size legend. New line character (\\\\n) can be used.\n    expression_cutoff\n        Expression cutoff that is used for binarizing the gene expression and\n        determining the fraction of cells expressing given genes. A gene is\n        expressed only if the expression value is greater than this threshold.\n    mean_only_expressed\n        If True, gene expression is averaged only over the cells\n        expressing the given genes.\n    dot_max\n        If none, the maximum dot size is set to the maximum fraction value found\n        (e.g. 0.6). If given, the value should be a number between 0 and 1.\n        All fractions larger than dot_max are clipped to this value.\n    dot_min\n        If none, the minimum dot size is set to 0. If given,\n        the value should be a number between 0 and 1.\n        All fractions smaller than dot_min are clipped to this value.\n    smallest_dot\n        If none, the smallest dot has size 0.\n        All expression levels with `dot_min` are plotted with this size.\n    {show_save_ax}\n    {vminmax}\n    kwds\n        Are passed to :func:`matplotlib.pyplot.scatter`.\n\n    Returns\n    -------\n    If `return_fig` is `True`, returns a :class:`~scanpy.pl.DotPlot` object,\n    else if `show` is false, return axes dict\n\n    See also\n    --------\n    :class:`~scanpy.pl.DotPlot`: The DotPlot class can be used to to control\n        several visual parameters not available in this function.\n    :func:`~scanpy.pl.rank_genes_groups_dotplot`: to plot marker genes\n        identified using the :func:`~scanpy.tl.rank_genes_groups` function.\n\n    Examples\n    --------\n\n    Create a dot plot using the given markers and the PBMC example dataset grouped by\n    the category 'bulk_labels'.\n\n    .. plot::\n        :context: close-figs\n\n        import scanpy as sc\n        adata = sc.datasets.pbmc68k_reduced()\n        markers = ['C1QA', 'PSAP', 'CD79A', 'CD79B', 'CST3', 'LYZ']\n        sc.pl.dotplot(adata, markers, groupby='bulk_labels', dendrogram=True)\n\n    Using var_names as dict:\n\n    .. plot::\n        :context: close-figs\n\n        markers = {{'T-cell': 'CD3D', 'B-cell': 'CD79A', 'myeloid': 'CST3'}}\n        sc.pl.dotplot(adata, markers, groupby='bulk_labels', dendrogram=True)\n\n    Get DotPlot object for fine tuning\n\n    .. plot::\n        :context: close-figs\n\n        dp = sc.pl.dotplot(adata, markers, 'bulk_labels', return_fig=True)\n        dp.add_totals().style(dot_edge_color='black', dot_edge_lw=0.5).show()\n\n    The axes used can be obtained using the get_axes() method\n\n    .. code-block:: python\n\n        axes_dict = dp.get_axes()\n        print(axes_dict)\n\n    \"\"\"\n\n    # backwards compatibility: previous version of dotplot used `color_map`\n    # instead of `cmap`\n    cmap = kwds.get('color_map', cmap)\n    if 'color_map' in kwds:\n        del kwds['color_map']\n\n    dp = DotPlot(\n        adata,\n        var_names,\n        groupby,\n        use_raw=use_raw,\n        log=log,\n        num_categories=num_categories,\n        expression_cutoff=expression_cutoff,\n        mean_only_expressed=mean_only_expressed,\n        standard_scale=standard_scale,\n        title=title,\n        figsize=figsize,\n        gene_symbols=gene_symbols,\n        var_group_positions=var_group_positions,\n        var_group_labels=var_group_labels,\n        var_group_rotation=var_group_rotation,\n        layer=layer,\n        dot_color_df=dot_color_df,\n        ax=ax,\n        vmin=vmin,\n        vmax=vmax,\n        vcenter=vcenter,\n        norm=norm,\n        **kwds,\n    )\n\n    if dendrogram:\n        dp.add_dendrogram(dendrogram_key=dendrogram)\n    if swap_axes:\n        dp.swap_axes()\n\n    dp = dp.style(\n        cmap=cmap,\n        dot_max=dot_max,\n        dot_min=dot_min,\n        smallest_dot=smallest_dot,\n        dot_edge_lw=kwds.pop('linewidth', DotPlot.DEFAULT_DOT_EDGELW),\n    ).legend(colorbar_title=colorbar_title, size_title=size_title)\n\n    if return_fig:\n        return dp\n    else:\n        dp.make_figure()\n        savefig_or_show(DotPlot.DEFAULT_SAVE_PREFIX, show=show, save=save)\n        show = settings.autoshow if show is None else show\n        if not show:\n            return dp.get_axes()", "idx": 196}
{"project": "Scanpy", "commit_id": "279_scanpy_1.9.0__dotplot.py___init__.py", "target": 0, "func": "def __init__(\n        self,\n        adata: AnnData,\n        var_names: Union[_VarNames, Mapping[str, _VarNames]],\n        groupby: Union[str, Sequence[str]],\n        use_raw: Optional[bool] = None,\n        log: bool = False,\n        num_categories: int = 7,\n        categories_order: Optional[Sequence[str]] = None,\n        title: Optional[str] = None,\n        figsize: Optional[Tuple[float, float]] = None,\n        gene_symbols: Optional[str] = None,\n        var_group_positions: Optional[Sequence[Tuple[int, int]]] = None,\n        var_group_labels: Optional[Sequence[str]] = None,\n        var_group_rotation: Optional[float] = None,\n        layer: Optional[str] = None,\n        expression_cutoff: float = 0.0,\n        mean_only_expressed: bool = False,\n        standard_scale: Literal['var', 'group'] = None,\n        dot_color_df: Optional[pd.DataFrame] = None,\n        dot_size_df: Optional[pd.DataFrame] = None,\n        ax: Optional[_AxesSubplot] = None,\n        vmin: Optional[float] = None,\n        vmax: Optional[float] = None,\n        vcenter: Optional[float] = None,\n        norm: Optional[Normalize] = None,\n        **kwds,\n    ):\n        BasePlot.__init__(\n            self,\n            adata,\n            var_names,\n            groupby,\n            use_raw=use_raw,\n            log=log,\n            num_categories=num_categories,\n            categories_order=categories_order,\n            title=title,\n            figsize=figsize,\n            gene_symbols=gene_symbols,\n            var_group_positions=var_group_positions,\n            var_group_labels=var_group_labels,\n            var_group_rotation=var_group_rotation,\n            layer=layer,\n            ax=ax,\n            vmin=vmin,\n            vmax=vmax,\n            vcenter=vcenter,\n            norm=norm,\n            **kwds,\n        )\n\n        # for if category defined by groupby (if any) compute for each var_name\n        # 1. the fraction of cells in the category having a value >expression_cutoff\n        # 2. the mean value over the category\n\n        # 1. compute fraction of cells having value > expression_cutoff\n        # transform obs_tidy into boolean matrix using the expression_cutoff\n        obs_bool = self.obs_tidy > expression_cutoff\n\n        # compute the sum per group which in the boolean matrix this is the number\n        # of values >expression_cutoff, and divide the result by the total number of\n        # values in the group (given by `count()`)\n        if dot_size_df is None:\n            dot_size_df = (\n                obs_bool.groupby(level=0).sum() / obs_bool.groupby(level=0).count()\n            )\n\n        if dot_color_df is None:\n            # 2. compute mean expression value value\n            if mean_only_expressed:\n                dot_color_df = (\n                    self.obs_tidy.mask(~obs_bool).groupby(level=0).mean().fillna(0)\n                )\n            else:\n                dot_color_df = self.obs_tidy.groupby(level=0).mean()\n\n            if standard_scale == 'group':\n                dot_color_df = dot_color_df.sub(dot_color_df.min(1), axis=0)\n                dot_color_df = dot_color_df.div(dot_color_df.max(1), axis=0).fillna(0)\n            elif standard_scale == 'var':\n                dot_color_df -= dot_color_df.min(0)\n                dot_color_df = (dot_color_df / dot_color_df.max(0)).fillna(0)\n            elif standard_scale is None:\n                pass\n            else:\n                logg.warning('Unknown type for standard_scale, ignored')\n        else:\n            # check that both matrices have the same shape\n            if dot_color_df.shape != dot_size_df.shape:\n                logg.error(\n                    \"the given dot_color_df data frame has a different shape than\"\n                    \"the data frame used for the dot size. Both data frames need\"\n                    \"to have the same index and columns\"\n                )\n\n            # Because genes (columns) can be duplicated (e.g. when the\n            # same gene is reported as marker gene in two clusters)\n            # they need to be removed first,\n            # otherwise, the duplicated genes are further duplicated when reordering\n            # Eg. A df with columns ['a', 'b', 'a'] after reordering columns\n            # with df[['a', 'a', 'b']], results in a df with columns:\n            # ['a', 'a', 'a', 'a', 'b']\n\n            unique_var_names, unique_idx = np.unique(\n                dot_color_df.columns, return_index=True\n            )\n            # remove duplicate columns\n            if len(unique_var_names) != len(self.var_names):\n                dot_color_df = dot_color_df.iloc[:, unique_idx]\n\n            # get the same order for rows and columns in the dot_color_df\n            # using the order from the doc_size_df\n            dot_color_df = dot_color_df.loc[dot_size_df.index][dot_size_df.columns]\n\n        self.dot_color_df = dot_color_df\n        self.dot_size_df = dot_size_df\n\n        # Set default style parameters\n        self.cmap = self.DEFAULT_COLORMAP\n        self.dot_max = self.DEFAULT_DOT_MAX\n        self.dot_min = self.DEFAULT_DOT_MIN\n        self.smallest_dot = self.DEFAULT_SMALLEST_DOT\n        self.largest_dot = self.DEFAULT_LARGEST_DOT\n        self.color_on = self.DEFAULT_COLOR_ON\n        self.size_exponent = self.DEFAULT_SIZE_EXPONENT\n        self.grid = False\n        self.plot_x_padding = self.DEFAULT_PLOT_X_PADDING\n        self.plot_y_padding = self.DEFAULT_PLOT_Y_PADDING\n\n        self.dot_edge_color = self.DEFAULT_DOT_EDGECOLOR\n        self.dot_edge_lw = self.DEFAULT_DOT_EDGELW\n\n        # set legend defaults\n        self.color_legend_title = self.DEFAULT_COLOR_LEGEND_TITLE\n        self.size_title = self.DEFAULT_SIZE_LEGEND_TITLE\n        self.legends_width = self.DEFAULT_LEGENDS_WIDTH\n        self.show_size_legend = True\n        self.show_colorbar = True", "idx": 197}
{"project": "Scanpy", "commit_id": "27_scanpy_1.9.0_cli.py___getitem__.py", "target": 0, "func": "def __getitem__(self, k: str) -> ArgumentParser:\n        try:\n            return self.parser_map[k]\n        except KeyError:\n            if which(f'{self.command}-{k}'):\n                return _DelegatingParser(self, k)\n            # Only here is the command list retrieved\n            raise ArgumentError(\n                self.action, f'No command \u201c{k}\u201d. Choose from {set(self)}'", "idx": 198}
{"project": "Scanpy", "commit_id": "280_scanpy_1.9.0__dotplot.py_style.py", "target": 0, "func": "def style(\n        self,\n        cmap: str = DEFAULT_COLORMAP,\n        color_on: Optional[Literal['dot', 'square']] = DEFAULT_COLOR_ON,\n        dot_max: Optional[float] = DEFAULT_DOT_MAX,\n        dot_min: Optional[float] = DEFAULT_DOT_MIN,\n        smallest_dot: Optional[float] = DEFAULT_SMALLEST_DOT,\n        largest_dot: Optional[float] = DEFAULT_LARGEST_DOT,\n        dot_edge_color: Optional[ColorLike] = DEFAULT_DOT_EDGECOLOR,\n        dot_edge_lw: Optional[float] = DEFAULT_DOT_EDGELW,\n        size_exponent: Optional[float] = DEFAULT_SIZE_EXPONENT,\n        grid: Optional[float] = False,\n        x_padding: Optional[float] = DEFAULT_PLOT_X_PADDING,\n        y_padding: Optional[float] = DEFAULT_PLOT_Y_PADDING,\n    ):\n        \"\"\"\\\n        Modifies plot visual parameters\n\n        Parameters\n        ----------\n        cmap\n            String denoting matplotlib color map.\n        color_on\n            Options are 'dot' or 'square'. Be default the colomap is applied to\n            the color of the dot. Optionally, the colormap can be applied to an\n            square behind the dot, in which case the dot is transparent and only\n            the edge is shown.\n        dot_max\n            If none, the maximum dot size is set to the maximum fraction value found\n            (e.g. 0.6). If given, the value should be a number between 0 and 1.\n            All fractions larger than dot_max are clipped to this value.\n        dot_min\n            If none, the minimum dot size is set to 0. If given,\n            the value should be a number between 0 and 1.\n            All fractions smaller than dot_min are clipped to this value.\n        smallest_dot\n            If none, the smallest dot has size 0.\n            All expression fractions with `dot_min` are plotted with this size.\n        largest_dot\n            If none, the largest dot has size 200.\n            All expression fractions with `dot_max` are plotted with this size.\n        dot_edge_color\n            Dot edge color. When `color_on='dot'` the default is no edge. When\n            `color_on='square'`, edge color is white for darker colors and black\n            for lighter background square colors.\n        dot_edge_lw\n            Dot edge line width. When `color_on='dot'` the default is no edge. When\n            `color_on='square'`, line width = 1.5.\n        size_exponent\n            Dot size is computed as:\n            fraction  ** size exponent and afterwards scaled to match the\n            `smallest_dot` and `largest_dot` size parameters.\n            Using a different size exponent changes the relative sizes of the dots\n            to each other.\n        grid\n            Set to true to show grid lines. By default grid lines are not shown.\n            Further configuration of the grid lines can be achieved directly on the\n            returned ax.\n        x_padding\n            Space between the plot left/right borders and the dots center. A unit\n            is the distance between the x ticks. Only applied when color_on = dot\n        y_padding\n            Space between the plot top/bottom borders and the dots center. A unit is\n            the distance between the y ticks. Only applied when color_on = dot\n\n        Returns\n        -------\n        :class:`~scanpy.pl.DotPlot`\n\n        Examples\n        -------\n\n        >>> adata = sc.datasets.pbmc68k_reduced()\n        >>> markers = ['C1QA', 'PSAP', 'CD79A', 'CD79B', 'CST3', 'LYZ']\n\n        Change color map and apply it to the square behind the dot\n\n        >>> sc.pl.DotPlot(adata, markers, groupby='bulk_labels')\\\n        ...               .style(cmap='RdBu_r', color_on='square').show()\n\n        Add edge to dots and plot a grid\n\n        >>> sc.pl.DotPlot(adata, markers, groupby='bulk_labels')\\\n        ...               .style(dot_edge_color='black', dot_edge_lw=1, grid=True)\\\n        ...               .show()\n\n        \"\"\"\n\n        # change only the values that had changed\n        if cmap != self.cmap:\n            self.cmap = cmap\n        if dot_max != self.dot_max:\n            self.dot_max = dot_max\n        if dot_min != self.dot_min:\n            self.dot_min = dot_min\n        if smallest_dot != self.smallest_dot:\n            self.smallest_dot = smallest_dot\n        if largest_dot != self.largest_dot:\n            self.largest_dot = largest_dot\n        if color_on != self.color_on:\n            self.color_on = color_on\n        if size_exponent != self.size_exponent:\n            self.size_exponent = size_exponent\n        if dot_edge_color != self.dot_edge_color:\n            self.dot_edge_color = dot_edge_color\n        if dot_edge_lw != self.dot_edge_lw:\n            self.dot_edge_lw = dot_edge_lw\n        if grid != self.grid:\n            self.grid = grid\n        if x_padding != self.plot_x_padding:\n            self.plot_x_padding = x_padding\n        if y_padding != self.plot_y_padding:\n            self.plot_y_padding = y_padding\n\n        return self", "idx": 199}
{"project": "Scanpy", "commit_id": "281_scanpy_1.9.0__dotplot.py_legend.py", "target": 0, "func": "def legend(\n        self,\n        show: Optional[bool] = True,\n        show_size_legend: Optional[bool] = True,\n        show_colorbar: Optional[bool] = True,\n        size_title: Optional[str] = DEFAULT_SIZE_LEGEND_TITLE,\n        colorbar_title: Optional[str] = DEFAULT_COLOR_LEGEND_TITLE,\n        width: Optional[float] = DEFAULT_LEGENDS_WIDTH,\n    ):\n        \"\"\"\\\n        Configures dot size and the colorbar legends\n\n        Parameters\n        ----------\n        show\n            Set to `False` to hide the default plot of the legends. This sets the\n            legend width to zero, which will result in a wider main plot.\n        show_size_legend\n            Set to `False` to hide the dot size legend\n        show_colorbar\n            Set to `False` to hide the colorbar legend\n        size_title\n            Title for the dot size legend. Use '\\\\n' to add line breaks. Appears on top\n            of dot sizes\n        colorbar_title\n            Title for the color bar. Use '\\\\n' to add line breaks. Appears on top of the\n            color bar\n        width\n            Width of the legends area. The unit is the same as in matplotlib (inches).\n\n        Returns\n        -------\n        :class:`~scanpy.pl.DotPlot`\n\n        Examples\n        --------\n\n        Set color bar title:\n\n        >>> adata = sc.datasets.pbmc68k_reduced()\n        >>> markers = {{'T-cell': 'CD3D', 'B-cell': 'CD79A', 'myeloid': 'CST3'}}\n        >>> dp = sc.pl.DotPlot(adata, markers, groupby='bulk_labels')\n        >>> dp.legend(colorbar_title='log(UMI counts + 1)').show()\n        \"\"\"\n\n        if not show:\n            # turn of legends by setting width to 0\n            self.legends_width = 0\n        else:\n            self.color_legend_title = colorbar_title\n            self.size_title = size_title\n            self.legends_width = width\n            self.show_size_legend = show_size_legend\n            self.show_colorbar = show_colorbar\n\n        return self", "idx": 200}
{"project": "Scanpy", "commit_id": "282_scanpy_1.9.0__dotplot.py__plot_size_legend.py", "target": 0, "func": "def _plot_size_legend(self, size_legend_ax: Axes):\n        # for the dot size legend, use step between dot_max and dot_min\n        # based on how different they are.\n        diff = self.dot_max - self.dot_min\n        if 0.3 < diff <= 0.6:\n            step = 0.1\n        elif diff <= 0.3:\n            step = 0.05\n        else:\n            step = 0.2\n        # a descending range that is afterwards inverted is used\n        # to guarantee that dot_max is in the legend.\n        size_range = np.arange(self.dot_max, self.dot_min, step * -1)[::-1]\n        if self.dot_min != 0 or self.dot_max != 1:\n            dot_range = self.dot_max - self.dot_min\n            size_values = (size_range - self.dot_min) / dot_range\n        else:\n            size_values = size_range\n\n        size = size_values**self.size_exponent\n        size = size * (self.largest_dot - self.smallest_dot) + self.smallest_dot\n\n        # plot size bar\n        size_legend_ax.scatter(\n            np.arange(len(size)) + 0.5,\n            np.repeat(0, len(size)),\n            s=size,\n            color='gray',\n            edgecolor='black',\n            linewidth=self.dot_edge_lw,\n            zorder=100,\n        )\n        size_legend_ax.set_xticks(np.arange(len(size)) + 0.5)\n        labels = [\n            \"{}\".format(np.round((x * 100), decimals=0).astype(int)) for x in size_range\n        ]\n        size_legend_ax.set_xticklabels(labels, fontsize='small')\n\n        # remove y ticks and labels\n        size_legend_ax.tick_params(\n            axis='y', left=False, labelleft=False, labelright=False\n        )\n\n        # remove surrounding lines\n        size_legend_ax.spines['right'].set_visible(False)\n        size_legend_ax.spines['top'].set_visible(False)\n        size_legend_ax.spines['left'].set_visible(False)\n        size_legend_ax.spines['bottom'].set_visible(False)\n        size_legend_ax.grid(False)\n\n        ymax = size_legend_ax.get_ylim()[1]\n        size_legend_ax.set_ylim(-1.05 - self.largest_dot * 0.003, 4)\n        size_legend_ax.set_title(self.size_title, y=ymax + 0.45, size='small')\n\n        xmin, xmax = size_legend_ax.get_xlim()\n        size_legend_ax.set_xlim(xmin - 0.15, xmax + 0.5)", "idx": 201}
{"project": "Scanpy", "commit_id": "283_scanpy_1.9.0__dotplot.py__plot_legend.py", "target": 0, "func": "def _plot_legend(self, legend_ax, return_ax_dict, normalize):\n\n        # to maintain the fixed height size of the legends, a\n        # spacer of variable height is added at the bottom.\n        # The structure for the legends is:\n        # first row: variable space to keep the other rows of\n        #            the same size (avoid stretching)\n        # second row: legend for dot size\n        # third row: spacer to avoid color and size legend titles to overlap\n        # fourth row: colorbar\n\n        cbar_legend_height = self.min_figure_height * 0.08\n        size_legend_height = self.min_figure_height * 0.27\n        spacer_height = self.min_figure_height * 0.3\n\n        height_ratios = [\n            self.height - size_legend_height - cbar_legend_height - spacer_height,\n            size_legend_height,\n            spacer_height,\n            cbar_legend_height,\n        ]\n        fig, legend_gs = make_grid_spec(\n            legend_ax, nrows=4, ncols=1, height_ratios=height_ratios\n        )\n\n        if self.show_size_legend:\n            size_legend_ax = fig.add_subplot(legend_gs[1])\n            self._plot_size_legend(size_legend_ax)\n            return_ax_dict['size_legend_ax'] = size_legend_ax\n\n        if self.show_colorbar:\n            color_legend_ax = fig.add_subplot(legend_gs[3])\n\n            self._plot_colorbar(color_legend_ax, normalize)\n            return_ax_dict['color_legend_ax'] = color_legend_ax", "idx": 202}
{"project": "Scanpy", "commit_id": "284_scanpy_1.9.0__dotplot.py__mainplot.py", "target": 1, "func": "def _mainplot(self, ax):\n        # work on a copy of the dataframes. This is to avoid changes\n        # on the original data frames after repetitive calls to the\n        # DotPlot object, for example once with swap_axes and other without\n\n        _color_df = self.dot_color_df.copy()\n        _size_df = self.dot_size_df.copy()\n        if self.var_names_idx_order is not None:\n            _color_df = _color_df.iloc[:, self.var_names_idx_order]\n            _size_df = _size_df.iloc[:, self.var_names_idx_order]\n\n        if self.categories_order is not None:\n            _color_df = _color_df.loc[self.categories_order, :]\n            _size_df = _size_df.loc[self.categories_order, :]\n\n        if self.are_axes_swapped:\n            _size_df = _size_df.T\n            _color_df = _color_df.T\n        self.cmap = self.kwds.get('cmap', self.cmap)\n        if 'cmap' in self.kwds:\n            del self.kwds['cmap']\n\n        normalize, dot_min, dot_max = self._dotplot(\n            _size_df,\n            _color_df,\n            ax,\n            cmap=self.cmap,\n            dot_max=self.dot_max,\n            dot_min=self.dot_min,\n            color_on=self.color_on,\n            edge_color=self.dot_edge_color,\n            edge_lw=self.dot_edge_lw,\n            smallest_dot=self.smallest_dot,\n            largest_dot=self.largest_dot,\n            size_exponent=self.size_exponent,\n            grid=self.grid,\n            x_padding=self.plot_x_padding,\n            y_padding=self.plot_y_padding,\n            vmin=self.vboundnorm.vmin,\n            vmax=self.vboundnorm.vmax,\n            vcenter=self.vboundnorm.vcenter,\n            norm=self.vboundnorm.norm,\n            **self.kwds,\n        )\n\n        self.dot_min, self.dot_max = dot_min, dot_max\n        return normalize", "idx": 203}
{"project": "Scanpy", "commit_id": "285_scanpy_1.9.0__dotplot.py__dotplot.py", "target": 1, "func": "def _dotplot(\n        dot_size,\n        dot_color,\n        dot_ax,\n        cmap: str = 'Reds',\n        color_on: Optional[str] = 'dot',\n        y_label: Optional[str] = None,\n        dot_max: Optional[float] = None,\n        dot_min: Optional[float] = None,\n        standard_scale: Literal['var', 'group'] = None,\n        smallest_dot: Optional[float] = 0.0,\n        largest_dot: Optional[float] = 200,\n        size_exponent: Optional[float] = 2,\n        edge_color: Optional[ColorLike] = None,\n        edge_lw: Optional[float] = None,\n        grid: Optional[bool] = False,\n        x_padding: Optional[float] = 0.8,\n        y_padding: Optional[float] = 1.0,\n        vmin: Optional[float] = None,\n        vmax: Optional[float] = None,\n        vcenter: Optional[float] = None,\n        norm: Optional[Normalize] = None,\n        **kwds,\n    ):\n        \"\"\"\\\n        Makes a *dot plot* given two data frames, one containing\n        the doc size and other containing the dot color. The indices and\n        columns of the data frame are used to label the output image\n\n        The dots are plotted using :func:`matplotlib.pyplot.scatter`. Thus, additional\n        arguments can be passed.\n\n        Parameters\n        ----------\n        dot_size: Data frame containing the dot_size.\n        dot_color: Data frame containing the dot_color, should have the same,\n                shape, columns and indices as dot_size.\n        dot_ax: matplotlib axis\n        cmap\n            String denoting matplotlib color map.\n        color_on\n            Options are 'dot' or 'square'. Be default the colomap is applied to\n            the color of the dot. Optionally, the colormap can be applied to an\n            square behind the dot, in which case the dot is transparent and only\n            the edge is shown.\n        y_label: String. Label for y axis\n        dot_max\n            If none, the maximum dot size is set to the maximum fraction value found\n            (e.g. 0.6). If given, the value should be a number between 0 and 1.\n            All fractions larger than dot_max are clipped to this value.\n        dot_min\n            If none, the minimum dot size is set to 0. If given,\n            the value should be a number between 0 and 1.\n            All fractions smaller than dot_min are clipped to this value.\n        standard_scale\n            Whether or not to standardize that dimension between 0 and 1,\n            meaning for each variable or group,\n            subtract the minimum and divide each by its maximum.\n        smallest_dot\n            If none, the smallest dot has size 0.\n            All expression levels with `dot_min` are plotted with this size.\n        edge_color\n            Dot edge color. When `color_on='dot'` the default is no edge. When\n            `color_on='square'`, edge color is white\n        edge_lw\n            Dot edge line width. When `color_on='dot'` the default is no edge. When\n            `color_on='square'`, line width = 1.5\n        grid\n            Adds a grid to the plot\n        x_paddding\n            Space between the plot left/right borders and the dots center. A unit\n            is the distance between the x ticks. Only applied when color_on = dot\n        y_paddding\n            Space between the plot top/bottom borders and the dots center. A unit is\n            the distance between the y ticks. Only applied when color_on = dot\n        kwds\n            Are passed to :func:`matplotlib.pyplot.scatter`.\n\n        Returns\n        -------\n        matplotlib.colors.Normalize, dot_min, dot_max\n\n        \"\"\"\n        assert dot_size.shape == dot_color.shape, (\n            'please check that dot_size ' 'and dot_color dataframes have the same shape'\n        )\n\n        assert list(dot_size.index) == list(dot_color.index), (\n            'please check that dot_size ' 'and dot_color dataframes have the same index'\n        )\n\n        assert list(dot_size.columns) == list(dot_color.columns), (\n            'please check that the dot_size '\n            'and dot_color dataframes have the same columns'\n        )\n\n        if standard_scale == 'group':\n            dot_color = dot_color.sub(dot_color.min(1), axis=0)\n            dot_color = dot_color.div(dot_color.max(1), axis=0).fillna(0)\n        elif standard_scale == 'var':\n            dot_color -= dot_color.min(0)\n            dot_color = (dot_color / dot_color.max(0)).fillna(0)\n        elif standard_scale is None:\n            pass\n\n        # make scatter plot in which\n        # x = var_names\n        # y = groupby category\n        # size = fraction\n        # color = mean expression\n\n        # +0.5 in y and x to set the dot center at 0.5 multiples\n        # this facilitates dendrogram and totals alignment for\n        # matrixplot, dotplot and stackec_violin using the same coordinates.\n        y, x = np.indices(dot_color.shape)\n        y = y.flatten() + 0.5\n        x = x.flatten() + 0.5\n        frac = dot_size.values.flatten()\n        mean_flat = dot_color.values.flatten()\n        cmap = pl.get_cmap(kwds.get('cmap', cmap))\n        if 'cmap' in kwds:\n            del kwds['cmap']\n        if dot_max is None:\n            dot_max = np.ceil(max(frac) * 10) / 10\n        else:\n            if dot_max < 0 or dot_max > 1:\n                raise ValueError(\"`dot_max` value has to be between 0 and 1\")\n        if dot_min is None:\n            dot_min = 0\n        else:\n            if dot_min < 0 or dot_min > 1:\n                raise ValueError(\"`dot_min` value has to be between 0 and 1\")\n\n        if dot_min != 0 or dot_max != 1:\n            # clip frac between dot_min and  dot_max\n            frac = np.clip(frac, dot_min, dot_max)\n            old_range = dot_max - dot_min\n            # re-scale frac between 0 and 1\n            frac = (frac - dot_min) / old_range\n\n        size = frac**size_exponent\n        # rescale size to match smallest_dot and largest_dot\n        size = size * (largest_dot - smallest_dot) + smallest_dot\n        normalize = check_colornorm(vmin, vmax, vcenter, norm)\n\n        if color_on == 'square':\n            if edge_color is None:\n                from seaborn.utils import relative_luminance\n\n                # use either black or white for the edge color\n                # depending on the luminance of the background\n                # square color\n                edge_color = []\n                for color_value in cmap(normalize(mean_flat)):\n                    lum = relative_luminance(color_value)\n                    edge_color.append(\".15\" if lum > 0.408 else \"w\")\n\n            edge_lw = 1.5 if edge_lw is None else edge_lw\n\n            # first make a heatmap similar to `sc.pl.matrixplot`\n            # (squares with the asigned colormap). Circles will be plotted\n            # on top\n            dot_ax.pcolor(dot_color.values, cmap=cmap, norm=normalize)\n            for axis in ['top', 'bottom', 'left', 'right']:\n                dot_ax.spines[axis].set_linewidth(1.5)\n            kwds = fix_kwds(\n                kwds,\n                s=size,\n                cmap=cmap,\n                linewidth=edge_lw,\n                facecolor='none',\n                edgecolor=edge_color,\n                norm=normalize,\n            )\n            dot_ax.scatter(x, y, **kwds)\n        else:\n            edge_color = 'none' if edge_color is None else edge_color\n            edge_lw = 0.0 if edge_lw is None else edge_lw\n\n            color = cmap(normalize(mean_flat))\n            kwds = fix_kwds(\n                kwds,\n                s=size,\n                cmap=cmap,\n                color=color,\n                linewidth=edge_lw,\n                edgecolor=edge_color,\n                norm=normalize,\n            )\n\n            dot_ax.scatter(x, y, **kwds)\n\n        y_ticks = np.arange(dot_color.shape[0]) + 0.5\n        dot_ax.set_yticks(y_ticks)\n        dot_ax.set_yticklabels(\n            [dot_color.index[idx] for idx, _ in enumerate(y_ticks)], minor=False\n        )\n\n        x_ticks = np.arange(dot_color.shape[1]) + 0.5\n        dot_ax.set_xticks(x_ticks)\n        dot_ax.set_xticklabels(\n            [dot_color.columns[idx] for idx, _ in enumerate(x_ticks)],\n            rotation=90,\n            ha='center',\n            minor=False,\n        )\n        dot_ax.tick_params(axis='both', labelsize='small')\n        dot_ax.grid(False)\n        dot_ax.set_ylabel(y_label)\n\n        # to be consistent with the heatmap plot, is better to\n        # invert the order of the y-axis, such that the first group is on\n        # top\n        dot_ax.set_ylim(dot_color.shape[0], 0)\n        dot_ax.set_xlim(0, dot_color.shape[1])\n\n        if color_on == 'dot':\n            # add padding to the x and y lims when the color is not in the square\n            # default y range goes from 0.5 to num cols + 0.5\n            # and default x range goes from 0.5 to num rows + 0.5, thus\n            # the padding needs to be corrected.\n            x_padding = x_padding - 0.5\n            y_padding = y_padding - 0.5\n            dot_ax.set_ylim(dot_color.shape[0] + y_padding, -y_padding)\n\n            dot_ax.set_xlim(-x_padding, dot_color.shape[1] + x_padding)\n\n        if grid:\n            dot_ax.grid(True, color='gray', linewidth=0.1)\n            dot_ax.set_axisbelow(True)\n\n        return normalize, dot_min, dot_max", "idx": 204}
{"project": "Scanpy", "commit_id": "286_scanpy_1.9.0__matrixplot.py_matrixplot.py", "target": 0, "func": "def matrixplot(\n    adata: AnnData,\n    var_names: Union[_VarNames, Mapping[str, _VarNames]],\n    groupby: Union[str, Sequence[str]],\n    use_raw: Optional[bool] = None,\n    log: bool = False,\n    num_categories: int = 7,\n    figsize: Optional[Tuple[float, float]] = None,\n    dendrogram: Union[bool, str] = False,\n    title: Optional[str] = None,\n    cmap: Optional[str] = MatrixPlot.DEFAULT_COLORMAP,\n    colorbar_title: Optional[str] = MatrixPlot.DEFAULT_COLOR_LEGEND_TITLE,\n    gene_symbols: Optional[str] = None,\n    var_group_positions: Optional[Sequence[Tuple[int, int]]] = None,\n    var_group_labels: Optional[Sequence[str]] = None,\n    var_group_rotation: Optional[float] = None,\n    layer: Optional[str] = None,\n    standard_scale: Literal['var', 'group'] = None,\n    values_df: Optional[pd.DataFrame] = None,\n    swap_axes: bool = False,\n    show: Optional[bool] = None,\n    save: Union[str, bool, None] = None,\n    ax: Optional[_AxesSubplot] = None,\n    return_fig: Optional[bool] = False,\n    vmin: Optional[float] = None,\n    vmax: Optional[float] = None,\n    vcenter: Optional[float] = None,\n    norm: Optional[Normalize] = None,\n    **kwds,\n) -> Union[MatrixPlot, dict, None]:\n    \"\"\"\\\n    Creates a heatmap of the mean expression values per group of each var_names.\n\n    This function provides a convenient interface to the :class:`~scanpy.pl.MatrixPlot`\n    class. If you need more flexibility, you should use :class:`~scanpy.pl.MatrixPlot`\n    directly.\n\n    Parameters\n    ----------\n    {common_plot_args}\n    {groupby_plots_args}\n    {show_save_ax}\n    {vminmax}\n    kwds\n        Are passed to :func:`matplotlib.pyplot.pcolor`.\n\n    Returns\n    -------\n    If `return_fig` is `True`, returns a :class:`~scanpy.pl.MatrixPlot` object,\n    else if `show` is false, return axes dict\n\n    See also\n    --------\n    :class:`~scanpy.pl.MatrixPlot`: The MatrixPlot class can be used to to control\n        several visual parameters not available in this function.\n    :func:`~scanpy.pl.rank_genes_groups_matrixplot`: to plot marker genes\n        identified using the :func:`~scanpy.tl.rank_genes_groups` function.\n\n    Examples\n    --------\n\n    .. plot::\n        :context: close-figs\n\n        import scanpy as sc\n        adata = sc.datasets.pbmc68k_reduced()\n        markers = ['C1QA', 'PSAP', 'CD79A', 'CD79B', 'CST3', 'LYZ']\n        sc.pl.matrixplot(adata, markers, groupby='bulk_labels', dendrogram=True)\n\n    Using var_names as dict:\n\n    .. plot::\n        :context: close-figs\n\n        markers = {{'T-cell': 'CD3D', 'B-cell': 'CD79A', 'myeloid': 'CST3'}}\n        sc.pl.matrixplot(adata, markers, groupby='bulk_labels', dendrogram=True)\n\n    Get Matrix object for fine tuning:\n\n    .. plot::\n        :context: close-figs\n\n        mp = sc.pl.matrixplot(adata, markers, 'bulk_labels', return_fig=True)\n        mp.add_totals().style(edge_color='black').show()\n\n    The axes used can be obtained using the get_axes() method\n\n    .. plot::\n        :context: close-figs\n\n        axes_dict = mp.get_axes()\n    \"\"\"\n\n    mp = MatrixPlot(\n        adata,\n        var_names,\n        groupby=groupby,\n        use_raw=use_raw,\n        log=log,\n        num_categories=num_categories,\n        standard_scale=standard_scale,\n        title=title,\n        figsize=figsize,\n        gene_symbols=gene_symbols,\n        var_group_positions=var_group_positions,\n        var_group_labels=var_group_labels,\n        var_group_rotation=var_group_rotation,\n        layer=layer,\n        values_df=values_df,\n        ax=ax,\n        vmin=vmin,\n        vmax=vmax,\n        vcenter=vcenter,\n        norm=norm,\n        **kwds,\n    )\n\n    if dendrogram:\n        mp.add_dendrogram(dendrogram_key=dendrogram)\n    if swap_axes:\n        mp.swap_axes()\n\n    mp = mp.style(cmap=cmap).legend(title=colorbar_title)\n    if return_fig:\n        return mp\n    else:\n        mp.make_figure()\n        savefig_or_show(MatrixPlot.DEFAULT_SAVE_PREFIX, show=show, save=save)\n        show = settings.autoshow if show is None else show\n        if not show:\n            return mp.get_axes()", "idx": 205}
{"project": "Scanpy", "commit_id": "287_scanpy_1.9.0__matrixplot.py___init__.py", "target": 1, "func": "def __init__(\n        self,\n        adata: AnnData,\n        var_names: Union[_VarNames, Mapping[str, _VarNames]],\n        groupby: Union[str, Sequence[str]],\n        use_raw: Optional[bool] = None,\n        log: bool = False,\n        num_categories: int = 7,\n        categories_order: Optional[Sequence[str]] = None,\n        title: Optional[str] = None,\n        figsize: Optional[Tuple[float, float]] = None,\n        gene_symbols: Optional[str] = None,\n        var_group_positions: Optional[Sequence[Tuple[int, int]]] = None,\n        var_group_labels: Optional[Sequence[str]] = None,\n        var_group_rotation: Optional[float] = None,\n        layer: Optional[str] = None,\n        standard_scale: Literal['var', 'group'] = None,\n        ax: Optional[_AxesSubplot] = None,\n        values_df: Optional[pd.DataFrame] = None,\n        vmin: Optional[float] = None,\n        vmax: Optional[float] = None,\n        vcenter: Optional[float] = None,\n        norm: Optional[Normalize] = None,\n        **kwds,\n    ):\n        BasePlot.__init__(\n            self,\n            adata,\n            var_names,\n            groupby,\n            use_raw=use_raw,\n            log=log,\n            num_categories=num_categories,\n            categories_order=categories_order,\n            title=title,\n            figsize=figsize,\n            gene_symbols=gene_symbols,\n            var_group_positions=var_group_positions,\n            var_group_labels=var_group_labels,\n            var_group_rotation=var_group_rotation,\n            layer=layer,\n            ax=ax,\n            vmin=vmin,\n            vmax=vmax,\n            vcenter=vcenter,\n            norm=norm,\n            **kwds,\n        )\n\n        if values_df is None:\n            # compute mean value\n            values_df = self.obs_tidy.groupby(level=0).mean()\n\n            if standard_scale == 'group':\n                values_df = values_df.sub(values_df.min(1), axis=0)\n                values_df = values_df.div(values_df.max(1), axis=0).fillna(0)\n            elif standard_scale == 'var':\n                values_df -= values_df.min(0)\n                values_df = (values_df / values_df.max(0)).fillna(0)\n            elif standard_scale is None:\n                pass\n            else:\n                logg.warning('Unknown type for standard_scale, ignored')\n\n        self.values_df = values_df\n\n        self.cmap = self.DEFAULT_COLORMAP\n        self.edge_color = self.DEFAULT_EDGE_COLOR\n        self.edge_lw = self.DEFAULT_EDGE_LW", "idx": 206}
{"project": "Scanpy", "commit_id": "288_scanpy_1.9.0__matrixplot.py_style.py", "target": 0, "func": "def style(\n        self,\n        cmap: str = DEFAULT_COLORMAP,\n        edge_color: Optional[ColorLike] = DEFAULT_EDGE_COLOR,\n        edge_lw: Optional[float] = DEFAULT_EDGE_LW,\n    ):\n        \"\"\"\\\n        Modifies plot visual parameters.\n\n        Parameters\n        ----------\n        cmap\n            String denoting matplotlib color map.\n        edge_color\n            Edge color between the squares of matrix plot. Default is gray\n        edge_lw\n            Edge line width.\n\n        Returns\n        -------\n        :class:`~scanpy.pl.MatrixPlot`\n\n        Examples\n        -------\n\n        .. plot::\n            :context: close-figs\n\n            import scanpy as sc\n\n            adata = sc.datasets.pbmc68k_reduced()\n            markers = ['C1QA', 'PSAP', 'CD79A', 'CD79B', 'CST3', 'LYZ']\n\n        Change color map and turn off edges:\n\n\n        .. plot::\n            :context: close-figs\n\n            (\n                sc.pl.MatrixPlot(adata, markers, groupby='bulk_labels')\n                .style(cmap='Blues', edge_color='none')\n                .show()\n            )\n\n        \"\"\"\n\n        # change only the values that had changed\n        if cmap != self.cmap:\n            self.cmap = cmap\n        if edge_color != self.edge_color:\n            self.edge_color = edge_color\n        if edge_lw != self.edge_lw:\n            self.edge_lw = edge_lw\n\n        return self", "idx": 207}
{"project": "Scanpy", "commit_id": "289_scanpy_1.9.0__matrixplot.py__mainplot.py", "target": 0, "func": "def _mainplot(self, ax):\n        # work on a copy of the dataframes. This is to avoid changes\n        # on the original data frames after repetitive calls to the\n        # MatrixPlot object, for example once with swap_axes and other without\n\n        _color_df = self.values_df.copy()\n        if self.var_names_idx_order is not None:\n            _color_df = _color_df.iloc[:, self.var_names_idx_order]\n\n        if self.categories_order is not None:\n            _color_df = _color_df.loc[self.categories_order, :]\n\n        if self.are_axes_swapped:\n            _color_df = _color_df.T\n        cmap = pl.get_cmap(self.kwds.get('cmap', self.cmap))\n        if 'cmap' in self.kwds:\n            del self.kwds['cmap']\n        normalize = check_colornorm(\n            self.vboundnorm.vmin,\n            self.vboundnorm.vmax,\n            self.vboundnorm.vcenter,\n            self.vboundnorm.norm,\n        )\n\n        for axis in ['top', 'bottom', 'left', 'right']:\n            ax.spines[axis].set_linewidth(1.5)\n\n        kwds = fix_kwds(\n            self.kwds,\n            cmap=cmap,\n            edgecolor=self.edge_color,\n            linewidth=self.edge_lw,\n            norm=normalize,\n        )\n        _ = ax.pcolor(_color_df, **kwds)\n\n        y_labels = _color_df.index\n        x_labels = _color_df.columns\n\n        y_ticks = np.arange(len(y_labels)) + 0.5\n        ax.set_yticks(y_ticks)\n        ax.set_yticklabels(y_labels)\n\n        x_ticks = np.arange(len(x_labels)) + 0.5\n        ax.set_xticks(x_ticks)\n        ax.set_xticklabels(x_labels, rotation=90, ha='center', minor=False)\n\n        ax.tick_params(axis='both', labelsize='small')\n        ax.grid(False)\n\n        # to be consistent with the heatmap plot, is better to\n        # invert the order of the y-axis, such that the first group is on\n        # top\n        ax.set_ylim(len(y_labels), 0)\n        ax.set_xlim(0, len(x_labels))\n\n        return normalize", "idx": 208}
{"project": "Scanpy", "commit_id": "28_scanpy_1.9.0_cli.py___setitem__.py", "target": 0, "func": "def __setitem__(self, k: str, v: ArgumentParser) -> None:\n        self.parser_map[k] = v", "idx": 209}
{"project": "Scanpy", "commit_id": "290_scanpy_1.9.0__preprocessing.py_highly_variable_genes.py", "target": 0, "func": "def highly_variable_genes(\n    adata_or_result: Union[AnnData, pd.DataFrame, np.recarray],\n    log: bool = False,\n    show: Optional[bool] = None,\n    save: Union[bool, str, None] = None,\n    highly_variable_genes: bool = True,\n):\n    \"\"\"Plot dispersions or normalized variance versus means for genes.\n\n    Produces Supp. Fig. 5c of Zheng et al. (2017) and MeanVarPlot() and\n    VariableFeaturePlot() of Seurat.\n\n    Parameters\n    ----------\n    adata\n        Result of :func:`~scanpy.pp.highly_variable_genes`.\n    log\n        Plot on logarithmic axes.\n    show\n         Show the plot, do not return axis.\n    save\n        If `True` or a `str`, save the figure.\n        A string is appended to the default filename.\n        Infer the filetype if ending on {{`'.pdf'`, `'.png'`, `'.svg'`}}.\n    \"\"\"\n    if isinstance(adata_or_result, AnnData):\n        result = adata_or_result.var\n        seurat_v3_flavor = adata_or_result.uns[\"hvg\"][\"flavor\"] == \"seurat_v3\"\n    else:\n        result = adata_or_result\n        if isinstance(result, pd.DataFrame):\n            seurat_v3_flavor = \"variances_norm\" in result.columns\n        else:\n            seurat_v3_flavor = False\n    if highly_variable_genes:\n        gene_subset = result.highly_variable\n    else:\n        gene_subset = result.gene_subset\n    means = result.means\n\n    if seurat_v3_flavor:\n        var_or_disp = result.variances\n        var_or_disp_norm = result.variances_norm\n    else:\n        var_or_disp = result.dispersions\n        var_or_disp_norm = result.dispersions_norm\n    size = rcParams['figure.figsize']\n    pl.figure(figsize=(2 * size[0], size[1]))\n    pl.subplots_adjust(wspace=0.3)\n    for idx, d in enumerate([var_or_disp_norm, var_or_disp]):\n        pl.subplot(1, 2, idx + 1)\n        for label, color, mask in zip(\n            ['highly variable genes', 'other genes'],\n            ['black', 'grey'],\n            [gene_subset, ~gene_subset],\n        ):\n            if False:\n                means_, var_or_disps_ = np.log10(means[mask]), np.log10(d[mask])\n            else:\n                means_, var_or_disps_ = means[mask], d[mask]\n            pl.scatter(means_, var_or_disps_, label=label, c=color, s=1)\n        if log:  # there's a bug in autoscale\n            pl.xscale('log')\n            pl.yscale('log')\n            y_min = np.min(var_or_disp)\n            y_min = 0.95 * y_min if y_min > 0 else 1e-1\n            pl.xlim(0.95 * np.min(means), 1.05 * np.max(means))\n            pl.ylim(y_min, 1.05 * np.max(var_or_disp))\n        if idx == 0:\n            pl.legend()\n        pl.xlabel(('$log_{10}$ ' if False else '') + 'mean expressions of genes')\n        data_type = 'dispersions' if not seurat_v3_flavor else 'variances'\n        pl.ylabel(\n            ('$log_{10}$ ' if False else '')\n            + '{} of genes'.format(data_type)\n            + (' (normalized)' if idx == 0 else ' (not normalized)')\n        )\n\n    _utils.savefig_or_show('filter_genes_dispersion', show=show, save=save)\n    if show is False:\n        return pl.gca()", "idx": 210}
{"project": "Scanpy", "commit_id": "291_scanpy_1.9.0__preprocessing.py_filter_genes_dispersion.py", "target": 0, "func": "def filter_genes_dispersion(\n    result: np.recarray,\n    log: bool = False,\n    show: Optional[bool] = None,\n    save: Union[bool, str, None] = None,\n):\n    \"\"\"\\\n    Plot dispersions versus means for genes.\n\n    Produces Supp. Fig. 5c of Zheng et al. (2017) and MeanVarPlot() of Seurat.\n\n    Parameters\n    ----------\n    result\n        Result of :func:`~scanpy.pp.filter_genes_dispersion`.\n    log\n        Plot on logarithmic axes.\n    show\n         Show the plot, do not return axis.\n    save\n        If `True` or a `str`, save the figure.\n        A string is appended to the default filename.\n        Infer the filetype if ending on {{`'.pdf'`, `'.png'`, `'.svg'`}}.\n    \"\"\"\n    highly_variable_genes(\n        result, log=log, show=show, save=save, highly_variable_genes=False", "idx": 211}
{"project": "Scanpy", "commit_id": "292_scanpy_1.9.0__qc.py_highest_expr_genes.py", "target": 0, "func": "def highest_expr_genes(\n    adata: AnnData,\n    n_top: int = 30,\n    show: Optional[bool] = None,\n    save: Optional[Union[str, bool]] = None,\n    ax: Optional[Axes] = None,\n    gene_symbols: Optional[str] = None,\n    log: bool = False,\n    **kwds,\n):\n    \"\"\"\\\n    Fraction of counts assigned to each gene over all cells.\n\n    Computes, for each gene, the fraction of counts assigned to that gene within\n    a cell. The `n_top` genes with the highest mean fraction over all cells are\n    plotted as boxplots.\n\n    This plot is similar to the `scater` package function `plotHighestExprs(type\n    = \"highest-expression\")`, see `here\n    <https://bioconductor.org/packages/devel/bioc/vignettes/scater/inst/doc/vignette-qc.html>`__. Quoting\n    from there:\n\n        *We expect to see the \u201cusual suspects\u201d, i.e., mitochondrial genes, actin,\n        ribosomal protein, MALAT1. A few spike-in transcripts may also be\n        present here, though if all of the spike-ins are in the top 50, it\n        suggests that too much spike-in RNA was added. A large number of\n        pseudo-genes or predicted genes may indicate problems with alignment.*\n        -- Davis McCarthy and Aaron Lun\n\n    Parameters\n    ----------\n    adata\n        Annotated data matrix.\n    n_top\n        Number of top\n    {show_save_ax}\n    gene_symbols\n        Key for field in .var that stores gene symbols if you do not want to use .var_names.\n    log\n        Plot x-axis in log scale\n    **kwds\n        Are passed to :func:`~seaborn.boxplot`.\n\n    Returns\n    -------\n    If `show==False` a :class:`~matplotlib.axes.Axes`.\n    \"\"\"\n    import seaborn as sns  # Slow import, only import if called\n    from scipy.sparse import issparse\n\n    # compute the percentage of each gene per cell\n    norm_dict = normalize_total(adata, target_sum=100, inplace=False)\n\n    # identify the genes with the highest mean\n    if issparse(norm_dict['X']):\n        mean_percent = norm_dict['X'].mean(axis=0).A1\n        top_idx = np.argsort(mean_percent)[::-1][:n_top]\n        counts_top_genes = norm_dict['X'][:, top_idx].A\n    else:\n        mean_percent = norm_dict['X'].mean(axis=0)\n        top_idx = np.argsort(mean_percent)[::-1][:n_top]\n        counts_top_genes = norm_dict['X'][:, top_idx]\n    columns = (\n        adata.var_names[top_idx]\n        if gene_symbols is None\n        else adata.var[gene_symbols][top_idx]\n    )\n    counts_top_genes = pd.DataFrame(\n        counts_top_genes, index=adata.obs_names, columns=columns\n    )\n\n    if not ax:\n        # figsize is hardcoded to produce a tall image. To change the fig size,\n        # a matplotlib.axes.Axes object needs to be passed.\n        height = (n_top * 0.2) + 1.5\n        fig, ax = plt.subplots(figsize=(5, height))\n    sns.boxplot(data=counts_top_genes, orient='h', ax=ax, fliersize=1, **kwds)\n    ax.set_xlabel('% of total counts')\n    if log:\n        ax.set_xscale('log')\n    _utils.savefig_or_show('highest_expr_genes', show=show, save=save)\n    if show is False:\n        return ax", "idx": 212}
{"project": "Scanpy", "commit_id": "293_scanpy_1.9.0__rcmod.py_set_rcParams_scanpy.py", "target": 0, "func": "def set_rcParams_scanpy(fontsize=14, color_map=None):\n    \"\"\"Set matplotlib.rcParams to Scanpy defaults.\n\n    Call this through `settings.set_figure_params`.\n    \"\"\"\n\n    # figure\n    rcParams['figure.figsize'] = (4, 4)\n    rcParams['figure.subplot.left'] = 0.18\n    rcParams['figure.subplot.right'] = 0.96\n    rcParams['figure.subplot.bottom'] = 0.15\n    rcParams['figure.subplot.top'] = 0.91\n\n    rcParams['lines.linewidth'] = 1.5  # the line width of the frame\n    rcParams['lines.markersize'] = 6\n    rcParams['lines.markeredgewidth'] = 1\n\n    # font\n    rcParams['font.sans-serif'] = [\n        'Arial',\n        'Helvetica',\n        'DejaVu Sans',\n        'Bitstream Vera Sans',\n        'sans-serif',\n    ]\n    fontsize = fontsize\n    rcParams['font.size'] = fontsize\n    rcParams['legend.fontsize'] = 0.92 * fontsize\n    rcParams['axes.titlesize'] = fontsize\n    rcParams['axes.labelsize'] = fontsize\n\n    # legend\n    rcParams['legend.numpoints'] = 1\n    rcParams['legend.scatterpoints'] = 1\n    rcParams['legend.handlelength'] = 0.5\n    rcParams['legend.handletextpad'] = 0.4\n\n    # color cycle\n    rcParams['axes.prop_cycle'] = cycler(color=palettes.default_20)\n\n    # lines\n    rcParams['axes.linewidth'] = 0.8\n    rcParams['axes.edgecolor'] = 'black'\n    rcParams['axes.facecolor'] = 'white'\n\n    # ticks\n    rcParams['xtick.color'] = 'k'\n    rcParams['ytick.color'] = 'k'\n    rcParams['xtick.labelsize'] = fontsize\n    rcParams['ytick.labelsize'] = fontsize\n\n    # axes grid\n    rcParams['axes.grid'] = True\n    rcParams['grid.color'] = '.8'\n\n    # color map\n    rcParams['image.cmap'] = rcParams['image.cmap'] if color_map is None else color_map", "idx": 213}
{"project": "Scanpy", "commit_id": "294_scanpy_1.9.0__rcmod.py_set_rcParams_defaults.py", "target": 0, "func": "def set_rcParams_defaults():\n    \"\"\"Reset `matplotlib.rcParams` to defaults.\"\"\"\n    rcParams.update(matplotlib.rcParamsDefault)", "idx": 214}
{"project": "Scanpy", "commit_id": "295_scanpy_1.9.0__stacked_violin.py_stacked_violin.py", "target": 0, "func": "def stacked_violin(\n    adata: AnnData,\n    var_names: Union[_VarNames, Mapping[str, _VarNames]],\n    groupby: Union[str, Sequence[str]],\n    log: bool = False,\n    use_raw: Optional[bool] = None,\n    num_categories: int = 7,\n    title: Optional[str] = None,\n    colorbar_title: Optional[str] = StackedViolin.DEFAULT_COLOR_LEGEND_TITLE,\n    figsize: Optional[Tuple[float, float]] = None,\n    dendrogram: Union[bool, str] = False,\n    gene_symbols: Optional[str] = None,\n    var_group_positions: Optional[Sequence[Tuple[int, int]]] = None,\n    var_group_labels: Optional[Sequence[str]] = None,\n    standard_scale: Optional[Literal['var', 'obs']] = None,\n    var_group_rotation: Optional[float] = None,\n    layer: Optional[str] = None,\n    stripplot: bool = StackedViolin.DEFAULT_STRIPPLOT,\n    jitter: Union[float, bool] = StackedViolin.DEFAULT_JITTER,\n    size: int = StackedViolin.DEFAULT_JITTER_SIZE,\n    scale: Literal['area', 'count', 'width'] = StackedViolin.DEFAULT_SCALE,\n    yticklabels: Optional[bool] = StackedViolin.DEFAULT_PLOT_YTICKLABELS,\n    order: Optional[Sequence[str]] = None,\n    swap_axes: bool = False,\n    show: Optional[bool] = None,\n    save: Union[bool, str, None] = None,\n    return_fig: Optional[bool] = False,\n    row_palette: Optional[str] = StackedViolin.DEFAULT_ROW_PALETTE,\n    cmap: Optional[str] = StackedViolin.DEFAULT_COLORMAP,\n    ax: Optional[_AxesSubplot] = None,\n    vmin: Optional[float] = None,\n    vmax: Optional[float] = None,\n    vcenter: Optional[float] = None,\n    norm: Optional[Normalize] = None,\n    **kwds,\n) -> Union[StackedViolin, dict, None]:\n    \"\"\"\\\n    Stacked violin plots.\n\n    Makes a compact image composed of individual violin plots\n    (from :func:`~seaborn.violinplot`) stacked on top of each other.\n    Useful to visualize gene expression per cluster.\n\n    Wraps :func:`seaborn.violinplot` for :class:`~anndata.AnnData`.\n\n    This function provides a convenient interface to the\n    :class:`~scanpy.pl.StackedViolin` class. If you need more flexibility,\n    you should use :class:`~scanpy.pl.StackedViolin` directly.\n\n    Parameters\n    ----------\n    {common_plot_args}\n    {groupby_plots_args}\n    stripplot\n        Add a stripplot on top of the violin plot.\n        See :func:`~seaborn.stripplot`.\n    jitter\n        Add jitter to the stripplot (only when stripplot is True)\n        See :func:`~seaborn.stripplot`.\n    size\n        Size of the jitter points.\n    order\n        Order in which to show the categories. Note: if `dendrogram=True`\n        the categories order will be given by the dendrogram and `order`\n        will be ignored.\n    scale\n        The method used to scale the width of each violin.\n        If 'width' (the default), each violin will have the same width.\n        If 'area', each violin will have the same area.\n        If 'count', a violin\u2019s width corresponds to the number of observations.\n    yticklabels\n        Set to true to view the y tick labels.\n    row_palette\n        Be default, median values are mapped to the violin color using a\n        color map (see `cmap` argument). Alternatively, a 'row_palette` can\n        be given to color each violin plot row using a different colors.\n        The value should be a valid seaborn or matplotlib palette name\n        (see :func:`~seaborn.color_palette`).\n        Alternatively, a single color name or hex value can be passed,\n        e.g. `'red'` or `'#cc33ff'`.\n    {show_save_ax}\n    {vminmax}\n    kwds\n        Are passed to :func:`~seaborn.violinplot`.\n\n    Returns\n    -------\n    If `return_fig` is `True`, returns a :class:`~scanpy.pl.StackedViolin` object,\n    else if `show` is false, return axes dict\n\n    See also\n    --------\n    :class:`~scanpy.pl.StackedViolin`: The StackedViolin class can be used to to control\n        several visual parameters not available in this function.\n    :func:`~scanpy.pl.rank_genes_groups_stacked_violin` to plot marker genes identified\n        using the :func:`~scanpy.tl.rank_genes_groups` function.\n\n    Examples\n    -------\n\n    Visualization of violin plots of a few genes grouped by the category 'bulk_labels':\n\n    >>> import scanpy as sc\n    >>> adata = sc.datasets.pbmc68k_reduced()\n    >>> markers = ['C1QA', 'PSAP', 'CD79A', 'CD79B', 'CST3', 'LYZ']\n    >>> sc.pl.stacked_violin(adata, markers, groupby='bulk_labels', dendrogram=True)\n\n    Same visualization but passing var_names as dict, which adds a grouping of\n    the genes on top of the image:\n\n    >>> markers = {{'T-cell': 'CD3D', 'B-cell': 'CD79A', 'myeloid': 'CST3'}}\n    >>> sc.pl.stacked_violin(adata, markers, groupby='bulk_labels', dendrogram=True)\n\n    Get StackedViolin object for fine tuning\n\n    >>> vp = sc.pl.stacked_violin(adata, markers, 'bulk_labels', return_fig=True)\n    >>> vp.add_totals().style(ylim=(0,5)).show()\n\n    The axes used can be obtained using the get_axes() method:\n\n    >>> axes_dict = vp.get_axes()\n\n    \"\"\"\n\n    vp = StackedViolin(\n        adata,\n        var_names,\n        groupby=groupby,\n        use_raw=use_raw,\n        log=log,\n        num_categories=num_categories,\n        standard_scale=standard_scale,\n        title=title,\n        figsize=figsize,\n        gene_symbols=gene_symbols,\n        var_group_positions=var_group_positions,\n        var_group_labels=var_group_labels,\n        var_group_rotation=var_group_rotation,\n        layer=layer,\n        ax=ax,\n        vmin=vmin,\n        vmax=vmax,\n        vcenter=vcenter,\n        norm=norm,\n        **kwds,\n    )\n\n    if dendrogram:\n        vp.add_dendrogram(dendrogram_key=dendrogram)\n    if swap_axes:\n        vp.swap_axes()\n    vp = vp.style(\n        cmap=cmap,\n        stripplot=stripplot,\n        jitter=jitter,\n        jitter_size=size,\n        row_palette=row_palette,\n        scale=kwds.get('scale', scale),\n        yticklabels=yticklabels,\n        linewidth=kwds.get('linewidth', StackedViolin.DEFAULT_LINE_WIDTH),\n    ).legend(title=colorbar_title)\n    if return_fig:\n        return vp\n    else:\n        vp.make_figure()\n        savefig_or_show(StackedViolin.DEFAULT_SAVE_PREFIX, show=show, save=save)\n        show = settings.autoshow if show is None else show\n        if not show:\n            return vp.get_axes()", "idx": 215}
{"project": "Scanpy", "commit_id": "296_scanpy_1.9.0__stacked_violin.py___init__.py", "target": 0, "func": "def __init__(\n        self,\n        adata: AnnData,\n        var_names: Union[_VarNames, Mapping[str, _VarNames]],\n        groupby: Union[str, Sequence[str]],\n        use_raw: Optional[bool] = None,\n        log: bool = False,\n        num_categories: int = 7,\n        categories_order: Optional[Sequence[str]] = None,\n        title: Optional[str] = None,\n        figsize: Optional[Tuple[float, float]] = None,\n        gene_symbols: Optional[str] = None,\n        var_group_positions: Optional[Sequence[Tuple[int, int]]] = None,\n        var_group_labels: Optional[Sequence[str]] = None,\n        var_group_rotation: Optional[float] = None,\n        layer: Optional[str] = None,\n        standard_scale: Literal['var', 'group'] = None,\n        ax: Optional[_AxesSubplot] = None,\n        vmin: Optional[float] = None,\n        vmax: Optional[float] = None,\n        vcenter: Optional[float] = None,\n        norm: Optional[Normalize] = None,\n        **kwds,\n    ):\n        BasePlot.__init__(\n            self,\n            adata,\n            var_names,\n            groupby,\n            use_raw=use_raw,\n            log=log,\n            num_categories=num_categories,\n            categories_order=categories_order,\n            title=title,\n            figsize=figsize,\n            gene_symbols=gene_symbols,\n            var_group_positions=var_group_positions,\n            var_group_labels=var_group_labels,\n            var_group_rotation=var_group_rotation,\n            layer=layer,\n            ax=ax,\n            vmin=vmin,\n            vmax=vmax,\n            vcenter=vcenter,\n            norm=norm,\n            **kwds,\n        )\n\n        if standard_scale == 'obs':\n            self.obs_tidy = self.obs_tidy.sub(self.obs_tidy.min(1), axis=0)\n            self.obs_tidy = self.obs_tidy.div(self.obs_tidy.max(1), axis=0).fillna(0)\n        elif standard_scale == 'var':\n            self.obs_tidy -= self.obs_tidy.min(0)\n            self.obs_tidy = (self.obs_tidy / self.obs_tidy.max(0)).fillna(0)\n        elif standard_scale is None:\n            pass\n        else:\n            logg.warning('Unknown type for standard_scale, ignored')\n\n        # Set default style parameters\n        self.cmap = self.DEFAULT_COLORMAP\n        self.row_palette = self.DEFAULT_ROW_PALETTE\n        self.stripplot = self.DEFAULT_STRIPPLOT\n        self.jitter = self.DEFAULT_JITTER\n        self.jitter_size = self.DEFAULT_JITTER_SIZE\n        self.plot_yticklabels = self.DEFAULT_PLOT_YTICKLABELS\n        self.ylim = self.DEFAULT_YLIM\n        self.plot_x_padding = self.DEFAULT_PLOT_X_PADDING\n        self.plot_y_padding = self.DEFAULT_PLOT_Y_PADDING\n\n        self.kwds.setdefault('cut', self.DEFAULT_CUT)\n        self.kwds.setdefault('inner', self.DEFAULT_INNER)\n        self.kwds.setdefault('linewidth', self.DEFAULT_LINE_WIDTH)\n        self.kwds.setdefault('scale', self.DEFAULT_SCALE)", "idx": 216}
{"project": "Scanpy", "commit_id": "297_scanpy_1.9.0__stacked_violin.py_style.py", "target": 0, "func": "def style(\n        self,\n        cmap: Optional[str] = DEFAULT_COLORMAP,\n        stripplot: Optional[bool] = DEFAULT_STRIPPLOT,\n        jitter: Optional[Union[float, bool]] = DEFAULT_JITTER,\n        jitter_size: Optional[int] = DEFAULT_JITTER_SIZE,\n        linewidth: Optional[float] = DEFAULT_LINE_WIDTH,\n        row_palette: Optional[str] = DEFAULT_ROW_PALETTE,\n        scale: Optional[Literal['area', 'count', 'width']] = DEFAULT_SCALE,\n        yticklabels: Optional[bool] = DEFAULT_PLOT_YTICKLABELS,\n        ylim: Optional[Tuple[float, float]] = DEFAULT_YLIM,\n        x_padding: Optional[float] = DEFAULT_PLOT_X_PADDING,\n        y_padding: Optional[float] = DEFAULT_PLOT_Y_PADDING,\n    ):\n        \"\"\"\\\n        Modifies plot visual parameters\n\n        Parameters\n        ----------\n        cmap\n            String denoting matplotlib color map.\n        stripplot\n            Add a stripplot on top of the violin plot.\n            See :func:`~seaborn.stripplot`.\n        jitter\n            Add jitter to the stripplot (only when stripplot is True)\n            See :func:`~seaborn.stripplot`.\n        jitter_size\n            Size of the jitter points.\n        linewidth\n            linewidth for the violin plots.\n        row_palette\n            The row palette determines the colors to use for the stacked violins.\n            The value should be a valid seaborn or matplotlib palette name\n            (see :func:`~seaborn.color_palette`).\n            Alternatively, a single color name or hex value can be passed,\n            e.g. `'red'` or `'#cc33ff'`.\n        scale\n            The method used to scale the width of each violin.\n            If 'width' (the default), each violin will have the same width.\n            If 'area', each violin will have the same area.\n            If 'count', a violin\u2019s width corresponds to the number of observations.\n        yticklabels\n            Set to true to view the y tick labels.\n        ylim\n            minimum and maximum values for the y-axis. If set. All rows will have\n            the same y-axis range. Example: ylim=(0, 5)\n        x_padding\n            Space between the plot left/right borders and the violins. A unit\n            is the distance between the x ticks.\n        y_padding\n            Space between the plot top/bottom borders and the violins. A unit is\n            the distance between the y ticks.\n\n        Returns\n        -------\n        :class:`~scanpy.pl.StackedViolin`\n\n        Examples\n        -------\n        >>> adata = sc.datasets.pbmc68k_reduced()\n        >>> markers = ['C1QA', 'PSAP', 'CD79A', 'CD79B', 'CST3', 'LYZ']\n\n        Change color map and turn off edges\n\n        >>> sc.pl.MatrixPlot(adata, markers, groupby='bulk_labels')\\\n        ...               .style(row_palette='Blues', linewidth=0).show()\n\n        \"\"\"\n\n        # modify only values that had changed\n        if cmap != self.cmap:\n            self.cmap = cmap\n        if row_palette != self.row_palette:\n            self.row_palette = row_palette\n            self.kwds['color'] = self.row_palette\n        if stripplot != self.stripplot:\n            self.stripplot = stripplot\n        if jitter != self.jitter:\n            self.jitter = jitter\n        if jitter_size != self.jitter_size:\n            self.jitter_size = jitter_size\n        if yticklabels != self.plot_yticklabels:\n            self.plot_yticklabels = yticklabels\n            if self.plot_yticklabels:\n                # space needs to be added to avoid overlapping\n                # of labels and legend or dendrogram/totals.\n                self.wspace = 0.3\n            else:\n                self.wspace = StackedViolin.DEFAULT_WSPACE\n        if ylim != self.ylim:\n            self.ylim = ylim\n        if x_padding != self.plot_x_padding:\n            self.plot_x_padding = x_padding\n        if y_padding != self.plot_y_padding:\n            self.plot_y_padding = y_padding\n        if linewidth != self.kwds['linewidth'] and linewidth != self.DEFAULT_LINE_WIDTH:\n            self.kwds['linewidth'] = linewidth\n        if scale != self.kwds['scale'] and scale != self.DEFAULT_SCALE:\n            self.kwds['scale'] = scale\n\n        return self", "idx": 217}
{"project": "Scanpy", "commit_id": "298_scanpy_1.9.0__stacked_violin.py__mainplot.py", "target": 1, "func": "def _mainplot(self, ax):\n        # to make the stacked violin plots, the\n        # `ax` is subdivided horizontally and in each horizontal sub ax\n        # a seaborn violin plot is added.\n\n        # work on a copy of the dataframes. This is to avoid changes\n        # on the original data frames after repetitive calls to the\n        # StackedViolin object, for example once with swap_axes and other without\n        _matrix = self.obs_tidy.copy()\n\n        if self.var_names_idx_order is not None:\n            _matrix = _matrix.iloc[:, self.var_names_idx_order]\n\n        if self.categories_order is not None:\n            _matrix.index = _matrix.index.reorder_categories(\n                self.categories_order, ordered=True\n            )\n\n        # get mean values for color and transform to color values\n        # using colormap\n        _color_df = _matrix.groupby(level=0).median()\n        if self.are_axes_swapped:\n            _color_df = _color_df.T\n\n        cmap = pl.get_cmap(self.kwds.get('cmap', self.cmap))\n        if 'cmap' in self.kwds:\n            del self.kwds['cmap']\n        normalize = check_colornorm(\n            self.vboundnorm.vmin,\n            self.vboundnorm.vmax,\n            self.vboundnorm.vcenter,\n            self.vboundnorm.norm,\n        )\n        colormap_array = cmap(normalize(_color_df.values))\n        x_spacer_size = self.plot_x_padding\n        y_spacer_size = self.plot_y_padding\n        self._make_rows_of_violinplots(\n            ax, _matrix, colormap_array, _color_df, x_spacer_size, y_spacer_size\n        )\n\n        # turn on axis for `ax` as this is turned off\n        # by make_grid_spec when the axis is subdivided earlier.\n        ax.set_frame_on(True)\n        ax.axis('on')\n        ax.patch.set_alpha(0.0)\n\n        # add tick labels\n        ax.set_ylim(_color_df.shape[0] + y_spacer_size, -y_spacer_size)\n        ax.set_xlim(-x_spacer_size, _color_df.shape[1] + x_spacer_size)\n\n        # 0.5 to position the ticks on the center of the violins\n        y_ticks = np.arange(_color_df.shape[0]) + 0.5\n        ax.set_yticks(y_ticks)\n        ax.set_yticklabels(\n            [_color_df.index[idx] for idx, _ in enumerate(y_ticks)], minor=False\n        )\n\n        # 0.5 to position the ticks on the center of the violins\n        x_ticks = np.arange(_color_df.shape[1]) + 0.5\n        ax.set_xticks(x_ticks)\n        labels = _color_df.columns\n        ax.set_xticklabels(labels, minor=False, ha='center')\n        # rotate x tick labels if they are longer than 2 characters\n        if max([len(x) for x in labels]) > 2:\n            ax.tick_params(axis='x', labelrotation=90)\n        ax.tick_params(axis='both', labelsize='small')\n        ax.grid(False)\n\n        return normalize", "idx": 218}
{"project": "Scanpy", "commit_id": "299_scanpy_1.9.0__stacked_violin.py__make_rows_of_violinplots.py", "target": 1, "func": "def _make_rows_of_violinplots(\n        self, ax, _matrix, colormap_array, _color_df, x_spacer_size, y_spacer_size\n    ):\n        import seaborn as sns  # Slow import, only import if called\n\n        row_palette = self.kwds.get('color', self.row_palette)\n        if 'color' in self.kwds:\n            del self.kwds['color']\n        if row_palette is not None:\n            if is_color_like(row_palette):\n                row_colors = [row_palette] * _color_df.shape[0]\n            else:\n                row_colors = sns.color_palette(row_palette, n_colors=_color_df.shape[0])\n            # when row_palette is used, there is no need for a legend\n            self.legends_width = 0.0\n        else:\n            row_colors = [None] * _color_df.shape[0]\n\n        # All columns should have a unique name, yet, frequently\n        # gene names are repeated in self.var_names,  otherwise the\n        # violin plot will not distinguish those genes\n        _matrix.columns = [f\"{x}_{idx}\" for idx, x in enumerate(_matrix.columns)]\n\n        # transform the  dataframe into a dataframe having three columns:\n        # the categories name (from groupby),\n        # the gene name\n        # the expression value\n        # This format is convenient to aggregate per gene or per category\n        # while making the violin plots.\n\n        df = (\n            pd.DataFrame(_matrix.stack(dropna=False))\n            .reset_index()\n            .rename(\n                columns={\n                    'level_1': 'genes',\n                    _matrix.index.name: 'categories',\n                    0: 'values',\n                }\n            )\n        )\n        df['genes'] = (\n            df['genes'].astype('category').cat.reorder_categories(_matrix.columns)\n        )\n        df['categories'] = (\n            df['categories']\n            .astype('category')\n            .cat.reorder_categories(_matrix.index.categories)\n        )\n\n        # the ax need to be subdivided\n        # define a layout of nrows = len(categories) rows\n        # each row is one violin plot.\n        num_rows, num_cols = _color_df.shape\n        height_ratios = [y_spacer_size] + [1] * num_rows + [y_spacer_size]\n        width_ratios = [x_spacer_size] + [1] * num_cols + [x_spacer_size]\n\n        fig, gs = make_grid_spec(\n            ax,\n            nrows=num_rows + 2,\n            ncols=num_cols + 2,\n            hspace=0.2 if self.plot_yticklabels else 0,\n            wspace=0,\n            height_ratios=height_ratios,\n            width_ratios=width_ratios,\n        )\n        axs_list = []\n        for idx, row_label in enumerate(_color_df.index):\n\n            row_ax = fig.add_subplot(gs[idx + 1, 1:-1])\n            axs_list.append(row_ax)\n\n            if row_colors[idx] is None:\n                palette_colors = colormap_array[idx, :]\n            else:\n                palette_colors = None\n\n            if not self.are_axes_swapped:\n                x = 'genes'\n                _df = df[df.categories == row_label]\n            else:\n                x = 'categories'\n                # because of the renamed matrix columns here\n                # we need to use this instead of the 'row_label'\n                # (in _color_df the values are not renamed as those\n                # values will be used to label the ticks)\n                _df = df[df.genes == _matrix.columns[idx]]\n\n            row_ax = sns.violinplot(\n                x=x,\n                y='values',\n                data=_df,\n                orient='vertical',\n                ax=row_ax,\n                palette=palette_colors,\n                color=row_colors[idx],\n                **self.kwds,\n            )\n\n            if self.stripplot:\n                row_ax = sns.stripplot(\n                    x=x,\n                    y='values',\n                    data=_df,\n                    jitter=self.jitter,\n                    color='black',\n                    size=self.jitter_size,\n                    ax=row_ax,\n                )\n\n            self._setup_violin_axes_ticks(row_ax, num_cols)", "idx": 219}
{"project": "Scanpy", "commit_id": "29_scanpy_1.9.0_cli.py___delitem__.py", "target": 0, "func": "def __delitem__(self, k: str) -> None:\n        del self.parser_map[k]", "idx": 220}
{"project": "Scanpy", "commit_id": "2_scanpy_1.9.0_conftest.py_pytest_collection_modifyitems.py", "target": 0, "func": "def pytest_collection_modifyitems(config, items):\n    run_internet = config.getoption(\"--internet-tests\")\n    skip_internet = pytest.mark.skip(reason=\"need --internet-tests option to run\")\n    for item in items:\n        # All tests marked with `pytest.mark.internet` get skipped unless\n        # `--run-internet` passed\n        if not run_internet and (\"internet\" in item.keywords):\n            item.add_marker(skip_internet)", "idx": 221}
{"project": "Scanpy", "commit_id": "300_scanpy_1.9.0__stacked_violin.py__setup_violin_axes_ticks.py", "target": 0, "func": "def _setup_violin_axes_ticks(self, row_ax, num_cols):\n        \"\"\"\n        Configures each of the violin plot axes ticks like remove or add labels etc.\n\n        \"\"\"\n        # remove the default seaborn grids because in such a compact\n        # plot are unnecessary\n\n        row_ax.grid(False)\n        if self.ylim is not None:\n            row_ax.set_ylim(self.ylim)\n        if self.log:\n            row_ax.set_yscale('log')\n\n        if self.plot_yticklabels:\n            for spine in ['top', 'bottom', 'left']:\n                row_ax.spines[spine].set_visible(False)\n\n            # make line a bit ticker to see the extend of the yaxis in the\n            # final plot\n            row_ax.spines['right'].set_linewidth(1.5)\n            row_ax.spines['right'].set_position(('data', num_cols))\n\n            row_ax.tick_params(\n                axis='y',\n                left=False,\n                right=True,\n                labelright=True,\n                labelleft=False,\n                labelsize='x-small',\n            )\n            # use only the smallest and the largest y ticks\n            # and align the firts label on top of the tick and\n            # the second below the tick. This avoid overlapping\n            # of nearby ticks\n            import matplotlib.ticker as ticker\n\n            # use MaxNLocator to set 2 ticks\n            row_ax.yaxis.set_major_locator(\n                ticker.MaxNLocator(nbins=2, steps=[1, 1.2, 10])\n            )\n            yticks = row_ax.get_yticks()\n            row_ax.set_yticks([yticks[0], yticks[-1]])\n            ticklabels = row_ax.get_yticklabels()\n            ticklabels[0].set_va(\"bottom\")\n            ticklabels[-1].set_va(\"top\")\n        else:\n            row_ax.axis('off')\n            # remove labels\n            row_ax.set_yticklabels([])\n            row_ax.tick_params(axis='y', left=False, right=False)\n\n        row_ax.set_ylabel('')\n\n        row_ax.set_xlabel('')\n\n        row_ax.set_xticklabels([])\n        row_ax.tick_params(\n            axis='x', bottom=False, top=False, labeltop=False, labelbottom=False", "idx": 222}
{"project": "Scanpy", "commit_id": "301_scanpy_1.9.0__utils.py_matrix.py", "target": 0, "func": "def matrix(\n    matrix,\n    xlabel=None,\n    ylabel=None,\n    xticks=None,\n    yticks=None,\n    title=None,\n    colorbar_shrink=0.5,\n    color_map=None,\n    show=None,\n    save=None,\n    ax=None,\n):\n    \"\"\"Plot a matrix.\"\"\"\n    if ax is None:\n        ax = pl.gca()\n    img = ax.imshow(matrix, cmap=color_map)\n    if xlabel is not None:\n        ax.set_xlabel(xlabel)\n    if ylabel is not None:\n        ax.set_ylabel(ylabel)\n    if title is not None:\n        ax.set_title(title)\n    if xticks is not None:\n        ax.set_xticks(range(len(xticks)), xticks, rotation='vertical')\n    if yticks is not None:\n        ax.set_yticks(range(len(yticks)), yticks)\n    pl.colorbar(\n        img, shrink=colorbar_shrink, ax=ax\n    )  # need a figure instance for colorbar\n    savefig_or_show('matrix', show=show, save=save)", "idx": 223}
{"project": "Scanpy", "commit_id": "302_scanpy_1.9.0__utils.py_timeseries.py", "target": 0, "func": "def timeseries(X, **kwargs):\n    \"\"\"Plot X. See timeseries_subplot.\"\"\"\n    pl.figure(\n        figsize=tuple(2 * s for s in rcParams['figure.figsize']),\n        subplotpars=sppars(left=0.12, right=0.98, bottom=0.13),\n    )\n    timeseries_subplot(X, **kwargs)", "idx": 224}
{"project": "Scanpy", "commit_id": "303_scanpy_1.9.0__utils.py_timeseries_subplot.py", "target": 0, "func": "def timeseries_subplot(\n    X: np.ndarray,\n    time=None,\n    color=None,\n    var_names=(),\n    highlights_x=(),\n    xlabel='',\n    ylabel='gene expression',\n    yticks=None,\n    xlim=None,\n    legend=True,\n    palette: Union[Sequence[str], Cycler, None] = None,\n    color_map='viridis',\n    ax: Optional[Axes] = None,\n):\n    \"\"\"\\\n    Plot X.\n\n    Parameters\n    ----------\n    X\n        Call this with:\n        X with one column, color categorical.\n        X with one column, color continuous.\n        X with n columns, color is of length n.\n    \"\"\"\n\n    if color is not None:\n        use_color_map = isinstance(color[0], (float, np.floating))\n    palette = default_palette(palette)\n    x_range = np.arange(X.shape[0]) if time is None else time\n    if X.ndim == 1:\n        X = X[:, None]\n    if X.shape[1] > 1:\n        colors = palette[: X.shape[1]].by_key()['color']\n        subsets = [(x_range, X[:, i]) for i in range(X.shape[1])]\n    elif use_color_map:\n        colors = [color]\n        subsets = [(x_range, X[:, 0])]\n    else:\n        levels, _ = np.unique(color, return_inverse=True)\n        colors = np.array(palette[: len(levels)].by_key()['color'])\n        subsets = [(x_range[color == level], X[color == level, :]) for level in levels]\n\n    if ax is None:\n        ax = pl.subplot()\n    for i, (x, y) in enumerate(subsets):\n        ax.scatter(\n            x,\n            y,\n            marker='.',\n            edgecolor='face',\n            s=rcParams['lines.markersize'],\n            c=colors[i],\n            label=var_names[i] if len(var_names) > 0 else '',\n            cmap=color_map,\n            rasterized=settings._vector_friendly,\n        )\n    ylim = ax.get_ylim()\n    for h in highlights_x:\n        ax.plot([h, h], [ylim[0], ylim[1]], '--', color='black')\n    ax.set_ylim(ylim)\n    if xlim is not None:\n        ax.set_xlim(xlim)\n    ax.set_xlabel(xlabel)\n    ax.set_ylabel(ylabel)\n    if yticks is not None:\n        ax.set_yticks(yticks)\n    if len(var_names) > 0 and legend:\n        ax.legend(frameon=False)", "idx": 225}
{"project": "Scanpy", "commit_id": "304_scanpy_1.9.0__utils.py_timeseries_as_heatmap.py", "target": 0, "func": "def timeseries_as_heatmap(\n    X: np.ndarray, var_names: Collection[str] = (), highlights_x=(), color_map=None\n):\n    \"\"\"\\\n    Plot timeseries as heatmap.\n\n    Parameters\n    ----------\n    X\n        Data array.\n    var_names\n        Array of strings naming variables stored in columns of X.\n    \"\"\"\n    if len(var_names) == 0:\n        var_names = np.arange(X.shape[1])\n    if var_names.ndim == 2:\n        var_names = var_names[:, 0]\n\n    # transpose X\n    X = X.T\n    min_x = np.min(X)\n\n    # insert space into X\n    if False:\n        # generate new array with highlights_x\n        space = 10  # integer\n        x_new = np.zeros((X.shape[0], X.shape[1] + space * len(highlights_x)))\n        hold = 0\n        _hold = 0\n        space_sum = 0\n        for ih, h in enumerate(highlights_x):\n            _h = h + space_sum\n            x_new[:, _hold:_h] = X[:, hold:h]\n            x_new[:, _h : _h + space] = min_x * np.ones((X.shape[0], space))\n            # update variables\n            space_sum += space\n            _hold = _h + space\n            hold = h\n        x_new[:, _hold:] = X[:, hold:]\n\n    _, ax = pl.subplots(figsize=(1.5 * 4, 2 * 4))\n    img = ax.imshow(\n        np.array(X, dtype=np.float_),\n        aspect='auto',\n        interpolation='nearest',\n        cmap=color_map,\n    )\n    pl.colorbar(img, shrink=0.5)\n    pl.yticks(range(X.shape[0]), var_names)\n    for h in highlights_x:\n        pl.plot([h, h], [0, X.shape[0]], '--', color='black')\n    pl.xlim([0, X.shape[1] - 1])\n    pl.ylim([0, X.shape[0] - 1])", "idx": 226}
{"project": "Scanpy", "commit_id": "305_scanpy_1.9.0__utils.py_savefig.py", "target": 0, "func": "def savefig(writekey, dpi=None, ext=None):\n    \"\"\"Save current figure to file.\n\n    The `filename` is generated as follows:\n\n        filename = settings.figdir / (writekey + settings.plot_suffix + '.' + settings.file_format_figs)\n    \"\"\"\n    if dpi is None:\n        # we need this as in notebooks, the internal figures are also influenced by 'savefig.dpi' this...\n        if (\n            not isinstance(rcParams['savefig.dpi'], str)\n            and rcParams['savefig.dpi'] < 150\n        ):\n            if settings._low_resolution_warning:\n                logg.warning(\n                    'You are using a low resolution (dpi<150) for saving figures.\\n'\n                    'Consider running `set_figure_params(dpi_save=...)`, which will '\n                    \"adjust `matplotlib.rcParams['savefig.dpi']`\"\n                )\n                settings._low_resolution_warning = False\n        else:\n            dpi = rcParams['savefig.dpi']\n    settings.figdir.mkdir(parents=True, exist_ok=True)\n    if ext is None:\n        ext = settings.file_format_figs\n    filename = settings.figdir / f'{writekey}{settings.plot_suffix}.{ext}'\n    # output the following msg at warning level; it's really important for the user\n    logg.warning(f'saving figure to file {filename}')\n    pl.savefig(filename, dpi=dpi, bbox_inches='tight')", "idx": 227}
{"project": "Scanpy", "commit_id": "306_scanpy_1.9.0__utils.py_savefig_or_show.py", "target": 0, "func": "def savefig_or_show(\n    writekey: str,\n    show: Optional[bool] = None,\n    dpi: Optional[int] = None,\n    ext: str = None,\n    save: Union[bool, str, None] = None,\n):\n    if isinstance(save, str):\n        # check whether `save` contains a figure extension\n        if ext is None:\n            for try_ext in ['.svg', '.pdf', '.png']:\n                if save.endswith(try_ext):\n                    ext = try_ext[1:]\n                    save = save.replace(try_ext, '')\n                    break\n        # append it\n        writekey += save\n        save = True\n    save = settings.autosave if save is None else save\n    show = settings.autoshow if show is None else show\n    if save:\n        savefig(writekey, dpi=dpi, ext=ext)\n    if show:\n        pl.show()\n    if save:\n        pl.close()  # clear figure", "idx": 228}
{"project": "Scanpy", "commit_id": "307_scanpy_1.9.0__utils.py_default_palette.py", "target": 0, "func": "def default_palette(palette: Union[Sequence[str], Cycler, None] = None) -> Cycler:\n    if palette is None:\n        return rcParams['axes.prop_cycle']\n    elif not isinstance(palette, Cycler):\n        return cycler(color=palette)\n    else:\n        return palette", "idx": 229}
{"project": "Scanpy", "commit_id": "308_scanpy_1.9.0__utils.py__validate_palette.py", "target": 1, "func": "def _validate_palette(adata, key):\n    \"\"\"\n    checks if the list of colors in adata.uns[f'{key}_colors'] is valid\n    and updates the color list in adata.uns[f'{key}_colors'] if needed.\n\n    Not only valid matplotlib colors are checked but also if the color name\n    is a valid R color name, in which case it will be translated to a valid name\n    \"\"\"\n\n    _palette = []\n    color_key = f\"{key}_colors\"\n\n    for color in adata.uns[color_key]:\n        if not is_color_like(color):\n            # check if the color is a valid R color and translate it\n            # to a valid hex color value\n            if color in additional_colors:\n                color = additional_colors[color]\n            else:\n                logg.warning(\n                    f\"The following color value found in adata.uns['{key}_colors'] \"\n                    f\"is not valid: '{color}'. Default colors will be used instead.\"\n                )\n                _set_default_colors_for_categorical_obs(adata, key)\n                _palette = None\n                break\n        _palette.append(color)\n    # Don't modify if nothing changed\n    if _palette is not None and list(_palette) != list(adata.uns[color_key]):\n        adata.uns[color_key] = _palette", "idx": 230}
{"project": "Scanpy", "commit_id": "309_scanpy_1.9.0__utils.py__set_colors_for_categorical_obs.py", "target": 1, "func": "def _set_colors_for_categorical_obs(\n    adata, value_to_plot, palette: Union[str, Sequence[str], Cycler]\n):\n    \"\"\"\n    Sets the adata.uns[value_to_plot + '_colors'] according to the given palette\n\n    Parameters\n    ----------\n    adata\n        annData object\n    value_to_plot\n        name of a valid categorical observation\n    palette\n        Palette should be either a valid :func:`~matplotlib.pyplot.colormaps` string,\n        a sequence of colors (in a format that can be understood by matplotlib,\n        eg. RGB, RGBS, hex, or a cycler object with key='color'\n\n    Returns\n    -------\n    None\n    \"\"\"\n    from matplotlib.colors import to_hex\n\n    categories = adata.obs[value_to_plot].cat.categories\n    # check is palette is a valid matplotlib colormap\n    if isinstance(palette, str) and palette in pl.colormaps():\n        # this creates a palette from a colormap. E.g. 'Accent, Dark2, tab20'\n        cmap = pl.get_cmap(palette)\n        colors_list = [to_hex(x) for x in cmap(np.linspace(0, 1, len(categories)))]\n    elif isinstance(palette, cabc.Mapping):\n        colors_list = [to_hex(palette[k], keep_alpha=True) for k in categories]\n    else:\n        # check if palette is a list and convert it to a cycler, thus\n        # it doesnt matter if the list is shorter than the categories length:\n        if isinstance(palette, cabc.Sequence):\n            if len(palette) < len(categories):\n                logg.warning(\n                    \"Length of palette colors is smaller than the number of \"\n                    f\"categories (palette length: {len(palette)}, \"\n                    f\"categories length: {len(categories)}. \"\n                    \"Some categories will have the same color.\"\n                )\n            # check that colors are valid\n            _color_list = []\n            for color in palette:\n                if not is_color_like(color):\n                    # check if the color is a valid R color and translate it\n                    # to a valid hex color value\n                    if color in additional_colors:\n                        color = additional_colors[color]\n                    else:\n                        raise ValueError(\n                            \"The following color value of the given palette \"\n                            f\"is not valid: {color}\"\n                        )\n                _color_list.append(color)\n\n            palette = cycler(color=_color_list)\n        if not isinstance(palette, Cycler):\n            raise ValueError(\n                \"Please check that the value of 'palette' is a valid \"\n                \"matplotlib colormap string (eg. Set2), a  list of color names \"\n                \"or a cycler with a 'color' key.\"\n            )\n        if 'color' not in palette.keys:\n            raise ValueError(\"Please set the palette key 'color'.\")\n\n        cc = palette()\n        colors_list = [to_hex(next(cc)['color']) for x in range(len(categories))]\n\n    adata.uns[value_to_plot + '_colors'] = colors_list", "idx": 231}
{"project": "Scanpy", "commit_id": "30_scanpy_1.9.0_cli.py___iter__.py", "target": 0, "func": "def __iter__(self) -> Generator[str, None, None]:\n        yield from self.parser_map\n        yield from self.commands", "idx": 232}
{"project": "Scanpy", "commit_id": "310_scanpy_1.9.0__utils.py__set_default_colors_for_categorical_obs.py", "target": 1, "func": "def _set_default_colors_for_categorical_obs(adata, value_to_plot):\n    \"\"\"\n    Sets the adata.uns[value_to_plot + '_colors'] using default color palettes\n\n    Parameters\n    ----------\n    adata\n        AnnData object\n    value_to_plot\n        Name of a valid categorical observation\n\n    Returns\n    -------\n    None\n    \"\"\"\n    categories = adata.obs[value_to_plot].cat.categories\n    length = len(categories)\n\n    # check if default matplotlib palette has enough colors\n    if len(rcParams['axes.prop_cycle'].by_key()['color']) >= length:\n        cc = rcParams['axes.prop_cycle']()\n        palette = [next(cc)['color'] for _ in range(length)]\n\n    else:\n        if length <= 20:\n            palette = palettes.default_20\n        elif length <= 28:\n            palette = palettes.default_28\n        elif length <= len(palettes.default_102):  # 103 colors\n            palette = palettes.default_102\n        else:\n            palette = ['grey' for _ in range(length)]\n            logg.info(\n                f'the obs value {value_to_plot!r} has more than 103 categories. Uniform '\n                \"'grey' color will be used for all categories.\"\n            )\n\n    _set_colors_for_categorical_obs(adata, value_to_plot, palette[:length])", "idx": 233}
{"project": "Scanpy", "commit_id": "311_scanpy_1.9.0__utils.py_add_colors_for_categorical_sample_annotation.py", "target": 0, "func": "def add_colors_for_categorical_sample_annotation(\n    adata, key, palette=None, force_update_colors=False\n):\n\n    color_key = f\"{key}_colors\"\n    colors_needed = len(adata.obs[key].cat.categories)\n    if palette and force_update_colors:\n        _set_colors_for_categorical_obs(adata, key, palette)\n    elif color_key in adata.uns and len(adata.uns[color_key]) <= colors_needed:\n        _validate_palette(adata, key)\n    else:\n        _set_default_colors_for_categorical_obs(adata, key)", "idx": 234}
{"project": "Scanpy", "commit_id": "312_scanpy_1.9.0__utils.py_plot_edges.py", "target": 0, "func": "def plot_edges(axs, adata, basis, edges_width, edges_color, neighbors_key=None):\n    import networkx as nx\n\n    if not isinstance(axs, cabc.Sequence):\n        axs = [axs]\n\n    if neighbors_key is None:\n        neighbors_key = 'neighbors'\n    if neighbors_key not in adata.uns:\n        raise ValueError('`edges=True` requires `pp.neighbors` to be run before.')\n    neighbors = NeighborsView(adata, neighbors_key)\n    g = nx.Graph(neighbors['connectivities'])\n    basis_key = _get_basis(adata, basis)\n\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n        for ax in axs:\n            edge_collection = nx.draw_networkx_edges(\n                g,\n                adata.obsm[basis_key],\n                ax=ax,\n                width=edges_width,\n                edge_color=edges_color,\n            )\n            edge_collection.set_zorder(-2)\n            edge_collection.set_rasterized(settings._vector_friendly)", "idx": 235}
{"project": "Scanpy", "commit_id": "313_scanpy_1.9.0__utils.py_plot_arrows.py", "target": 0, "func": "def plot_arrows(axs, adata, basis, arrows_kwds=None):\n    if not isinstance(axs, cabc.Sequence):\n        axs = [axs]\n    v_prefix = next(\n        (p for p in ['velocity', 'Delta'] if f'{p}_{basis}' in adata.obsm), None\n    )\n    if v_prefix is None:\n        raise ValueError(\n            \"`arrows=True` requires \"\n            f\"`'velocity_{basis}'` from scvelo or \"\n            f\"`'Delta_{basis}'` from velocyto.\"\n        )\n    if v_prefix == 'velocity':\n        logg.warning(\n            'The module `scvelo` has improved plotting facilities. '\n            'Prefer using `scv.pl.velocity_embedding` to `arrows=True`.'\n        )\n\n    basis_key = _get_basis(adata, basis)\n    X = adata.obsm[basis_key]\n    V = adata.obsm[f'{v_prefix}_{basis}']\n    for ax in axs:\n        quiver_kwds = arrows_kwds if arrows_kwds is not None else {}\n        ax.quiver(\n            X[:, 0],\n            X[:, 1],\n            V[:, 0],\n            V[:, 1],\n            **quiver_kwds,\n            rasterized=settings._vector_friendly,", "idx": 236}
{"project": "Scanpy", "commit_id": "314_scanpy_1.9.0__utils.py_scatter_group.py", "target": 0, "func": "def scatter_group(ax, key, imask, adata, Y, projection='2d', size=3, alpha=None):\n    \"\"\"Scatter of group using representation of data Y.\"\"\"\n    mask = adata.obs[key].cat.categories[imask] == adata.obs[key].values\n    color = adata.uns[key + '_colors'][imask]\n    if not isinstance(color[0], str):\n        from matplotlib.colors import rgb2hex\n\n        color = rgb2hex(adata.uns[key + '_colors'][imask])\n    if not is_color_like(color):\n        raise ValueError('\"{}\" is not a valid matplotlib color.'.format(color))\n    data = [Y[mask, 0], Y[mask, 1]]\n    if projection == '3d':\n        data.append(Y[mask, 2])\n    ax.scatter(\n        *data,\n        marker='.',\n        alpha=alpha,\n        c=color,\n        edgecolors='none',\n        s=size,\n        label=adata.obs[key].cat.categories[imask],\n        rasterized=settings._vector_friendly,\n    )\n    return mask", "idx": 237}
{"project": "Scanpy", "commit_id": "315_scanpy_1.9.0__utils.py_setup_axes.py", "target": 0, "func": "def setup_axes(\n    ax: Union[Axes, Sequence[Axes]] = None,\n    panels='blue',\n    colorbars=(False,),\n    right_margin=None,\n    left_margin=None,\n    projection: Literal['2d', '3d'] = '2d',\n    show_ticks=False,\n):\n    \"\"\"Grid of axes for plotting, legends and colorbars.\"\"\"\n    check_projection(projection)\n    if left_margin is not None:\n        raise NotImplementedError('We currently don\u2019t support `left_margin`.')\n    if np.any(colorbars) and right_margin is None:\n        right_margin = 1 - rcParams['figure.subplot.right'] + 0.21  # 0.25\n    elif right_margin is None:\n        right_margin = 1 - rcParams['figure.subplot.right'] + 0.06  # 0.10\n    # make a list of right margins for each panel\n    if not isinstance(right_margin, list):\n        right_margin_list = [right_margin for i in range(len(panels))]\n    else:\n        right_margin_list = right_margin\n\n    # make a figure with len(panels) panels in a row side by side\n    top_offset = 1 - rcParams['figure.subplot.top']\n    bottom_offset = 0.15 if show_ticks else 0.08\n    left_offset = 1 if show_ticks else 0.3  # in units of base_height\n    base_height = rcParams['figure.figsize'][1]\n    height = base_height\n    base_width = rcParams['figure.figsize'][0]\n    if show_ticks:\n        base_width *= 1.1\n\n    draw_region_width = (\n        base_width - left_offset - top_offset - 0.5\n    )  # this is kept constant throughout\n\n    right_margin_factor = sum([1 + right_margin for right_margin in right_margin_list])\n    width_without_offsets = (\n        right_margin_factor * draw_region_width\n    )  # this is the total width that keeps draw_region_width\n\n    right_offset = (len(panels) - 1) * left_offset\n    figure_width = width_without_offsets + left_offset + right_offset\n    draw_region_width_frac = draw_region_width / figure_width\n    left_offset_frac = left_offset / figure_width\n    right_offset_frac = (  # noqa: F841  # TODO Does this need fixing?\n        1 - (len(panels) - 1) * left_offset_frac\n    )\n\n    if ax is None:\n        pl.figure(\n            figsize=(figure_width, height),\n            subplotpars=sppars(left=0, right=1, bottom=bottom_offset),\n        )\n    left_positions = [left_offset_frac, left_offset_frac + draw_region_width_frac]\n    for i in range(1, len(panels)):\n        right_margin = right_margin_list[i - 1]\n        left_positions.append(\n            left_positions[-1] + right_margin * draw_region_width_frac\n        )\n        left_positions.append(left_positions[-1] + draw_region_width_frac)\n    panel_pos = [[bottom_offset], [1 - top_offset], left_positions]\n\n    axs = []\n    if ax is None:\n        for icolor, color in enumerate(panels):\n            left = panel_pos[2][2 * icolor]\n            bottom = panel_pos[0][0]\n            width = draw_region_width / figure_width\n            height = panel_pos[1][0] - bottom\n            if projection == '2d':\n                ax = pl.axes([left, bottom, width, height])\n            elif projection == '3d':\n                ax = pl.axes([left, bottom, width, height], projection='3d')\n            axs.append(ax)\n    else:\n        axs = ax if isinstance(ax, cabc.Sequence) else [ax]\n\n    return axs, panel_pos, draw_region_width, figure_width", "idx": 238}
{"project": "Scanpy", "commit_id": "316_scanpy_1.9.0__utils.py_scatter_base.py", "target": 0, "func": "def scatter_base(\n    Y: np.ndarray,\n    colors='blue',\n    sort_order=True,\n    alpha=None,\n    highlights=(),\n    right_margin=None,\n    left_margin=None,\n    projection: Literal['2d', '3d'] = '2d',\n    title=None,\n    component_name='DC',\n    component_indexnames=(1, 2, 3),\n    axis_labels=None,\n    colorbars=(False,),\n    sizes=(1,),\n    color_map='viridis',\n    show_ticks=True,\n    ax=None,\n) -> Union[Axes, List[Axes]]:\n    \"\"\"Plot scatter plot of data.\n\n    Parameters\n    ----------\n    Y\n        Data array.\n    projection\n\n    Returns\n    -------\n    Depending on whether supplying a single array or a list of arrays,\n    return a single axis or a list of axes.\n    \"\"\"\n    if isinstance(highlights, cabc.Mapping):\n        highlights_indices = sorted(highlights)\n        highlights_labels = [highlights[i] for i in highlights_indices]\n    else:\n        highlights_indices = highlights\n        highlights_labels = []\n    # if we have a single array, transform it into a list with a single array\n    if isinstance(colors, str):\n        colors = [colors]\n    if len(sizes) != len(colors) and len(sizes) == 1:\n        sizes = [sizes[0] for _ in range(len(colors))]\n    axs, panel_pos, draw_region_width, figure_width = setup_axes(\n        ax=ax,\n        panels=colors,\n        colorbars=colorbars,\n        projection=projection,\n        right_margin=right_margin,\n        left_margin=left_margin,\n        show_ticks=show_ticks,\n    )\n    for icolor, color in enumerate(colors):\n        ax = axs[icolor]\n        bottom = panel_pos[0][0]\n        height = panel_pos[1][0] - bottom\n        Y_sort = Y\n        if not is_color_like(color) and sort_order:\n            sort = np.argsort(color)\n            color = color[sort]\n            Y_sort = Y[sort]\n        if projection == '2d':\n            data = Y_sort[:, 0], Y_sort[:, 1]\n        elif projection == '3d':\n            data = Y_sort[:, 0], Y_sort[:, 1], Y_sort[:, 2]\n        else:\n            raise ValueError(f\"Unknown projection {projection!r} not in '2d', '3d'\")\n        if not isinstance(color, str) or color != 'white':\n            sct = ax.scatter(\n                *data,\n                marker='.',\n                c=color,\n                alpha=alpha,\n                edgecolors='none',  # 'face',\n                s=sizes[icolor],\n                cmap=color_map,\n                rasterized=settings._vector_friendly,\n            )\n        if colorbars[icolor]:\n            width = 0.006 * draw_region_width / len(colors)\n            left = (\n                panel_pos[2][2 * icolor + 1]\n                + (1.2 if projection == '3d' else 0.2) * width\n            )\n            rectangle = [left, bottom, width, height]\n            fig = pl.gcf()\n            ax_cb = fig.add_axes(rectangle)\n            _ = pl.colorbar(\n                sct, format=ticker.FuncFormatter(ticks_formatter), cax=ax_cb\n            )\n        # set the title\n        if title is not None:\n            ax.set_title(title[icolor])\n        # output highlighted data points\n        for iihighlight, ihighlight in enumerate(highlights_indices):\n            ihighlight = ihighlight if isinstance(ihighlight, int) else int(ihighlight)\n            data = [Y[ihighlight, 0]], [Y[ihighlight, 1]]\n            if '3d' in projection:\n                data = [Y[ihighlight, 0]], [Y[ihighlight, 1]], [Y[ihighlight, 2]]\n            ax.scatter(\n                *data,\n                c='black',\n                facecolors='black',\n                edgecolors='black',\n                marker='x',\n                s=10,\n                zorder=20,\n            )\n            highlight_text = (\n                highlights_labels[iihighlight]\n                if len(highlights_labels) > 0\n                else str(ihighlight)\n            )\n            # the following is a Python 2 compatibility hack\n            ax.text(\n                *([d[0] for d in data] + [highlight_text]),\n                zorder=20,\n                fontsize=10,\n                color='black',\n            )\n        if not show_ticks:\n            ax.set_xticks([])\n            ax.set_yticks([])\n            if '3d' in projection:\n                ax.set_zticks([])\n    # set default axis_labels\n    if axis_labels is None:\n        axis_labels = [\n            [component_name + str(i) for i in component_indexnames]\n            for _ in range(len(axs))\n        ]\n    else:\n        axis_labels = [axis_labels for _ in range(len(axs))]\n    for iax, ax in enumerate(axs):\n        ax.set_xlabel(axis_labels[iax][0])\n        ax.set_ylabel(axis_labels[iax][1])\n        if '3d' in projection:\n            # shift the label closer to the axis\n            ax.set_zlabel(axis_labels[iax][2], labelpad=-7)\n    for ax in axs:\n        # scale limits to match data\n        ax.autoscale_view()\n    return axs", "idx": 239}
{"project": "Scanpy", "commit_id": "317_scanpy_1.9.0__utils.py_scatter_single.py", "target": 0, "func": "def scatter_single(ax: Axes, Y: np.ndarray, *args, **kwargs):\n    \"\"\"Plot scatter plot of data.\n\n    Parameters\n    ----------\n    ax\n        Axis to plot on.\n    Y\n        Data array, data to be plotted needs to be in the first two columns.\n    \"\"\"\n    if 's' not in kwargs:\n        kwargs['s'] = 2 if Y.shape[0] > 500 else 10\n    if 'edgecolors' not in kwargs:\n        kwargs['edgecolors'] = 'face'\n    ax.scatter(Y[:, 0], Y[:, 1], **kwargs, rasterized=settings._vector_friendly)\n    ax.set_xticks([])\n    ax.set_yticks([])", "idx": 240}
{"project": "Scanpy", "commit_id": "318_scanpy_1.9.0__utils.py_arrows_transitions.py", "target": 0, "func": "def arrows_transitions(ax: Axes, X: np.ndarray, indices: Sequence[int], weight=None):\n    \"\"\"\n    Plot arrows of transitions in data matrix.\n\n    Parameters\n    ----------\n    ax\n        Axis object from matplotlib.\n    X\n        Data array, any representation wished (X, psi, phi, etc).\n    indices\n        Indices storing the transitions.\n    \"\"\"\n    step = 1\n    width = axis_to_data(ax, 0.001)\n    if X.shape[0] > 300:\n        step = 5\n        width = axis_to_data(ax, 0.0005)\n    if X.shape[0] > 500:\n        step = 30\n        width = axis_to_data(ax, 0.0001)\n    head_width = 10 * width\n    for ix, x in enumerate(X):\n        if ix % step != 0:\n            continue\n        X_step = X[indices[ix]] - x\n        # don't plot arrow of length 0\n        for itrans in range(X_step.shape[0]):\n            alphai = 1\n            widthi = width\n            head_widthi = head_width\n            if weight is not None:\n                alphai *= weight[ix, itrans]\n                widthi *= weight[ix, itrans]\n            if not np.any(X_step[itrans, :1]):\n                continue\n            ax.arrow(\n                x[0],\n                x[1],\n                X_step[itrans, 0],\n                X_step[itrans, 1],\n                length_includes_head=True,\n                width=widthi,\n                head_width=head_widthi,\n                alpha=alphai,\n                color='grey',", "idx": 241}
{"project": "Scanpy", "commit_id": "319_scanpy_1.9.0__utils.py_ticks_formatter.py", "target": 0, "func": "def ticks_formatter(x, pos):\n    # pretty scientific notation\n    if False:\n        a, b = f'{x:.2e}'.split('e')\n        b = int(b)\n        return fr'${a} \\times 10^{{{b}}}$'\n    else:\n        return f'{x:.3f}'.rstrip('0').rstrip('.')", "idx": 242}
{"project": "Scanpy", "commit_id": "31_scanpy_1.9.0_cli.py___len__.py", "target": 0, "func": "def __len__(self) -> int:\n        return len(self.parser_map) + len(self.commands)", "idx": 243}
{"project": "Scanpy", "commit_id": "320_scanpy_1.9.0__utils.py_pimp_axis.py", "target": 0, "func": "def pimp_axis(x_or_y_ax):\n    \"\"\"Remove trailing zeros.\"\"\"\n    x_or_y_ax.set_major_formatter(ticker.FuncFormatter(ticks_formatter))", "idx": 244}
{"project": "Scanpy", "commit_id": "321_scanpy_1.9.0__utils.py_scale_to_zero_one.py", "target": 0, "func": "def scale_to_zero_one(x):\n    \"\"\"Take some 1d data and scale it so that min matches 0 and max 1.\"\"\"\n    xscaled = x - np.min(x)\n    xscaled /= np.max(xscaled)\n    return xscaled", "idx": 245}
{"project": "Scanpy", "commit_id": "322_scanpy_1.9.0__utils.py_hierarchy_pos.py", "target": 0, "func": "def hierarchy_pos(G, root, levels=None, width=1.0, height=1.0):\n    \"\"\"Tree layout for networkx graph.\n\n    See https://stackoverflow.com/questions/29586520/can-one-get-hierarchical-graphs-from-networkx-with-python-3\n    answer by burubum.\n\n    If there is a cycle that is reachable from root, then this will see\n    infinite recursion.\n\n    Parameters\n    ----------\n    G: the graph\n    root: the root node\n    levels: a dictionary\n            key: level number (starting from 0)\n            value: number of nodes in this level\n    width: horizontal space allocated for drawing\n    height: vertical space allocated for drawing\n    \"\"\"\n    TOTAL = \"total\"\n    CURRENT = \"current\"\n\n    def make_levels(levels, node=root, currentLevel=0, parent=None):\n        \"\"\"Compute the number of nodes for each level\"\"\"\n        if currentLevel not in levels:\n            levels[currentLevel] = {TOTAL: 0, CURRENT: 0}\n        levels[currentLevel][TOTAL] += 1\n        neighbors = list(G.neighbors(node))\n        if parent is not None:\n            neighbors.remove(parent)\n        for neighbor in neighbors:\n            levels = make_levels(levels, neighbor, currentLevel + 1, node)\n        return levels\n\n    def make_pos(pos, node=root, currentLevel=0, parent=None, vert_loc=0):\n        dx = 1 / levels[currentLevel][TOTAL]\n        left = dx / 2\n        pos[node] = ((left + dx * levels[currentLevel][CURRENT]) * width, vert_loc)\n        levels[currentLevel][CURRENT] += 1\n        neighbors = list(G.neighbors(node))\n        if parent is not None:\n            neighbors.remove(parent)\n        for neighbor in neighbors:\n            pos = make_pos(pos, neighbor, currentLevel + 1, node, vert_loc - vert_gap)\n        return pos\n\n    if levels is None:\n        levels = make_levels({})\n    else:\n        levels = {k: {TOTAL: v, CURRENT: 0} for k, v in levels.items()}\n    vert_gap = height / (max(levels.keys()) + 1)\n    return make_pos({})", "idx": 246}
{"project": "Scanpy", "commit_id": "323_scanpy_1.9.0__utils.py_hierarchy_sc.py", "target": 0, "func": "def hierarchy_sc(G, root, node_sets):\n    import networkx as nx\n\n    def make_sc_tree(sc_G, node=root, parent=None):\n        sc_G.add_node(node)\n        neighbors = G.neighbors(node)\n        if parent is not None:\n            sc_G.add_edge(parent, node)\n            neighbors.remove(parent)\n        old_node = node\n        for n in node_sets[int(node)]:\n            new_node = str(node) + '_' + str(n)\n            sc_G.add_node(new_node)\n            sc_G.add_edge(old_node, new_node)\n            old_node = new_node\n        for neighbor in neighbors:\n            sc_G = make_sc_tree(sc_G, neighbor, node)\n        return sc_G\n\n    return make_sc_tree(nx.Graph())", "idx": 247}
{"project": "Scanpy", "commit_id": "324_scanpy_1.9.0__utils.py_zoom.py", "target": 0, "func": "def zoom(ax, xy='x', factor=1):\n    \"\"\"Zoom into axis.\n\n    Parameters\n    ----------\n    \"\"\"\n    limits = ax.get_xlim() if xy == 'x' else ax.get_ylim()\n    new_limits = 0.5 * (limits[0] + limits[1]) + 1.0 / factor * np.array(\n        (-0.5, 0.5)\n    ) * (limits[1] - limits[0])\n    if xy == 'x':\n        ax.set_xlim(new_limits)\n    else:\n        ax.set_ylim(new_limits)", "idx": 248}
{"project": "Scanpy", "commit_id": "325_scanpy_1.9.0__utils.py_get_ax_size.py", "target": 0, "func": "def get_ax_size(ax: Axes, fig: Figure):\n    \"\"\"Get axis size\n\n    Parameters\n    ----------\n    ax\n        Axis object from matplotlib.\n    fig\n        Figure.\n    \"\"\"\n    bbox = ax.get_window_extent().transformed(fig.dpi_scale_trans.inverted())\n    width, height = bbox.width, bbox.height\n    width *= fig.dpi\n    height *= fig.dpi", "idx": 249}
{"project": "Scanpy", "commit_id": "326_scanpy_1.9.0__utils.py_axis_to_data.py", "target": 0, "func": "def axis_to_data(ax: Axes, width: float):\n    \"\"\"For a width in axis coordinates, return the corresponding in data\n    coordinates.\n\n    Parameters\n    ----------\n    ax\n        Axis object from matplotlib.\n    width\n        Width in xaxis coordinates.\n    \"\"\"\n    xlim = ax.get_xlim()\n    widthx = width * (xlim[1] - xlim[0])\n    ylim = ax.get_ylim()\n    widthy = width * (ylim[1] - ylim[0])\n    return 0.5 * (widthx + widthy)", "idx": 250}
{"project": "Scanpy", "commit_id": "327_scanpy_1.9.0__utils.py_axis_to_data_points.py", "target": 0, "func": "def axis_to_data_points(ax: Axes, points_axis: np.ndarray):\n    \"\"\"Map points in axis coordinates to data coordinates.\n\n    Uses matplotlib.transform.\n\n    Parameters\n    ----------\n    ax\n        Axis object from matplotlib.\n    points_axis\n        Points in axis coordinates.\n    \"\"\"\n    axis_to_data = ax.transAxes + ax.transData.inverted()\n    return axis_to_data.transform(points_axis)", "idx": 251}
{"project": "Scanpy", "commit_id": "328_scanpy_1.9.0__utils.py_data_to_axis_points.py", "target": 0, "func": "def data_to_axis_points(ax: Axes, points_data: np.ndarray):\n    \"\"\"Map points in data coordinates to axis coordinates.\n\n    Uses matplotlib.transform.\n\n    Parameters\n    ----------\n    ax\n        Axis object from matplotlib.\n    points_data\n        Points in data coordinates.\n    \"\"\"\n    data_to_axis = axis_to_data.inverted()\n    return data_to_axis(points_data)", "idx": 252}
{"project": "Scanpy", "commit_id": "329_scanpy_1.9.0__utils.py_check_projection.py", "target": 0, "func": "def check_projection(projection):\n    \"\"\"Validation for projection argument.\"\"\"\n    if projection not in {\"2d\", \"3d\"}:\n        raise ValueError(f\"Projection must be '2d' or '3d', was '{projection}'.\")\n    if projection == \"3d\":\n        from packaging.version import parse\n\n        mpl_version = parse(mpl.__version__)\n        if mpl_version < parse(\"3.3.3\"):\n            raise ImportError(\n                f\"3d plotting requires matplotlib > 3.3.3. Found {mpl.__version__}\"", "idx": 253}
{"project": "Scanpy", "commit_id": "32_scanpy_1.9.0_cli.py___hash__.py", "target": 0, "func": "def __hash__(self) -> int:\n        return hash(self.command)", "idx": 254}
{"project": "Scanpy", "commit_id": "330_scanpy_1.9.0__utils.py_circles.py", "target": 0, "func": "def circles(\n    x, y, s, ax, marker=None, c='b', vmin=None, vmax=None, scale_factor=1.0, **kwargs\n):\n    \"\"\"\n    Taken from here: https://gist.github.com/syrte/592a062c562cd2a98a83\n    Make a scatter plot of circles.\n    Similar to pl.scatter, but the size of circles are in data scale.\n    Parameters\n    ----------\n    x, y : scalar or array_like, shape (n, )\n        Input data\n    s : scalar or array_like, shape (n, )\n        Radius of circles.\n    c : color or sequence of color, optional, default : 'b'\n        `c` can be a single color format string, or a sequence of color\n        specifications of length `N`, or a sequence of `N` numbers to be\n        mapped to colors using the `cmap` and `norm` specified via kwargs.\n        Note that `c` should not be a single numeric RGB or RGBA sequence\n        because that is indistinguishable from an array of values\n        to be colormapped. (If you insist, use `color` instead.)\n        `c` can be a 2-D array in which the rows are RGB or RGBA, however.\n    vmin, vmax : scalar, optional, default: None\n        `vmin` and `vmax` are used in conjunction with `norm` to normalize\n        luminance data.  If either are `None`, the min and max of the\n        color array is used.\n    kwargs : `~matplotlib.collections.Collection` properties\n        Eg. alpha, edgecolor(ec), facecolor(fc), linewidth(lw), linestyle(ls),\n        norm, cmap, transform, etc.\n    Returns\n    -------\n    paths : `~matplotlib.collections.PathCollection`\n    Examples\n    --------\n    a = np.arange(11)\n    circles(a, a, s=a*0.2, c=a, alpha=0.5, ec='none')\n    pl.colorbar()\n    License\n    --------\n    This code is under [The BSD 3-Clause License]\n    (http://opensource.org/licenses/BSD-3-Clause)\n    \"\"\"\n\n    # You can set `facecolor` with an array for each patch,\n    # while you can only set `facecolors` with a value for all.\n    if scale_factor != 1.0:\n        x = x * scale_factor\n        y = y * scale_factor\n    zipped = np.broadcast(x, y, s)\n    patches = [Circle((x_, y_), s_) for x_, y_, s_ in zipped]\n    collection = PatchCollection(patches, **kwargs)\n    if isinstance(c, np.ndarray) and np.issubdtype(c.dtype, np.number):\n        collection.set_array(np.ma.masked_invalid(c))\n        collection.set_clim(vmin, vmax)\n    else:\n        collection.set_facecolor(c)\n\n    ax.add_collection(collection)\n\n    return collection", "idx": 255}
{"project": "Scanpy", "commit_id": "331_scanpy_1.9.0__utils.py_make_grid_spec.py", "target": 0, "func": "def make_grid_spec(\n    ax_or_figsize: Union[Tuple[int, int], _AxesSubplot],\n    nrows: int,\n    ncols: int,\n    wspace: Optional[float] = None,\n    hspace: Optional[float] = None,\n    width_ratios: Optional[Sequence[float]] = None,\n    height_ratios: Optional[Sequence[float]] = None,\n) -> Tuple[Figure, gridspec.GridSpecBase]:\n    kw = dict(\n        wspace=wspace,\n        hspace=hspace,\n        width_ratios=width_ratios,\n        height_ratios=height_ratios,\n    )\n    if isinstance(ax_or_figsize, tuple):\n        fig = pl.figure(figsize=ax_or_figsize)\n        return fig, gridspec.GridSpec(nrows, ncols, **kw)\n    else:\n        ax = ax_or_figsize\n        ax.axis('off')\n        ax.set_frame_on(False)\n        ax.set_xticks([])\n        ax.set_yticks([])\n        return ax.figure, ax.get_subplotspec().subgridspec(nrows, ncols, **kw)", "idx": 256}
{"project": "Scanpy", "commit_id": "332_scanpy_1.9.0__utils.py_fix_kwds.py", "target": 0, "func": "def fix_kwds(kwds_dict, **kwargs):\n    \"\"\"\n    Given a dictionary of plot parameters (kwds_dict) and a dict of kwds,\n    merge the parameters into a single consolidated dictionary to avoid\n    argument duplication errors.\n\n    If kwds_dict an kwargs have the same key, only the value in kwds_dict is kept.\n\n    Parameters\n    ----------\n    kwds_dict kwds_dictionary\n    kwargs\n\n    Returns\n    -------\n    kwds_dict merged with kwargs\n\n    Examples\n    --------\n\n    >>> def _example(**kwds):\n    ...     return fix_kwds(kwds, key1=\"value1\", key2=\"value2\")\n    >>> example(key1=\"value10\", key3=\"value3\")\n        {'key1': 'value10, 'key2': 'value2', 'key3': 'value3'}\n\n    \"\"\"\n\n    kwargs.update(kwds_dict)\n\n    return kwargs", "idx": 257}
{"project": "Scanpy", "commit_id": "333_scanpy_1.9.0__utils.py__get_basis.py", "target": 0, "func": "def _get_basis(adata: anndata.AnnData, basis: str):\n\n    if basis in adata.obsm.keys():\n        basis_key = basis\n\n    elif f\"X_{basis}\" in adata.obsm.keys():\n        basis_key = f\"X_{basis}\"\n\n    return basis_key", "idx": 258}
{"project": "Scanpy", "commit_id": "334_scanpy_1.9.0__utils.py_check_colornorm.py", "target": 0, "func": "def check_colornorm(vmin=None, vmax=None, vcenter=None, norm=None):\n    from matplotlib.colors import Normalize\n\n    try:\n        from matplotlib.colors import TwoSlopeNorm as DivNorm\n    except ImportError:\n        # matplotlib<3.2\n        from matplotlib.colors import DivergingNorm as DivNorm\n\n    if norm is not None:\n        if (vmin is not None) or (vmax is not None) or (vcenter is not None):\n            raise ValueError('Passing both norm and vmin/vmax/vcenter is not allowed.')\n    else:\n        if vcenter is not None:\n            norm = DivNorm(vmin=vmin, vmax=vmax, vcenter=vcenter)\n        else:\n            norm = Normalize(vmin=vmin, vmax=vmax)\n\n    return norm", "idx": 259}
{"project": "Scanpy", "commit_id": "335_scanpy_1.9.0__utils.py_make_levels.py", "target": 0, "func": "def make_levels(levels, node=root, currentLevel=0, parent=None):\n        \"\"\"Compute the number of nodes for each level\"\"\"\n        if currentLevel not in levels:\n            levels[currentLevel] = {TOTAL: 0, CURRENT: 0}\n        levels[currentLevel][TOTAL] += 1\n        neighbors = list(G.neighbors(node))\n        if parent is not None:\n            neighbors.remove(parent)\n        for neighbor in neighbors:\n            levels = make_levels(levels, neighbor, currentLevel + 1, node)\n        return levels", "idx": 260}
{"project": "Scanpy", "commit_id": "336_scanpy_1.9.0__utils.py_make_pos.py", "target": 0, "func": "def make_pos(pos, node=root, currentLevel=0, parent=None, vert_loc=0):\n        dx = 1 / levels[currentLevel][TOTAL]\n        left = dx / 2\n        pos[node] = ((left + dx * levels[currentLevel][CURRENT]) * width, vert_loc)\n        levels[currentLevel][CURRENT] += 1\n        neighbors = list(G.neighbors(node))\n        if parent is not None:\n            neighbors.remove(parent)\n        for neighbor in neighbors:\n            pos = make_pos(pos, neighbor, currentLevel + 1, node, vert_loc - vert_gap)\n        return pos", "idx": 261}
{"project": "Scanpy", "commit_id": "337_scanpy_1.9.0__utils.py_make_sc_tree.py", "target": 0, "func": "def make_sc_tree(sc_G, node=root, parent=None):\n        sc_G.add_node(node)\n        neighbors = G.neighbors(node)\n        if parent is not None:\n            sc_G.add_edge(parent, node)\n            neighbors.remove(parent)\n        old_node = node\n        for n in node_sets[int(node)]:\n            new_node = str(node) + '_' + str(n)\n            sc_G.add_node(new_node)\n            sc_G.add_edge(old_node, new_node)\n            old_node = new_node\n        for neighbor in neighbors:\n            sc_G = make_sc_tree(sc_G, neighbor, node)\n        return sc_G", "idx": 262}
{"project": "Scanpy", "commit_id": "338_scanpy_1.9.0_paga.py_paga_compare.py", "target": 0, "func": "def paga_compare(\n    adata: AnnData,\n    basis=None,\n    edges=False,\n    color=None,\n    alpha=None,\n    groups=None,\n    components=None,\n    projection: Literal['2d', '3d'] = '2d',\n    legend_loc='on data',\n    legend_fontsize: Union[int, float, _FontSize, None] = None,\n    legend_fontweight: Union[int, _FontWeight] = 'bold',\n    legend_fontoutline=None,\n    color_map=None,\n    palette=None,\n    frameon=False,\n    size=None,\n    title=None,\n    right_margin=None,\n    left_margin=0.05,\n    show=None,\n    save=None,\n    title_graph=None,\n    groups_graph=None,\n    *,\n    pos=None,\n    **paga_graph_params,\n):\n    \"\"\"\\\n    Scatter and PAGA graph side-by-side.\n\n    Consists in a scatter plot and the abstracted graph. See\n    :func:`~scanpy.pl.paga` for all related parameters.\n\n    See :func:`~scanpy.pl.paga_path` for visualizing gene changes along paths\n    through the abstracted graph.\n\n    Additional parameters are as follows.\n\n    Parameters\n    ----------\n    adata\n        Annotated data matrix.\n    kwds_scatter\n        Keywords for :func:`~scanpy.pl.scatter`.\n    kwds_paga\n        Keywords for :func:`~scanpy.pl.paga`.\n\n    Returns\n    -------\n    A list of :class:`~matplotlib.axes.Axes` if `show` is `False`.\n    \"\"\"\n    axs, _, _, _ = _utils.setup_axes(\n        panels=[0, 1],\n        right_margin=right_margin,\n    )\n    if color is None:\n        color = adata.uns['paga']['groups']\n    suptitle = None  # common title for entire figure\n    if title_graph is None:\n        suptitle = color if title is None else title\n        title, title_graph = '', ''\n    if basis is None:\n        if 'X_draw_graph_fa' in adata.obsm:\n            basis = 'draw_graph_fa'\n        elif 'X_umap' in adata.obsm:\n            basis = 'umap'\n        elif 'X_tsne' in adata.obsm:\n            basis = 'tsne'\n        elif 'X_draw_graph_fr' in adata.obsm:\n            basis = 'draw_graph_fr'\n        else:\n            basis = 'umap'\n\n    from .scatterplots import embedding, _get_basis, _components_to_dimensions\n\n    embedding(\n        adata,\n        ax=axs[0],\n        basis=basis,\n        color=color,\n        edges=edges,\n        alpha=alpha,\n        groups=groups,\n        components=components,\n        legend_loc=legend_loc,\n        legend_fontsize=legend_fontsize,\n        legend_fontweight=legend_fontweight,\n        legend_fontoutline=legend_fontoutline,\n        color_map=color_map,\n        palette=palette,\n        frameon=frameon,\n        size=size,\n        title=title,\n        show=False,\n        save=False,\n    )\n\n    if pos is None:\n        if color == adata.uns['paga']['groups']:\n            # TODO: Use dimensions here\n            _basis = _get_basis(adata, basis)\n            dims = _components_to_dimensions(\n                components=components, dimensions=None, total_dims=_basis.shape[1]\n            )[0]\n            coords = _basis[:, dims]\n            pos = (\n                pd.DataFrame(coords, columns=[\"x\", \"y\"], index=adata.obs_names)\n                .groupby(adata.obs[color], observed=True)\n                .median()\n                .sort_index()\n            ).to_numpy()\n        else:\n            pos = adata.uns['paga']['pos']\n    xlim, ylim = axs[0].get_xlim(), axs[0].get_ylim()\n    axs[1].set_xlim(xlim)\n    axs[1].set_ylim(ylim)\n    if 'labels' in paga_graph_params:\n        labels = paga_graph_params.pop('labels')\n    else:\n        labels = groups_graph\n    if legend_fontsize is not None:\n        paga_graph_params['fontsize'] = legend_fontsize\n    if legend_fontweight is not None:\n        paga_graph_params['fontweight'] = legend_fontweight\n    if legend_fontoutline is not None:\n        paga_graph_params['fontoutline'] = legend_fontoutline\n    paga(\n        adata,\n        ax=axs[1],\n        show=False,\n        save=False,\n        title=title_graph,\n        labels=labels,\n        colors=color,\n        frameon=frameon,\n        pos=pos,\n        **paga_graph_params,\n    )\n    if suptitle is not None:\n        pl.suptitle(suptitle)\n    _utils.savefig_or_show('paga_compare', show=show, save=save)\n    if show is False:\n        return axs", "idx": 263}
{"project": "Scanpy", "commit_id": "339_scanpy_1.9.0_paga.py__compute_pos.py", "target": 0, "func": "def _compute_pos(\n    adjacency_solid,\n    layout=None,\n    random_state=0,\n    init_pos=None,\n    adj_tree=None,\n    root=0,\n    layout_kwds: Mapping[str, Any] = MappingProxyType({}),\n):\n    import random\n    import networkx as nx\n\n    random_state = check_random_state(random_state)\n\n    nx_g_solid = nx.Graph(adjacency_solid)\n    if layout is None:\n        layout = 'fr'\n    if layout == 'fa':\n        try:\n            from fa2 import ForceAtlas2\n        except ImportError:\n            logg.warning(\n                \"Package 'fa2' is not installed, falling back to layout 'fr'.\"\n                'To use the faster and better ForceAtlas2 layout, '\n                \"install package 'fa2' (`pip install fa2`).\"\n            )\n            layout = 'fr'\n    if layout == 'fa':\n        # np.random.seed(random_state)\n        if init_pos is None:\n            init_coords = random_state.random_sample((adjacency_solid.shape[0], 2))\n        else:\n            init_coords = init_pos.copy()\n        forceatlas2 = ForceAtlas2(\n            # Behavior alternatives\n            outboundAttractionDistribution=False,  # Dissuade hubs\n            linLogMode=False,  # NOT IMPLEMENTED\n            adjustSizes=False,  # Prevent overlap (NOT IMPLEMENTED)\n            edgeWeightInfluence=1.0,\n            # Performance\n            jitterTolerance=1.0,  # Tolerance\n            barnesHutOptimize=True,\n            barnesHutTheta=1.2,\n            multiThreaded=False,  # NOT IMPLEMENTED\n            # Tuning\n            scalingRatio=2.0,\n            strongGravityMode=False,\n            gravity=1.0,\n            # Log\n            verbose=False,\n        )\n        if 'maxiter' in layout_kwds:\n            iterations = layout_kwds['maxiter']\n        elif 'iterations' in layout_kwds:\n            iterations = layout_kwds['iterations']\n        else:\n            iterations = 500\n        pos_list = forceatlas2.forceatlas2(\n            adjacency_solid, pos=init_coords, iterations=iterations\n        )\n        pos = {n: [p[0], -p[1]] for n, p in enumerate(pos_list)}\n    elif layout == 'eq_tree':\n        nx_g_tree = nx.Graph(adj_tree)\n        pos = _utils.hierarchy_pos(nx_g_tree, root)\n        if len(pos) < adjacency_solid.shape[0]:\n            raise ValueError(\n                'This is a forest and not a single tree. '\n                'Try another `layout`, e.g., {\\'fr\\'}.'\n            )\n    else:\n        # igraph layouts\n        random.seed(random_state.bytes(8))\n        g = _sc_utils.get_igraph_from_adjacency(adjacency_solid)\n        if 'rt' in layout:\n            g_tree = _sc_utils.get_igraph_from_adjacency(adj_tree)\n            pos_list = g_tree.layout(\n                layout, root=root if isinstance(root, list) else [root]\n            ).coords\n        elif layout == 'circle':\n            pos_list = g.layout(layout).coords\n        else:\n            # I don't know why this is necessary\n            # np.random.seed(random_state)\n            if init_pos is None:\n                init_coords = random_state.random_sample(\n                    (adjacency_solid.shape[0], 2)\n                ).tolist()\n            else:\n                init_pos = init_pos.copy()\n                # this is a super-weird hack that is necessary as igraph\u2019s\n                # layout function seems to do some strange stuff here\n                init_pos[:, 1] *= -1\n                init_coords = init_pos.tolist()\n            try:\n                pos_list = g.layout(\n                    layout, seed=init_coords, weights='weight', **layout_kwds\n                ).coords\n            except AttributeError:  # hack for empty graphs...\n                pos_list = g.layout(layout, seed=init_coords, **layout_kwds).coords\n        pos = {n: [p[0], -p[1]] for n, p in enumerate(pos_list)}\n    if len(pos) == 1:\n        pos[0] = (0.5, 0.5)\n    pos_array = np.array([pos[n] for count, n in enumerate(nx_g_solid)])\n    return pos_array", "idx": 264}
{"project": "Scanpy", "commit_id": "33_scanpy_1.9.0_cli.py___eq__.py", "target": 0, "func": "def __eq__(self, other: Mapping[str, ArgumentParser]):\n        if isinstance(other, _CommandDelegator):\n            return all(\n                getattr(self, attr) == getattr(other, attr)\n                for attr in ['command', 'action', 'parser_map', 'runargs']\n            )\n        return self.parser_map == other", "idx": 265}
{"project": "Scanpy", "commit_id": "340_scanpy_1.9.0_paga.py_paga.py", "target": 0, "func": "def paga(\n    adata: AnnData,\n    threshold: Optional[float] = None,\n    color: Optional[Union[str, Mapping[Union[str, int], Mapping[Any, float]]]] = None,\n    layout: Optional[_IGraphLayout] = None,\n    layout_kwds: Mapping[str, Any] = MappingProxyType({}),\n    init_pos: Optional[np.ndarray] = None,\n    root: Union[int, str, Sequence[int], None] = 0,\n    labels: Union[str, Sequence[str], Mapping[str, str], None] = None,\n    single_component: bool = False,\n    solid_edges: str = 'connectivities',\n    dashed_edges: Optional[str] = None,\n    transitions: Optional[str] = None,\n    fontsize: Optional[int] = None,\n    fontweight: str = 'bold',\n    fontoutline: Optional[int] = None,\n    text_kwds: Mapping[str, Any] = MappingProxyType({}),\n    node_size_scale: float = 1.0,\n    node_size_power: float = 0.5,\n    edge_width_scale: float = 1.0,\n    min_edge_width: Optional[float] = None,\n    max_edge_width: Optional[float] = None,\n    arrowsize: int = 30,\n    title: Optional[str] = None,\n    left_margin: float = 0.01,\n    random_state: Optional[int] = 0,\n    pos: Union[np.ndarray, str, Path, None] = None,\n    normalize_to_color: bool = False,\n    cmap: Union[str, Colormap] = None,\n    cax: Optional[Axes] = None,\n    colorbar=None,  # TODO: this seems to be unused\n    cb_kwds: Mapping[str, Any] = MappingProxyType({}),\n    frameon: Optional[bool] = None,\n    add_pos: bool = True,\n    export_to_gexf: bool = False,\n    use_raw: bool = True,\n    colors=None,  # backwards compat\n    groups=None,  # backwards compat\n    plot: bool = True,\n    show: Optional[bool] = None,\n    save: Union[bool, str, None] = None,\n    ax: Optional[Axes] = None,\n) -> Union[Axes, List[Axes], None]:\n    \"\"\"\\\n    Plot the PAGA graph through thresholding low-connectivity edges.\n\n    Compute a coarse-grained layout of the data. Reuse this by passing\n    `init_pos='paga'` to :func:`~scanpy.tl.umap` or\n    :func:`~scanpy.tl.draw_graph` and obtain embeddings with more meaningful\n    global topology [Wolf19]_.\n\n    This uses ForceAtlas2 or igraph's layout algorithms for most layouts [Csardi06]_.\n\n    Parameters\n    ----------\n    adata\n        Annotated data matrix.\n    threshold\n        Do not draw edges for weights below this threshold. Set to 0 if you want\n        all edges. Discarding low-connectivity edges helps in getting a much\n        clearer picture of the graph.\n    color\n        Gene name or `obs` annotation defining the node colors.\n        Also plots the degree of the abstracted graph when\n        passing {`'degree_dashed'`, `'degree_solid'`}.\n\n        Can be also used to visualize pie chart at each node in the following form:\n        `{<group name or index>: {<color>: <fraction>, ...}, ...}`. If the fractions\n        do not sum to 1, a new category called `'rest'` colored grey will be created.\n    labels\n        The node labels. If `None`, this defaults to the group labels stored in\n        the categorical for which :func:`~scanpy.tl.paga` has been computed.\n    pos\n        Two-column array-like storing the x and y coordinates for drawing.\n        Otherwise, path to a `.gdf` file that has been exported from Gephi or\n        a similar graph visualization software.\n    layout\n        Plotting layout that computes positions.\n        `'fa'` stands for \u201cForceAtlas2\u201d,\n        `'fr'` stands for \u201cFruchterman-Reingold\u201d,\n        `'rt'` stands for \u201cReingold-Tilford\u201d,\n        `'eq_tree'` stands for \u201ceqally spaced tree\u201d.\n        All but `'fa'` and `'eq_tree'` are igraph layouts.\n        All other igraph layouts are also permitted.\n        See also parameter `pos` and :func:`~scanpy.tl.draw_graph`.\n    layout_kwds\n        Keywords for the layout.\n    init_pos\n        Two-column array storing the x and y coordinates for initializing the\n        layout.\n    random_state\n        For layouts with random initialization like `'fr'`, change this to use\n        different intial states for the optimization. If `None`, the initial\n        state is not reproducible.\n    root\n        If choosing a tree layout, this is the index of the root node or a list\n        of root node indices. If this is a non-empty vector then the supplied\n        node IDs are used as the roots of the trees (or a single tree if the\n        graph is connected). If this is `None` or an empty list, the root\n        vertices are automatically calculated based on topological sorting.\n    transitions\n        Key for `.uns['paga']` that specifies the matrix that stores the\n        arrows, for instance `'transitions_confidence'`.\n    solid_edges\n        Key for `.uns['paga']` that specifies the matrix that stores the edges\n        to be drawn solid black.\n    dashed_edges\n        Key for `.uns['paga']` that specifies the matrix that stores the edges\n        to be drawn dashed grey. If `None`, no dashed edges are drawn.\n    single_component\n        Restrict to largest connected component.\n    fontsize\n        Font size for node labels.\n    fontoutline\n        Width of the white outline around fonts.\n    text_kwds\n        Keywords for :meth:`~matplotlib.axes.Axes.text`.\n    node_size_scale\n        Increase or decrease the size of the nodes.\n    node_size_power\n        The power with which groups sizes influence the radius of the nodes.\n    edge_width_scale\n        Edge with scale in units of `rcParams['lines.linewidth']`.\n    min_edge_width\n        Min width of solid edges.\n    max_edge_width\n        Max width of solid and dashed edges.\n    arrowsize\n       For directed graphs, choose the size of the arrow head head's length and\n       width. See :py:class: `matplotlib.patches.FancyArrowPatch` for attribute\n       `mutation_scale` for more info.\n    export_to_gexf\n        Export to gexf format to be read by graph visualization programs such as\n        Gephi.\n    normalize_to_color\n        Whether to normalize categorical plots to `color` or the underlying\n        grouping.\n    cmap\n        The color map.\n    cax\n        A matplotlib axes object for a potential colorbar.\n    cb_kwds\n        Keyword arguments for :class:`~matplotlib.colorbar.Colorbar`,\n        for instance, `ticks`.\n    add_pos\n        Add the positions to `adata.uns['paga']`.\n    title\n        Provide a title.\n    frameon\n        Draw a frame around the PAGA graph.\n    plot\n        If `False`, do not create the figure, simply compute the layout.\n    save\n        If `True` or a `str`, save the figure.\n        A string is appended to the default filename.\n        Infer the filetype if ending on \\\\{`'.pdf'`, `'.png'`, `'.svg'`\\\\}.\n    ax\n        A matplotlib axes object.\n\n    Returns\n    -------\n    If `show==False`, one or more :class:`~matplotlib.axes.Axes` objects.\n    Adds `'pos'` to `adata.uns['paga']` if `add_pos` is `True`.\n\n    Examples\n    --------\n\n    .. plot::\n        :context: close-figs\n\n        import scanpy as sc\n        adata = sc.datasets.pbmc3k_processed()\n        sc.tl.paga(adata, groups='louvain')\n        sc.pl.paga(adata)\n\n    You can increase node and edge sizes by specifying additional arguments.\n\n    .. plot::\n        :context: close-figs\n\n        sc.pl.paga(adata, node_size_scale=10, edge_width_scale=2)\n\n    Notes\n    -----\n    When initializing the positions, note that \u2013 for some reason \u2013 igraph\n    mirrors coordinates along the x axis... that is, you should increase the\n    `maxiter` parameter by 1 if the layout is flipped.\n\n    .. currentmodule:: scanpy\n\n    See also\n    --------\n    tl.paga\n    pl.paga_compare\n    pl.paga_path\n    \"\"\"\n\n    if groups is not None:  # backwards compat\n        labels = groups\n        logg.warning('`groups` is deprecated in `pl.paga`: use `labels` instead')\n    if colors is None:\n        colors = color\n\n    groups_key = adata.uns['paga']['groups']\n\n    def is_flat(x):\n        has_one_per_category = isinstance(x, cabc.Collection) and len(x) == len(\n            adata.obs[groups_key].cat.categories\n        )\n        return has_one_per_category or x is None or isinstance(x, str)\n\n    if isinstance(colors, cabc.Mapping) and isinstance(\n        colors[next(iter(colors))], cabc.Mapping\n    ):\n        # handle paga pie, remap string keys to integers\n        names_to_ixs = {\n            n: i for i, n in enumerate(adata.obs[groups_key].cat.categories)\n        }\n        colors = {names_to_ixs.get(n, n): v for n, v in colors.items()}\n    if is_flat(colors):\n        colors = [colors]\n\n    if frameon is None:\n        frameon = settings._frameon\n    # labels is a list that contains no lists\n    if is_flat(labels):\n        labels = [labels for _ in range(len(colors))]\n\n    if title is None and len(colors) > 1:\n        title = [c for c in colors]\n    elif isinstance(title, str):\n        title = [title for c in colors]\n    elif title is None:\n        title = [None for c in colors]\n\n    if colorbar is None:\n        var_names = adata.var_names if adata.raw is None else adata.raw.var_names\n        colorbars = [\n            (\n                (c in adata.obs_keys() and adata.obs[c].dtype.name != 'category')\n                or (c in var_names)\n            )\n            for c in colors\n        ]\n    else:\n        colorbars = [False for _ in colors]\n\n    if isinstance(root, str):\n        if root not in labels:\n            raise ValueError(\n                'If `root` is a string, '\n                f'it needs to be one of {labels} not {root!r}.'\n            )\n        root = list(labels).index(root)\n    if isinstance(root, cabc.Sequence) and root[0] in labels:\n        root = [list(labels).index(r) for r in root]\n\n    # define the adjacency matrices\n    adjacency_solid = adata.uns['paga'][solid_edges].copy()\n    adjacency_dashed = None\n    if threshold is None:\n        threshold = 0.01  # default threshold\n    if threshold > 0:\n        adjacency_solid.data[adjacency_solid.data < threshold] = 0\n        adjacency_solid.eliminate_zeros()\n    if dashed_edges is not None:\n        adjacency_dashed = adata.uns['paga'][dashed_edges].copy()\n        if threshold > 0:\n            adjacency_dashed.data[adjacency_dashed.data < threshold] = 0\n            adjacency_dashed.eliminate_zeros()\n\n    # compute positions\n    if pos is None:\n        adj_tree = None\n        if layout in {'rt', 'rt_circular', 'eq_tree'}:\n            adj_tree = adata.uns['paga']['connectivities_tree']\n        pos = _compute_pos(\n            adjacency_solid,\n            layout=layout,\n            random_state=random_state,\n            init_pos=init_pos,\n            layout_kwds=layout_kwds,\n            adj_tree=adj_tree,\n            root=root,\n        )\n\n    if plot:\n        axs, panel_pos, draw_region_width, figure_width = _utils.setup_axes(\n            ax=ax,\n            panels=colors,\n            colorbars=colorbars,\n        )\n\n        if len(colors) == 1 and not isinstance(axs, list):\n            axs = [axs]\n\n        for icolor, c in enumerate(colors):\n            if title[icolor] is not None:\n                axs[icolor].set_title(title[icolor])\n            sct = _paga_graph(\n                adata,\n                axs[icolor],\n                colors=colors if isinstance(colors, cabc.Mapping) else c,\n                solid_edges=solid_edges,\n                dashed_edges=dashed_edges,\n                transitions=transitions,\n                threshold=threshold,\n                adjacency_solid=adjacency_solid,\n                adjacency_dashed=adjacency_dashed,\n                root=root,\n                labels=labels[icolor],\n                fontsize=fontsize,\n                fontweight=fontweight,\n                fontoutline=fontoutline,\n                text_kwds=text_kwds,\n                node_size_scale=node_size_scale,\n                node_size_power=node_size_power,\n                edge_width_scale=edge_width_scale,\n                min_edge_width=min_edge_width,\n                max_edge_width=max_edge_width,\n                normalize_to_color=normalize_to_color,\n                frameon=frameon,\n                cmap=cmap,\n                colorbar=colorbars[icolor],\n                cb_kwds=cb_kwds,\n                use_raw=use_raw,\n                title=title[icolor],\n                export_to_gexf=export_to_gexf,\n                single_component=single_component,\n                arrowsize=arrowsize,\n                pos=pos,\n            )\n            if colorbars[icolor]:\n                if cax is None:\n                    bottom = panel_pos[0][0]\n                    height = panel_pos[1][0] - bottom\n                    width = 0.006 * draw_region_width / len(colors)\n                    left = panel_pos[2][2 * icolor + 1] + 0.2 * width\n                    rectangle = [left, bottom, width, height]\n                    fig = pl.gcf()\n                    ax_cb = fig.add_axes(rectangle)\n                else:\n                    ax_cb = cax[icolor]\n\n                _ = pl.colorbar(\n                    sct,\n                    format=ticker.FuncFormatter(_utils.ticks_formatter),\n                    cax=ax_cb,\n                )\n    if add_pos:\n        adata.uns['paga']['pos'] = pos\n        logg.hint(\"added 'pos', the PAGA positions (adata.uns['paga'])\")\n    if plot:\n        _utils.savefig_or_show('paga', show=show, save=save)\n        if len(colors) == 1 and isinstance(axs, list):\n            axs = axs[0]\n        if show is False:\n            return axs", "idx": 266}
{"project": "Scanpy", "commit_id": "341_scanpy_1.9.0_paga.py__paga_graph.py", "target": 0, "func": "def _paga_graph(\n    adata,\n    ax,\n    solid_edges=None,\n    dashed_edges=None,\n    adjacency_solid=None,\n    adjacency_dashed=None,\n    transitions=None,\n    threshold=None,\n    root=0,\n    colors=None,\n    labels=None,\n    fontsize=None,\n    fontweight=None,\n    fontoutline=None,\n    text_kwds: Mapping[str, Any] = MappingProxyType({}),\n    node_size_scale=1.0,\n    node_size_power=0.5,\n    edge_width_scale=1.0,\n    normalize_to_color='reference',\n    title=None,\n    pos=None,\n    cmap=None,\n    frameon=True,\n    min_edge_width=None,\n    max_edge_width=None,\n    export_to_gexf=False,\n    colorbar=None,\n    use_raw=True,\n    cb_kwds: Mapping[str, Any] = MappingProxyType({}),\n    single_component=False,\n    arrowsize=30,\n):\n    import networkx as nx\n\n    node_labels = labels  # rename for clarity\n    if (\n        node_labels is not None\n        and isinstance(node_labels, str)\n        and node_labels != adata.uns['paga']['groups']\n    ):\n        raise ValueError(\n            'Provide a list of group labels for the PAGA groups {}, not {}.'.format(\n                adata.uns['paga']['groups'], node_labels\n            )\n        )\n    groups_key = adata.uns['paga']['groups']\n    if node_labels is None:\n        node_labels = adata.obs[groups_key].cat.categories\n\n    if (colors is None or colors == groups_key) and groups_key is not None:\n        if groups_key + '_colors' not in adata.uns or len(\n            adata.obs[groups_key].cat.categories\n        ) != len(adata.uns[groups_key + '_colors']):\n            _utils.add_colors_for_categorical_sample_annotation(adata, groups_key)\n        colors = adata.uns[groups_key + '_colors']\n        for iname, name in enumerate(adata.obs[groups_key].cat.categories):\n            if name in settings.categories_to_ignore:\n                colors[iname] = 'grey'\n\n    nx_g_solid = nx.Graph(adjacency_solid)\n    if dashed_edges is not None:\n        nx_g_dashed = nx.Graph(adjacency_dashed)\n\n    # convert pos to array and dict\n    if not isinstance(pos, (Path, str)):\n        pos_array = pos\n    else:\n        pos = Path(pos)\n        if pos.suffix != '.gdf':\n            raise ValueError(\n                'Currently only supporting reading positions from .gdf files. '\n                'Consider generating them using, for instance, Gephi.'\n            )\n        s = ''  # read the node definition from the file\n        with pos.open() as f:\n            f.readline()\n            for line in f:\n                if line.startswith('edgedef>'):\n                    break\n                s += line\n        from io import StringIO\n\n        df = pd.read_csv(StringIO(s), header=-1)\n        pos_array = df[[4, 5]].values\n\n    # convert to dictionary\n    pos = {n: [p[0], p[1]] for n, p in enumerate(pos_array)}\n\n    # uniform color\n    if isinstance(colors, str) and is_color_like(colors):\n        colors = [colors for c in range(len(node_labels))]\n\n    # color degree of the graph\n    if isinstance(colors, str) and colors.startswith('degree'):\n        # see also tools.paga.paga_degrees\n        if colors == 'degree_dashed':\n            colors = [d for _, d in nx_g_dashed.degree(weight='weight')]\n        elif colors == 'degree_solid':\n            colors = [d for _, d in nx_g_solid.degree(weight='weight')]\n        else:\n            raise ValueError('`degree` either \"degree_dashed\" or \"degree_solid\".')\n        colors = (np.array(colors) - np.min(colors)) / (np.max(colors) - np.min(colors))\n\n    # plot gene expression\n    var_names = adata.var_names if adata.raw is None else adata.raw.var_names\n    if isinstance(colors, str) and colors in var_names:\n        x_color = []\n        cats = adata.obs[groups_key].cat.categories\n        for icat, cat in enumerate(cats):\n            subset = (cat == adata.obs[groups_key]).values\n            if adata.raw is not None and use_raw:\n                adata_gene = adata.raw[:, colors]\n            else:\n                adata_gene = adata[:, colors]\n            x_color.append(np.mean(adata_gene.X[subset]))\n        colors = x_color\n\n    # plot continuous annotation\n    if (\n        isinstance(colors, str)\n        and colors in adata.obs\n        and not is_categorical_dtype(adata.obs[colors])\n    ):\n        x_color = []\n        cats = adata.obs[groups_key].cat.categories\n        for icat, cat in enumerate(cats):\n            subset = (cat == adata.obs[groups_key]).values\n            x_color.append(adata.obs.loc[subset, colors].mean())\n        colors = x_color\n\n    # plot categorical annotation\n    if (\n        isinstance(colors, str)\n        and colors in adata.obs\n        and is_categorical_dtype(adata.obs[colors])\n    ):\n        asso_names, asso_matrix = _sc_utils.compute_association_matrix_of_groups(\n            adata,\n            prediction=groups_key,\n            reference=colors,\n            normalization='reference' if normalize_to_color else 'prediction',\n        )\n        _utils.add_colors_for_categorical_sample_annotation(adata, colors)\n        asso_colors = _sc_utils.get_associated_colors_of_groups(\n            adata.uns[colors + '_colors'], asso_matrix\n        )\n        colors = asso_colors\n\n    if len(colors) != len(node_labels):\n        raise ValueError(\n            f'Expected `colors` to be of length `{len(node_labels)}`, '\n            f'found `{len(colors)}`.'\n        )\n\n    # count number of connected components\n    n_components, labels = scipy.sparse.csgraph.connected_components(adjacency_solid)\n    if n_components > 1 and not single_component:\n        logg.debug(\n            'Graph has more than a single connected component. '\n            'To restrict to this component, pass `single_component=True`.'\n        )\n    if n_components > 1 and single_component:\n        component_sizes = np.bincount(labels)\n        largest_component = np.where(component_sizes == component_sizes.max())[0][0]\n        adjacency_solid = adjacency_solid.tocsr()[labels == largest_component, :]\n        adjacency_solid = adjacency_solid.tocsc()[:, labels == largest_component]\n        colors = np.array(colors)[labels == largest_component]\n        node_labels = np.array(node_labels)[labels == largest_component]\n        cats_dropped = (\n            adata.obs[groups_key].cat.categories[labels != largest_component].tolist()\n        )\n        logg.info(\n            'Restricting graph to largest connected component by dropping categories\\n'\n            f'{cats_dropped}'\n        )\n        nx_g_solid = nx.Graph(adjacency_solid)\n        if dashed_edges is not None:\n            raise ValueError('`single_component` only if `dashed_edges` is `None`.')\n\n    # edge widths\n    base_edge_width = edge_width_scale * 5 * rcParams['lines.linewidth']\n\n    # draw dashed edges\n    if dashed_edges is not None:\n        widths = [x[-1]['weight'] for x in nx_g_dashed.edges(data=True)]\n        widths = base_edge_width * np.array(widths)\n        if max_edge_width is not None:\n            widths = np.clip(widths, None, max_edge_width)\n        nx.draw_networkx_edges(\n            nx_g_dashed,\n            pos,\n            ax=ax,\n            width=widths,\n            edge_color='grey',\n            style='dashed',\n            alpha=0.5,\n        )\n\n    # draw solid edges\n    if transitions is None:\n        widths = [x[-1]['weight'] for x in nx_g_solid.edges(data=True)]\n        widths = base_edge_width * np.array(widths)\n        if min_edge_width is not None or max_edge_width is not None:\n            widths = np.clip(widths, min_edge_width, max_edge_width)\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\")\n            nx.draw_networkx_edges(\n                nx_g_solid, pos, ax=ax, width=widths, edge_color='black'\n            )\n    # draw directed edges\n    else:\n        adjacency_transitions = adata.uns['paga'][transitions].copy()\n        if threshold is None:\n            threshold = 0.01\n        adjacency_transitions.data[adjacency_transitions.data < threshold] = 0\n        adjacency_transitions.eliminate_zeros()\n        g_dir = nx.DiGraph(adjacency_transitions.T)\n        widths = [x[-1]['weight'] for x in g_dir.edges(data=True)]\n        widths = base_edge_width * np.array(widths)\n        if min_edge_width is not None or max_edge_width is not None:\n            widths = np.clip(widths, min_edge_width, max_edge_width)\n        nx.draw_networkx_edges(\n            g_dir, pos, ax=ax, width=widths, edge_color='black', arrowsize=arrowsize\n        )\n\n    if export_to_gexf:\n        if isinstance(colors[0], tuple):\n            from matplotlib.colors import rgb2hex\n\n            colors = [rgb2hex(c) for c in colors]\n        for count, n in enumerate(nx_g_solid.nodes()):\n            nx_g_solid.node[count]['label'] = str(node_labels[count])\n            nx_g_solid.node[count]['color'] = str(colors[count])\n            nx_g_solid.node[count]['viz'] = dict(\n                position=dict(\n                    x=1000 * pos[count][0],\n                    y=1000 * pos[count][1],\n                    z=0,\n                )\n            )\n        filename = settings.writedir / 'paga_graph.gexf'\n        logg.warning(f'exporting to {filename}')\n        settings.writedir.mkdir(parents=True, exist_ok=True)\n        nx.write_gexf(nx_g_solid, settings.writedir / 'paga_graph.gexf')\n\n    ax.set_frame_on(frameon)\n    ax.set_xticks([])\n    ax.set_yticks([])\n\n    # groups sizes\n    if groups_key is not None and groups_key + '_sizes' in adata.uns:\n        groups_sizes = adata.uns[groups_key + '_sizes']\n    else:\n        groups_sizes = np.ones(len(node_labels))\n    base_scale_scatter = 2000\n    base_pie_size = (\n        base_scale_scatter / (np.sqrt(adjacency_solid.shape[0]) + 10) * node_size_scale\n    )\n    median_group_size = np.median(groups_sizes)\n    groups_sizes = base_pie_size * np.power(\n        groups_sizes / median_group_size, node_size_power\n    )\n\n    if fontsize is None:\n        fontsize = rcParams['legend.fontsize']\n    if fontoutline is not None:\n        text_kwds = dict(text_kwds)\n        text_kwds['path_effects'] = [\n            patheffects.withStroke(linewidth=fontoutline, foreground='w')\n        ]\n    # usual scatter plot\n    if not isinstance(colors[0], cabc.Mapping):\n        n_groups = len(pos_array)\n        sct = ax.scatter(\n            pos_array[:, 0],\n            pos_array[:, 1],\n            c=colors[:n_groups],\n            edgecolors='face',\n            s=groups_sizes,\n            cmap=cmap,\n        )\n        for count, group in enumerate(node_labels):\n            ax.text(\n                pos_array[count, 0],\n                pos_array[count, 1],\n                group,\n                verticalalignment='center',\n                horizontalalignment='center',\n                size=fontsize,\n                fontweight=fontweight,\n                **text_kwds,\n            )\n    # else pie chart plot\n    else:\n        for ix, (xx, yy) in enumerate(zip(pos_array[:, 0], pos_array[:, 1])):\n            if not isinstance(colors[ix], cabc.Mapping):\n                raise ValueError(\n                    f'{colors[ix]} is neither a dict of valid '\n                    'matplotlib colors nor a valid matplotlib color.'\n                )\n            color_single = colors[ix].keys()\n            fracs = [colors[ix][c] for c in color_single]\n            total = sum(fracs)\n\n            if total < 1:\n                color_single = list(color_single)\n                color_single.append('grey')\n                fracs.append(1 - sum(fracs))\n            elif not np.isclose(total, 1):\n                raise ValueError(\n                    f'Expected fractions for node `{ix}` to be '\n                    f'close to 1, found `{total}`.'\n                )\n\n            cumsum = np.cumsum(fracs)\n            cumsum = cumsum / cumsum[-1]\n            cumsum = [0] + cumsum.tolist()\n\n            for r1, r2, color in zip(cumsum[:-1], cumsum[1:], color_single):\n                angles = np.linspace(2 * np.pi * r1, 2 * np.pi * r2, 20)\n                x = [0] + np.cos(angles).tolist()\n                y = [0] + np.sin(angles).tolist()\n\n                xy = np.column_stack([x, y])\n                s = np.abs(xy).max()\n\n                sct = ax.scatter(\n                    [xx], [yy], marker=xy, s=s**2 * groups_sizes[ix], color=color\n                )\n\n            if node_labels is not None:\n                ax.text(\n                    xx,\n                    yy,\n                    node_labels[ix],\n                    verticalalignment='center',\n                    horizontalalignment='center',\n                    size=fontsize,\n                    fontweight=fontweight,\n                    **text_kwds,\n                )\n\n    return sct", "idx": 267}
{"project": "Scanpy", "commit_id": "342_scanpy_1.9.0_paga.py_paga_path.py", "target": 0, "func": "def paga_path(\n    adata: AnnData,\n    nodes: Sequence[Union[str, int]],\n    keys: Sequence[str],\n    use_raw: bool = True,\n    annotations: Sequence[str] = ('dpt_pseudotime',),\n    color_map: Union[str, Colormap, None] = None,\n    color_maps_annotations: Mapping[str, Union[str, Colormap]] = MappingProxyType(\n        dict(dpt_pseudotime='Greys')\n    ),\n    palette_groups: Optional[Sequence[str]] = None,\n    n_avg: int = 1,\n    groups_key: Optional[str] = None,\n    xlim: Tuple[Optional[int], Optional[int]] = (None, None),\n    title: Optional[str] = None,\n    left_margin=None,\n    ytick_fontsize: Optional[int] = None,\n    title_fontsize: Optional[int] = None,\n    show_node_names: bool = True,\n    show_yticks: bool = True,\n    show_colorbar: bool = True,\n    legend_fontsize: Union[int, float, _FontSize, None] = None,\n    legend_fontweight: Union[int, _FontWeight, None] = None,\n    normalize_to_zero_one: bool = False,\n    as_heatmap: bool = True,\n    return_data: bool = False,\n    show: Optional[bool] = None,\n    save: Union[bool, str, None] = None,\n    ax: Optional[Axes] = None,\n) -> Optional[Axes]:\n    \"\"\"\\\n    Gene expression and annotation changes along paths in the abstracted graph.\n\n    Parameters\n    ----------\n    adata\n        An annotated data matrix.\n    nodes\n        A path through nodes of the abstracted graph, that is, names or indices\n        (within `.categories`) of groups that have been used to run PAGA.\n    keys\n        Either variables in `adata.var_names` or annotations in\n        `adata.obs`. They are plotted using `color_map`.\n    use_raw\n        Use `adata.raw` for retrieving gene expressions if it has been set.\n    annotations\n        Plot these keys with `color_maps_annotations`. Need to be keys for\n        `adata.obs`.\n    color_map\n        Matplotlib colormap.\n    color_maps_annotations\n        Color maps for plotting the annotations. Keys of the dictionary must\n        appear in `annotations`.\n    palette_groups\n        Ususally, use the same `sc.pl.palettes...` as used for coloring the\n        abstracted graph.\n    n_avg\n        Number of data points to include in computation of running average.\n    groups_key\n        Key of the grouping used to run PAGA. If `None`, defaults to\n        `adata.uns['paga']['groups']`.\n    as_heatmap\n        Plot the timeseries as heatmap. If not plotting as heatmap,\n        `annotations` have no effect.\n    show_node_names\n        Plot the node names on the nodes bar.\n    show_colorbar\n        Show the colorbar.\n    show_yticks\n        Show the y ticks.\n    normalize_to_zero_one\n        Shift and scale the running average to [0, 1] per gene.\n    return_data\n        Return the timeseries data in addition to the axes if `True`.\n    show\n         Show the plot, do not return axis.\n    save\n        If `True` or a `str`, save the figure.\n        A string is appended to the default filename.\n        Infer the filetype if ending on \\\\{`'.pdf'`, `'.png'`, `'.svg'`\\\\}.\n    ax\n         A matplotlib axes object.\n\n    Returns\n    -------\n    A :class:`~matplotlib.axes.Axes` object, if `ax` is `None`, else `None`.\n    If `return_data`, return the timeseries data in addition to an axes.\n    \"\"\"\n    ax_was_none = ax is None\n\n    if groups_key is None:\n        if 'groups' not in adata.uns['paga']:\n            raise KeyError(\n                'Pass the key of the grouping with which you ran PAGA, '\n                'using the parameter `groups_key`.'\n            )\n        groups_key = adata.uns['paga']['groups']\n    groups_names = adata.obs[groups_key].cat.categories\n\n    if 'dpt_pseudotime' not in adata.obs.keys():\n        raise ValueError(\n            '`pl.paga_path` requires computation of a pseudotime `tl.dpt` '\n            'for ordering at single-cell resolution'\n        )\n\n    if palette_groups is None:\n        _utils.add_colors_for_categorical_sample_annotation(adata, groups_key)\n        palette_groups = adata.uns[f'{groups_key}_colors']\n\n    def moving_average(a):\n        return _sc_utils.moving_average(a, n_avg)\n\n    ax = pl.gca() if ax is None else ax\n\n    X = []\n    x_tick_locs = [0]\n    x_tick_labels = []\n    groups = []\n    anno_dict = {anno: [] for anno in annotations}\n    if isinstance(nodes[0], str):\n        nodes_ints = []\n        groups_names_set = set(groups_names)\n        for node in nodes:\n            if node not in groups_names_set:\n                raise ValueError(\n                    f'Each node/group needs to be in {groups_names.tolist()} '\n                    f'(`groups_key`={groups_key!r}) not {node!r}.'\n                )\n            nodes_ints.append(groups_names.get_loc(node))\n        nodes_strs = nodes\n    else:\n        nodes_ints = nodes\n        nodes_strs = [groups_names[node] for node in nodes]\n\n    adata_X = adata\n    if use_raw and adata.raw is not None:\n        adata_X = adata.raw\n\n    for ikey, key in enumerate(keys):\n        x = []\n        for igroup, group in enumerate(nodes_ints):\n            idcs = np.arange(adata.n_obs)[\n                adata.obs[groups_key].values == nodes_strs[igroup]\n            ]\n            if len(idcs) == 0:\n                raise ValueError(\n                    'Did not find data points that match '\n                    f'`adata.obs[{groups_key!r}].values == {str(group)!r}`. '\n                    f'Check whether `adata.obs[{groups_key!r}]` '\n                    'actually contains what you expect.'\n                )\n            idcs_group = np.argsort(\n                adata.obs['dpt_pseudotime'].values[\n                    adata.obs[groups_key].values == nodes_strs[igroup]\n                ]\n            )\n            idcs = idcs[idcs_group]\n            values = (\n                adata.obs[key].values if key in adata.obs_keys() else adata_X[:, key].X\n            )[idcs]\n            x += (values.A if issparse(values) else values).tolist()\n            if ikey == 0:\n                groups += [group] * len(idcs)\n                x_tick_locs.append(len(x))\n                for anno in annotations:\n                    series = adata.obs[anno]\n                    if is_categorical_dtype(series):\n                        series = series.cat.codes\n                    anno_dict[anno] += list(series.values[idcs])\n        if n_avg > 1:\n            x = moving_average(x)\n            if ikey == 0:\n                for key in annotations:\n                    if not isinstance(anno_dict[key][0], str):\n                        anno_dict[key] = moving_average(anno_dict[key])\n        if normalize_to_zero_one:\n            x -= np.min(x)\n            x /= np.max(x)\n        X.append(x)\n        if not as_heatmap:\n            ax.plot(x[xlim[0] : xlim[1]], label=key)\n        if ikey == 0:\n            for igroup, group in enumerate(nodes):\n                if len(groups_names) > 0 and group not in groups_names:\n                    label = groups_names[group]\n                else:\n                    label = group\n                x_tick_labels.append(label)\n    X = np.asarray(X).squeeze()\n    if as_heatmap:\n        img = ax.imshow(X, aspect='auto', interpolation='nearest', cmap=color_map)\n        if show_yticks:\n            ax.set_yticks(range(len(X)))\n            ax.set_yticklabels(keys, fontsize=ytick_fontsize)\n        else:\n            ax.set_yticks([])\n        ax.set_frame_on(False)\n        ax.set_xticks([])\n        ax.tick_params(axis='both', which='both', length=0)\n        ax.grid(False)\n        if show_colorbar:\n            pl.colorbar(img, ax=ax)\n        left_margin = 0.2 if left_margin is None else left_margin\n        pl.subplots_adjust(left=left_margin)\n    else:\n        left_margin = 0.4 if left_margin is None else left_margin\n        if len(keys) > 1:\n            pl.legend(\n                frameon=False,\n                loc='center left',\n                bbox_to_anchor=(-left_margin, 0.5),\n                fontsize=legend_fontsize,\n            )\n    xlabel = groups_key\n    if not as_heatmap:\n        ax.set_xlabel(xlabel)\n        pl.yticks([])\n        if len(keys) == 1:\n            pl.ylabel(keys[0] + ' (a.u.)')\n    else:\n        import matplotlib.colors\n\n        # groups bar\n        ax_bounds = ax.get_position().bounds\n        groups_axis = pl.axes(\n            (\n                ax_bounds[0],\n                ax_bounds[1] - ax_bounds[3] / len(keys),\n                ax_bounds[2],\n                ax_bounds[3] / len(keys),\n            )\n        )\n        groups = np.array(groups)[None, :]\n        groups_axis.imshow(\n            groups,\n            aspect='auto',\n            interpolation=\"nearest\",\n            cmap=matplotlib.colors.ListedColormap(\n                # the following line doesn't work because of normalization\n                # adata.uns['paga_groups_colors'])\n                palette_groups[np.min(groups).astype(int) :],\n                N=int(np.max(groups) + 1 - np.min(groups)),\n            ),\n        )\n        if show_yticks:\n            groups_axis.set_yticklabels(['', xlabel, ''], fontsize=ytick_fontsize)\n        else:\n            groups_axis.set_yticks([])\n        groups_axis.set_frame_on(False)\n        if show_node_names:\n            ypos = (groups_axis.get_ylim()[1] + groups_axis.get_ylim()[0]) / 2\n            x_tick_locs = _sc_utils.moving_average(x_tick_locs, n=2)\n            for ilabel, label in enumerate(x_tick_labels):\n                groups_axis.text(\n                    x_tick_locs[ilabel],\n                    ypos,\n                    x_tick_labels[ilabel],\n                    fontdict=dict(\n                        horizontalalignment='center',\n                        verticalalignment='center',\n                    ),\n                )\n        groups_axis.set_xticks([])\n        groups_axis.grid(False)\n        groups_axis.tick_params(axis='both', which='both', length=0)\n        # further annotations\n        y_shift = ax_bounds[3] / len(keys)\n        for ianno, anno in enumerate(annotations):\n            if ianno > 0:\n                y_shift = ax_bounds[3] / len(keys) / 2\n            anno_axis = pl.axes(\n                (\n                    ax_bounds[0],\n                    ax_bounds[1] - (ianno + 2) * y_shift,\n                    ax_bounds[2],\n                    y_shift,\n                )\n            )\n            arr = np.array(anno_dict[anno])[None, :]\n            if anno not in color_maps_annotations:\n                color_map_anno = (\n                    'Vega10' if is_categorical_dtype(adata.obs[anno]) else 'Greys'\n                )\n            else:\n                color_map_anno = color_maps_annotations[anno]\n            img = anno_axis.imshow(\n                arr,\n                aspect='auto',\n                interpolation='nearest',\n                cmap=color_map_anno,\n            )\n            if show_yticks:\n                anno_axis.set_yticklabels(['', anno, ''], fontsize=ytick_fontsize)\n                anno_axis.tick_params(axis='both', which='both', length=0)\n            else:\n                anno_axis.set_yticks([])\n            anno_axis.set_frame_on(False)\n            anno_axis.set_xticks([])\n            anno_axis.grid(False)\n    if title is not None:\n        ax.set_title(title, fontsize=title_fontsize)\n    if show is None and not ax_was_none:\n        show = False\n    else:\n        show = settings.autoshow if show is None else show\n    _utils.savefig_or_show('paga_path', show=show, save=save)\n    if return_data:\n        df = pd.DataFrame(data=X.T, columns=keys)\n        df['groups'] = moving_average(groups)  # groups is without moving average, yet\n        if 'dpt_pseudotime' in anno_dict:\n            df['distance'] = anno_dict['dpt_pseudotime'].T\n        return ax, df if ax_was_none and not show else df\n    else:\n        return ax if ax_was_none and not show else None", "idx": 268}
{"project": "Scanpy", "commit_id": "343_scanpy_1.9.0_paga.py_paga_adjacency.py", "target": 0, "func": "def paga_adjacency(\n    adata,\n    adjacency='connectivities',\n    adjacency_tree='connectivities_tree',\n    as_heatmap=True,\n    color_map=None,\n    show=None,\n    save=None,\n):\n    \"\"\"Connectivity of paga groups.\"\"\"\n    connectivity = adata.uns[adjacency].toarray()\n    connectivity_select = adata.uns[adjacency_tree]\n    if as_heatmap:\n        matrix(connectivity, color_map=color_map, show=False)\n        for i in range(connectivity_select.shape[0]):\n            neighbors = connectivity_select[i].nonzero()[1]\n            pl.scatter([i for j in neighbors], neighbors, color='black', s=1)\n    # as a stripplot\n    else:\n        pl.figure()\n        for i, cs in enumerate(connectivity):\n            x = [i for j, d in enumerate(cs) if i != j]\n            y = [c for j, c in enumerate(cs) if i != j]\n            pl.scatter(x, y, color='gray', s=1)\n            neighbors = connectivity_select[i].nonzero()[1]\n            pl.scatter([i for j in neighbors], cs[neighbors], color='black', s=1)\n    _utils.savefig_or_show('paga_connectivity', show=show, save=save)", "idx": 269}
{"project": "Scanpy", "commit_id": "344_scanpy_1.9.0_paga.py_is_flat.py", "target": 0, "func": "def is_flat(x):\n        has_one_per_category = isinstance(x, cabc.Collection) and len(x) == len(\n            adata.obs[groups_key].cat.categories\n        )\n        return has_one_per_category or x is None or isinstance(x, str)", "idx": 270}
{"project": "Scanpy", "commit_id": "345_scanpy_1.9.0_paga.py_moving_average.py", "target": 0, "func": "def moving_average(a):\n        return _sc_utils.moving_average(a, n_avg)", "idx": 271}
{"project": "Scanpy", "commit_id": "346_scanpy_1.9.0_scatterplots.py_embedding.py", "target": 1, "func": "def embedding(\n    adata: AnnData,\n    basis: str,\n    *,\n    color: Union[str, Sequence[str], None] = None,\n    gene_symbols: Optional[str] = None,\n    use_raw: Optional[bool] = None,\n    sort_order: bool = True,\n    edges: bool = False,\n    edges_width: float = 0.1,\n    edges_color: Union[str, Sequence[float], Sequence[str]] = 'grey',\n    neighbors_key: Optional[str] = None,\n    arrows: bool = False,\n    arrows_kwds: Optional[Mapping[str, Any]] = None,\n    groups: Optional[str] = None,\n    components: Union[str, Sequence[str]] = None,\n    dimensions: Optional[Union[Tuple[int, int], Sequence[Tuple[int, int]]]] = None,\n    layer: Optional[str] = None,\n    projection: Literal['2d', '3d'] = '2d',\n    scale_factor: Optional[float] = None,\n    color_map: Union[Colormap, str, None] = None,\n    cmap: Union[Colormap, str, None] = None,\n    palette: Union[str, Sequence[str], Cycler, None] = None,\n    na_color: ColorLike = \"lightgray\",\n    na_in_legend: bool = True,\n    size: Union[float, Sequence[float], None] = None,\n    frameon: Optional[bool] = None,\n    legend_fontsize: Union[int, float, _FontSize, None] = None,\n    legend_fontweight: Union[int, _FontWeight] = 'bold',\n    legend_loc: str = 'right margin',\n    legend_fontoutline: Optional[int] = None,\n    colorbar_loc: Optional[str] = \"right\",\n    vmax: Union[VBound, Sequence[VBound], None] = None,\n    vmin: Union[VBound, Sequence[VBound], None] = None,\n    vcenter: Union[VBound, Sequence[VBound], None] = None,\n    norm: Union[Normalize, Sequence[Normalize], None] = None,\n    add_outline: Optional[bool] = False,\n    outline_width: Tuple[float, float] = (0.3, 0.05),\n    outline_color: Tuple[str, str] = ('black', 'white'),\n    ncols: int = 4,\n    hspace: float = 0.25,\n    wspace: Optional[float] = None,\n    title: Union[str, Sequence[str], None] = None,\n    show: Optional[bool] = None,\n    save: Union[bool, str, None] = None,\n    ax: Optional[Axes] = None,\n    return_fig: Optional[bool] = None,\n    **kwargs,\n) -> Union[Figure, Axes, None]:\n    \"\"\"\\\n    Scatter plot for user specified embedding basis (e.g. umap, pca, etc)\n\n    Parameters\n    ----------\n    basis\n        Name of the `obsm` basis to use.\n    {adata_color_etc}\n    {edges_arrows}\n    {scatter_bulk}\n    {show_save_ax}\n\n    Returns\n    -------\n    If `show==False` a :class:`~matplotlib.axes.Axes` or a list of it.\n    \"\"\"\n    #####################\n    # Argument handling #\n    #####################\n\n    check_projection(projection)\n    sanitize_anndata(adata)\n\n    basis_values = _get_basis(adata, basis)\n    dimensions = _components_to_dimensions(\n        components, dimensions, projection=projection, total_dims=basis_values.shape[1]\n    )\n    args_3d = dict(projection='3d') if projection == '3d' else {}\n\n    # Figure out if we're using raw\n    if use_raw is None:\n        # check if adata.raw is set\n        use_raw = layer is None and adata.raw is not None\n    if use_raw and layer is not None:\n        raise ValueError(\n            \"Cannot use both a layer and the raw representation. Was passed:\"\n            f\"use_raw={use_raw}, layer={layer}.\"\n        )\n    if use_raw and adata.raw is None:\n        raise ValueError(\n            \"`use_raw` is set to True but AnnData object does not have raw. \"\n            \"Please check.\"\n        )\n\n    if isinstance(groups, str):\n        groups = [groups]\n\n    # Color map\n    if color_map is not None:\n        if cmap is not None:\n            raise ValueError(\"Cannot specify both `color_map` and `cmap`.\")\n        else:\n            cmap = color_map\n    cmap = copy(get_cmap(cmap))\n    cmap.set_bad(na_color)\n    kwargs[\"cmap\"] = cmap\n    # Prevents warnings during legend creation\n    na_color = colors.to_hex(na_color, keep_alpha=True)\n\n    if 'edgecolor' not in kwargs:\n        # by default turn off edge color. Otherwise, for\n        # very small sizes the edge will not reduce its size\n        # (https://github.com/theislab/scanpy/issues/293)\n        kwargs['edgecolor'] = 'none'\n\n    # Vectorized arguments\n\n    # turn color into a python list\n    color = [color] if isinstance(color, str) or color is None else list(color)\n    if title is not None:\n        # turn title into a python list if not None\n        title = [title] if isinstance(title, str) else list(title)\n\n    # turn vmax and vmin into a sequence\n    if isinstance(vmax, str) or not isinstance(vmax, cabc.Sequence):\n        vmax = [vmax]\n    if isinstance(vmin, str) or not isinstance(vmin, cabc.Sequence):\n        vmin = [vmin]\n    if isinstance(vcenter, str) or not isinstance(vcenter, cabc.Sequence):\n        vcenter = [vcenter]\n    if isinstance(norm, Normalize) or not isinstance(norm, cabc.Sequence):\n        norm = [norm]\n\n    # Size\n    if 's' in kwargs and size is None:\n        size = kwargs.pop('s')\n    if size is not None:\n        # check if size is any type of sequence, and if so\n        # set as ndarray\n        if (\n            size is not None\n            and isinstance(size, (cabc.Sequence, pd.Series, np.ndarray))\n            and len(size) == adata.shape[0]\n        ):\n            size = np.array(size, dtype=float)\n    else:\n        size = 120000 / adata.shape[0]\n\n    ##########\n    # Layout #\n    ##########\n    # Most of the code is for the case when multiple plots are required\n\n    if wspace is None:\n        #  try to set a wspace that is not too large or too small given the\n        #  current figure size\n        wspace = 0.75 / rcParams['figure.figsize'][0] + 0.02\n\n    if components is not None:\n        color, dimensions = list(zip(*product(color, dimensions)))\n\n    color, dimensions = _broadcast_args(color, dimensions)\n\n    # 'color' is a list of names that want to be plotted.\n    # Eg. ['Gene1', 'louvain', 'Gene2'].\n    # component_list is a list of components [[0,1], [1,2]]\n    if (\n        not isinstance(color, str)\n        and isinstance(color, cabc.Sequence)\n        and len(color) > 1\n    ) or len(dimensions) > 1:\n        if ax is not None:\n            raise ValueError(\n                \"Cannot specify `ax` when plotting multiple panels \"\n                \"(each for a given value of 'color').\"\n            )\n\n        # each plot needs to be its own panel\n        fig, grid = _panel_grid(hspace, wspace, ncols, len(color))\n    else:\n        grid = None\n        if ax is None:\n            fig = pl.figure()\n            ax = fig.add_subplot(111, **args_3d)\n\n    ############\n    # Plotting #\n    ############\n    axs = []\n\n    # use itertools.product to make a plot for each color and for each component\n    # For example if color=[gene1, gene2] and components=['1,2, '2,3'].\n    # The plots are: [\n    #     color=gene1, components=[1,2], color=gene1, components=[2,3],\n    #     color=gene2, components = [1, 2], color=gene2, components=[2,3],\n    # ]\n    for count, (value_to_plot, dims) in enumerate(zip(color, dimensions)):\n        color_source_vector = _get_color_source_vector(\n            adata,\n            value_to_plot,\n            layer=layer,\n            use_raw=use_raw,\n            gene_symbols=gene_symbols,\n            groups=groups,\n        )\n        color_vector, categorical = _color_vector(\n            adata,\n            value_to_plot,\n            color_source_vector,\n            palette=palette,\n            na_color=na_color,\n        )\n\n        # Order points\n        order = slice(None)\n        if sort_order is True and value_to_plot is not None and categorical is False:\n            # Higher values plotted on top, null values on bottom\n            order = np.argsort(-color_vector, kind=\"stable\")[::-1]\n        elif sort_order and categorical:\n            # Null points go on bottom\n            order = np.argsort(~pd.isnull(color_source_vector), kind=\"stable\")\n        # Set orders\n        if isinstance(size, np.ndarray):\n            size = np.array(size)[order]\n        color_source_vector = color_source_vector[order]\n        color_vector = color_vector[order]\n        coords = basis_values[:, dims][order, :]\n\n        # if plotting multiple panels, get the ax from the grid spec\n        # else use the ax value (either user given or created previously)\n        if grid:\n            ax = pl.subplot(grid[count], **args_3d)\n            axs.append(ax)\n        if not (settings._frameon if frameon is None else frameon):\n            ax.axis('off')\n        if title is None:\n            if value_to_plot is not None:\n                ax.set_title(value_to_plot)\n            else:\n                ax.set_title('')\n        else:\n            try:\n                ax.set_title(title[count])\n            except IndexError:\n                logg.warning(\n                    \"The title list is shorter than the number of panels. \"\n                    \"Using 'color' value instead for some plots.\"\n                )\n                ax.set_title(value_to_plot)\n\n        if not categorical:\n            vmin_float, vmax_float, vcenter_float, norm_obj = _get_vboundnorm(\n                vmin, vmax, vcenter, norm, count, color_vector\n            )\n            normalize = check_colornorm(\n                vmin_float,\n                vmax_float,\n                vcenter_float,\n                norm_obj,\n            )\n        else:\n            normalize = None\n\n        # make the scatter plot\n        if projection == '3d':\n            cax = ax.scatter(\n                coords[:, 0],\n                coords[:, 1],\n                coords[:, 2],\n                marker=\".\",\n                c=color_vector,\n                rasterized=settings._vector_friendly,\n                norm=normalize,\n                **kwargs,\n            )\n        else:\n            scatter = (\n                partial(ax.scatter, s=size, plotnonfinite=True)\n                if scale_factor is None\n                else partial(\n                    circles, s=size, ax=ax, scale_factor=scale_factor\n                )  # size in circles is radius\n            )\n\n            if add_outline:\n                # the default outline is a black edge followed by a\n                # thin white edged added around connected clusters.\n                # To add an outline\n                # three overlapping scatter plots are drawn:\n                # First black dots with slightly larger size,\n                # then, white dots a bit smaller, but still larger\n                # than the final dots. Then the final dots are drawn\n                # with some transparency.\n\n                bg_width, gap_width = outline_width\n                point = np.sqrt(size)\n                gap_size = (point + (point * gap_width) * 2) ** 2\n                bg_size = (np.sqrt(gap_size) + (point * bg_width) * 2) ** 2\n                # the default black and white colors can be changes using\n                # the contour_config parameter\n                bg_color, gap_color = outline_color\n\n                # remove edge from kwargs if present\n                # because edge needs to be set to None\n                kwargs['edgecolor'] = 'none'\n\n                # remove alpha for outline\n                alpha = kwargs.pop('alpha') if 'alpha' in kwargs else None\n\n                ax.scatter(\n                    coords[:, 0],\n                    coords[:, 1],\n                    s=bg_size,\n                    marker=\".\",\n                    c=bg_color,\n                    rasterized=settings._vector_friendly,\n                    norm=normalize,\n                    **kwargs,\n                )\n                ax.scatter(\n                    coords[:, 0],\n                    coords[:, 1],\n                    s=gap_size,\n                    marker=\".\",\n                    c=gap_color,\n                    rasterized=settings._vector_friendly,\n                    norm=normalize,\n                    **kwargs,\n                )\n                # if user did not set alpha, set alpha to 0.7\n                kwargs['alpha'] = 0.7 if alpha is None else alpha\n\n            cax = scatter(\n                coords[:, 0],\n                coords[:, 1],\n                marker=\".\",\n                c=color_vector,\n                rasterized=settings._vector_friendly,\n                norm=normalize,\n                **kwargs,\n            )\n\n        # remove y and x ticks\n        ax.set_yticks([])\n        ax.set_xticks([])\n        if projection == '3d':\n            ax.set_zticks([])\n\n        # set default axis_labels\n        name = _basis2name(basis)\n        axis_labels = [name + str(d + 1) for d in dims]\n\n        ax.set_xlabel(axis_labels[0])\n        ax.set_ylabel(axis_labels[1])\n        if projection == '3d':\n            # shift the label closer to the axis\n            ax.set_zlabel(axis_labels[2], labelpad=-7)\n        ax.autoscale_view()\n\n        if edges:\n            _utils.plot_edges(ax, adata, basis, edges_width, edges_color, neighbors_key)\n        if arrows:\n            _utils.plot_arrows(ax, adata, basis, arrows_kwds)\n\n        if value_to_plot is None:\n            # if only dots were plotted without an associated value\n            # there is not need to plot a legend or a colorbar\n            continue\n\n        if legend_fontoutline is not None:\n            path_effect = [\n                patheffects.withStroke(linewidth=legend_fontoutline, foreground='w')\n            ]\n        else:\n            path_effect = None\n\n        # Adding legends\n        if categorical:\n            _add_categorical_legend(\n                ax,\n                color_source_vector,\n                palette=_get_palette(adata, value_to_plot),\n                scatter_array=coords,\n                legend_loc=legend_loc,\n                legend_fontweight=legend_fontweight,\n                legend_fontsize=legend_fontsize,\n                legend_fontoutline=path_effect,\n                na_color=na_color,\n                na_in_legend=na_in_legend,\n                multi_panel=bool(grid),\n            )\n        elif colorbar_loc is not None:\n            pl.colorbar(\n                cax, ax=ax, pad=0.01, fraction=0.08, aspect=30, location=colorbar_loc\n            )\n\n    if return_fig is True:\n        return fig\n    axs = axs if grid else ax\n    _utils.savefig_or_show(basis, show=show, save=save)\n    if show is False:\n        return axs", "idx": 272}
{"project": "Scanpy", "commit_id": "347_scanpy_1.9.0_scatterplots.py__panel_grid.py", "target": 0, "func": "def _panel_grid(hspace, wspace, ncols, num_panels):\n    from matplotlib import gridspec\n\n    n_panels_x = min(ncols, num_panels)\n    n_panels_y = np.ceil(num_panels / n_panels_x).astype(int)\n    # each panel will have the size of rcParams['figure.figsize']\n    fig = pl.figure(\n        figsize=(\n            n_panels_x * rcParams['figure.figsize'][0] * (1 + wspace),\n            n_panels_y * rcParams['figure.figsize'][1],\n        ),\n    )\n    left = 0.2 / n_panels_x\n    bottom = 0.13 / n_panels_y\n    gs = gridspec.GridSpec(\n        nrows=n_panels_y,\n        ncols=n_panels_x,\n        left=left,\n        right=1 - (n_panels_x - 1) * left - 0.01 / n_panels_x,\n        bottom=bottom,\n        top=1 - (n_panels_y - 1) * bottom - 0.1 / n_panels_y,\n        hspace=hspace,\n        wspace=wspace,\n    )\n    return fig, gs", "idx": 273}
{"project": "Scanpy", "commit_id": "348_scanpy_1.9.0_scatterplots.py__get_vboundnorm.py", "target": 0, "func": "def _get_vboundnorm(\n    vmin: Sequence[VBound],\n    vmax: Sequence[VBound],\n    vcenter: Sequence[VBound],\n    norm: Sequence[Normalize],\n    index: int,\n    color_vector: Sequence[float],\n) -> Tuple[Union[float, None], Union[float, None]]:\n    \"\"\"\n    Evaluates the value of vmin, vmax and vcenter, which could be a\n    str in which case is interpreted as a percentile and should\n    be specified in the form 'pN' where N is the percentile.\n    Eg. for a percentile of 85 the format would be 'p85'.\n    Floats are accepted as p99.9\n\n    Alternatively, vmin/vmax could be a function that is applied to\n    the list of color values (`color_vector`).  E.g.\n\n    def my_vmax(color_vector): np.percentile(color_vector, p=80)\n\n\n    Parameters\n    ----------\n    index\n        This index of the plot\n    color_vector\n        List or values for the plot\n\n    Returns\n    -------\n\n    (vmin, vmax, vcenter, norm) containing None or float values for\n    vmin, vmax, vcenter and matplotlib.colors.Normalize  or None for norm.\n\n    \"\"\"\n    out = []\n    for v_name, v in [('vmin', vmin), ('vmax', vmax), ('vcenter', vcenter)]:\n        if len(v) == 1:\n            # this case usually happens when the user sets eg vmax=0.9, which\n            # is internally converted into list of len=1, but is expected that this\n            # value applies to all plots.\n            v_value = v[0]\n        else:\n            try:\n                v_value = v[index]\n            except IndexError:\n                logg.error(\n                    f\"The parameter {v_name} is not valid. If setting multiple {v_name} values,\"\n                    f\"check that the length of the {v_name} list is equal to the number \"\n                    \"of plots. \"\n                )\n                v_value = None\n\n        if v_value is not None:\n            if isinstance(v_value, str) and v_value.startswith('p'):\n                try:\n                    float(v_value[1:])\n                except ValueError:\n                    logg.error(\n                        f\"The parameter {v_name}={v_value} for plot number {index + 1} is not valid. \"\n                        f\"Please check the correct format for percentiles.\"\n                    )\n                # interpret value of vmin/vmax as quantile with the following syntax 'p99.9'\n                v_value = np.nanpercentile(color_vector, q=float(v_value[1:]))\n            elif callable(v_value):\n                # interpret vmin/vmax as function\n                v_value = v_value(color_vector)\n                if not isinstance(v_value, float):\n                    logg.error(\n                        f\"The return of the function given for {v_name} is not valid. \"\n                        \"Please check that the function returns a number.\"\n                    )\n                    v_value = None\n            else:\n                try:\n                    float(v_value)\n                except ValueError:\n                    logg.error(\n                        f\"The given {v_name}={v_value} for plot number {index + 1} is not valid. \"\n                        f\"Please check that the value given is a valid number, a string \"\n                        f\"starting with 'p' for percentiles or a valid function.\"\n                    )\n                    v_value = None\n        out.append(v_value)\n    out.append(norm[0] if len(norm) == 1 else norm[index])\n    return tuple(out)", "idx": 274}
{"project": "Scanpy", "commit_id": "349_scanpy_1.9.0_scatterplots.py__wraps_plot_scatter.py", "target": 0, "func": "def _wraps_plot_scatter(wrapper):\n    import inspect\n\n    params = inspect.signature(embedding).parameters.copy()\n    wrapper_sig = inspect.signature(wrapper)\n    wrapper_params = wrapper_sig.parameters.copy()\n\n    params.pop(\"basis\")\n    params.pop(\"kwargs\")\n    wrapper_params.pop(\"adata\")\n\n    params.update(wrapper_params)\n    annotations = {\n        k: v.annotation\n        for k, v in params.items()\n        if v.annotation != inspect.Parameter.empty\n    }\n    if wrapper_sig.return_annotation is not inspect.Signature.empty:\n        annotations[\"return\"] = wrapper_sig.return_annotation\n\n    wrapper.__signature__ = inspect.Signature(\n        list(params.values()), return_annotation=wrapper_sig.return_annotation\n    )\n    wrapper.__annotations__ = annotations\n\n    return wrapper", "idx": 275}
{"project": "Scanpy", "commit_id": "34_scanpy_1.9.0_cli.py_commands.py", "target": 0, "func": "def commands(self) -> FrozenSet[str]:\n        return frozenset(\n            binary.name[len(self.command) + 1 :]\n            for bin_dir in os.environ['PATH'].split(os.pathsep)\n            for binary in Path(bin_dir).glob(f'{self.command}-*')\n            if os.access(binary, os.X_OK)", "idx": 276}
{"project": "Scanpy", "commit_id": "350_scanpy_1.9.0_scatterplots.py_umap.py", "target": 0, "func": "def umap(adata, **kwargs) -> Union[Axes, List[Axes], None]:\n    \"\"\"\\\n    Scatter plot in UMAP basis.\n\n    Parameters\n    ----------\n    {adata_color_etc}\n    {edges_arrows}\n    {scatter_bulk}\n    {show_save_ax}\n\n    Returns\n    -------\n    If `show==False` a :class:`~matplotlib.axes.Axes` or a list of it.\n\n    Examples\n    --------\n\n    .. plot::\n        :context: close-figs\n\n        import scanpy as sc\n        adata = sc.datasets.pbmc68k_reduced()\n        sc.pl.umap(adata)\n\n    Colour points by discrete variable (Louvain clusters).\n\n    .. plot::\n        :context: close-figs\n\n        sc.pl.umap(adata, color=\"louvain\")\n\n    Colour points by gene expression.\n\n    .. plot::\n        :context: close-figs\n\n        sc.pl.umap(adata, color=\"HES4\")\n\n    Plot muliple umaps for different gene expressions.\n\n    .. plot::\n        :context: close-figs\n\n        sc.pl.umap(adata, color=[\"HES4\", \"TNFRSF4\"])\n\n    .. currentmodule:: scanpy\n\n    See also\n    --------\n    tl.umap\n    \"\"\"\n    return embedding(adata, 'umap', **kwargs)", "idx": 277}
{"project": "Scanpy", "commit_id": "351_scanpy_1.9.0_scatterplots.py_tsne.py", "target": 0, "func": "def tsne(adata, **kwargs) -> Union[Axes, List[Axes], None]:\n    \"\"\"\\\n    Scatter plot in tSNE basis.\n\n    Parameters\n    ----------\n    {adata_color_etc}\n    {edges_arrows}\n    {scatter_bulk}\n    {show_save_ax}\n\n    Returns\n    -------\n    If `show==False` a :class:`~matplotlib.axes.Axes` or a list of it.\n\n    Examples\n    --------\n    .. plot::\n        :context: close-figs\n\n        import scanpy as sc\n        adata = sc.datasets.pbmc68k_reduced()\n        sc.tl.tsne(adata)\n        sc.pl.tsne(adata, color='bulk_labels')\n\n    .. currentmodule:: scanpy\n\n    See also\n    --------\n    tl.tsne\n    \"\"\"\n    return embedding(adata, 'tsne', **kwargs)", "idx": 278}
{"project": "Scanpy", "commit_id": "352_scanpy_1.9.0_scatterplots.py_diffmap.py", "target": 0, "func": "def diffmap(adata, **kwargs) -> Union[Axes, List[Axes], None]:\n    \"\"\"\\\n    Scatter plot in Diffusion Map basis.\n\n    Parameters\n    ----------\n    {adata_color_etc}\n    {scatter_bulk}\n    {show_save_ax}\n\n    Returns\n    -------\n    If `show==False` a :class:`~matplotlib.axes.Axes` or a list of it.\n\n    Examples\n    --------\n    .. plot::\n        :context: close-figs\n\n        import scanpy as sc\n        adata = sc.datasets.pbmc68k_reduced()\n        sc.tl.diffmap(adata)\n        sc.pl.diffmap(adata, color='bulk_labels')\n\n    .. currentmodule:: scanpy\n\n    See also\n    --------\n    tl.diffmap\n    \"\"\"\n    return embedding(adata, 'diffmap', **kwargs)", "idx": 279}
{"project": "Scanpy", "commit_id": "353_scanpy_1.9.0_scatterplots.py_draw_graph.py", "target": 0, "func": "def draw_graph(\n    adata: AnnData, *, layout: Optional[_IGraphLayout] = None, **kwargs\n) -> Union[Axes, List[Axes], None]:\n    \"\"\"\\\n    Scatter plot in graph-drawing basis.\n\n    Parameters\n    ----------\n    {adata_color_etc}\n    layout\n        One of the :func:`~scanpy.tl.draw_graph` layouts.\n        By default, the last computed layout is used.\n    {edges_arrows}\n    {scatter_bulk}\n    {show_save_ax}\n\n    Returns\n    -------\n    If `show==False` a :class:`~matplotlib.axes.Axes` or a list of it.\n\n    Examples\n    --------\n    .. plot::\n        :context: close-figs\n\n        import scanpy as sc\n        adata = sc.datasets.pbmc68k_reduced()\n        sc.tl.draw_graph(adata)\n        sc.pl.draw_graph(adata, color=['phase', 'bulk_labels'])\n\n    .. currentmodule:: scanpy\n\n    See also\n    --------\n    tl.draw_graph\n    \"\"\"\n    if layout is None:\n        layout = str(adata.uns['draw_graph']['params']['layout'])\n    basis = 'draw_graph_' + layout\n    if 'X_' + basis not in adata.obsm_keys():\n        raise ValueError(\n            'Did not find {} in adata.obs. Did you compute layout {}?'.format(\n                'draw_graph_' + layout, layout\n            )\n        )\n\n    return embedding(adata, basis, **kwargs)", "idx": 280}
{"project": "Scanpy", "commit_id": "354_scanpy_1.9.0_scatterplots.py_pca.py", "target": 1, "func": "def pca(\n    adata,\n    *,\n    annotate_var_explained: bool = False,\n    show: Optional[bool] = None,\n    return_fig: Optional[bool] = None,\n    save: Union[bool, str, None] = None,\n    **kwargs,\n) -> Union[Axes, List[Axes], None]:\n    \"\"\"\\\n    Scatter plot in PCA coordinates.\n\n    Use the parameter `annotate_var_explained` to annotate the explained variance.\n\n    Parameters\n    ----------\n    {adata_color_etc}\n    annotate_var_explained\n    {scatter_bulk}\n    {show_save_ax}\n\n    Returns\n    -------\n    If `show==False` a :class:`~matplotlib.axes.Axes` or a list of it.\n\n    Examples\n    --------\n\n    .. plot::\n        :context: close-figs\n\n        import scanpy as sc\n        adata = sc.datasets.pbmc3k_processed()\n        sc.pl.pca(adata)\n\n    Colour points by discrete variable (Louvain clusters).\n\n    .. plot::\n        :context: close-figs\n\n        sc.pl.pca(adata, color=\"louvain\")\n\n    Colour points by gene expression.\n\n    .. plot::\n        :context: close-figs\n\n        sc.pl.pca(adata, color=\"CST3\")\n\n    .. currentmodule:: scanpy\n\n    See also\n    --------\n    tl.pca\n    pp.pca\n    \"\"\"\n    if not annotate_var_explained:\n        return embedding(\n            adata, 'pca', show=show, return_fig=return_fig, save=save, **kwargs\n        )\n    else:\n\n        if 'pca' not in adata.obsm.keys() and 'X_pca' not in adata.obsm.keys():\n            raise KeyError(\n                f\"Could not find entry in `obsm` for 'pca'.\\n\"\n                f\"Available keys are: {list(adata.obsm.keys())}.\"\n            )\n\n        label_dict = {\n            'PC{}'.format(i + 1): 'PC{} ({}%)'.format(i + 1, round(v * 100, 2))\n            for i, v in enumerate(adata.uns['pca']['variance_ratio'])\n        }\n\n        if return_fig is True:\n            # edit axis labels in returned figure\n            fig = embedding(adata, 'pca', return_fig=return_fig, **kwargs)\n            for ax in fig.axes:\n                ax.set_xlabel(label_dict[ax.xaxis.get_label().get_text()])\n                ax.set_ylabel(label_dict[ax.yaxis.get_label().get_text()])\n            return fig\n\n        else:\n            # get the axs, edit the labels and apply show and save from user\n            axs = embedding(adata, 'pca', show=False, save=False, **kwargs)\n            if isinstance(axs, list):\n                for ax in axs:\n                    ax.set_xlabel(label_dict[ax.xaxis.get_label().get_text()])\n                    ax.set_ylabel(label_dict[ax.yaxis.get_label().get_text()])\n            else:\n                axs.set_xlabel(label_dict[axs.xaxis.get_label().get_text()])\n                axs.set_ylabel(label_dict[axs.yaxis.get_label().get_text()])\n            _utils.savefig_or_show('pca', show=show, save=save)\n            if show is False:\n                return axs", "idx": 281}
{"project": "Scanpy", "commit_id": "355_scanpy_1.9.0_scatterplots.py_spatial.py", "target": 0, "func": "def spatial(\n    adata,\n    *,\n    basis: str = \"spatial\",\n    img: Union[np.ndarray, None] = None,\n    img_key: Union[str, None, Empty] = _empty,\n    library_id: Union[str, Empty] = _empty,\n    crop_coord: Tuple[int, int, int, int] = None,\n    alpha_img: float = 1.0,\n    bw: Optional[bool] = False,\n    size: float = 1.0,\n    scale_factor: Optional[float] = None,\n    spot_size: Optional[float] = None,\n    na_color: Optional[ColorLike] = None,\n    show: Optional[bool] = None,\n    return_fig: Optional[bool] = None,\n    save: Union[bool, str, None] = None,\n    **kwargs,\n) -> Union[Axes, List[Axes], None]:\n    \"\"\"\\\n    Scatter plot in spatial coordinates.\n\n    This function allows overlaying data on top of images.\n    Use the parameter `img_key` to see the image in the background\n    And the parameter `library_id` to select the image.\n    By default, `'hires'` and `'lowres'` are attempted.\n\n    Use `crop_coord`, `alpha_img`, and `bw` to control how it is displayed.\n    Use `size` to scale the size of the Visium spots plotted on top.\n\n    As this function is designed to for imaging data, there are two key assumptions\n    about how coordinates are handled:\n\n    1. The origin (e.g `(0, 0)`) is at the top left \u2013 as is common convention\n    with image data.\n\n    2. Coordinates are in the pixel space of the source image, so an equal\n    aspect ratio is assumed.\n\n    If your anndata object has a `\"spatial\"` entry in `.uns`, the `img_key`\n    and `library_id` parameters to find values for `img`, `scale_factor`,\n    and `spot_size` arguments. Alternatively, these values be passed directly.\n\n    Parameters\n    ----------\n    {adata_color_etc}\n    {scatter_spatial}\n    {scatter_bulk}\n    {show_save_ax}\n\n    Returns\n    -------\n    If `show==False` a :class:`~matplotlib.axes.Axes` or a list of it.\n\n    Examples\n    --------\n    This function behaves very similarly to other embedding plots like\n    :func:`~scanpy.pl.umap`\n\n    >>> adata = sc.datasets.visium_sge(\"Targeted_Visium_Human_Glioblastoma_Pan_Cancer\")\n    >>> sc.pp.calculate_qc_metrics(adata, inplace=True)\n    >>> sc.pl.spatial(adata, color=\"log1p_n_genes_by_counts\")\n\n    See Also\n    --------\n    :func:`scanpy.datasets.visium_sge`\n        Example visium data.\n    :tutorial:`spatial/basic-analysis`\n        Tutorial on spatial analysis.\n    \"\"\"\n    # get default image params if available\n    library_id, spatial_data = _check_spatial_data(adata.uns, library_id)\n    img, img_key = _check_img(spatial_data, img, img_key, bw=bw)\n    spot_size = _check_spot_size(spatial_data, spot_size)\n    scale_factor = _check_scale_factor(\n        spatial_data, img_key=img_key, scale_factor=scale_factor\n    )\n    crop_coord = _check_crop_coord(crop_coord, scale_factor)\n    na_color = _check_na_color(na_color, img=img)\n\n    if bw:\n        cmap_img = \"gray\"\n    else:\n        cmap_img = None\n    circle_radius = size * scale_factor * spot_size * 0.5\n\n    axs = embedding(\n        adata,\n        basis=basis,\n        scale_factor=scale_factor,\n        size=circle_radius,\n        na_color=na_color,\n        show=False,\n        save=False,\n        **kwargs,\n    )\n    if not isinstance(axs, list):\n        axs = [axs]\n    for ax in axs:\n        cur_coords = np.concatenate([ax.get_xlim(), ax.get_ylim()])\n        if img is not None:\n            ax.imshow(img, cmap=cmap_img, alpha=alpha_img)\n        else:\n            ax.set_aspect(\"equal\")\n            ax.invert_yaxis()\n        if crop_coord is not None:\n            ax.set_xlim(crop_coord[0], crop_coord[1])\n            ax.set_ylim(crop_coord[3], crop_coord[2])\n        else:\n            ax.set_xlim(cur_coords[0], cur_coords[1])\n            ax.set_ylim(cur_coords[3], cur_coords[2])\n    _utils.savefig_or_show('show', show=show, save=save)\n    if show is False or return_fig is True:\n        return axs", "idx": 282}
{"project": "Scanpy", "commit_id": "356_scanpy_1.9.0_scatterplots.py__components_to_dimensions.py", "target": 0, "func": "def _components_to_dimensions(\n    components: Optional[Union[str, Collection[str]]],\n    dimensions: Optional[Union[Collection[int], Collection[Collection[int]]]],\n    *,\n    projection: Literal[\"2d\", \"3d\"] = \"2d\",\n    total_dims: int,\n) -> List[Collection[int]]:\n    \"\"\"Normalize components/ dimensions args for embedding plots.\"\"\"\n    # TODO: Deprecate components kwarg\n    ndims = {\"2d\": 2, \"3d\": 3}[projection]\n    if components is None and dimensions is None:\n        dimensions = [tuple(i for i in range(ndims))]\n    elif components is not None and dimensions is not None:\n        raise ValueError(\"Cannot provide both dimensions and components\")\n\n    # TODO: Consider deprecating this\n    # If components is not None, parse them and set dimensions\n    if components == \"all\":\n        dimensions = list(combinations(range(total_dims), ndims))\n    elif components is not None:\n        if isinstance(components, str):\n            components = [components]\n        # Components use 1 based indexing\n        dimensions = [[int(dim) - 1 for dim in c.split(\",\")] for c in components]\n\n    if all(isinstance(el, Integral) for el in dimensions):\n        dimensions = [dimensions]\n    # if all(isinstance(el, Collection) for el in dimensions):\n    for dims in dimensions:\n        if len(dims) != ndims or not all(isinstance(d, Integral) for d in dims):\n            raise ValueError()\n\n    return dimensions", "idx": 283}
{"project": "Scanpy", "commit_id": "357_scanpy_1.9.0_scatterplots.py__add_categorical_legend.py", "target": 1, "func": "def _add_categorical_legend(\n    ax,\n    color_source_vector,\n    palette: dict,\n    legend_loc: str,\n    legend_fontweight,\n    legend_fontsize,\n    legend_fontoutline,\n    multi_panel,\n    na_color,\n    na_in_legend: bool,\n    scatter_array=None,\n):\n    \"\"\"Add a legend to the passed Axes.\"\"\"\n    if na_in_legend and pd.isnull(color_source_vector).any():\n        if \"NA\" in color_source_vector:\n            raise NotImplementedError(\n                \"No fallback for null labels has been defined if NA already in categories.\"\n            )\n        color_source_vector = color_source_vector.add_categories(\"NA\").fillna(\"NA\")\n        palette = palette.copy()\n        palette[\"NA\"] = na_color\n    cats = color_source_vector.categories\n\n    if multi_panel is True:\n        # Shrink current axis by 10% to fit legend and match\n        # size of plots that are not categorical\n        box = ax.get_position()\n        ax.set_position([box.x0, box.y0, box.width * 0.91, box.height])\n\n    if legend_loc == 'right margin':\n        for label in cats:\n            ax.scatter([], [], c=palette[label], label=label)\n        ax.legend(\n            frameon=False,\n            loc='center left',\n            bbox_to_anchor=(1, 0.5),\n            ncol=(1 if len(cats) <= 14 else 2 if len(cats) <= 30 else 3),\n            fontsize=legend_fontsize,\n        )\n    elif legend_loc == 'on data':\n        # identify centroids to put labels\n\n        all_pos = (\n            pd.DataFrame(scatter_array, columns=[\"x\", \"y\"])\n            .groupby(color_source_vector, observed=True)\n            .median()\n            # Have to sort_index since if observed=True and categorical is unordered\n            # the order of values in .index is undefined. Related issue:\n            # https://github.com/pandas-dev/pandas/issues/25167\n            .sort_index()\n        )\n\n        for label, x_pos, y_pos in all_pos.itertuples():\n            ax.text(\n                x_pos,\n                y_pos,\n                label,\n                weight=legend_fontweight,\n                verticalalignment='center',\n                horizontalalignment='center',\n                fontsize=legend_fontsize,\n                path_effects=legend_fontoutline,", "idx": 284}
{"project": "Scanpy", "commit_id": "358_scanpy_1.9.0_scatterplots.py__get_basis.py", "target": 0, "func": "def _get_basis(adata: AnnData, basis: str) -> np.ndarray:\n    \"\"\"Get array for basis from anndata. Just tries to add 'X_'.\"\"\"\n    if basis in adata.obsm:\n        return adata.obsm[basis]\n    elif f\"X_{basis}\" in adata.obsm:\n        return adata.obsm[f\"X_{basis}\"]\n    else:\n        raise KeyError(f\"Could not find '{basis}' or 'X_{basis}' in .obsm\")", "idx": 285}
{"project": "Scanpy", "commit_id": "359_scanpy_1.9.0_scatterplots.py__get_color_source_vector.py", "target": 0, "func": "def _get_color_source_vector(\n    adata, value_to_plot, use_raw=False, gene_symbols=None, layer=None, groups=None\n):\n    \"\"\"\n    Get array from adata that colors will be based on.\n    \"\"\"\n    if value_to_plot is None:\n        # Points will be plotted with `na_color`. Ideally this would work\n        # with the \"bad color\" in a color map but that throws a warning. Instead\n        # _color_vector handles this.\n        # https://github.com/matplotlib/matplotlib/issues/18294\n        return np.broadcast_to(np.nan, adata.n_obs)\n    if (\n        gene_symbols is not None\n        and value_to_plot not in adata.obs.columns\n        and value_to_plot not in adata.var_names\n    ):\n        # We should probably just make an index for this, and share it over runs\n        value_to_plot = adata.var.index[adata.var[gene_symbols] == value_to_plot][\n            0\n        ]  # TODO: Throw helpful error if this doesn't work\n    if use_raw and value_to_plot not in adata.obs.columns:\n        values = adata.raw.obs_vector(value_to_plot)\n    else:\n        values = adata.obs_vector(value_to_plot, layer=layer)\n    if groups and is_categorical_dtype(values):\n        values = values.replace(values.categories.difference(groups), np.nan)\n    return values", "idx": 286}
{"project": "Scanpy", "commit_id": "35_scanpy_1.9.0_cli.py_parse_known_args.py", "target": 0, "func": "def parse_known_args(\n        self,\n        args: Optional[Sequence[str]] = None,\n        namespace: Optional[Namespace] = None,\n    ) -> Tuple[Namespace, List[str]]:\n        assert (\n            args is not None and namespace is None\n        ), 'Only use DelegatingParser as subparser'\n        return Namespace(func=partial(run, [self.prog, *args], **self.cd.runargs)), []", "idx": 287}
{"project": "Scanpy", "commit_id": "360_scanpy_1.9.0_scatterplots.py__get_palette.py", "target": 1, "func": "def _get_palette(adata, values_key: str, palette=None):\n    color_key = f\"{values_key}_colors\"\n    values = pd.Categorical(adata.obs[values_key])\n    if palette:\n        _utils._set_colors_for_categorical_obs(adata, values_key, palette)\n    elif color_key not in adata.uns or len(adata.uns[color_key]) < len(\n        values.categories\n    ):\n        #  set a default palette in case that no colors or few colors are found\n        _utils._set_default_colors_for_categorical_obs(adata, values_key)\n    else:\n        _utils._validate_palette(adata, values_key)\n    return dict(zip(values.categories, adata.uns[color_key]))", "idx": 288}
{"project": "Scanpy", "commit_id": "361_scanpy_1.9.0_scatterplots.py__color_vector.py", "target": 1, "func": "def _color_vector(\n    adata, values_key: str, values, palette, na_color=\"lightgray\"\n) -> Tuple[np.ndarray, bool]:\n    \"\"\"\n    Map array of values to array of hex (plus alpha) codes.\n\n    For categorical data, the return value is list of colors taken\n    from the category palette or from the given `palette` value.\n\n    For continuous values, the input array is returned (may change in future).\n    \"\"\"\n    ###\n    # when plotting, the color of the dots is determined for each plot\n    # the data is either categorical or continuous and the data could be in\n    # 'obs' or in 'var'\n    to_hex = partial(colors.to_hex, keep_alpha=True)\n    if values_key is None:\n        return np.broadcast_to(to_hex(na_color), adata.n_obs), False\n    if not is_categorical_dtype(values):\n        return values, False\n    else:  # is_categorical_dtype(values)\n        color_map = {\n            k: to_hex(v)\n            for k, v in _get_palette(adata, values_key, palette=palette).items()\n        }\n        # If color_map does not have unique values, this can be slow as the\n        # result is not categorical\n        color_vector = pd.Categorical(values.map(color_map))\n\n        # Set color to 'missing color' for all missing values\n        if color_vector.isna().any():\n            color_vector = color_vector.add_categories([to_hex(na_color)])\n            color_vector = color_vector.fillna(to_hex(na_color))\n        return color_vector, True", "idx": 289}
{"project": "Scanpy", "commit_id": "362_scanpy_1.9.0_scatterplots.py__basis2name.py", "target": 0, "func": "def _basis2name(basis):\n    \"\"\"\n    converts the 'basis' into the proper name.\n    \"\"\"\n\n    component_name = (\n        'DC'\n        if basis == 'diffmap'\n        else 'tSNE'\n        if basis == 'tsne'\n        else 'UMAP'\n        if basis == 'umap'\n        else 'PC'\n        if basis == 'pca'\n        else basis.replace('draw_graph_', '').upper()\n        if 'draw_graph' in basis\n        else basis\n    )\n    return component_name", "idx": 290}
{"project": "Scanpy", "commit_id": "363_scanpy_1.9.0_scatterplots.py__check_spot_size.py", "target": 0, "func": "def _check_spot_size(\n    spatial_data: Optional[Mapping], spot_size: Optional[float]\n) -> float:\n    \"\"\"\n    Resolve spot_size value.\n\n    This is a required argument for spatial plots.\n    \"\"\"\n    if spatial_data is None and spot_size is None:\n        raise ValueError(\n            \"When .uns['spatial'][library_id] does not exist, spot_size must be \"\n            \"provided directly.\"\n        )\n    elif spot_size is None:\n        return spatial_data['scalefactors']['spot_diameter_fullres']\n    else:\n        return spot_size", "idx": 291}
{"project": "Scanpy", "commit_id": "364_scanpy_1.9.0_scatterplots.py__check_scale_factor.py", "target": 0, "func": "def _check_scale_factor(\n    spatial_data: Optional[Mapping],\n    img_key: Optional[str],\n    scale_factor: Optional[float],\n) -> float:\n    \"\"\"Resolve scale_factor, defaults to 1.\"\"\"\n    if scale_factor is not None:\n        return scale_factor\n    elif spatial_data is not None and img_key is not None:\n        return spatial_data['scalefactors'][f\"tissue_{img_key}_scalef\"]\n    else:\n        return 1.0", "idx": 292}
{"project": "Scanpy", "commit_id": "365_scanpy_1.9.0_scatterplots.py__check_spatial_data.py", "target": 0, "func": "def _check_spatial_data(\n    uns: Mapping, library_id: Union[Empty, None, str]\n) -> Tuple[Optional[str], Optional[Mapping]]:\n    \"\"\"\n    Given a mapping, try and extract a library id/ mapping with spatial data.\n\n    Assumes this is `.uns` from how we parse visium data.\n    \"\"\"\n    spatial_mapping = uns.get(\"spatial\", {})\n    if library_id is _empty:\n        if len(spatial_mapping) > 1:\n            raise ValueError(\n                \"Found multiple possible libraries in `.uns['spatial']. Please specify.\"\n                f\" Options are:\\n\\t{list(spatial_mapping.keys())}\"\n            )\n        elif len(spatial_mapping) == 1:\n            library_id = list(spatial_mapping.keys())[0]\n        else:\n            library_id = None\n    if library_id is not None:\n        spatial_data = spatial_mapping[library_id]\n    else:\n        spatial_data = None\n    return library_id, spatial_data", "idx": 293}
{"project": "Scanpy", "commit_id": "366_scanpy_1.9.0_scatterplots.py__check_img.py", "target": 0, "func": "def _check_img(\n    spatial_data: Optional[Mapping],\n    img: Optional[np.ndarray],\n    img_key: Union[None, str, Empty],\n    bw: bool = False,\n) -> Tuple[Optional[np.ndarray], Optional[str]]:\n    \"\"\"\n    Resolve image for spatial plots.\n    \"\"\"\n    if img is None and spatial_data is not None and img_key is _empty:\n        img_key = next(\n            (k for k in ['hires', 'lowres'] if k in spatial_data['images']),\n        )  # Throws StopIteration Error if keys not present\n    if img is None and spatial_data is not None and img_key is not None:\n        img = spatial_data[\"images\"][img_key]\n    if bw:\n        img = np.dot(img[..., :3], [0.2989, 0.5870, 0.1140])\n    return img, img_key", "idx": 294}
{"project": "Scanpy", "commit_id": "367_scanpy_1.9.0_scatterplots.py__check_crop_coord.py", "target": 0, "func": "def _check_crop_coord(\n    crop_coord: Optional[tuple],\n    scale_factor: float,\n) -> Tuple[float, float, float, float]:\n    \"\"\"Handle cropping with image or basis.\"\"\"\n    if crop_coord is None:\n        return None\n    if len(crop_coord) != 4:\n        raise ValueError(\"Invalid crop_coord of length {len(crop_coord)}(!=4)\")\n    crop_coord = tuple(c * scale_factor for c in crop_coord)\n    return crop_coord", "idx": 295}
{"project": "Scanpy", "commit_id": "368_scanpy_1.9.0_scatterplots.py__check_na_color.py", "target": 0, "func": "def _check_na_color(\n    na_color: Optional[ColorLike], *, img: Optional[np.ndarray] = None\n) -> ColorLike:\n    if na_color is None:\n        if img is not None:\n            na_color = (0.0, 0.0, 0.0, 0.0)\n        else:\n            na_color = \"lightgray\"\n    return na_color", "idx": 296}
{"project": "Scanpy", "commit_id": "369_scanpy_1.9.0_scatterplots.py__broadcast_args.py", "target": 0, "func": "def _broadcast_args(*args):\n    \"\"\"Broadcasts arguments to a common length.\"\"\"\n    from itertools import repeat\n\n    lens = [len(arg) for arg in args]\n    longest = max(lens)\n    if not (set(lens) == {1, longest} or set(lens) == {longest}):\n        raise ValueError(f\"Could not broadast together arguments with shapes: {lens}.\")\n    return list(\n        [[arg[0] for _ in range(longest)] if len(arg) == 1 else arg for arg in args]", "idx": 297}
{"project": "Scanpy", "commit_id": "36_scanpy_1.9.0_logging.py__set_log_file.py", "target": 0, "func": "def _set_log_file(settings):\n    file = settings.logfile\n    name = settings.logpath\n    root = settings._root_logger\n    h = logging.StreamHandler(file) if name is None else logging.FileHandler(name)\n    h.setFormatter(_LogFormatter())\n    h.setLevel(root.level)\n    if len(root.handlers) == 1:\n        root.removeHandler(root.handlers[0])\n    elif len(root.handlers) > 1:\n        raise RuntimeError('Scanpy\u2019s root logger somehow got more than one handler')\n    root.addHandler(h)", "idx": 298}
{"project": "Scanpy", "commit_id": "370_scanpy_1.9.0___init__.py_pca_overview.py", "target": 0, "func": "def pca_overview(adata: AnnData, **params):\n    \"\"\"\\\n    Plot PCA results.\n\n    The parameters are the ones of the scatter plot. Call pca_ranking separately\n    if you want to change the default settings.\n\n    Parameters\n    ----------\n    adata\n        Annotated data matrix.\n    color\n        Keys for observation/cell annotation either as list `[\"ann1\", \"ann2\"]` or\n        string `\"ann1,ann2,...\"`.\n    use_raw\n        Use `raw` attribute of `adata` if present.\n    {scatter_bulk}\n    show\n         Show the plot, do not return axis.\n    save\n        If `True` or a `str`, save the figure.\n        A string is appended to the default filename.\n        Infer the filetype if ending on {{`'.pdf'`, `'.png'`, `'.svg'`}}.\n    Examples\n    --------\n    .. plot::\n        :context: close-figs\n\n        import scanpy as sc\n        adata = sc.datasets.pbmc3k_processed()\n        sc.pl.pca_overview(adata, color=\"louvain\")\n\n    .. currentmodule:: scanpy\n\n    See also\n    --------\n    tl.pca\n    pp.pca\n    \"\"\"\n    show = params['show'] if 'show' in params else None\n    if 'show' in params:\n        del params['show']\n    pca(adata, **params, show=False)\n    pca_loadings(adata, show=False)\n    pca_variance_ratio(adata, show=show)", "idx": 299}
{"project": "Scanpy", "commit_id": "371_scanpy_1.9.0___init__.py_pca_loadings.py", "target": 0, "func": "def pca_loadings(\n    adata: AnnData,\n    components: Union[str, Sequence[int], None] = None,\n    include_lowest: bool = True,\n    n_points: Union[int, None] = None,\n    show: Optional[bool] = None,\n    save: Union[str, bool, None] = None,\n):\n    \"\"\"\\\n    Rank genes according to contributions to PCs.\n\n    Parameters\n    ----------\n    adata\n        Annotated data matrix.\n    components\n        For example, ``'1,2,3'`` means ``[1, 2, 3]``, first, second, third\n        principal component.\n    include_lowest\n        Whether to show the variables with both highest and lowest loadings.\n    show\n        Show the plot, do not return axis.\n    n_points\n        Number of variables to plot for each component.\n    save\n        If `True` or a `str`, save the figure.\n        A string is appended to the default filename.\n        Infer the filetype if ending on {`'.pdf'`, `'.png'`, `'.svg'`}.\n\n    Examples\n    --------\n    .. plot::\n        :context: close-figs\n\n        import scanpy as sc\n        adata = sc.datasets.pbmc3k_processed()\n\n    Show first 3 components loadings\n\n    .. plot::\n        :context: close-figs\n\n        sc.pl.pca_loadings(adata, components = '1,2,3')\n\n\n    \"\"\"\n    if components is None:\n        components = [1, 2, 3]\n    elif isinstance(components, str):\n        components = [int(x) for x in components.split(',')]\n    components = np.array(components) - 1\n\n    if np.any(components < 0):\n        raise ValueError(\"Component indices must be greater than zero.\")\n\n    if n_points is None:\n        n_points = min(30, adata.n_vars)\n    elif adata.n_vars < n_points:\n        raise ValueError(\n            f\"Tried to plot {n_points} variables, but passed anndata only has {adata.n_vars}.\"\n        )\n\n    ranking(\n        adata,\n        'varm',\n        'PCs',\n        n_points=n_points,\n        indices=components,\n        include_lowest=include_lowest,\n    )\n    savefig_or_show('pca_loadings', show=show, save=save)", "idx": 300}
{"project": "Scanpy", "commit_id": "372_scanpy_1.9.0___init__.py_pca_variance_ratio.py", "target": 0, "func": "def pca_variance_ratio(\n    adata: AnnData,\n    n_pcs: int = 30,\n    log: bool = False,\n    show: Optional[bool] = None,\n    save: Union[bool, str, None] = None,\n):\n    \"\"\"\\\n    Plot the variance ratio.\n\n    Parameters\n    ----------\n    n_pcs\n         Number of PCs to show.\n    log\n         Plot on logarithmic scale..\n    show\n         Show the plot, do not return axis.\n    save\n        If `True` or a `str`, save the figure.\n        A string is appended to the default filename.\n        Infer the filetype if ending on {`'.pdf'`, `'.png'`, `'.svg'`}.\n    \"\"\"\n    ranking(\n        adata,\n        'uns',\n        'variance_ratio',\n        n_points=n_pcs,\n        dictionary='pca',\n        labels='PC',\n        log=log,\n    )\n    savefig_or_show('pca_variance_ratio', show=show, save=save)", "idx": 301}
{"project": "Scanpy", "commit_id": "373_scanpy_1.9.0___init__.py_dpt_timeseries.py", "target": 0, "func": "def dpt_timeseries(\n    adata: AnnData,\n    color_map: Union[str, Colormap] = None,\n    show: Optional[bool] = None,\n    save: Optional[bool] = None,\n    as_heatmap: bool = True,\n):\n    \"\"\"\\\n    Heatmap of pseudotime series.\n\n    Parameters\n    ----------\n    as_heatmap\n        Plot the timeseries as heatmap.\n    \"\"\"\n    if adata.n_vars > 100:\n        logg.warning(\n            'Plotting more than 100 genes might take some while, '\n            'consider selecting only highly variable genes, for example.'\n        )\n    # only if number of genes is not too high\n    if as_heatmap:\n        # plot time series as heatmap, as in Haghverdi et al. (2016), Fig. 1d\n        timeseries_as_heatmap(\n            adata.X[adata.obs['dpt_order_indices'].values],\n            var_names=adata.var_names,\n            highlights_x=adata.uns['dpt_changepoints'],\n            color_map=color_map,\n        )\n    else:\n        # plot time series as gene expression vs time\n        timeseries(\n            adata.X[adata.obs['dpt_order_indices'].values],\n            var_names=adata.var_names,\n            highlights_x=adata.uns['dpt_changepoints'],\n            xlim=[0, 1.3 * adata.X.shape[0]],\n        )\n    pl.xlabel('dpt order')\n    savefig_or_show('dpt_timeseries', save=save, show=show)", "idx": 302}
{"project": "Scanpy", "commit_id": "374_scanpy_1.9.0___init__.py_dpt_groups_pseudotime.py", "target": 0, "func": "def dpt_groups_pseudotime(\n    adata: AnnData,\n    color_map: Union[str, Colormap, None] = None,\n    palette: Union[Sequence[str], Cycler, None] = None,\n    show: Optional[bool] = None,\n    save: Union[bool, str, None] = None,\n):\n    \"\"\"Plot groups and pseudotime.\"\"\"\n    _, (ax_grp, ax_ord) = pl.subplots(2, 1)\n    timeseries_subplot(\n        adata.obs['dpt_groups'].cat.codes,\n        time=adata.obs['dpt_order'].values,\n        color=np.asarray(adata.obs['dpt_groups']),\n        highlights_x=adata.uns['dpt_changepoints'],\n        ylabel='dpt groups',\n        yticks=(\n            np.arange(len(adata.obs['dpt_groups'].cat.categories), dtype=int)\n            if len(adata.obs['dpt_groups'].cat.categories) < 5\n            else None\n        ),\n        palette=palette,\n        ax=ax_grp,\n    )\n    timeseries_subplot(\n        adata.obs['dpt_pseudotime'].values,\n        time=adata.obs['dpt_order'].values,\n        color=adata.obs['dpt_pseudotime'].values,\n        xlabel='dpt order',\n        highlights_x=adata.uns['dpt_changepoints'],\n        ylabel='pseudotime',\n        yticks=[0, 1],\n        color_map=color_map,\n        ax=ax_ord,\n    )\n    savefig_or_show('dpt_groups_pseudotime', save=save, show=show)", "idx": 303}
{"project": "Scanpy", "commit_id": "375_scanpy_1.9.0___init__.py_rank_genes_groups.py", "target": 0, "func": "def rank_genes_groups(\n    adata: AnnData,\n    groups: Union[str, Sequence[str]] = None,\n    n_genes: int = 20,\n    gene_symbols: Optional[str] = None,\n    key: Optional[str] = 'rank_genes_groups',\n    fontsize: int = 8,\n    ncols: int = 4,\n    sharey: bool = True,\n    show: Optional[bool] = None,\n    save: Optional[bool] = None,\n    ax: Optional[Axes] = None,\n    **kwds,\n):\n    \"\"\"\\\n    Plot ranking of genes.\n\n    Parameters\n    ----------\n    adata\n        Annotated data matrix.\n    groups\n        The groups for which to show the gene ranking.\n    gene_symbols\n        Key for field in `.var` that stores gene symbols if you do not want to\n        use `.var_names`.\n    n_genes\n        Number of genes to show.\n    fontsize\n        Fontsize for gene names.\n    ncols\n        Number of panels shown per row.\n    sharey\n        Controls if the y-axis of each panels should be shared. But passing\n        `sharey=False`, each panel has its own y-axis range.\n    {show_save_ax}\n\n\n    Examples\n    --------\n\n    .. plot::\n        :context: close-figs\n\n        import scanpy as sc\n        adata = sc.datasets.pbmc68k_reduced()\n        sc.pl.rank_genes_groups(adata)\n\n\n    Plot top 10 genes (default 20 genes)\n\n    .. plot::\n        :context: close-figs\n\n        sc.pl.rank_genes_groups(adata, n_genes=10)\n\n    .. currentmodule:: scanpy\n\n    See also\n    --------\n    tl.rank_genes_groups\n\n    \"\"\"\n    if 'n_panels_per_row' in kwds:\n        n_panels_per_row = kwds['n_panels_per_row']\n    else:\n        n_panels_per_row = ncols\n    if n_genes < 1:\n        raise NotImplementedError(\n            \"Specifying a negative number for n_genes has not been implemented for \"\n            f\"this plot. Received n_genes={n_genes}.\"\n        )\n\n    reference = str(adata.uns[key]['params']['reference'])\n    group_names = adata.uns[key]['names'].dtype.names if groups is None else groups\n    # one panel for each group\n    # set up the figure\n    n_panels_x = min(n_panels_per_row, len(group_names))\n    n_panels_y = np.ceil(len(group_names) / n_panels_x).astype(int)\n\n    from matplotlib import gridspec\n\n    fig = pl.figure(\n        figsize=(\n            n_panels_x * rcParams['figure.figsize'][0],\n            n_panels_y * rcParams['figure.figsize'][1],\n        )\n    )\n    gs = gridspec.GridSpec(nrows=n_panels_y, ncols=n_panels_x, wspace=0.22, hspace=0.3)\n\n    ax0 = None\n    ymin = np.Inf\n    ymax = -np.Inf\n    for count, group_name in enumerate(group_names):\n        gene_names = adata.uns[key]['names'][group_name][:n_genes]\n        scores = adata.uns[key]['scores'][group_name][:n_genes]\n\n        # Setting up axis, calculating y bounds\n        if sharey:\n            ymin = min(ymin, np.min(scores))\n            ymax = max(ymax, np.max(scores))\n\n            if ax0 is None:\n                ax = fig.add_subplot(gs[count])\n                ax0 = ax\n            else:\n                ax = fig.add_subplot(gs[count], sharey=ax0)\n        else:\n            ymin = np.min(scores)\n            ymax = np.max(scores)\n            ymax += 0.3 * (ymax - ymin)\n\n            ax = fig.add_subplot(gs[count])\n            ax.set_ylim(ymin, ymax)\n\n        ax.set_xlim(-0.9, n_genes - 0.1)\n\n        # Mapping to gene_symbols\n        if gene_symbols is not None:\n            if adata.raw is not None and adata.uns[key]['params']['use_raw']:\n                gene_names = adata.raw.var[gene_symbols][gene_names]\n            else:\n                gene_names = adata.var[gene_symbols][gene_names]\n\n        # Making labels\n        for ig, gene_name in enumerate(gene_names):\n            ax.text(\n                ig,\n                scores[ig],\n                gene_name,\n                rotation='vertical',\n                verticalalignment='bottom',\n                horizontalalignment='center',\n                fontsize=fontsize,\n            )\n\n        ax.set_title('{} vs. {}'.format(group_name, reference))\n        if count >= n_panels_x * (n_panels_y - 1):\n            ax.set_xlabel('ranking')\n\n        # print the 'score' label only on the first panel per row.\n        if count % n_panels_x == 0:\n            ax.set_ylabel('score')\n\n    if sharey is True:\n        ymax += 0.3 * (ymax - ymin)\n        ax.set_ylim(ymin, ymax)\n\n    writekey = f\"rank_genes_groups_{adata.uns[key]['params']['groupby']}\"\n    savefig_or_show(writekey, show=show, save=save)", "idx": 304}
{"project": "Scanpy", "commit_id": "376_scanpy_1.9.0___init__.py__fig_show_save_or_axes.py", "target": 0, "func": "def _fig_show_save_or_axes(plot_obj, return_fig, show, save):\n    \"\"\"\n    Decides what to return\n    \"\"\"\n    if return_fig:\n        return plot_obj\n    else:\n        plot_obj.make_figure()\n        savefig_or_show(plot_obj.DEFAULT_SAVE_PREFIX, show=show, save=save)\n        show = settings.autoshow if show is None else show\n        if show is False:\n            return plot_obj.get_axes()", "idx": 305}
{"project": "Scanpy", "commit_id": "377_scanpy_1.9.0___init__.py__rank_genes_groups_plot.py", "target": 0, "func": "def _rank_genes_groups_plot(\n    adata: AnnData,\n    plot_type: str = 'heatmap',\n    groups: Union[str, Sequence[str]] = None,\n    n_genes: Optional[int] = None,\n    groupby: Optional[str] = None,\n    values_to_plot: Optional[str] = None,\n    var_names: Optional[Union[Sequence[str], Mapping[str, Sequence[str]]]] = None,\n    min_logfoldchange: Optional[float] = None,\n    key: Optional[str] = None,\n    show: Optional[bool] = None,\n    save: Optional[bool] = None,\n    return_fig: Optional[bool] = False,\n    gene_symbols: Optional[str] = None,\n    **kwds,\n):\n    \"\"\"\\\n    Common function to call the different rank_genes_groups_* plots\n    \"\"\"\n    if var_names is not None and n_genes is not None:\n        raise ValueError(\n            \"The arguments n_genes and var_names are mutually exclusive. Please \"\n            \"select only one.\"\n        )\n\n    if var_names is None and n_genes is None:\n        # set n_genes = 10 as default when none of the options is given\n        n_genes = 10\n\n    if key is None:\n        key = 'rank_genes_groups'\n\n    if groupby is None:\n        groupby = str(adata.uns[key]['params']['groupby'])\n    group_names = adata.uns[key]['names'].dtype.names if groups is None else groups\n\n    if var_names is not None:\n        if isinstance(var_names, Mapping):\n            # get a single list of all gene names in the dictionary\n            var_names_list = sum([list(x) for x in var_names.values()], [])\n        elif isinstance(var_names, str):\n            var_names_list = [var_names]\n        else:\n            var_names_list = var_names\n    else:\n        # dict in which each group is the key and the n_genes are the values\n        var_names = {}\n        var_names_list = []\n        for group in group_names:\n            df = rank_genes_groups_df(\n                adata,\n                group,\n                key=key,\n                gene_symbols=gene_symbols,\n                log2fc_min=min_logfoldchange,\n            )\n\n            if gene_symbols is not None:\n                df['names'] = df[gene_symbols]\n\n            genes_list = df.names[df.names.notnull()].tolist()\n\n            if len(genes_list) == 0:\n                logg.warning(f'No genes found for group {group}')\n                continue\n            if n_genes < 0:\n                genes_list = genes_list[n_genes:]\n            else:\n                genes_list = genes_list[:n_genes]\n            var_names[group] = genes_list\n            var_names_list.extend(genes_list)\n\n    # by default add dendrogram to plots\n    kwds.setdefault('dendrogram', True)\n\n    if plot_type in ['dotplot', 'matrixplot']:\n        # these two types of plots can also\n        # show score, logfoldchange and pvalues, in general any value from rank\n        # genes groups\n        title = None\n        values_df = None\n        if values_to_plot is not None:\n            values_df = _get_values_to_plot(\n                adata,\n                values_to_plot,\n                var_names_list,\n                key=key,\n                gene_symbols=gene_symbols,\n            )\n            title = values_to_plot\n            if values_to_plot == 'logfoldchanges':\n                title = 'log fold change'\n            else:\n                title = values_to_plot.replace(\"_\", \" \").replace('pvals', 'p-value')\n\n        if plot_type == 'dotplot':\n            from .._dotplot import dotplot\n\n            _pl = dotplot(\n                adata,\n                var_names,\n                groupby,\n                dot_color_df=values_df,\n                return_fig=True,\n                gene_symbols=gene_symbols,\n                **kwds,\n            )\n            if title is not None and 'colorbar_title' not in kwds:\n                _pl.legend(colorbar_title=title)\n        elif plot_type == 'matrixplot':\n            from .._matrixplot import matrixplot\n\n            _pl = matrixplot(\n                adata,\n                var_names,\n                groupby,\n                values_df=values_df,\n                return_fig=True,\n                gene_symbols=gene_symbols,\n                **kwds,\n            )\n\n            if title is not None and 'colorbar_title' not in kwds:\n                _pl.legend(title=title)\n\n        return _fig_show_save_or_axes(_pl, return_fig, show, save)\n\n    elif plot_type == 'stacked_violin':\n        from .._stacked_violin import stacked_violin\n\n        _pl = stacked_violin(\n            adata,\n            var_names,\n            groupby,\n            return_fig=True,\n            gene_symbols=gene_symbols,\n            **kwds,\n        )\n        return _fig_show_save_or_axes(_pl, return_fig, show, save)\n    elif plot_type == 'heatmap':\n        from .._anndata import heatmap\n\n        return heatmap(\n            adata,\n            var_names,\n            groupby,\n            show=show,\n            save=save,\n            gene_symbols=gene_symbols,\n            **kwds,\n        )\n\n    elif plot_type == 'tracksplot':\n        from .._anndata import tracksplot\n\n        return tracksplot(\n            adata,\n            var_names,\n            groupby,\n            show=show,\n            save=save,\n            gene_symbols=gene_symbols,\n            **kwds,", "idx": 306}
{"project": "Scanpy", "commit_id": "378_scanpy_1.9.0___init__.py_rank_genes_groups_heatmap.py", "target": 0, "func": "def rank_genes_groups_heatmap(\n    adata: AnnData,\n    groups: Union[str, Sequence[str]] = None,\n    n_genes: Optional[int] = None,\n    groupby: Optional[str] = None,\n    gene_symbols: Optional[str] = None,\n    var_names: Optional[Union[Sequence[str], Mapping[str, Sequence[str]]]] = None,\n    min_logfoldchange: Optional[float] = None,\n    key: str = None,\n    show: Optional[bool] = None,\n    save: Optional[bool] = None,\n    **kwds,\n):\n    \"\"\"\\\n    Plot ranking of genes using heatmap plot (see :func:`~scanpy.pl.heatmap`)\n\n    Parameters\n    ----------\n    {params}\n    {show_save_ax}\n    **kwds\n        Are passed to :func:`~scanpy.pl.heatmap`.\n    {show_save_ax}\n\n    Examples\n    --------\n\n    .. plot::\n        :context: close-figs\n\n        import scanpy as sc\n        adata = sc.datasets.pbmc68k_reduced()\n        sc.tl.rank_genes_groups(adata, 'bulk_labels')\n        sc.pl.rank_genes_groups_heatmap(adata)\n\n    Show gene names per group on the heatmap\n\n    .. plot::\n        :context: close-figs\n\n        sc.pl.rank_genes_groups_heatmap(adata, show_gene_labels=True)\n\n    Plot top 5 genes per group (default 10 genes)\n\n    .. plot::\n        :context: close-figs\n\n        sc.pl.rank_genes_groups_heatmap(adata, n_genes=5, show_gene_labels=True)\n\n    .. currentmodule:: scanpy\n\n    See also\n    --------\n    tl.rank_genes_groups\n    tl.dendrogram\n    \"\"\"\n    return _rank_genes_groups_plot(\n        adata,\n        plot_type='heatmap',\n        groups=groups,\n        n_genes=n_genes,\n        gene_symbols=gene_symbols,\n        groupby=groupby,\n        var_names=var_names,\n        key=key,\n        min_logfoldchange=min_logfoldchange,\n        show=show,\n        save=save,\n        **kwds,", "idx": 307}
{"project": "Scanpy", "commit_id": "379_scanpy_1.9.0___init__.py_rank_genes_groups_tracksplot.py", "target": 0, "func": "def rank_genes_groups_tracksplot(\n    adata: AnnData,\n    groups: Union[str, Sequence[str]] = None,\n    n_genes: Optional[int] = None,\n    groupby: Optional[str] = None,\n    var_names: Optional[Union[Sequence[str], Mapping[str, Sequence[str]]]] = None,\n    gene_symbols: Optional[str] = None,\n    min_logfoldchange: Optional[float] = None,\n    key: Optional[str] = None,\n    show: Optional[bool] = None,\n    save: Optional[bool] = None,\n    **kwds,\n):\n    \"\"\"\\\n    Plot ranking of genes using heatmap plot (see :func:`~scanpy.pl.heatmap`)\n\n    Parameters\n    ----------\n    {params}\n    {show_save_ax}\n    **kwds\n        Are passed to :func:`~scanpy.pl.tracksplot`.\n    {show_save_ax}\n\n    Examples\n    --------\n\n    .. plot::\n        :context: close-figs\n\n        import scanpy as sc\n        adata = sc.datasets.pbmc68k_reduced()\n        sc.tl.rank_genes_groups(adata, 'bulk_labels')\n        sc.pl.rank_genes_groups_tracksplot(adata)\n    \"\"\"\n\n    return _rank_genes_groups_plot(\n        adata,\n        plot_type='tracksplot',\n        groups=groups,\n        n_genes=n_genes,\n        var_names=var_names,\n        gene_symbols=gene_symbols,\n        groupby=groupby,\n        key=key,\n        min_logfoldchange=min_logfoldchange,\n        show=show,\n        save=save,\n        **kwds,", "idx": 308}
{"project": "Scanpy", "commit_id": "37_scanpy_1.9.0_logging.py__set_log_level.py", "target": 0, "func": "def _set_log_level(settings, level: int):\n    root = settings._root_logger\n    root.setLevel(level)\n    (h,) = root.handlers  # may only be 1\n    h.setLevel(level)", "idx": 309}
{"project": "Scanpy", "commit_id": "380_scanpy_1.9.0___init__.py_rank_genes_groups_dotplot.py", "target": 0, "func": "def rank_genes_groups_dotplot(\n    adata: AnnData,\n    groups: Union[str, Sequence[str]] = None,\n    n_genes: Optional[int] = None,\n    groupby: Optional[str] = None,\n    values_to_plot: Optional[\n        Literal[\n            'scores',\n            'logfoldchanges',\n            'pvals',\n            'pvals_adj',\n            'log10_pvals',\n            'log10_pvals_adj',\n        ]\n    ] = None,\n    var_names: Optional[Union[Sequence[str], Mapping[str, Sequence[str]]]] = None,\n    gene_symbols: Optional[str] = None,\n    min_logfoldchange: Optional[float] = None,\n    key: Optional[str] = None,\n    show: Optional[bool] = None,\n    save: Optional[bool] = None,\n    return_fig: Optional[bool] = False,\n    **kwds,\n):\n    \"\"\"\\\n    Plot ranking of genes using dotplot plot (see :func:`~scanpy.pl.dotplot`)\n\n    Parameters\n    ----------\n    {params}\n    {vals_to_plot}\n    {show_save_ax}\n    return_fig\n        Returns :class:`DotPlot` object. Useful for fine-tuning\n        the plot. Takes precedence over `show=False`.\n    **kwds\n        Are passed to :func:`~scanpy.pl.dotplot`.\n\n    Returns\n    -------\n    If `return_fig` is `True`, returns a :class:`DotPlot` object,\n    else if `show` is false, return axes dict\n\n    Examples\n    --------\n\n    .. plot::\n        :context: close-figs\n\n        import scanpy as sc\n        adata = sc.datasets.pbmc68k_reduced()\n        sc.tl.rank_genes_groups(adata, 'bulk_labels', n_genes=adata.raw.shape[1])\n\n    Plot top 2 genes per group.\n\n    .. plot::\n        :context: close-figs\n\n        sc.pl.rank_genes_groups_dotplot(adata,n_genes=2)\n\n    Plot with scaled expressions for easier identification of differences.\n\n    .. plot::\n        :context: close-figs\n\n        sc.pl.rank_genes_groups_dotplot(adata, n_genes=2, standard_scale='var')\n\n    Plot `logfoldchanges` instead of gene expression. In this case a diverging colormap\n    like `bwr` or `seismic` works better. To center the colormap in zero, the minimum\n    and maximum values to plot are set to -4 and 4 respectively.\n    Also, only genes with a log fold change of 3 or more are shown.\n\n    .. plot::\n        :context: close-figs\n\n        sc.pl.rank_genes_groups_dotplot(\n            adata,\n            n_genes=4,\n            values_to_plot=\"logfoldchanges\", cmap='bwr',\n            vmin=-4,\n            vmax=4,\n            min_logfoldchange=3,\n            colorbar_title='log fold change'\n        )\n\n    Also, the last genes can be plotted. This can be useful to identify genes\n    that are lowly expressed in a group. For this `n_genes=-4` is used\n\n    .. plot::\n        :context: close-figs\n\n        sc.pl.rank_genes_groups_dotplot(\n            adata,\n            n_genes=-4,\n            values_to_plot=\"logfoldchanges\",\n            cmap='bwr',\n            vmin=-4,\n            vmax=4,\n            min_logfoldchange=3,\n            colorbar_title='log fold change',\n        )\n\n    A list specific genes can be given to check their log fold change. If a\n    dictionary, the dictionary keys will be added as labels in the plot.\n\n    .. plot::\n        :context: close-figs\n\n        var_names = {{'T-cell': ['CD3D', 'CD3E', 'IL32'],\n                      'B-cell': ['CD79A', 'CD79B', 'MS4A1'],\n                      'myeloid': ['CST3', 'LYZ'] }}\n        sc.pl.rank_genes_groups_dotplot(\n            adata,\n            var_names=var_names,\n            values_to_plot=\"logfoldchanges\",\n            cmap='bwr',\n            vmin=-4,\n            vmax=4,\n            min_logfoldchange=3,\n            colorbar_title='log fold change',\n        )\n\n    .. currentmodule:: scanpy\n\n    See also\n    --------\n    tl.rank_genes_groups\n    \"\"\"\n    return _rank_genes_groups_plot(\n        adata,\n        plot_type='dotplot',\n        groups=groups,\n        n_genes=n_genes,\n        groupby=groupby,\n        values_to_plot=values_to_plot,\n        var_names=var_names,\n        gene_symbols=gene_symbols,\n        key=key,\n        min_logfoldchange=min_logfoldchange,\n        show=show,\n        save=save,\n        return_fig=return_fig,\n        **kwds,", "idx": 310}
{"project": "Scanpy", "commit_id": "381_scanpy_1.9.0___init__.py_rank_genes_groups_stacked_violin.py", "target": 0, "func": "def rank_genes_groups_stacked_violin(\n    adata: AnnData,\n    groups: Union[str, Sequence[str]] = None,\n    n_genes: Optional[int] = None,\n    groupby: Optional[str] = None,\n    gene_symbols: Optional[str] = None,\n    *,\n    var_names: Optional[Union[Sequence[str], Mapping[str, Sequence[str]]]] = None,\n    min_logfoldchange: Optional[float] = None,\n    key: Optional[str] = None,\n    show: Optional[bool] = None,\n    save: Optional[bool] = None,\n    return_fig: Optional[bool] = False,\n    **kwds,\n):\n    \"\"\"\\\n    Plot ranking of genes using stacked_violin plot\n    (see :func:`~scanpy.pl.stacked_violin`)\n\n    Parameters\n    ----------\n    {params}\n    {show_save_ax}\n    return_fig\n        Returns :class:`StackedViolin` object. Useful for fine-tuning\n        the plot. Takes precedence over `show=False`.\n    **kwds\n        Are passed to :func:`~scanpy.pl.stacked_violin`.\n\n    Returns\n    -------\n    If `return_fig` is `True`, returns a :class:`StackedViolin` object,\n    else if `show` is false, return axes dict\n\n    Examples\n    --------\n    >>> import scanpy as sc\n    >>> adata = sc.datasets.pbmc68k_reduced()\n    >>> sc.tl.rank_genes_groups(adata, 'bulk_labels')\n\n    >>> sc.pl.rank_genes_groups_stacked_violin(adata, n_genes=4,\n    ... min_logfoldchange=4, figsize=(8,6))\n\n    \"\"\"\n\n    return _rank_genes_groups_plot(\n        adata,\n        plot_type='stacked_violin',\n        groups=groups,\n        n_genes=n_genes,\n        gene_symbols=gene_symbols,\n        groupby=groupby,\n        var_names=var_names,\n        key=key,\n        min_logfoldchange=min_logfoldchange,\n        show=show,\n        save=save,\n        return_fig=return_fig,\n        **kwds,", "idx": 311}
{"project": "Scanpy", "commit_id": "382_scanpy_1.9.0___init__.py_rank_genes_groups_matrixplot.py", "target": 0, "func": "def rank_genes_groups_matrixplot(\n    adata: AnnData,\n    groups: Union[str, Sequence[str]] = None,\n    n_genes: Optional[int] = None,\n    groupby: Optional[str] = None,\n    values_to_plot: Optional[\n        Literal[\n            'scores',\n            'logfoldchanges',\n            'pvals',\n            'pvals_adj',\n            'log10_pvals',\n            'log10_pvals_adj',\n        ]\n    ] = None,\n    var_names: Optional[Union[Sequence[str], Mapping[str, Sequence[str]]]] = None,\n    gene_symbols: Optional[str] = None,\n    min_logfoldchange: Optional[float] = None,\n    key: Optional[str] = None,\n    show: Optional[bool] = None,\n    save: Optional[bool] = None,\n    return_fig: Optional[bool] = False,\n    **kwds,\n):\n    \"\"\"\\\n    Plot ranking of genes using matrixplot plot (see :func:`~scanpy.pl.matrixplot`)\n\n    Parameters\n    ----------\n    {params}\n    {vals_to_plot}\n    {show_save_ax}\n    return_fig\n        Returns :class:`MatrixPlot` object. Useful for fine-tuning\n        the plot. Takes precedence over `show=False`.\n    **kwds\n        Are passed to :func:`~scanpy.pl.matrixplot`.\n\n    Returns\n    -------\n    If `return_fig` is `True`, returns a :class:`MatrixPlot` object,\n    else if `show` is false, return axes dict\n\n    Examples\n    --------\n\n    .. plot::\n        :context: close-figs\n\n        import scanpy as sc\n        adata = sc.datasets.pbmc68k_reduced()\n        sc.tl.rank_genes_groups(adata, 'bulk_labels', n_genes=adata.raw.shape[1])\n\n    Plot `logfoldchanges` instead of gene expression. In this case a diverging colormap\n    like `bwr` or `seismic` works better. To center the colormap in zero, the minimum\n    and maximum values to plot are set to -4 and 4 respectively.\n    Also, only genes with a log fold change of 3 or more are shown.\n\n\n    .. plot::\n        :context: close-figs\n\n        sc.pl.rank_genes_groups_matrixplot(\n            adata,\n            n_genes=4,\n            values_to_plot=\"logfoldchanges\",\n            cmap='bwr',\n            vmin=-4,\n            vmax=4,\n            min_logfoldchange=3,\n            colorbar_title='log fold change',\n        )\n\n    Also, the last genes can be plotted. This can be useful to identify genes\n    that are lowly expressed in a group. For this `n_genes=-4` is used\n\n    .. plot::\n        :context: close-figs\n\n        sc.pl.rank_genes_groups_matrixplot(\n            adata,\n            n_genes=-4,\n            values_to_plot=\"logfoldchanges\",\n            cmap='bwr',\n            vmin=-4,\n            vmax=4,\n            min_logfoldchange=3,\n            colorbar_title='log fold change',\n        )\n\n    A list specific genes can be given to check their log fold change. If a\n    dictionary, the dictionary keys will be added as labels in the plot.\n\n    .. plot::\n        :context: close-figs\n\n        var_names = {{\"T-cell\": ['CD3D', 'CD3E', 'IL32'],\n                      'B-cell': ['CD79A', 'CD79B', 'MS4A1'],\n                      'myeloid': ['CST3', 'LYZ'] }}\n        sc.pl.rank_genes_groups_matrixplot(\n            adata,\n            var_names=var_names,\n            values_to_plot=\"logfoldchanges\",\n            cmap='bwr',\n            vmin=-4,\n            vmax=4,\n            min_logfoldchange=3,\n            colorbar_title='log fold change',\n        )\n    \"\"\"\n\n    return _rank_genes_groups_plot(\n        adata,\n        plot_type='matrixplot',\n        groups=groups,\n        n_genes=n_genes,\n        groupby=groupby,\n        values_to_plot=values_to_plot,\n        var_names=var_names,\n        gene_symbols=gene_symbols,\n        key=key,\n        min_logfoldchange=min_logfoldchange,\n        show=show,\n        save=save,\n        return_fig=return_fig,\n        **kwds,", "idx": 312}
{"project": "Scanpy", "commit_id": "383_scanpy_1.9.0___init__.py_rank_genes_groups_violin.py", "target": 0, "func": "def rank_genes_groups_violin(\n    adata: AnnData,\n    groups: Optional[Sequence[str]] = None,\n    n_genes: int = 20,\n    gene_names: Optional[Iterable[str]] = None,\n    gene_symbols: Optional[str] = None,\n    use_raw: Optional[bool] = None,\n    key: Optional[str] = None,\n    split: bool = True,\n    scale: str = 'width',\n    strip: bool = True,\n    jitter: Union[int, float, bool] = True,\n    size: int = 1,\n    ax: Optional[Axes] = None,\n    show: Optional[bool] = None,\n    save: Optional[bool] = None,\n):\n    \"\"\"\\\n    Plot ranking of genes for all tested comparisons.\n\n    Parameters\n    ----------\n    adata\n        Annotated data matrix.\n    groups\n        List of group names.\n    n_genes\n        Number of genes to show. Is ignored if `gene_names` is passed.\n    gene_names\n        List of genes to plot. Is only useful if interested in a custom gene list,\n        which is not the result of :func:`scanpy.tl.rank_genes_groups`.\n    gene_symbols\n        Key for field in `.var` that stores gene symbols if you do not want to\n        use `.var_names` displayed in the plot.\n    use_raw\n        Use `raw` attribute of `adata` if present. Defaults to the value that\n        was used in :func:`~scanpy.tl.rank_genes_groups`.\n    split\n        Whether to split the violins or not.\n    scale\n        See :func:`~seaborn.violinplot`.\n    strip\n        Show a strip plot on top of the violin plot.\n    jitter\n        If set to 0, no points are drawn. See :func:`~seaborn.stripplot`.\n    size\n        Size of the jitter points.\n    {show_save_ax}\n    \"\"\"\n    if key is None:\n        key = 'rank_genes_groups'\n    groups_key = str(adata.uns[key]['params']['groupby'])\n    if use_raw is None:\n        use_raw = bool(adata.uns[key]['params']['use_raw'])\n    reference = str(adata.uns[key]['params']['reference'])\n    groups_names = adata.uns[key]['names'].dtype.names if groups is None else groups\n    if isinstance(groups_names, str):\n        groups_names = [groups_names]\n    axs = []\n    for group_name in groups_names:\n        if gene_names is None:\n            _gene_names = adata.uns[key]['names'][group_name][:n_genes]\n        else:\n            _gene_names = gene_names\n        if isinstance(_gene_names, np.ndarray):\n            _gene_names = _gene_names.tolist()\n        df = obs_df(adata, _gene_names, use_raw=use_raw, gene_symbols=gene_symbols)\n        new_gene_names = df.columns\n        df['hue'] = adata.obs[groups_key].astype(str).values\n        if reference == 'rest':\n            df.loc[df['hue'] != group_name, 'hue'] = 'rest'\n        else:\n            df.loc[~df['hue'].isin([group_name, reference]), 'hue'] = np.nan\n        df['hue'] = df['hue'].astype('category')\n        df_tidy = pd.melt(df, id_vars='hue', value_vars=new_gene_names)\n        x = 'variable'\n        y = 'value'\n        hue_order = [group_name, reference]\n        import seaborn as sns\n\n        _ax = sns.violinplot(\n            x=x,\n            y=y,\n            data=df_tidy,\n            inner=None,\n            hue_order=hue_order,\n            hue='hue',\n            split=split,\n            scale=scale,\n            orient='vertical',\n            ax=ax,\n        )\n        if strip:\n            _ax = sns.stripplot(\n                x=x,\n                y=y,\n                data=df_tidy,\n                hue='hue',\n                dodge=True,\n                hue_order=hue_order,\n                jitter=jitter,\n                color='black',\n                size=size,\n                ax=_ax,\n            )\n        _ax.set_xlabel('genes')\n        _ax.set_title('{} vs. {}'.format(group_name, reference))\n        _ax.legend_.remove()\n        _ax.set_ylabel('expression')\n        _ax.set_xticklabels(new_gene_names, rotation='vertical')\n        writekey = (\n            f\"rank_genes_groups_\"\n            f\"{adata.uns[key]['params']['groupby']}_\"\n            f\"{group_name}\"\n        )\n        savefig_or_show(writekey, show=show, save=save)\n        axs.append(_ax)\n    if show is False:\n        return axs", "idx": 313}
{"project": "Scanpy", "commit_id": "384_scanpy_1.9.0___init__.py_sim.py", "target": 0, "func": "def sim(\n    adata,\n    tmax_realization: Optional[int] = None,\n    as_heatmap: bool = False,\n    shuffle: bool = False,\n    show: Optional[bool] = None,\n    save: Union[bool, str, None] = None,\n):\n    \"\"\"\\\n    Plot results of simulation.\n\n    Parameters\n    ----------\n    tmax_realization\n        Number of observations in one realization of the time series. The data matrix\n        adata.X consists in concatenated realizations.\n    as_heatmap\n        Plot the timeseries as heatmap.\n    shuffle\n        Shuffle the data.\n    show\n        Show the plot, do not return axis.\n    save\n        If `True` or a `str`, save the figure.\n        A string is appended to the default filename.\n        Infer the filetype if ending on {{`'.pdf'`, `'.png'`, `'.svg'`}}.\n    \"\"\"\n    if tmax_realization is not None:\n        tmax = tmax_realization\n    elif 'tmax_write' in adata.uns:\n        tmax = adata.uns['tmax_write']\n    else:\n        tmax = adata.n_obs\n    n_realizations = adata.n_obs / tmax\n    if not shuffle:\n        if not as_heatmap:\n            timeseries(\n                adata.X,\n                var_names=adata.var_names,\n                xlim=[0, 1.25 * adata.n_obs],\n                highlights_x=np.arange(tmax, n_realizations * tmax, tmax),\n                xlabel='realizations',\n            )\n        else:\n            # plot time series as heatmap, as in Haghverdi et al. (2016), Fig. 1d\n            timeseries_as_heatmap(\n                adata.X,\n                var_names=adata.var_names,\n                highlights_x=np.arange(tmax, n_realizations * tmax, tmax),\n            )\n        pl.xticks(\n            np.arange(0, n_realizations * tmax, tmax),\n            np.arange(n_realizations).astype(int) + 1,\n        )\n        savefig_or_show('sim', save=save, show=show)\n    else:\n        # shuffled data\n        X = adata.X\n        X, rows = subsample(X, seed=1)\n        timeseries(\n            X,\n            var_names=adata.var_names,\n            xlim=[0, 1.25 * adata.n_obs],\n            highlights_x=np.arange(tmax, n_realizations * tmax, tmax),\n            xlabel='index (arbitrary order)',\n        )\n        savefig_or_show('sim_shuffled', save=save, show=show)", "idx": 314}
{"project": "Scanpy", "commit_id": "385_scanpy_1.9.0___init__.py_embedding_density.py", "target": 0, "func": "def embedding_density(\n    adata: AnnData,\n    # on purpose, there is no asterisk here (for backward compat)\n    basis: str = 'umap',  # was positional before 1.4.5\n    key: Optional[str] = None,  # was positional before 1.4.5\n    groupby: Optional[str] = None,\n    group: Optional[Union[str, List[str], None]] = 'all',\n    color_map: Union[Colormap, str] = 'YlOrRd',\n    bg_dotsize: Optional[int] = 80,\n    fg_dotsize: Optional[int] = 180,\n    vmax: Optional[int] = 1,\n    vmin: Optional[int] = 0,\n    vcenter: Optional[int] = None,\n    norm: Optional[Normalize] = None,\n    ncols: Optional[int] = 4,\n    hspace: Optional[float] = 0.25,\n    wspace: Optional[None] = None,\n    title: str = None,\n    show: Optional[bool] = None,\n    save: Union[bool, str, None] = None,\n    ax: Optional[Axes] = None,\n    return_fig: Optional[bool] = None,\n    **kwargs,\n) -> Union[Figure, Axes, None]:\n    \"\"\"\\\n    Plot the density of cells in an embedding (per condition).\n\n    Plots the gaussian kernel density estimates (over condition) from the\n    `sc.tl.embedding_density()` output.\n\n    This function was written by Sophie Tritschler and implemented into\n    Scanpy by Malte Luecken.\n\n    Parameters\n    ----------\n    adata\n        The annotated data matrix.\n    basis\n        The embedding over which the density was calculated. This embedded\n        representation should be found in `adata.obsm['X_[basis]']``.\n    key\n        Name of the `.obs` covariate that contains the density estimates. Alternatively, pass `groupby`.\n    groupby\n        Name of the condition used in `tl.embedding_density`. Alternatively, pass `key`.\n    group\n        The category in the categorical observation annotation to be plotted.\n        For example, 'G1' in the cell cycle 'phase' covariate. If all categories\n        are to be plotted use group='all' (default), If multiple categories\n        want to be plotted use a list (e.g.: ['G1', 'S']. If the overall density\n        wants to be ploted set group to 'None'.\n    color_map\n        Matplolib color map to use for density plotting.\n    bg_dotsize\n        Dot size for background data points not in the `group`.\n    fg_dotsize\n        Dot size for foreground data points in the `group`.\n    {vminmax}\n    {panels}\n    {show_save_ax}\n\n    Examples\n    --------\n\n    .. plot::\n        :context: close-figs\n\n        import scanpy as sc\n        adata = sc.datasets.pbmc68k_reduced()\n        sc.tl.umap(adata)\n        sc.tl.embedding_density(adata, basis='umap', groupby='phase')\n\n    Plot all categories be default\n\n    .. plot::\n        :context: close-figs\n\n        sc.pl.embedding_density(adata, basis='umap', key='umap_density_phase')\n\n    Plot selected categories\n\n    .. plot::\n        :context: close-figs\n\n        sc.pl.embedding_density(\n            adata,\n            basis='umap',\n            key='umap_density_phase',\n            group=['G1', 'S'],\n        )\n\n    .. currentmodule:: scanpy\n\n    See also\n    --------\n    tl.embedding_density\n    \"\"\"\n    sanitize_anndata(adata)\n\n    # Test user inputs\n    basis = basis.lower()\n\n    if basis == 'fa':\n        basis = 'draw_graph_fa'\n\n    if key is not None and groupby is not None:\n        raise ValueError('either pass key or groupby but not both')\n\n    if key is None:\n        key = 'umap_density'\n    if groupby is not None:\n        key += f'_{groupby}'\n\n    if f'X_{basis}' not in adata.obsm_keys():\n        raise ValueError(\n            f'Cannot find the embedded representation `adata.obsm[X_{basis!r}]`. '\n            'Compute the embedding first.'\n        )\n\n    if key not in adata.obs or f'{key}_params' not in adata.uns:\n        raise ValueError(\n            'Please run `sc.tl.embedding_density()` first '\n            'and specify the correct key.'\n        )\n\n    if 'components' in kwargs:\n        logg.warning(\n            'Components were specified, but will be ignored. Only the '\n            'components used to calculate the density can be plotted.'\n        )\n        del kwargs['components']\n\n    components = adata.uns[f'{key}_params']['components']\n    groupby = adata.uns[f'{key}_params']['covariate']\n\n    # turn group into a list if needed\n    if group == 'all':\n        if groupby is None:\n            group = None\n        else:\n            group = list(adata.obs[groupby].cat.categories)\n    elif isinstance(group, str):\n        group = [group]\n\n    if group is None and groupby is not None:\n        raise ValueError(\n            'Densities were calculated over an `.obs` covariate. '\n            'Please specify a group from this covariate to plot.'\n        )\n\n    if group is not None and groupby is None:\n        logg.warning(\n            \"value of 'group' is ignored because densities \"\n            \"were not calculated for an `.obs` covariate.\"\n        )\n        group = None\n\n    if np.min(adata.obs[key]) < 0 or np.max(adata.obs[key]) > 1:\n        raise ValueError('Densities should be scaled between 0 and 1.')\n\n    if wspace is None:\n        #  try to set a wspace that is not too large or too small given the\n        #  current figure size\n        wspace = 0.75 / rcParams['figure.figsize'][0] + 0.02\n\n    # Make the color map\n    if isinstance(color_map, str):\n        color_map = copy(cm.get_cmap(color_map))\n\n    color_map.set_over('black')\n    color_map.set_under('lightgray')\n    # a name to store the density values is needed. To avoid\n    # overwriting a user name a new random name is created\n    while True:\n        col_id = np.random.randint(1000, 10000)\n        density_col_name = f'_tmp_embedding_density_column_{col_id}_'\n        if density_col_name not in adata.obs.columns:\n            break\n\n    # if group is set, then plot it using multiple panels\n    # (even if only one group is set)\n    if (\n        group is not None\n        and not isinstance(group, str)\n        and isinstance(group, cabc.Sequence)\n    ):\n        if ax is not None:\n            raise ValueError(\"Can only specify `ax` if no `group` sequence is given.\")\n        fig, gs = _panel_grid(hspace, wspace, ncols, len(group))\n\n        axs = []\n        for count, group_name in enumerate(group):\n            if group_name not in adata.obs[groupby].cat.categories:\n                raise ValueError(\n                    'Please specify a group from the `.obs` category '\n                    'over which the density was calculated. '\n                    f'Invalid group name: {group_name}'\n                )\n\n            ax = pl.subplot(gs[count])\n            # Define plotting data\n            dot_sizes = np.ones(adata.n_obs) * bg_dotsize\n            group_mask = adata.obs[groupby] == group_name\n            dens_values = -np.ones(adata.n_obs)\n            dens_values[group_mask] = adata.obs[key][group_mask]\n            adata.obs[density_col_name] = dens_values\n            dot_sizes[group_mask] = np.ones(sum(group_mask)) * fg_dotsize\n\n            if title is None:\n                _title = group_name\n            else:\n                _title = title\n\n            ax = embedding(\n                adata,\n                basis,\n                dimensions=np.array(components) - 1,  # Saved with 1 based indexing\n                color=density_col_name,\n                color_map=color_map,\n                size=dot_sizes,\n                vmax=vmax,\n                vmin=vmin,\n                vcenter=vcenter,\n                norm=norm,\n                save=False,\n                title=_title,\n                ax=ax,\n                show=False,\n                **kwargs,\n            )\n            axs.append(ax)\n\n        ax = axs\n    else:\n        dens_values = adata.obs[key]\n        dot_sizes = np.ones(adata.n_obs) * fg_dotsize\n\n        adata.obs[density_col_name] = dens_values\n\n        # Ensure title is blank as default\n        if title is None:\n            title = group if group is not None else \"\"\n\n        # Plot the graph\n        fig_or_ax = embedding(\n            adata,\n            basis,\n            dimensions=np.array(components) - 1,  # Saved with 1 based indexing\n            color=density_col_name,\n            color_map=color_map,\n            size=dot_sizes,\n            vmax=vmax,\n            vmin=vmin,\n            vcenter=vcenter,\n            norm=norm,\n            save=False,\n            show=False,\n            title=title,\n            ax=ax,\n            return_fig=return_fig,\n            **kwargs,\n        )\n        if return_fig:\n            fig = fig_or_ax\n        else:\n            ax = fig_or_ax\n\n    # remove temporary column name\n    adata.obs = adata.obs.drop(columns=[density_col_name])\n\n    if return_fig:\n        return fig\n    savefig_or_show(f\"{key}_\", show=show, save=save)\n    if show is False:\n        return ax", "idx": 315}
{"project": "Scanpy", "commit_id": "386_scanpy_1.9.0___init__.py__get_values_to_plot.py", "target": 0, "func": "def _get_values_to_plot(\n    adata,\n    values_to_plot: Literal[\n        'scores',\n        'logfoldchanges',\n        'pvals',\n        'pvals_adj',\n        'log10_pvals',\n        'log10_pvals_adj',\n    ],\n    gene_names: Sequence[str],\n    groups: Optional[Sequence[str]] = None,\n    key: Optional[str] = 'rank_genes_groups',\n    gene_symbols: Optional[str] = None,\n):\n    \"\"\"\n    If rank_genes_groups has been called, this function\n    prepares a dataframe containing scores, pvalues, logfoldchange etc to be plotted\n    as dotplot or matrixplot.\n\n    The dataframe index are the given groups and the columns are the gene_names\n\n    used by rank_genes_groups_dotplot\n\n    Parameters\n    ----------\n    adata\n    values_to_plot\n        name of the value to plot\n    gene_names\n        gene names\n    groups\n        groupby categories\n    key\n        adata.uns key where the rank_genes_groups is stored.\n        By default 'rank_genes_groups'\n    gene_symbols\n        Key for field in .var that stores gene symbols.\n    Returns\n    -------\n    pandas DataFrame index=groups, columns=gene_names\n\n    \"\"\"\n    valid_options = [\n        'scores',\n        'logfoldchanges',\n        'pvals',\n        'pvals_adj',\n        'log10_pvals',\n        'log10_pvals_adj',\n    ]\n    if values_to_plot not in valid_options:\n        raise ValueError(\n            f\"given value_to_plot: '{values_to_plot}' is not valid. Valid options are {valid_options}\"\n        )\n\n    values_df = None\n    check_done = False\n    if groups is None:\n        groups = adata.uns[key]['names'].dtype.names\n    if values_to_plot is not None:\n\n        df_list = []\n        for group in groups:\n            df = rank_genes_groups_df(adata, group, key=key, gene_symbols=gene_symbols)\n            if gene_symbols is not None:\n                df['names'] = df[gene_symbols]\n            # check that all genes are present in the df as sc.tl.rank_genes_groups\n            # can be called with only top genes\n            if not check_done:\n                if df.shape[0] < adata.shape[1]:\n                    message = (\n                        \"Please run `sc.tl.rank_genes_groups` with \"\n                        \"'n_genes=adata.shape[1]' to save all gene \"\n                        f\"scores. Currently, only {df.shape[0]} \"\n                        \"are found\"\n                    )\n                    logg.error(message)\n                    raise ValueError(message)\n            df['group'] = group\n            df_list.append(df)\n\n        values_df = pd.concat(df_list)\n        if values_to_plot.startswith('log10'):\n            column = values_to_plot.replace('log10_', '')\n        else:\n            column = values_to_plot\n        values_df = pd.pivot(\n            values_df, index='names', columns='group', values=column\n        ).fillna(1)\n\n        if values_to_plot in ['log10_pvals', 'log10_pvals_adj']:\n            values_df = -1 * np.log10(values_df)\n\n        values_df = values_df.loc[gene_names].T\n\n    return values_df", "idx": 316}
{"project": "Scanpy", "commit_id": "387_scanpy_1.9.0__combat.py__design_matrix.py", "target": 0, "func": "def _design_matrix(\n    model: pd.DataFrame, batch_key: str, batch_levels: Collection[str]\n) -> pd.DataFrame:\n    \"\"\"\\\n    Computes a simple design matrix.\n\n    Parameters\n    --------\n    model\n        Contains the batch annotation\n    batch_key\n        Name of the batch column\n    batch_levels\n        Levels of the batch annotation\n\n    Returns\n    --------\n    The design matrix for the regression problem\n    \"\"\"\n    import patsy\n\n    design = patsy.dmatrix(\n        \"~ 0 + C(Q('{}'), levels=batch_levels)\".format(batch_key),\n        model,\n        return_type=\"dataframe\",\n    )\n    model = model.drop([batch_key], axis=1)\n    numerical_covariates = model.select_dtypes('number').columns.values\n\n    logg.info(f\"Found {design.shape[1]} batches\\n\")\n    other_cols = [c for c in model.columns.values if c not in numerical_covariates]\n\n    if other_cols:\n        col_repr = \" + \".join(\"Q('{}')\".format(x) for x in other_cols)\n        factor_matrix = patsy.dmatrix(\n            \"~ 0 + {}\".format(col_repr), model[other_cols], return_type=\"dataframe\"\n        )\n\n        design = pd.concat((design, factor_matrix), axis=1)\n        logg.info(f\"Found {len(other_cols)} categorical variables:\")\n        logg.info(\"\\t\" + \", \".join(other_cols) + '\\n')\n\n    if numerical_covariates is not None:\n        logg.info(f\"Found {len(numerical_covariates)} numerical variables:\")\n        logg.info(\"\\t\" + \", \".join(numerical_covariates) + '\\n')\n\n        for nC in numerical_covariates:\n            design[nC] = model[nC]\n\n    return design", "idx": 317}
{"project": "Scanpy", "commit_id": "388_scanpy_1.9.0__combat.py__standardize_data.py", "target": 0, "func": "def _standardize_data(\n    model: pd.DataFrame, data: pd.DataFrame, batch_key: str\n) -> Tuple[pd.DataFrame, pd.DataFrame, np.ndarray, np.ndarray]:\n    \"\"\"\\\n    Standardizes the data per gene.\n\n    The aim here is to make mean and variance be comparable across batches.\n\n    Parameters\n    --------\n    model\n        Contains the batch annotation\n    data\n        Contains the Data\n    batch_key\n        Name of the batch column in the model matrix\n\n    Returns\n    --------\n    s_data\n        Standardized Data\n    design\n        Batch assignment as one-hot encodings\n    var_pooled\n        Pooled variance per gene\n    stand_mean\n        Gene-wise mean\n    \"\"\"\n\n    # compute the design matrix\n    batch_items = model.groupby(batch_key).groups.items()\n    batch_levels, batch_info = zip(*batch_items)\n    n_batch = len(batch_info)\n    n_batches = np.array([len(v) for v in batch_info])\n    n_array = float(sum(n_batches))\n\n    design = _design_matrix(model, batch_key, batch_levels)\n\n    # compute pooled variance estimator\n    B_hat = np.dot(np.dot(la.inv(np.dot(design.T, design)), design.T), data.T)\n    grand_mean = np.dot((n_batches / n_array).T, B_hat[:n_batch, :])\n    var_pooled = (data - np.dot(design, B_hat).T) ** 2\n    var_pooled = np.dot(var_pooled, np.ones((int(n_array), 1)) / int(n_array))\n\n    # Compute the means\n    if np.sum(var_pooled == 0) > 0:\n        print(f'Found {np.sum(var_pooled == 0)} genes with zero variance.')\n    stand_mean = np.dot(\n        grand_mean.T.reshape((len(grand_mean), 1)), np.ones((1, int(n_array)))\n    )\n    tmp = np.array(design.copy())\n    tmp[:, :n_batch] = 0\n    stand_mean += np.dot(tmp, B_hat).T\n\n    # need to be a bit careful with the zero variance genes\n    # just set the zero variance genes to zero in the standardized data\n    s_data = np.where(\n        var_pooled == 0,\n        0,\n        ((data - stand_mean) / np.dot(np.sqrt(var_pooled), np.ones((1, int(n_array))))),\n    )\n    s_data = pd.DataFrame(s_data, index=data.index, columns=data.columns)\n\n    return s_data, design, var_pooled, stand_mean", "idx": 318}
{"project": "Scanpy", "commit_id": "389_scanpy_1.9.0__combat.py_combat.py", "target": 1, "func": "def combat(\n    adata: AnnData,\n    key: str = 'batch',\n    covariates: Optional[Collection[str]] = None,\n    inplace: bool = True,\n) -> Union[AnnData, np.ndarray, None]:\n    \"\"\"\\\n    ComBat function for batch effect correction [Johnson07]_ [Leek12]_\n    [Pedersen12]_.\n\n    Corrects for batch effects by fitting linear models, gains statistical power\n    via an EB framework where information is borrowed across genes.\n    This uses the implementation `combat.py`_ [Pedersen12]_.\n\n    .. _combat.py: https://github.com/brentp/combat.py\n\n    Parameters\n    ----------\n    adata\n        Annotated data matrix\n    key\n        Key to a categorical annotation from :attr:`~anndata.AnnData.obs`\n        that will be used for batch effect removal.\n    covariates\n        Additional covariates besides the batch variable such as adjustment\n        variables or biological condition. This parameter refers to the design\n        matrix `X` in Equation 2.1 in [Johnson07]_ and to the `mod` argument in\n        the original combat function in the sva R package.\n        Note that not including covariates may introduce bias or lead to the\n        removal of biological signal in unbalanced designs.\n    inplace\n        Whether to replace adata.X or to return the corrected data\n\n    Returns\n    -------\n    Depending on the value of `inplace`, either returns the corrected matrix or\n    or modifies `adata.X`.\n    \"\"\"\n\n    # check the input\n    if key not in adata.obs_keys():\n        raise ValueError('Could not find the key {!r} in adata.obs'.format(key))\n\n    if covariates is not None:\n        cov_exist = np.isin(covariates, adata.obs_keys())\n        if np.any(~cov_exist):\n            missing_cov = np.array(covariates)[~cov_exist].tolist()\n            raise ValueError(\n                'Could not find the covariate(s) {!r} in adata.obs'.format(missing_cov)\n            )\n\n        if key in covariates:\n            raise ValueError('Batch key and covariates cannot overlap')\n\n        if len(covariates) != len(set(covariates)):\n            raise ValueError('Covariates must be unique')\n\n    # only works on dense matrices so far\n    if issparse(adata.X):\n        X = adata.X.A.T\n    else:\n        X = adata.X.T\n    data = pd.DataFrame(data=X, index=adata.var_names, columns=adata.obs_names)\n\n    sanitize_anndata(adata)\n\n    # construct a pandas series of the batch annotation\n    model = adata.obs[[key] + (covariates if covariates else [])]\n    batch_info = model.groupby(key).indices.values()\n    n_batch = len(batch_info)\n    n_batches = np.array([len(v) for v in batch_info])\n    n_array = float(sum(n_batches))\n\n    # standardize across genes using a pooled variance estimator\n    logg.info(\"Standardizing Data across genes.\\n\")\n    s_data, design, var_pooled, stand_mean = _standardize_data(model, data, key)\n\n    # fitting the parameters on the standardized data\n    logg.info(\"Fitting L/S model and finding priors\\n\")\n    batch_design = design[design.columns[:n_batch]]\n    # first estimate of the additive batch effect\n    gamma_hat = (\n        la.inv(batch_design.T @ batch_design) @ batch_design.T @ s_data.T\n    ).values\n    delta_hat = []\n\n    # first estimate for the multiplicative batch effect\n    for i, batch_idxs in enumerate(batch_info):\n        delta_hat.append(s_data.iloc[:, batch_idxs].var(axis=1))\n\n    # empirically fix the prior hyperparameters\n    gamma_bar = gamma_hat.mean(axis=1)\n    t2 = gamma_hat.var(axis=1)\n    # a_prior and b_prior are the priors on lambda and theta from Johnson and Li (2006)\n    a_prior = list(map(_aprior, delta_hat))\n    b_prior = list(map(_bprior, delta_hat))\n\n    logg.info(\"Finding parametric adjustments\\n\")\n    # gamma star and delta star will be our empirical bayes (EB) estimators\n    # for the additive and multiplicative batch effect per batch and cell\n    gamma_star, delta_star = [], []\n    for i, batch_idxs in enumerate(batch_info):\n        # temp stores our estimates for the batch effect parameters.\n        # temp[0] is the additive batch effect\n        # temp[1] is the multiplicative batch effect\n        gamma, delta = _it_sol(\n            s_data.iloc[:, batch_idxs].values,\n            gamma_hat[i],\n            delta_hat[i].values,\n            gamma_bar[i],\n            t2[i],\n            a_prior[i],\n            b_prior[i],\n        )\n\n        gamma_star.append(gamma)\n        delta_star.append(delta)\n\n    logg.info(\"Adjusting data\\n\")\n    bayesdata = s_data\n    gamma_star = np.array(gamma_star)\n    delta_star = np.array(delta_star)\n\n    # we now apply the parametric adjustment to the standardized data from above\n    # loop over all batches in the data\n    for j, batch_idxs in enumerate(batch_info):\n        # we basically substract the additive batch effect, rescale by the ratio\n        # of multiplicative batch effect to pooled variance and add the overall gene\n        # wise mean\n        dsq = np.sqrt(delta_star[j, :])\n        dsq = dsq.reshape((len(dsq), 1))\n        denom = np.dot(dsq, np.ones((1, n_batches[j])))\n        numer = np.array(\n            bayesdata.iloc[:, batch_idxs]\n            - np.dot(batch_design.iloc[batch_idxs], gamma_star).T\n        )\n        bayesdata.iloc[:, batch_idxs] = numer / denom\n\n    vpsq = np.sqrt(var_pooled).reshape((len(var_pooled), 1))\n    bayesdata = bayesdata * np.dot(vpsq, np.ones((1, int(n_array)))) + stand_mean\n\n    # put back into the adata object or return\n    if inplace:\n        adata.X = bayesdata.values.transpose()\n    else:\n        return bayesdata.values.transpose()", "idx": 319}
{"project": "Scanpy", "commit_id": "38_scanpy_1.9.0_logging.py__versions_dependencies.py", "target": 0, "func": "def _versions_dependencies(dependencies):\n    # this is not the same as the requirements!\n    for mod in dependencies:\n        mod_name, dist_name = mod if isinstance(mod, tuple) else (mod, mod)\n        try:\n            imp = __import__(mod_name)\n            yield dist_name, imp.__version__\n        except (ImportError, AttributeError):\n            pass", "idx": 320}
{"project": "Scanpy", "commit_id": "390_scanpy_1.9.0__combat.py__it_sol.py", "target": 0, "func": "def _it_sol(\n    s_data: np.ndarray,\n    g_hat: np.ndarray,\n    d_hat: np.ndarray,\n    g_bar: float,\n    t2: float,\n    a: float,\n    b: float,\n    conv: float = 0.0001,\n) -> Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\\\n    Iteratively compute the conditional posterior means for gamma and delta.\n\n    gamma is an estimator for the additive batch effect, deltat is an estimator\n    for the multiplicative batch effect. We use an EB framework to estimate these\n    two. Analytical expressions exist for both parameters, which however depend on each other.\n    We therefore iteratively evalutate these two expressions until convergence is reached.\n\n    Parameters\n    --------\n    s_data\n        Contains the standardized Data\n    g_hat\n        Initial guess for gamma\n    d_hat\n        Initial guess for delta\n    g_bar, t_2, a, b\n        Hyperparameters\n    conv: float, optional (default: `0.0001`)\n        convergence criterium\n\n    Returns:\n    --------\n    gamma\n        estimated value for gamma\n    delta\n        estimated value for delta\n    \"\"\"\n\n    n = (1 - np.isnan(s_data)).sum(axis=1)\n    g_old = g_hat.copy()\n    d_old = d_hat.copy()\n\n    change = 1\n    count = 0\n\n    # They need to be initialized for numba to properly infer types\n    g_new = g_old\n    d_new = d_old\n    # we place a normally distributed prior on gamma and and inverse gamma prior on delta\n    # in the loop, gamma and delta are updated together. they depend on each other. we iterate until convergence.\n    while change > conv:\n        g_new = (t2 * n * g_hat + d_old * g_bar) / (t2 * n + d_old)\n        sum2 = s_data - g_new.reshape((g_new.shape[0], 1)) @ np.ones(\n            (1, s_data.shape[1])\n        )\n        sum2 = sum2**2\n        sum2 = sum2.sum(axis=1)\n        d_new = (0.5 * sum2 + b) / (n / 2.0 + a - 1.0)\n\n        change = max(\n            (abs(g_new - g_old) / g_old).max(), (abs(d_new - d_old) / d_old).max()\n        )\n        g_old = g_new  # .copy()\n        d_old = d_new  # .copy()\n        count = count + 1\n\n    return g_new, d_new", "idx": 321}
{"project": "Scanpy", "commit_id": "391_scanpy_1.9.0__combat.py__aprior.py", "target": 0, "func": "def _aprior(delta_hat):\n    m = delta_hat.mean()\n    s2 = delta_hat.var()\n    return (2 * s2 + m**2) / s2", "idx": 322}
{"project": "Scanpy", "commit_id": "392_scanpy_1.9.0__combat.py__bprior.py", "target": 0, "func": "def _bprior(delta_hat):\n    m = delta_hat.mean()\n    s2 = delta_hat.var()\n    return (m * s2 + m**3) / s2", "idx": 323}
{"project": "Scanpy", "commit_id": "393_scanpy_1.9.0__distributed.py_materialize_as_ndarray.py", "target": 0, "func": "def materialize_as_ndarray(a):\n    \"\"\"Convert distributed arrays to ndarrays.\"\"\"\n    if type(a) in (list, tuple):\n        if da is not None and any(isinstance(arr, da.Array) for arr in a):\n            return da.compute(*a, sync=True)\n        return tuple(np.asarray(arr) for arr in a)\n    return np.asarray(a)", "idx": 324}
{"project": "Scanpy", "commit_id": "394_scanpy_1.9.0__highly_variable_genes.py__highly_variable_genes_seurat_v3.py", "target": 1, "func": "def _highly_variable_genes_seurat_v3(\n    adata: AnnData,\n    layer: Optional[str] = None,\n    n_top_genes: int = 2000,\n    batch_key: Optional[str] = None,\n    check_values: bool = True,\n    span: float = 0.3,\n    subset: bool = False,\n    inplace: bool = True,\n) -> Optional[pd.DataFrame]:\n    \"\"\"\\\n    See `highly_variable_genes`.\n\n    For further implementation details see https://www.overleaf.com/read/ckptrbgzzzpg\n\n    Returns\n    -------\n    Depending on `inplace` returns calculated metrics (:class:`~pd.DataFrame`) or\n    updates `.var` with the following fields:\n\n    highly_variable : bool\n        boolean indicator of highly-variable genes.\n    **means**\n        means per gene.\n    **variances**\n        variance per gene.\n    **variances_norm**\n        normalized variance per gene, averaged in the case of multiple batches.\n    highly_variable_rank : float\n        Rank of the gene according to normalized variance, median rank in the case of multiple batches.\n    highly_variable_nbatches : int\n        If batch_key is given, this denotes in how many batches genes are detected as HVG.\n    \"\"\"\n\n    try:\n        from skmisc.loess import loess\n    except ImportError:\n        raise ImportError(\n            'Please install skmisc package via `pip install --user scikit-misc'\n        )\n    df = pd.DataFrame(index=adata.var_names)\n    X = adata.layers[layer] if layer is not None else adata.X\n\n    if check_values and not check_nonnegative_integers(X):\n        warnings.warn(\n            \"`flavor='seurat_v3'` expects raw count data, but non-integers were found.\",\n            UserWarning,\n        )\n\n    df['means'], df['variances'] = _get_mean_var(X)\n\n    if batch_key is None:\n        batch_info = pd.Categorical(np.zeros(adata.shape[0], dtype=int))\n    else:\n        batch_info = adata.obs[batch_key].values\n\n    norm_gene_vars = []\n    for b in np.unique(batch_info):\n        X_batch = X[batch_info == b]\n\n        mean, var = _get_mean_var(X_batch)\n        not_const = var > 0\n        estimat_var = np.zeros(X.shape[1], dtype=np.float64)\n\n        y = np.log10(var[not_const])\n        x = np.log10(mean[not_const])\n        model = loess(x, y, span=span, degree=2)\n        model.fit()\n        estimat_var[not_const] = model.outputs.fitted_values\n        reg_std = np.sqrt(10**estimat_var)\n\n        batch_counts = X_batch.astype(np.float64).copy()\n        # clip large values as in Seurat\n        N = X_batch.shape[0]\n        vmax = np.sqrt(N)\n        clip_val = reg_std * vmax + mean\n        if sp_sparse.issparse(batch_counts):\n            batch_counts = sp_sparse.csr_matrix(batch_counts)\n            mask = batch_counts.data > clip_val[batch_counts.indices]\n            batch_counts.data[mask] = clip_val[batch_counts.indices[mask]]\n\n            squared_batch_counts_sum = np.array(batch_counts.power(2).sum(axis=0))\n            batch_counts_sum = np.array(batch_counts.sum(axis=0))\n        else:\n            clip_val_broad = np.broadcast_to(clip_val, batch_counts.shape)\n            np.putmask(\n                batch_counts,\n                batch_counts > clip_val_broad,\n                clip_val_broad,\n            )\n\n            squared_batch_counts_sum = np.square(batch_counts).sum(axis=0)\n            batch_counts_sum = batch_counts.sum(axis=0)\n\n        norm_gene_var = (1 / ((N - 1) * np.square(reg_std))) * (\n            (N * np.square(mean))\n            + squared_batch_counts_sum\n            - 2 * batch_counts_sum * mean\n        )\n        norm_gene_vars.append(norm_gene_var.reshape(1, -1))\n\n    norm_gene_vars = np.concatenate(norm_gene_vars, axis=0)\n    # argsort twice gives ranks, small rank means most variable\n    ranked_norm_gene_vars = np.argsort(np.argsort(-norm_gene_vars, axis=1), axis=1)\n\n    # this is done in SelectIntegrationFeatures() in Seurat v3\n    ranked_norm_gene_vars = ranked_norm_gene_vars.astype(np.float32)\n    num_batches_high_var = np.sum(\n        (ranked_norm_gene_vars < n_top_genes).astype(int), axis=0\n    )\n    ranked_norm_gene_vars[ranked_norm_gene_vars >= n_top_genes] = np.nan\n    ma_ranked = np.ma.masked_invalid(ranked_norm_gene_vars)\n    median_ranked = np.ma.median(ma_ranked, axis=0).filled(np.nan)\n\n    df['highly_variable_nbatches'] = num_batches_high_var\n    df['highly_variable_rank'] = median_ranked\n    df['variances_norm'] = np.mean(norm_gene_vars, axis=0)\n\n    sorted_index = (\n        df[['highly_variable_rank', 'highly_variable_nbatches']]\n        .sort_values(\n            ['highly_variable_rank', 'highly_variable_nbatches'],\n            ascending=[True, False],\n            na_position='last',\n        )\n        .index\n    )\n    df['highly_variable'] = False\n    df.loc[sorted_index[: int(n_top_genes)], 'highly_variable'] = True\n\n    if inplace or subset:\n        adata.uns['hvg'] = {'flavor': 'seurat_v3'}\n        logg.hint(\n            'added\\n'\n            '    \\'highly_variable\\', boolean vector (adata.var)\\n'\n            '    \\'highly_variable_rank\\', float vector (adata.var)\\n'\n            '    \\'means\\', float vector (adata.var)\\n'\n            '    \\'variances\\', float vector (adata.var)\\n'\n            '    \\'variances_norm\\', float vector (adata.var)'\n        )\n        adata.var['highly_variable'] = df['highly_variable'].values\n        adata.var['highly_variable_rank'] = df['highly_variable_rank'].values\n        adata.var['means'] = df['means'].values\n        adata.var['variances'] = df['variances'].values\n        adata.var['variances_norm'] = df['variances_norm'].values.astype(\n            'float64', copy=False\n        )\n        if batch_key is not None:\n            adata.var['highly_variable_nbatches'] = df[\n                'highly_variable_nbatches'\n            ].values\n        if subset:\n            adata._inplace_subset_var(df['highly_variable'].values)\n    else:\n        if batch_key is None:\n            df = df.drop(['highly_variable_nbatches'], axis=1)\n        return df", "idx": 325}
{"project": "Scanpy", "commit_id": "395_scanpy_1.9.0__highly_variable_genes.py__highly_variable_genes_single_batch.py", "target": 1, "func": "def _highly_variable_genes_single_batch(\n    adata: AnnData,\n    layer: Optional[str] = None,\n    min_disp: Optional[float] = 0.5,\n    max_disp: Optional[float] = np.inf,\n    min_mean: Optional[float] = 0.0125,\n    max_mean: Optional[float] = 3,\n    n_top_genes: Optional[int] = None,\n    n_bins: int = 20,\n    flavor: Literal['seurat', 'cell_ranger'] = 'seurat',\n) -> pd.DataFrame:\n    \"\"\"\\\n    See `highly_variable_genes`.\n\n    Returns\n    -------\n    A DataFrame that contains the columns\n    `highly_variable`, `means`, `dispersions`, and `dispersions_norm`.\n    \"\"\"\n    X = adata.layers[layer] if layer is not None else adata.X\n    if flavor == 'seurat':\n        if 'log1p' in adata.uns_keys() and adata.uns['log1p']['base'] is not None:\n            X *= np.log(adata.uns['log1p']['base'])\n        X = np.expm1(X)\n\n    mean, var = materialize_as_ndarray(_get_mean_var(X))\n    # now actually compute the dispersion\n    mean[mean == 0] = 1e-12  # set entries equal to zero to small value\n    dispersion = var / mean\n    if flavor == 'seurat':  # logarithmized mean as in Seurat\n        dispersion[dispersion == 0] = np.nan\n        dispersion = np.log(dispersion)\n        mean = np.log1p(mean)\n    # all of the following quantities are \"per-gene\" here\n    df = pd.DataFrame()\n    df['means'] = mean\n    df['dispersions'] = dispersion\n    if flavor == 'seurat':\n        df['mean_bin'] = pd.cut(df['means'], bins=n_bins)\n        disp_grouped = df.groupby('mean_bin')['dispersions']\n        disp_mean_bin = disp_grouped.mean()\n        disp_std_bin = disp_grouped.std(ddof=1)\n        # retrieve those genes that have nan std, these are the ones where\n        # only a single gene fell in the bin and implicitly set them to have\n        # a normalized disperion of 1\n        one_gene_per_bin = disp_std_bin.isnull()\n        gen_indices = np.where(one_gene_per_bin[df['mean_bin'].values])[0].tolist()\n        if len(gen_indices) > 0:\n            logg.debug(\n                f'Gene indices {gen_indices} fell into a single bin: their '\n                'normalized dispersion was set to 1.\\n    '\n                'Decreasing `n_bins` will likely avoid this effect.'\n            )\n        # Circumvent pandas 0.23 bug. Both sides of the assignment have dtype==float32,\n        # but there\u2019s still a dtype error without \u201c.value\u201d.\n        disp_std_bin[one_gene_per_bin.values] = disp_mean_bin[\n            one_gene_per_bin.values\n        ].values\n        disp_mean_bin[one_gene_per_bin.values] = 0\n        # actually do the normalization\n        df['dispersions_norm'] = (\n            df['dispersions'].values  # use values here as index differs\n            - disp_mean_bin[df['mean_bin'].values].values\n        ) / disp_std_bin[df['mean_bin'].values].values\n    elif flavor == 'cell_ranger':\n        from statsmodels import robust\n\n        df['mean_bin'] = pd.cut(\n            df['means'],\n            np.r_[-np.inf, np.percentile(df['means'], np.arange(10, 105, 5)), np.inf],\n        )\n        disp_grouped = df.groupby('mean_bin')['dispersions']\n        disp_median_bin = disp_grouped.median()\n        # the next line raises the warning: \"Mean of empty slice\"\n        with warnings.catch_warnings():\n            warnings.simplefilter('ignore')\n            disp_mad_bin = disp_grouped.apply(robust.mad)\n            df['dispersions_norm'] = (\n                df['dispersions'].values - disp_median_bin[df['mean_bin'].values].values\n            ) / disp_mad_bin[df['mean_bin'].values].values\n    else:\n        raise ValueError('`flavor` needs to be \"seurat\" or \"cell_ranger\"')\n    dispersion_norm = df['dispersions_norm'].values\n    if n_top_genes is not None:\n        dispersion_norm = dispersion_norm[~np.isnan(dispersion_norm)]\n        dispersion_norm[\n            ::-1\n        ].sort()  # interestingly, np.argpartition is slightly slower\n        if n_top_genes > adata.n_vars:\n            logg.info('`n_top_genes` > `adata.n_var`, returning all genes.')\n            n_top_genes = adata.n_vars\n        disp_cut_off = dispersion_norm[n_top_genes - 1]\n        gene_subset = np.nan_to_num(df['dispersions_norm'].values) >= disp_cut_off\n        logg.debug(\n            f'the {n_top_genes} top genes correspond to a '\n            f'normalized dispersion cutoff of {disp_cut_off}'\n        )\n    else:\n        dispersion_norm[np.isnan(dispersion_norm)] = 0  # similar to Seurat\n        gene_subset = np.logical_and.reduce(\n            (\n                mean > min_mean,\n                mean < max_mean,\n                dispersion_norm > min_disp,\n                dispersion_norm < max_disp,\n            )\n        )\n\n    df['highly_variable'] = gene_subset\n    return df", "idx": 326}
{"project": "Scanpy", "commit_id": "396_scanpy_1.9.0__highly_variable_genes.py_highly_variable_genes.py", "target": 1, "func": "def highly_variable_genes(\n    adata: AnnData,\n    layer: Optional[str] = None,\n    n_top_genes: Optional[int] = None,\n    min_disp: Optional[float] = 0.5,\n    max_disp: Optional[float] = np.inf,\n    min_mean: Optional[float] = 0.0125,\n    max_mean: Optional[float] = 3,\n    span: Optional[float] = 0.3,\n    n_bins: int = 20,\n    flavor: Literal['seurat', 'cell_ranger', 'seurat_v3'] = 'seurat',\n    subset: bool = False,\n    inplace: bool = True,\n    batch_key: Optional[str] = None,\n    check_values: bool = True,\n) -> Optional[pd.DataFrame]:\n    \"\"\"\\\n    Annotate highly variable genes [Satija15]_ [Zheng17]_ [Stuart19]_.\n\n    Expects logarithmized data, except when `flavor='seurat_v3'`, in which count\n    data is expected.\n\n    Depending on `flavor`, this reproduces the R-implementations of Seurat\n    [Satija15]_, Cell Ranger [Zheng17]_, and Seurat v3 [Stuart19]_.\n\n    For the dispersion-based methods ([Satija15]_ and [Zheng17]_), the normalized\n    dispersion is obtained by scaling with the mean and standard deviation of\n    the dispersions for genes falling into a given bin for mean expression of\n    genes. This means that for each bin of mean expression, highly variable\n    genes are selected.\n\n    For [Stuart19]_, a normalized variance for each gene is computed. First, the data\n    are standardized (i.e., z-score normalization per feature) with a regularized\n    standard deviation. Next, the normalized variance is computed as the variance\n    of each gene after the transformation. Genes are ranked by the normalized variance.\n\n    See also `scanpy.experimental.pp._highly_variable_genes` for additional flavours\n    (e.g. Pearson residuals).\n\n    Parameters\n    ----------\n    adata\n        The annotated data matrix of shape `n_obs` \u00d7 `n_vars`. Rows correspond\n        to cells and columns to genes.\n    layer\n        If provided, use `adata.layers[layer]` for expression values instead of `adata.X`.\n    n_top_genes\n        Number of highly-variable genes to keep. Mandatory if `flavor='seurat_v3'`.\n    min_mean\n        If `n_top_genes` unequals `None`, this and all other cutoffs for the means and the\n        normalized dispersions are ignored. Ignored if `flavor='seurat_v3'`.\n    max_mean\n        If `n_top_genes` unequals `None`, this and all other cutoffs for the means and the\n        normalized dispersions are ignored. Ignored if `flavor='seurat_v3'`.\n    min_disp\n        If `n_top_genes` unequals `None`, this and all other cutoffs for the means and the\n        normalized dispersions are ignored. Ignored if `flavor='seurat_v3'`.\n    max_disp\n        If `n_top_genes` unequals `None`, this and all other cutoffs for the means and the\n        normalized dispersions are ignored. Ignored if `flavor='seurat_v3'`.\n    span\n        The fraction of the data (cells) used when estimating the variance in the loess\n        model fit if `flavor='seurat_v3'`.\n    n_bins\n        Number of bins for binning the mean gene expression. Normalization is\n        done with respect to each bin. If just a single gene falls into a bin,\n        the normalized dispersion is artificially set to 1. You'll be informed\n        about this if you set `settings.verbosity = 4`.\n    flavor\n        Choose the flavor for identifying highly variable genes. For the dispersion\n        based methods in their default workflows, Seurat passes the cutoffs whereas\n        Cell Ranger passes `n_top_genes`.\n    subset\n        Inplace subset to highly-variable genes if `True` otherwise merely indicate\n        highly variable genes.\n    inplace\n        Whether to place calculated metrics in `.var` or return them.\n    batch_key\n        If specified, highly-variable genes are selected within each batch separately and merged.\n        This simple process avoids the selection of batch-specific genes and acts as a\n        lightweight batch correction method. For all flavors, genes are first sorted\n        by how many batches they are a HVG. For dispersion-based flavors ties are broken\n        by normalized dispersion. If `flavor = 'seurat_v3'`, ties are broken by the median\n        (across batches) rank based on within-batch normalized variance.\n    check_values\n        Check if counts in selected layer are integers. A Warning is returned if set to True.\n        Only used if `flavor='seurat_v3'`.\n\n    Returns\n    -------\n    Depending on `inplace` returns calculated metrics (:class:`~pandas.DataFrame`) or\n    updates `.var` with the following fields\n\n    highly_variable : bool\n        boolean indicator of highly-variable genes\n    **means**\n        means per gene\n    **dispersions**\n        For dispersion-based flavors, dispersions per gene\n    **dispersions_norm**\n        For dispersion-based flavors, normalized dispersions per gene\n    **variances**\n        For `flavor='seurat_v3'`, variance per gene\n    **variances_norm**\n        For `flavor='seurat_v3'`, normalized variance per gene, averaged in\n        the case of multiple batches\n    highly_variable_rank : float\n        For `flavor='seurat_v3'`, rank of the gene according to normalized\n        variance, median rank in the case of multiple batches\n    highly_variable_nbatches : int\n        If batch_key is given, this denotes in how many batches genes are detected as HVG\n    highly_variable_intersection : bool\n        If batch_key is given, this denotes the genes that are highly variable in all batches\n\n    Notes\n    -----\n    This function replaces :func:`~scanpy.pp.filter_genes_dispersion`.\n    \"\"\"\n\n    if n_top_genes is not None and not all(\n        m is None for m in [min_disp, max_disp, min_mean, max_mean]\n    ):\n        logg.info('If you pass `n_top_genes`, all cutoffs are ignored.')\n\n    start = logg.info('extracting highly variable genes')\n\n    if not isinstance(adata, AnnData):\n        raise ValueError(\n            '`pp.highly_variable_genes` expects an `AnnData` argument, '\n            'pass `inplace=False` if you want to return a `pd.DataFrame`.'\n        )\n\n    if flavor == 'seurat_v3':\n        return _highly_variable_genes_seurat_v3(\n            adata,\n            layer=layer,\n            n_top_genes=n_top_genes,\n            batch_key=batch_key,\n            check_values=check_values,\n            span=span,\n            subset=subset,\n            inplace=inplace,\n        )\n\n    if batch_key is None:\n        df = _highly_variable_genes_single_batch(\n            adata,\n            layer=layer,\n            min_disp=min_disp,\n            max_disp=max_disp,\n            min_mean=min_mean,\n            max_mean=max_mean,\n            n_top_genes=n_top_genes,\n            n_bins=n_bins,\n            flavor=flavor,\n        )\n    else:\n        sanitize_anndata(adata)\n        batches = adata.obs[batch_key].cat.categories\n        df = []\n        gene_list = adata.var_names\n        for batch in batches:\n            adata_subset = adata[adata.obs[batch_key] == batch]\n\n            # Filter to genes that are in the dataset\n            with settings.verbosity.override(Verbosity.error):\n                filt = filter_genes(adata_subset, min_cells=1, inplace=False)[0]\n\n            adata_subset = adata_subset[:, filt]\n\n            hvg = _highly_variable_genes_single_batch(\n                adata_subset,\n                min_disp=min_disp,\n                max_disp=max_disp,\n                min_mean=min_mean,\n                max_mean=max_mean,\n                n_top_genes=n_top_genes,\n                n_bins=n_bins,\n                flavor=flavor,\n            )\n\n            # Add 0 values for genes that were filtered out\n            missing_hvg = pd.DataFrame(\n                np.zeros((np.sum(~filt), len(hvg.columns))),\n                columns=hvg.columns,\n            )\n            missing_hvg['highly_variable'] = missing_hvg['highly_variable'].astype(bool)\n            missing_hvg['gene'] = gene_list[~filt]\n            hvg['gene'] = adata_subset.var_names.values\n            hvg = hvg.append(missing_hvg, ignore_index=True)\n\n            # Order as before filtering\n            idxs = np.concatenate((np.where(filt)[0], np.where(~filt)[0]))\n            hvg = hvg.loc[np.argsort(idxs)]\n\n            df.append(hvg)\n\n        df = pd.concat(df, axis=0)\n        df['highly_variable'] = df['highly_variable'].astype(int)\n        df = df.groupby('gene').agg(\n            dict(\n                means=np.nanmean,\n                dispersions=np.nanmean,\n                dispersions_norm=np.nanmean,\n                highly_variable=np.nansum,\n            )\n        )\n        df.rename(\n            columns=dict(highly_variable='highly_variable_nbatches'), inplace=True\n        )\n        df['highly_variable_intersection'] = df['highly_variable_nbatches'] == len(\n            batches\n        )\n\n        if n_top_genes is not None:\n            # sort genes by how often they selected as hvg within each batch and\n            # break ties with normalized dispersion across batches\n            df.sort_values(\n                ['highly_variable_nbatches', 'dispersions_norm'],\n                ascending=False,\n                na_position='last',\n                inplace=True,\n            )\n            high_var = np.zeros(df.shape[0])\n            high_var[:n_top_genes] = True\n            df['highly_variable'] = high_var.astype(bool)\n            df = df.loc[adata.var_names, :]\n        else:\n            df = df.loc[adata.var_names]\n            dispersion_norm = df.dispersions_norm.values\n            dispersion_norm[np.isnan(dispersion_norm)] = 0  # similar to Seurat\n            gene_subset = np.logical_and.reduce(\n                (\n                    df.means > min_mean,\n                    df.means < max_mean,\n                    df.dispersions_norm > min_disp,\n                    df.dispersions_norm < max_disp,\n                )\n            )\n            df['highly_variable'] = gene_subset\n\n    logg.info('    finished', time=start)\n\n    if inplace or subset:\n        adata.uns['hvg'] = {'flavor': flavor}\n        logg.hint(\n            'added\\n'\n            '    \\'highly_variable\\', boolean vector (adata.var)\\n'\n            '    \\'means\\', float vector (adata.var)\\n'\n            '    \\'dispersions\\', float vector (adata.var)\\n'\n            '    \\'dispersions_norm\\', float vector (adata.var)'\n        )\n        adata.var['highly_variable'] = df['highly_variable'].values\n        adata.var['means'] = df['means'].values\n        adata.var['dispersions'] = df['dispersions'].values\n        adata.var['dispersions_norm'] = df['dispersions_norm'].values.astype(\n            'float32', copy=False\n        )\n        if batch_key is not None:\n            adata.var['highly_variable_nbatches'] = df[\n                'highly_variable_nbatches'\n            ].values\n            adata.var['highly_variable_intersection'] = df[\n                'highly_variable_intersection'\n            ].values\n        if subset:\n            adata._inplace_subset_var(df['highly_variable'].values)\n    else:\n        return df", "idx": 327}
{"project": "Scanpy", "commit_id": "397_scanpy_1.9.0__normalization.py__normalize_data.py", "target": 0, "func": "def _normalize_data(X, counts, after=None, copy=False):\n    X = X.copy() if copy else X\n    if issubclass(X.dtype.type, (int, np.integer)):\n        X = X.astype(np.float32)  # TODO: Check if float64 should be used\n    if isinstance(counts, DaskArray):\n        counts_greater_than_zero = counts[counts > 0].compute_chunk_sizes()\n    else:\n        counts_greater_than_zero = counts[counts > 0]\n\n    after = np.median(counts_greater_than_zero, axis=0) if after is None else after\n    counts += counts == 0\n    counts = counts / after\n    if issparse(X):\n        sparsefuncs.inplace_row_scale(X, 1 / counts)\n    elif isinstance(counts, np.ndarray):\n        np.divide(X, counts[:, None], out=X)\n    else:\n        X = np.divide(X, counts[:, None])  # dask does not support kwarg \"out\"\n    return X", "idx": 328}
{"project": "Scanpy", "commit_id": "398_scanpy_1.9.0__normalization.py_normalize_total.py", "target": 1, "func": "def normalize_total(\n    adata: AnnData,\n    target_sum: Optional[float] = None,\n    exclude_highly_expressed: bool = False,\n    max_fraction: float = 0.05,\n    key_added: Optional[str] = None,\n    layer: Optional[str] = None,\n    layers: Union[Literal['all'], Iterable[str]] = None,\n    layer_norm: Optional[str] = None,\n    inplace: bool = True,\n    copy: bool = False,\n) -> Optional[Dict[str, np.ndarray]]:\n    \"\"\"\\\n    Normalize counts per cell.\n\n    Normalize each cell by total counts over all genes,\n    so that every cell has the same total count after normalization.\n    If choosing `target_sum=1e6`, this is CPM normalization.\n\n    If `exclude_highly_expressed=True`, very highly expressed genes are excluded\n    from the computation of the normalization factor (size factor) for each\n    cell. This is meaningful as these can strongly influence the resulting\n    normalized values for all other genes [Weinreb17]_.\n\n    Similar functions are used, for example, by Seurat [Satija15]_, Cell Ranger\n    [Zheng17]_ or SPRING [Weinreb17]_.\n\n    Params\n    ------\n    adata\n        The annotated data matrix of shape `n_obs` \u00d7 `n_vars`.\n        Rows correspond to cells and columns to genes.\n    target_sum\n        If `None`, after normalization, each observation (cell) has a total\n        count equal to the median of total counts for observations (cells)\n        before normalization.\n    exclude_highly_expressed\n        Exclude (very) highly expressed genes for the computation of the\n        normalization factor (size factor) for each cell. A gene is considered\n        highly expressed, if it has more than `max_fraction` of the total counts\n        in at least one cell. The not-excluded genes will sum up to\n        `target_sum`.\n    max_fraction\n        If `exclude_highly_expressed=True`, consider cells as highly expressed\n        that have more counts than `max_fraction` of the original total counts\n        in at least one cell.\n    key_added\n        Name of the field in `adata.obs` where the normalization factor is\n        stored.\n    layer\n        Layer to normalize instead of `X`. If `None`, `X` is normalized.\n    inplace\n        Whether to update `adata` or return dictionary with normalized copies of\n        `adata.X` and `adata.layers`.\n    copy\n        Whether to modify copied input object. Not compatible with inplace=False.\n\n    Returns\n    -------\n    Returns dictionary with normalized copies of `adata.X` and `adata.layers`\n    or updates `adata` with normalized version of the original\n    `adata.X` and `adata.layers`, depending on `inplace`.\n\n    Example\n    --------\n    >>> from anndata import AnnData\n    >>> import scanpy as sc\n    >>> sc.settings.verbosity = 2\n    >>> np.set_printoptions(precision=2)\n    >>> adata = AnnData(np.array([\n    ...    [3, 3, 3, 6, 6],\n    ...    [1, 1, 1, 2, 2],\n    ...    [1, 22, 1, 2, 2],\n    ... ]))\n    >>> adata.X\n    array([[ 3.,  3.,  3.,  6.,  6.],\n           [ 1.,  1.,  1.,  2.,  2.],\n           [ 1., 22.,  1.,  2.,  2.]], dtype=float32)\n    >>> X_norm = sc.pp.normalize_total(adata, target_sum=1, inplace=False)['X']\n    >>> X_norm\n    array([[0.14, 0.14, 0.14, 0.29, 0.29],\n           [0.14, 0.14, 0.14, 0.29, 0.29],\n           [0.04, 0.79, 0.04, 0.07, 0.07]], dtype=float32)\n    >>> X_norm = sc.pp.normalize_total(\n    ...     adata, target_sum=1, exclude_highly_expressed=True,\n    ...     max_fraction=0.2, inplace=False\n    ... )['X']\n    The following highly-expressed genes are not considered during normalization factor computation:\n    ['1', '3', '4']\n    >>> X_norm\n    array([[ 0.5,  0.5,  0.5,  1. ,  1. ],\n           [ 0.5,  0.5,  0.5,  1. ,  1. ],\n           [ 0.5, 11. ,  0.5,  1. ,  1. ]], dtype=float32)\n    \"\"\"\n    if copy:\n        if not inplace:\n            raise ValueError(\"`copy=True` cannot be used with `inplace=False`.\")\n        adata = adata.copy()\n\n    if max_fraction < 0 or max_fraction > 1:\n        raise ValueError('Choose max_fraction between 0 and 1.')\n\n    # Deprecated features\n    if layers is not None:\n        warn(\n            FutureWarning(\n                \"The `layers` argument is deprecated. Instead, specify individual \"\n                \"layers to normalize with `layer`.\"\n            )\n        )\n    if layer_norm is not None:\n        warn(\n            FutureWarning(\n                \"The `layer_norm` argument is deprecated. Specify the target size \"\n                \"factor directly with `target_sum`.\"\n            )\n        )\n\n    if layers == 'all':\n        layers = adata.layers.keys()\n    elif isinstance(layers, str):\n        raise ValueError(\n            f\"`layers` needs to be a list of strings or 'all', not {layers!r}\"\n        )\n\n    view_to_actual(adata)\n\n    X = _get_obs_rep(adata, layer=layer)\n\n    gene_subset = None\n    msg = 'normalizing counts per cell'\n    if exclude_highly_expressed:\n        counts_per_cell = X.sum(1)  # original counts per cell\n        counts_per_cell = np.ravel(counts_per_cell)\n\n        # at least one cell as more than max_fraction of counts per cell\n\n        gene_subset = (X > counts_per_cell[:, None] * max_fraction).sum(0)\n        gene_subset = np.ravel(gene_subset) == 0\n\n        msg += (\n            ' The following highly-expressed genes are not considered during '\n            f'normalization factor computation:\\n{adata.var_names[~gene_subset].tolist()}'\n        )\n        counts_per_cell = X[:, gene_subset].sum(1)\n    else:\n        counts_per_cell = X.sum(1)\n    start = logg.info(msg)\n    counts_per_cell = np.ravel(counts_per_cell)\n\n    cell_subset = counts_per_cell > 0\n    if not np.all(cell_subset):\n        warn(UserWarning('Some cells have zero counts'))\n\n    if inplace:\n        if key_added is not None:\n            adata.obs[key_added] = counts_per_cell\n        _set_obs_rep(\n            adata, _normalize_data(X, counts_per_cell, target_sum), layer=layer\n        )\n    else:\n        # not recarray because need to support sparse\n        dat = dict(\n            X=_normalize_data(X, counts_per_cell, target_sum, copy=True),\n            norm_factor=counts_per_cell,\n        )\n\n    # Deprecated features\n    if layer_norm == 'after':\n        after = target_sum\n    elif layer_norm == 'X':\n        after = np.median(counts_per_cell[cell_subset])\n    elif layer_norm is None:\n        after = None\n    else:\n        raise ValueError('layer_norm should be \"after\", \"X\" or None')\n\n    for layer_to_norm in layers if layers is not None else ():\n        res = normalize_total(\n            adata, layer=layer_to_norm, target_sum=after, inplace=inplace\n        )\n        if not inplace:\n            dat[layer_to_norm] = res[\"X\"]\n\n    logg.info(\n        '    finished ({time_passed})',\n        time=start,\n    )\n    if key_added is not None:\n        logg.debug(\n            f'and added {key_added!r}, counts per cell before normalization (adata.obs)'\n        )\n\n    if copy:\n        return adata\n    elif not inplace:\n        return dat", "idx": 329}
{"project": "Scanpy", "commit_id": "399_scanpy_1.9.0__pca.py_pca.py", "target": 1, "func": "def pca(\n    data: Union[AnnData, np.ndarray, spmatrix],\n    n_comps: Optional[int] = None,\n    zero_center: Optional[bool] = True,\n    svd_solver: str = 'arpack',\n    random_state: AnyRandom = 0,\n    return_info: bool = False,\n    use_highly_variable: Optional[bool] = None,\n    dtype: str = 'float32',\n    copy: bool = False,\n    chunked: bool = False,\n    chunk_size: Optional[int] = None,\n) -> Union[AnnData, np.ndarray, spmatrix]:\n    \"\"\"\\\n    Principal component analysis [Pedregosa11]_.\n\n    Computes PCA coordinates, loadings and variance decomposition.\n    Uses the implementation of *scikit-learn* [Pedregosa11]_.\n\n    .. versionchanged:: 1.5.0\n\n        In previous versions, computing a PCA on a sparse matrix would make a dense copy of\n        the array for mean centering.\n        As of scanpy 1.5.0, mean centering is implicit.\n        While results are extremely similar, they are not exactly the same.\n        If you would like to reproduce the old results, pass a dense array.\n\n    Parameters\n    ----------\n    data\n        The (annotated) data matrix of shape `n_obs` \u00d7 `n_vars`.\n        Rows correspond to cells and columns to genes.\n    n_comps\n        Number of principal components to compute. Defaults to 50, or 1 - minimum\n        dimension size of selected representation.\n    zero_center\n        If `True`, compute standard PCA from covariance matrix.\n        If `False`, omit zero-centering variables\n        (uses :class:`~sklearn.decomposition.TruncatedSVD`),\n        which allows to handle sparse input efficiently.\n        Passing `None` decides automatically based on sparseness of the data.\n    svd_solver\n        SVD solver to use:\n\n        `'arpack'` (the default)\n          for the ARPACK wrapper in SciPy (:func:`~scipy.sparse.linalg.svds`)\n        `'randomized'`\n          for the randomized algorithm due to Halko (2009).\n        `'auto'`\n          chooses automatically depending on the size of the problem.\n        `'lobpcg'`\n          An alternative SciPy solver.\n\n        .. versionchanged:: 1.4.5\n           Default value changed from `'auto'` to `'arpack'`.\n\n        Efficient computation of the principal components of a sparse matrix\n        currently only works with the `'arpack`' or `'lobpcg'` solvers.\n\n    random_state\n        Change to use different initial states for the optimization.\n    return_info\n        Only relevant when not passing an :class:`~anndata.AnnData`:\n        see \u201c**Returns**\u201d.\n    use_highly_variable\n        Whether to use highly variable genes only, stored in\n        `.var['highly_variable']`.\n        By default uses them if they have been determined beforehand.\n    dtype\n        Numpy data type string to which to convert the result.\n    copy\n        If an :class:`~anndata.AnnData` is passed, determines whether a copy\n        is returned. Is ignored otherwise.\n    chunked\n        If `True`, perform an incremental PCA on segments of `chunk_size`.\n        The incremental PCA automatically zero centers and ignores settings of\n        `random_seed` and `svd_solver`. If `False`, perform a full PCA.\n    chunk_size\n        Number of observations to include in each chunk.\n        Required if `chunked=True` was passed.\n\n    Returns\n    -------\n    X_pca : :class:`~scipy.sparse.spmatrix`, :class:`~numpy.ndarray`\n        If `data` is array-like and `return_info=False` was passed,\n        this function only returns `X_pca`\u2026\n    adata : anndata.AnnData\n        \u2026otherwise if `copy=True` it returns or else adds fields to `adata`:\n\n        `.obsm['X_pca']`\n             PCA representation of data.\n        `.varm['PCs']`\n             The principal components containing the loadings.\n        `.uns['pca']['variance_ratio']`\n             Ratio of explained variance.\n        `.uns['pca']['variance']`\n             Explained variance, equivalent to the eigenvalues of the\n             covariance matrix.\n    \"\"\"\n    logg_start = logg.info('computing PCA')\n\n    # chunked calculation is not randomized, anyways\n    if svd_solver in {'auto', 'randomized'} and not chunked:\n        logg.info(\n            'Note that scikit-learn\\'s randomized PCA might not be exactly '\n            'reproducible across different computational platforms. For exact '\n            'reproducibility, choose `svd_solver=\\'arpack\\'.`'\n        )\n    data_is_AnnData = isinstance(data, AnnData)\n    if data_is_AnnData:\n        adata = data.copy() if copy else data\n    else:\n        adata = AnnData(data, dtype=data.dtype)\n\n    if use_highly_variable is True and 'highly_variable' not in adata.var.keys():\n        raise ValueError(\n            'Did not find adata.var[\\'highly_variable\\']. '\n            'Either your data already only consists of highly-variable genes '\n            'or consider running `pp.highly_variable_genes` first.'\n        )\n    if use_highly_variable is None:\n        use_highly_variable = True if 'highly_variable' in adata.var.keys() else False\n    if use_highly_variable:\n        logg.info('    on highly variable genes')\n    adata_comp = (\n        adata[:, adata.var['highly_variable']] if use_highly_variable else adata\n    )\n\n    if n_comps is None:\n        min_dim = min(adata_comp.n_vars, adata_comp.n_obs)\n        if settings.N_PCS >= min_dim:\n            n_comps = min_dim - 1\n        else:\n            n_comps = settings.N_PCS\n\n    logg.info(f'    with n_comps={n_comps}')\n\n    random_state = check_random_state(random_state)\n\n    X = adata_comp.X\n\n    if chunked:\n        if not zero_center or random_state or svd_solver != 'arpack':\n            logg.debug('Ignoring zero_center, random_state, svd_solver')\n\n        from sklearn.decomposition import IncrementalPCA\n\n        X_pca = np.zeros((X.shape[0], n_comps), X.dtype)\n\n        pca_ = IncrementalPCA(n_components=n_comps)\n\n        for chunk, _, _ in adata_comp.chunked_X(chunk_size):\n            chunk = chunk.toarray() if issparse(chunk) else chunk\n            pca_.partial_fit(chunk)\n\n        for chunk, start, end in adata_comp.chunked_X(chunk_size):\n            chunk = chunk.toarray() if issparse(chunk) else chunk\n            X_pca[start:end] = pca_.transform(chunk)\n    elif (not issparse(X) or svd_solver == \"randomized\") and zero_center:\n        from sklearn.decomposition import PCA\n\n        if issparse(X) and svd_solver == \"randomized\":\n            # This  is for backwards compat. Better behaviour would be to either error or use arpack.\n            logg.warning(\n                \"svd_solver 'randomized' does not work with sparse input. Densifying the array. \"\n                \"This may take a very large amount of memory.\"\n            )\n            X = X.toarray()\n        pca_ = PCA(\n            n_components=n_comps, svd_solver=svd_solver, random_state=random_state\n        )\n        X_pca = pca_.fit_transform(X)\n    elif issparse(X) and zero_center:\n        from sklearn.decomposition import PCA\n\n        if svd_solver == \"auto\":\n            svd_solver = \"arpack\"\n        if svd_solver not in {'lobpcg', 'arpack'}:\n            raise ValueError(\n                'svd_solver: {svd_solver} can not be used with sparse input.\\n'\n                'Use \"arpack\" (the default) or \"lobpcg\" instead.'\n            )\n\n        output = _pca_with_sparse(\n            X, n_comps, solver=svd_solver, random_state=random_state\n        )\n        # this is just a wrapper for the results\n        X_pca = output['X_pca']\n        pca_ = PCA(n_components=n_comps, svd_solver=svd_solver)\n        pca_.components_ = output['components']\n        pca_.explained_variance_ = output['variance']\n        pca_.explained_variance_ratio_ = output['variance_ratio']\n    elif not zero_center:\n        from sklearn.decomposition import TruncatedSVD\n\n        logg.debug(\n            '    without zero-centering: \\n'\n            '    the explained variance does not correspond to the exact statistical defintion\\n'\n            '    the first component, e.g., might be heavily influenced by different means\\n'\n            '    the following components often resemble the exact PCA very closely'\n        )\n        pca_ = TruncatedSVD(\n            n_components=n_comps, random_state=random_state, algorithm=svd_solver\n        )\n        X_pca = pca_.fit_transform(X)\n    else:\n        raise Exception(\"This shouldn't happen. Please open a bug report.\")\n\n    if X_pca.dtype.descr != np.dtype(dtype).descr:\n        X_pca = X_pca.astype(dtype)\n\n    if data_is_AnnData:\n        adata.obsm['X_pca'] = X_pca\n        adata.uns['pca'] = {}\n        adata.uns['pca']['params'] = {\n            'zero_center': zero_center,\n            'use_highly_variable': use_highly_variable,\n        }\n        if use_highly_variable:\n            adata.varm['PCs'] = np.zeros(shape=(adata.n_vars, n_comps))\n            adata.varm['PCs'][adata.var['highly_variable']] = pca_.components_.T\n        else:\n            adata.varm['PCs'] = pca_.components_.T\n        adata.uns['pca']['variance'] = pca_.explained_variance_\n        adata.uns['pca']['variance_ratio'] = pca_.explained_variance_ratio_\n        logg.info('    finished', time=logg_start)\n        logg.debug(\n            'and added\\n'\n            '    \\'X_pca\\', the PCA coordinates (adata.obs)\\n'\n            '    \\'PC1\\', \\'PC2\\', ..., the loadings (adata.var)\\n'\n            '    \\'pca_variance\\', the variance / eigenvalues (adata.uns)\\n'\n            '    \\'pca_variance_ratio\\', the variance ratio (adata.uns)'\n        )\n        return adata if copy else None\n    else:\n        logg.info('    finished', time=logg_start)\n        if return_info:\n            return (\n                X_pca,\n                pca_.components_,\n                pca_.explained_variance_ratio_,\n                pca_.explained_variance_,\n            )\n        else:\n            return X_pca", "idx": 330}
{"project": "Scanpy", "commit_id": "39_scanpy_1.9.0_logging.py_print_header.py", "target": 0, "func": "def print_header(*, file=None):\n    \"\"\"\\\n    Versions that might influence the numerical results.\n    Matplotlib and Seaborn are excluded from this.\n    \"\"\"\n\n    modules = ['scanpy'] + _DEPENDENCIES_NUMERICS\n    print(\n        ' '.join(f'{mod}=={ver}' for mod, ver in _versions_dependencies(modules)),\n        file=file or sys.stdout,", "idx": 331}
{"project": "Scanpy", "commit_id": "3_scanpy_1.9.0_conftest.py__pbmc3k_normalized.py", "target": 0, "func": "def _pbmc3k_normalized():\n    import scanpy as sc\n\n    pbmc = sc.datasets.pbmc3k()\n    pbmc.X = pbmc.X.astype(\"float64\")  # For better accuracy\n    sc.pp.filter_genes(pbmc, min_counts=1)\n    sc.pp.log1p(pbmc)\n    sc.pp.normalize_total(pbmc)\n    sc.pp.highly_variable_genes(pbmc)\n    return pbmc", "idx": 332}
{"project": "Scanpy", "commit_id": "400_scanpy_1.9.0__pca.py__pca_with_sparse.py", "target": 0, "func": "def _pca_with_sparse(X, npcs, solver='arpack', mu=None, random_state=None):\n    random_state = check_random_state(random_state)\n    np.random.set_state(random_state.get_state())\n    random_init = np.random.rand(np.min(X.shape))\n    X = check_array(X, accept_sparse=['csr', 'csc'])\n\n    if mu is None:\n        mu = X.mean(0).A.flatten()[None, :]\n    mdot = mu.dot\n    mmat = mdot\n    mhdot = mu.T.dot\n    mhmat = mu.T.dot\n    Xdot = X.dot\n    Xmat = Xdot\n    XHdot = X.T.conj().dot\n    XHmat = XHdot\n    ones = np.ones(X.shape[0])[None, :].dot\n\n    def matvec(x):\n        return Xdot(x) - mdot(x)\n\n    def matmat(x):\n        return Xmat(x) - mmat(x)\n\n    def rmatvec(x):\n        return XHdot(x) - mhdot(ones(x))\n\n    def rmatmat(x):\n        return XHmat(x) - mhmat(ones(x))\n\n    XL = LinearOperator(\n        matvec=matvec,\n        dtype=X.dtype,\n        matmat=matmat,\n        shape=X.shape,\n        rmatvec=rmatvec,\n        rmatmat=rmatmat,\n    )\n\n    u, s, v = svds(XL, solver=solver, k=npcs, v0=random_init)\n    u, v = svd_flip(u, v)\n    idx = np.argsort(-s)\n    v = v[idx, :]\n\n    X_pca = (u * s)[:, idx]\n    ev = s[idx] ** 2 / (X.shape[0] - 1)\n\n    total_var = _get_mean_var(X)[1].sum()\n    ev_ratio = ev / total_var\n\n    output = {\n        'X_pca': X_pca,\n        'variance': ev,\n        'variance_ratio': ev_ratio,\n        'components': v,\n    }\n    return output", "idx": 333}
{"project": "Scanpy", "commit_id": "401_scanpy_1.9.0__pca.py_matvec.py", "target": 0, "func": "def matvec(x):\n        return Xdot(x) - mdot(x)", "idx": 334}
{"project": "Scanpy", "commit_id": "402_scanpy_1.9.0__pca.py_matmat.py", "target": 0, "func": "def matmat(x):\n        return Xmat(x) - mmat(x)", "idx": 335}
{"project": "Scanpy", "commit_id": "403_scanpy_1.9.0__pca.py_rmatvec.py", "target": 0, "func": "def rmatvec(x):\n        return XHdot(x) - mhdot(ones(x))", "idx": 336}
{"project": "Scanpy", "commit_id": "404_scanpy_1.9.0__pca.py_rmatmat.py", "target": 0, "func": "def rmatmat(x):\n        return XHmat(x) - mhmat(ones(x))", "idx": 337}
{"project": "Scanpy", "commit_id": "405_scanpy_1.9.0__qc.py__choose_mtx_rep.py", "target": 0, "func": "def _choose_mtx_rep(adata, use_raw=False, layer=None):\n    is_layer = layer is not None\n    if use_raw and is_layer:\n        raise ValueError(\n            \"Cannot use expression from both layer and raw. You provided:\"\n            f\"'use_raw={use_raw}' and 'layer={layer}'\"\n        )\n    if is_layer:\n        return adata.layers[layer]\n    elif use_raw:\n        return adata.raw.X\n    else:\n        return adata.X", "idx": 338}
{"project": "Scanpy", "commit_id": "406_scanpy_1.9.0__qc.py_describe_obs.py", "target": 0, "func": "def describe_obs(\n    adata: AnnData,\n    *,\n    expr_type: str = \"counts\",\n    var_type: str = \"genes\",\n    qc_vars: Collection[str] = (),\n    percent_top: Optional[Collection[int]] = (50, 100, 200, 500),\n    layer: Optional[str] = None,\n    use_raw: bool = False,\n    log1p: Optional[str] = True,\n    inplace: bool = False,\n    X=None,\n    parallel=None,\n) -> Optional[pd.DataFrame]:\n    \"\"\"\\\n    Describe observations of anndata.\n\n    Calculates a number of qc metrics for observations in AnnData object. See\n    section `Returns` for a description of those metrics.\n\n    Note that this method can take a while to compile on the first call. That\n    result is then cached to disk to be used later.\n\n    Params\n    ------\n    {doc_adata_basic}\n    {doc_qc_metric_naming}\n    {doc_obs_qc_args}\n    {doc_expr_reps}\n    log1p\n        Add `log1p` transformed metrics.\n    inplace\n        Whether to place calculated metrics in `adata.obs`.\n    X\n        Matrix to calculate values on. Meant for internal usage.\n\n    Returns\n    -------\n    QC metrics for observations in adata. If inplace, values are placed into\n    the AnnData's `.obs` dataframe.\n\n    {doc_obs_qc_returns}\n    \"\"\"\n    if parallel is not None:\n        warn(\n            \"Argument `parallel` is deprecated, and currently has no effect.\",\n            FutureWarning,\n        )\n    # Handle whether X is passed\n    if X is None:\n        X = _choose_mtx_rep(adata, use_raw, layer)\n        if isspmatrix_coo(X):\n            X = csr_matrix(X)  # COO not subscriptable\n        if issparse(X):\n            X.eliminate_zeros()\n    obs_metrics = pd.DataFrame(index=adata.obs_names)\n    if issparse(X):\n        obs_metrics[f\"n_{var_type}_by_{expr_type}\"] = X.getnnz(axis=1)\n    else:\n        obs_metrics[f\"n_{var_type}_by_{expr_type}\"] = np.count_nonzero(X, axis=1)\n    if log1p:\n        obs_metrics[f\"log1p_n_{var_type}_by_{expr_type}\"] = np.log1p(\n            obs_metrics[f\"n_{var_type}_by_{expr_type}\"]\n        )\n    obs_metrics[f\"total_{expr_type}\"] = np.ravel(X.sum(axis=1))\n    if log1p:\n        obs_metrics[f\"log1p_total_{expr_type}\"] = np.log1p(\n            obs_metrics[f\"total_{expr_type}\"]\n        )\n    if percent_top:\n        percent_top = sorted(percent_top)\n        proportions = top_segment_proportions(X, percent_top)\n        for i, n in enumerate(percent_top):\n            obs_metrics[f\"pct_{expr_type}_in_top_{n}_{var_type}\"] = (\n                proportions[:, i] * 100\n            )\n    for qc_var in qc_vars:\n        obs_metrics[f\"total_{expr_type}_{qc_var}\"] = np.ravel(\n            X[:, adata.var[qc_var].values].sum(axis=1)\n        )\n        if log1p:\n            obs_metrics[f\"log1p_total_{expr_type}_{qc_var}\"] = np.log1p(\n                obs_metrics[f\"total_{expr_type}_{qc_var}\"]\n            )\n        obs_metrics[f\"pct_{expr_type}_{qc_var}\"] = (\n            obs_metrics[f\"total_{expr_type}_{qc_var}\"]\n            / obs_metrics[f\"total_{expr_type}\"]\n            * 100\n        )\n    if inplace:\n        adata.obs[obs_metrics.columns] = obs_metrics\n    else:\n        return obs_metrics", "idx": 339}
{"project": "Scanpy", "commit_id": "407_scanpy_1.9.0__qc.py_describe_var.py", "target": 0, "func": "def describe_var(\n    adata: AnnData,\n    *,\n    expr_type: str = \"counts\",\n    var_type: str = \"genes\",\n    layer: Optional[str] = None,\n    use_raw: bool = False,\n    inplace=False,\n    log1p=True,\n    X=None,\n) -> Optional[pd.DataFrame]:\n    \"\"\"\\\n    Describe variables of anndata.\n\n    Calculates a number of qc metrics for variables in AnnData object. See\n    section `Returns` for a description of those metrics.\n\n    Params\n    ------\n    {doc_adata_basic}\n    {doc_qc_metric_naming}\n    {doc_expr_reps}\n    inplace\n        Whether to place calculated metrics in `adata.var`.\n    X\n        Matrix to calculate values on. Meant for internal usage.\n\n    Returns\n    -------\n    QC metrics for variables in adata. If inplace, values are placed into the\n    AnnData's `.var` dataframe.\n\n    {doc_var_qc_returns}\n    \"\"\"\n    # Handle whether X is passed\n    if X is None:\n        X = _choose_mtx_rep(adata, use_raw, layer)\n        if isspmatrix_coo(X):\n            X = csr_matrix(X)  # COO not subscriptable\n        if issparse(X):\n            X.eliminate_zeros()\n    var_metrics = pd.DataFrame(index=adata.var_names)\n    if issparse(X):\n        # Current memory bottleneck for csr matrices:\n        var_metrics[\"n_cells_by_{expr_type}\"] = X.getnnz(axis=0)\n        var_metrics[\"mean_{expr_type}\"] = mean_variance_axis(X, axis=0)[0]\n    else:\n        var_metrics[\"n_cells_by_{expr_type}\"] = np.count_nonzero(X, axis=0)\n        var_metrics[\"mean_{expr_type}\"] = X.mean(axis=0)\n    if log1p:\n        var_metrics[\"log1p_mean_{expr_type}\"] = np.log1p(\n            var_metrics[\"mean_{expr_type}\"]\n        )\n    var_metrics[\"pct_dropout_by_{expr_type}\"] = (\n        1 - var_metrics[\"n_cells_by_{expr_type}\"] / X.shape[0]\n    ) * 100\n    var_metrics[\"total_{expr_type}\"] = np.ravel(X.sum(axis=0))\n    if log1p:\n        var_metrics[\"log1p_total_{expr_type}\"] = np.log1p(\n            var_metrics[\"total_{expr_type}\"]\n        )\n    # Relabel\n    new_colnames = []\n    for col in var_metrics.columns:\n        new_colnames.append(col.format(**locals()))\n    var_metrics.columns = new_colnames\n    if inplace:\n        adata.var[var_metrics.columns] = var_metrics\n    else:\n        return var_metrics", "idx": 340}
{"project": "Scanpy", "commit_id": "408_scanpy_1.9.0__qc.py_calculate_qc_metrics.py", "target": 0, "func": "def calculate_qc_metrics(\n    adata: AnnData,\n    *,\n    expr_type: str = \"counts\",\n    var_type: str = \"genes\",\n    qc_vars: Collection[str] = (),\n    percent_top: Optional[Collection[int]] = (50, 100, 200, 500),\n    layer: Optional[str] = None,\n    use_raw: bool = False,\n    inplace: bool = False,\n    log1p: bool = True,\n    parallel: Optional[bool] = None,\n) -> Optional[Tuple[pd.DataFrame, pd.DataFrame]]:\n    \"\"\"\\\n    Calculate quality control metrics.\n\n    Calculates a number of qc metrics for an AnnData object, see section\n    `Returns` for specifics. Largely based on `calculateQCMetrics` from scater\n    [McCarthy17]_. Currently is most efficient on a sparse CSR or dense matrix.\n\n    Note that this method can take a while to compile on the first call. That\n    result is then cached to disk to be used later.\n\n    Parameters\n    ----------\n    {doc_adata_basic}\n    {doc_qc_metric_naming}\n    {doc_obs_qc_args}\n    {doc_expr_reps}\n    inplace\n        Whether to place calculated metrics in `adata`'s `.obs` and `.var`.\n    log1p\n        Set to `False` to skip computing `log1p` transformed annotations.\n\n    Returns\n    -------\n    Depending on `inplace` returns calculated metrics\n    (as :class:`~pandas.DataFrame`) or updates `adata`'s `obs` and `var`.\n\n    {doc_obs_qc_returns}\n\n    {doc_var_qc_returns}\n\n    Example\n    -------\n    Calculate qc metrics for visualization.\n\n    .. plot::\n        :context: close-figs\n\n        import scanpy as sc\n        import seaborn as sns\n\n        pbmc = sc.datasets.pbmc3k()\n        pbmc.var[\"mito\"] = pbmc.var_names.str.startswith(\"MT-\")\n        sc.pp.calculate_qc_metrics(pbmc, qc_vars=[\"mito\"], inplace=True)\n        sns.jointplot(\n            data=pbmc.obs,\n            x=\"log1p_total_counts\",\n            y=\"log1p_n_genes_by_counts\",\n            kind=\"hex\",\n        )\n\n    .. plot::\n        :context: close-figs\n\n        sns.histplot(pbmc.obs[\"pct_counts_mito\"])\n    \"\"\"\n    if parallel is not None:\n        warn(\n            \"Argument `parallel` is deprecated, and currently has no effect.\",\n            FutureWarning,\n        )\n    # Pass X so I only have to do it once\n    X = _choose_mtx_rep(adata, use_raw, layer)\n    if isspmatrix_coo(X):\n        X = csr_matrix(X)  # COO not subscriptable\n    if issparse(X):\n        X.eliminate_zeros()\n\n    obs_metrics = describe_obs(\n        adata,\n        expr_type=expr_type,\n        var_type=var_type,\n        qc_vars=qc_vars,\n        percent_top=percent_top,\n        inplace=inplace,\n        X=X,\n        log1p=log1p,\n    )\n    var_metrics = describe_var(\n        adata,\n        expr_type=expr_type,\n        var_type=var_type,\n        inplace=inplace,\n        X=X,\n        log1p=log1p,\n    )\n\n    if not inplace:\n        return obs_metrics, var_metrics", "idx": 341}
{"project": "Scanpy", "commit_id": "409_scanpy_1.9.0__qc.py_top_proportions.py", "target": 0, "func": "def top_proportions(mtx: Union[np.array, spmatrix], n: int):\n    \"\"\"\\\n    Calculates cumulative proportions of top expressed genes\n\n    Parameters\n    ----------\n    mtx\n        Matrix, where each row is a sample, each column a feature.\n    n\n        Rank to calculate proportions up to. Value is treated as 1-indexed,\n        `n=50` will calculate cumulative proportions up to the 50th most\n        expressed gene.\n    \"\"\"\n    if issparse(mtx):\n        if not isspmatrix_csr(mtx):\n            mtx = csr_matrix(mtx)\n        # Allowing numba to do more\n        return top_proportions_sparse_csr(mtx.data, mtx.indptr, np.array(n))\n    else:\n        return top_proportions_dense(mtx, n)", "idx": 342}
{"project": "Scanpy", "commit_id": "40_scanpy_1.9.0_logging.py_print_versions.py", "target": 0, "func": "def print_versions(*, file: Optional[IO[str]] = None):\n    \"\"\"\\\n    Print versions of imported packages, OS, and jupyter environment.\n\n    For more options (including rich output) use `session_info.show` directly.\n    \"\"\"\n    import session_info\n\n    if file is not None:\n        from contextlib import redirect_stdout\n\n        warnings.warn(\n            \"Passing argument 'file' to print_versions is deprecated, and will be \"\n            \"removed in a future version.\",\n            FutureWarning,\n        )\n        with redirect_stdout(file):\n            print_versions()\n    else:\n        session_info.show(\n            dependencies=True,\n            html=False,\n            excludes=[\n                'builtins',\n                'stdlib_list',\n                'importlib_metadata',\n                # Special module present if test coverage being calculated\n                # https://gitlab.com/joelostblom/session_info/-/issues/10\n                \"$coverage\",", "idx": 343}
{"project": "Scanpy", "commit_id": "410_scanpy_1.9.0__qc.py_top_proportions_dense.py", "target": 0, "func": "def top_proportions_dense(mtx, n):\n    sums = mtx.sum(axis=1)\n    partitioned = np.apply_along_axis(np.argpartition, 1, -mtx, n - 1)\n    partitioned = partitioned[:, :n]\n    values = np.zeros_like(partitioned, dtype=np.float64)\n    for i in range(partitioned.shape[0]):\n        vec = mtx[i, partitioned[i, :]]  # Not a view\n        vec[::-1].sort()  # Sorting on a reversed view (e.g. a descending sort)\n        vec = np.cumsum(vec) / sums[i]\n        values[i, :] = vec\n    return values", "idx": 344}
{"project": "Scanpy", "commit_id": "411_scanpy_1.9.0__qc.py_top_proportions_sparse_csr.py", "target": 0, "func": "def top_proportions_sparse_csr(data, indptr, n):\n    values = np.zeros((indptr.size - 1, n), dtype=np.float64)\n    for i in numba.prange(indptr.size - 1):\n        start, end = indptr[i], indptr[i + 1]\n        vec = np.zeros(n, dtype=np.float64)\n        if end - start <= n:\n            vec[: end - start] = data[start:end]\n            total = vec.sum()\n        else:\n            vec[:] = -(np.partition(-data[start:end], n - 1)[:n])\n            total = (data[start:end]).sum()  # Is this not just vec.sum()?\n        vec[::-1].sort()\n        values[i, :] = vec.cumsum() / total\n    return values", "idx": 345}
{"project": "Scanpy", "commit_id": "412_scanpy_1.9.0__qc.py_top_segment_proportions.py", "target": 0, "func": "def top_segment_proportions(\n    mtx: Union[np.array, spmatrix], ns: Collection[int]\n) -> np.ndarray:\n    \"\"\"\n    Calculates total percentage of counts in top ns genes.\n\n    Parameters\n    ----------\n    mtx\n        Matrix, where each row is a sample, each column a feature.\n    ns\n        Positions to calculate cumulative proportion at. Values are considered\n        1-indexed, e.g. `ns=[50]` will calculate cumulative proportion up to\n        the 50th most expressed gene.\n    \"\"\"\n    # Pretty much just does dispatch\n    if not (max(ns) <= mtx.shape[1] and min(ns) > 0):\n        raise IndexError(\"Positions outside range of features.\")\n    if issparse(mtx):\n        if not isspmatrix_csr(mtx):\n            mtx = csr_matrix(mtx)\n        return top_segment_proportions_sparse_csr(mtx.data, mtx.indptr, np.array(ns))\n    else:\n        return top_segment_proportions_dense(mtx, ns)", "idx": 346}
{"project": "Scanpy", "commit_id": "413_scanpy_1.9.0__qc.py_top_segment_proportions_dense.py", "target": 0, "func": "def top_segment_proportions_dense(\n    mtx: Union[np.array, spmatrix], ns: Collection[int]\n) -> np.ndarray:\n    # Currently ns is considered to be 1 indexed\n    ns = np.sort(ns)\n    sums = mtx.sum(axis=1)\n    partitioned = np.apply_along_axis(np.partition, 1, mtx, mtx.shape[1] - ns)[:, ::-1][\n        :, : ns[-1]\n    ]\n    values = np.zeros((mtx.shape[0], len(ns)))\n    acc = np.zeros(mtx.shape[0])\n    prev = 0\n    for j, n in enumerate(ns):\n        acc += partitioned[:, prev:n].sum(axis=1)\n        values[:, j] = acc\n        prev = n\n    return values / sums[:, None]", "idx": 347}
{"project": "Scanpy", "commit_id": "414_scanpy_1.9.0__qc.py_top_segment_proportions_sparse_csr.py", "target": 0, "func": "def top_segment_proportions_sparse_csr(data, indptr, ns):\n    # work around https://github.com/numba/numba/issues/5056\n    indptr = indptr.astype(np.int64)\n    ns = ns.astype(np.int64)\n    ns = np.sort(ns)\n    maxidx = ns[-1]\n    sums = np.zeros((indptr.size - 1), dtype=data.dtype)\n    values = np.zeros((indptr.size - 1, len(ns)), dtype=np.float64)\n    # Just to keep it simple, as a dense matrix\n    partitioned = np.zeros((indptr.size - 1, maxidx), dtype=data.dtype)\n    for i in numba.prange(indptr.size - 1):\n        start, end = indptr[i], indptr[i + 1]\n        sums[i] = np.sum(data[start:end])\n        if end - start <= maxidx:\n            partitioned[i, : end - start] = data[start:end]\n        elif (end - start) > maxidx:\n            partitioned[i, :] = -(np.partition(-data[start:end], maxidx))[:maxidx]\n        partitioned[i, :] = np.partition(partitioned[i, :], maxidx - ns)\n    partitioned = partitioned[:, ::-1][:, : ns[-1]]\n    acc = np.zeros((indptr.size - 1), dtype=data.dtype)\n    prev = 0\n    # can\u2019t use enumerate due to https://github.com/numba/numba/issues/2625\n    for j in range(ns.size):\n        acc += partitioned[:, prev : ns[j]].sum(axis=1)\n        values[:, j] = acc\n        prev = ns[j]\n    return values / sums.reshape((indptr.size - 1, 1))", "idx": 348}
{"project": "Scanpy", "commit_id": "415_scanpy_1.9.0__recipes.py_recipe_weinreb17.py", "target": 0, "func": "def recipe_weinreb17(\n    adata: AnnData,\n    log: bool = True,\n    mean_threshold: float = 0.01,\n    cv_threshold: int = 2,\n    n_pcs: int = 50,\n    svd_solver='randomized',\n    random_state: AnyRandom = 0,\n    copy: bool = False,\n) -> Optional[AnnData]:\n    \"\"\"\\\n    Normalization and filtering as of [Weinreb17]_.\n\n    Expects non-logarithmized data.\n    If using logarithmized data, pass `log=False`.\n\n    Parameters\n    ----------\n    adata\n        Annotated data matrix.\n    log\n        Logarithmize data?\n    copy\n        Return a copy if true.\n    \"\"\"\n    from ._deprecated import normalize_per_cell_weinreb16_deprecated, zscore_deprecated\n    from scipy.sparse import issparse\n\n    if issparse(adata.X):\n        raise ValueError('`recipe_weinreb16 does not support sparse matrices.')\n    if copy:\n        adata = adata.copy()\n    if log:\n        pp.log1p(adata)\n    adata.X = normalize_per_cell_weinreb16_deprecated(\n        adata.X, max_fraction=0.05, mult_with_mean=True\n    )\n    gene_subset = filter_genes_cv_deprecated(adata.X, mean_threshold, cv_threshold)\n    adata._inplace_subset_var(gene_subset)  # this modifies the object itself\n    X_pca = pp.pca(\n        zscore_deprecated(adata.X),\n        n_comps=n_pcs,\n        svd_solver=svd_solver,\n        random_state=random_state,\n    )\n    # update adata\n    adata.obsm['X_pca'] = X_pca\n    return adata if copy else None", "idx": 349}
{"project": "Scanpy", "commit_id": "416_scanpy_1.9.0__recipes.py_recipe_seurat.py", "target": 0, "func": "def recipe_seurat(\n    adata: AnnData, log: bool = True, plot: bool = False, copy: bool = False\n) -> Optional[AnnData]:\n    \"\"\"\\\n    Normalization and filtering as of Seurat [Satija15]_.\n\n    This uses a particular preprocessing.\n\n    Expects non-logarithmized data.\n    If using logarithmized data, pass `log=False`.\n    \"\"\"\n    if copy:\n        adata = adata.copy()\n    pp.filter_cells(adata, min_genes=200)\n    pp.filter_genes(adata, min_cells=3)\n    normalize_total(adata, target_sum=1e4)\n    filter_result = filter_genes_dispersion(\n        adata.X, min_mean=0.0125, max_mean=3, min_disp=0.5, log=not log\n    )\n    if plot:\n        from ..plotting import (\n            _preprocessing as ppp,\n        )  # should not import at the top of the file\n\n        ppp.filter_genes_dispersion(filter_result, log=not log)\n    adata._inplace_subset_var(filter_result.gene_subset)  # filter genes\n    if log:\n        pp.log1p(adata)\n    pp.scale(adata, max_value=10)\n    return adata if copy else None", "idx": 350}
{"project": "Scanpy", "commit_id": "417_scanpy_1.9.0__recipes.py_recipe_zheng17.py", "target": 0, "func": "def recipe_zheng17(\n    adata: AnnData,\n    n_top_genes: int = 1000,\n    log: bool = True,\n    plot: bool = False,\n    copy: bool = False,\n) -> Optional[AnnData]:\n    \"\"\"\\\n    Normalization and filtering as of [Zheng17]_.\n\n    Reproduces the preprocessing of [Zheng17]_ \u2013 the Cell Ranger R Kit of 10x\n    Genomics.\n\n    Expects non-logarithmized data.\n    If using logarithmized data, pass `log=False`.\n\n    The recipe runs the following steps\n\n    .. code:: python\n\n        sc.pp.filter_genes(adata, min_counts=1)         # only consider genes with more than 1 count\n        sc.pp.normalize_per_cell(                       # normalize with total UMI count per cell\n             adata, key_n_counts='n_counts_all'\n        )\n        filter_result = sc.pp.filter_genes_dispersion(  # select highly-variable genes\n            adata.X, flavor='cell_ranger', n_top_genes=n_top_genes, log=False\n        )\n        adata = adata[:, filter_result.gene_subset]     # subset the genes\n        sc.pp.normalize_per_cell(adata)                 # renormalize after filtering\n        if log: sc.pp.log1p(adata)                      # log transform: adata.X = log(adata.X + 1)\n        sc.pp.scale(adata)                              # scale to unit variance and shift to zero mean\n\n\n    Parameters\n    ----------\n    adata\n        Annotated data matrix.\n    n_top_genes\n        Number of genes to keep.\n    log\n        Take logarithm.\n    plot\n        Show a plot of the gene dispersion vs. mean relation.\n    copy\n        Return a copy of `adata` instead of updating it.\n\n    Returns\n    -------\n    Returns or updates `adata` depending on `copy`.\n    \"\"\"\n    start = logg.info('running recipe zheng17')\n    if copy:\n        adata = adata.copy()\n    # only consider genes with more than 1 count\n    pp.filter_genes(adata, min_counts=1)\n    # normalize with total UMI count per cell\n    normalize_total(adata, key_added='n_counts_all')\n    filter_result = filter_genes_dispersion(\n        adata.X, flavor='cell_ranger', n_top_genes=n_top_genes, log=False\n    )\n    if plot:  # should not import at the top of the file\n        from ..plotting import _preprocessing as ppp\n\n        ppp.filter_genes_dispersion(filter_result, log=True)\n    # actually filter the genes, the following is the inplace version of\n    #     adata = adata[:, filter_result.gene_subset]\n    adata._inplace_subset_var(filter_result.gene_subset)  # filter genes\n    normalize_total(adata)  # renormalize after filtering\n    if log:\n        pp.log1p(adata)  # log transform: X = log(X + 1)\n    pp.scale(adata)\n    logg.info('    finished', time=start)\n    return adata if copy else None", "idx": 351}
{"project": "Scanpy", "commit_id": "418_scanpy_1.9.0__simple.py_filter_cells.py", "target": 0, "func": "def filter_cells(\n    data: AnnData,\n    min_counts: Optional[int] = None,\n    min_genes: Optional[int] = None,\n    max_counts: Optional[int] = None,\n    max_genes: Optional[int] = None,\n    inplace: bool = True,\n    copy: bool = False,\n) -> Optional[Tuple[np.ndarray, np.ndarray]]:\n    \"\"\"\\\n    Filter cell outliers based on counts and numbers of genes expressed.\n\n    For instance, only keep cells with at least `min_counts` counts or\n    `min_genes` genes expressed. This is to filter measurement outliers,\n    i.e. \u201cunreliable\u201d observations.\n\n    Only provide one of the optional parameters `min_counts`, `min_genes`,\n    `max_counts`, `max_genes` per call.\n\n    Parameters\n    ----------\n    data\n        The (annotated) data matrix of shape `n_obs` \u00d7 `n_vars`.\n        Rows correspond to cells and columns to genes.\n    min_counts\n        Minimum number of counts required for a cell to pass filtering.\n    min_genes\n        Minimum number of genes expressed required for a cell to pass filtering.\n    max_counts\n        Maximum number of counts required for a cell to pass filtering.\n    max_genes\n        Maximum number of genes expressed required for a cell to pass filtering.\n    inplace\n        Perform computation inplace or return result.\n\n    Returns\n    -------\n    Depending on `inplace`, returns the following arrays or directly subsets\n    and annotates the data matrix:\n\n    cells_subset\n        Boolean index mask that does filtering. `True` means that the\n        cell is kept. `False` means the cell is removed.\n    number_per_cell\n        Depending on what was tresholded (`counts` or `genes`),\n        the array stores `n_counts` or `n_cells` per gene.\n\n    Examples\n    --------\n    >>> import scanpy as sc\n    >>> adata = sc.datasets.krumsiek11()\n    >>> adata.n_obs\n    640\n    >>> adata.var_names\n    ['Gata2' 'Gata1' 'Fog1' 'EKLF' 'Fli1' 'SCL' 'Cebpa'\n     'Pu.1' 'cJun' 'EgrNab' 'Gfi1']\n    >>> # add some true zeros\n    >>> adata.X[adata.X < 0.3] = 0\n    >>> # simply compute the number of genes per cell\n    >>> sc.pp.filter_cells(adata, min_genes=0)\n    >>> adata.n_obs\n    640\n    >>> adata.obs['n_genes'].min()\n    1\n    >>> # filter manually\n    >>> adata_copy = adata[adata.obs['n_genes'] >= 3]\n    >>> adata_copy.obs['n_genes'].min()\n    >>> adata.n_obs\n    554\n    >>> adata.obs['n_genes'].min()\n    3\n    >>> # actually do some filtering\n    >>> sc.pp.filter_cells(adata, min_genes=3)\n    >>> adata.n_obs\n    554\n    >>> adata.obs['n_genes'].min()\n    3\n    \"\"\"\n    if copy:\n        logg.warning('`copy` is deprecated, use `inplace` instead.')\n    n_given_options = sum(\n        option is not None for option in [min_genes, min_counts, max_genes, max_counts]\n    )\n    if n_given_options != 1:\n        raise ValueError(\n            'Only provide one of the optional parameters `min_counts`, '\n            '`min_genes`, `max_counts`, `max_genes` per call.'\n        )\n    if isinstance(data, AnnData):\n        adata = data.copy() if copy else data\n        cell_subset, number = materialize_as_ndarray(\n            filter_cells(adata.X, min_counts, min_genes, max_counts, max_genes)\n        )\n        if not inplace:\n            return cell_subset, number\n        if min_genes is None and max_genes is None:\n            adata.obs['n_counts'] = number\n        else:\n            adata.obs['n_genes'] = number\n        adata._inplace_subset_obs(cell_subset)\n        return adata if copy else None\n    X = data  # proceed with processing the data matrix\n    min_number = min_counts if min_genes is None else min_genes\n    max_number = max_counts if max_genes is None else max_genes\n    number_per_cell = np.sum(\n        X if min_genes is None and max_genes is None else X > 0, axis=1\n    )\n    if issparse(X):\n        number_per_cell = number_per_cell.A1\n    if min_number is not None:\n        cell_subset = number_per_cell >= min_number\n    if max_number is not None:\n        cell_subset = number_per_cell <= max_number\n\n    s = np.sum(~cell_subset)\n    if s > 0:\n        msg = f'filtered out {s} cells that have '\n        if min_genes is not None or min_counts is not None:\n            msg += 'less than '\n            msg += (\n                f'{min_genes} genes expressed'\n                if min_counts is None\n                else f'{min_counts} counts'\n            )\n        if max_genes is not None or max_counts is not None:\n            msg += 'more than '\n            msg += (\n                f'{max_genes} genes expressed'\n                if max_counts is None\n                else f'{max_counts} counts'\n            )\n        logg.info(msg)\n    return cell_subset, number_per_cell", "idx": 352}
{"project": "Scanpy", "commit_id": "419_scanpy_1.9.0__simple.py_filter_genes.py", "target": 0, "func": "def filter_genes(\n    data: AnnData,\n    min_counts: Optional[int] = None,\n    min_cells: Optional[int] = None,\n    max_counts: Optional[int] = None,\n    max_cells: Optional[int] = None,\n    inplace: bool = True,\n    copy: bool = False,\n) -> Union[AnnData, None, Tuple[np.ndarray, np.ndarray]]:\n    \"\"\"\\\n    Filter genes based on number of cells or counts.\n\n    Keep genes that have at least `min_counts` counts or are expressed in at\n    least `min_cells` cells or have at most `max_counts` counts or are expressed\n    in at most `max_cells` cells.\n\n    Only provide one of the optional parameters `min_counts`, `min_cells`,\n    `max_counts`, `max_cells` per call.\n\n    Parameters\n    ----------\n    data\n        An annotated data matrix of shape `n_obs` \u00d7 `n_vars`. Rows correspond\n        to cells and columns to genes.\n    min_counts\n        Minimum number of counts required for a gene to pass filtering.\n    min_cells\n        Minimum number of cells expressed required for a gene to pass filtering.\n    max_counts\n        Maximum number of counts required for a gene to pass filtering.\n    max_cells\n        Maximum number of cells expressed required for a gene to pass filtering.\n    inplace\n        Perform computation inplace or return result.\n\n    Returns\n    -------\n    Depending on `inplace`, returns the following arrays or directly subsets\n    and annotates the data matrix\n\n    gene_subset\n        Boolean index mask that does filtering. `True` means that the\n        gene is kept. `False` means the gene is removed.\n    number_per_gene\n        Depending on what was tresholded (`counts` or `cells`), the array stores\n        `n_counts` or `n_cells` per gene.\n    \"\"\"\n    if copy:\n        logg.warning('`copy` is deprecated, use `inplace` instead.')\n    n_given_options = sum(\n        option is not None for option in [min_cells, min_counts, max_cells, max_counts]\n    )\n    if n_given_options != 1:\n        raise ValueError(\n            'Only provide one of the optional parameters `min_counts`, '\n            '`min_cells`, `max_counts`, `max_cells` per call.'\n        )\n\n    if isinstance(data, AnnData):\n        adata = data.copy() if copy else data\n        gene_subset, number = materialize_as_ndarray(\n            filter_genes(\n                adata.X,\n                min_cells=min_cells,\n                min_counts=min_counts,\n                max_cells=max_cells,\n                max_counts=max_counts,\n            )\n        )\n        if not inplace:\n            return gene_subset, number\n        if min_cells is None and max_cells is None:\n            adata.var['n_counts'] = number\n        else:\n            adata.var['n_cells'] = number\n        adata._inplace_subset_var(gene_subset)\n        return adata if copy else None\n\n    X = data  # proceed with processing the data matrix\n    min_number = min_counts if min_cells is None else min_cells\n    max_number = max_counts if max_cells is None else max_cells\n    number_per_gene = np.sum(\n        X if min_cells is None and max_cells is None else X > 0, axis=0\n    )\n    if issparse(X):\n        number_per_gene = number_per_gene.A1\n    if min_number is not None:\n        gene_subset = number_per_gene >= min_number\n    if max_number is not None:\n        gene_subset = number_per_gene <= max_number\n\n    s = np.sum(~gene_subset)\n    if s > 0:\n        msg = f'filtered out {s} genes that are detected '\n        if min_cells is not None or min_counts is not None:\n            msg += 'in less than '\n            msg += (\n                f'{min_cells} cells' if min_counts is None else f'{min_counts} counts'\n            )\n        if max_cells is not None or max_counts is not None:\n            msg += 'in more than '\n            msg += (\n                f'{max_cells} cells' if max_counts is None else f'{max_counts} counts'\n            )\n        logg.info(msg)\n    return gene_subset, number_per_gene", "idx": 353}
{"project": "Scanpy", "commit_id": "41_scanpy_1.9.0_logging.py_print_version_and_date.py", "target": 0, "func": "def print_version_and_date(*, file=None):\n    \"\"\"\\\n    Useful for starting a notebook so you see when you started working.\n    \"\"\"\n    from . import __version__\n\n    if file is None:\n        file = sys.stdout\n    print(\n        f'Running Scanpy {__version__}, ' f'on {datetime.now():%Y-%m-%d %H:%M}.',\n        file=file,", "idx": 354}
{"project": "Scanpy", "commit_id": "420_scanpy_1.9.0__simple.py_log1p.py", "target": 0, "func": "def log1p(\n    X: Union[AnnData, np.ndarray, spmatrix],\n    *,\n    base: Optional[Number] = None,\n    copy: bool = False,\n    chunked: bool = None,\n    chunk_size: Optional[int] = None,\n    layer: Optional[str] = None,\n    obsm: Optional[str] = None,\n):\n    \"\"\"\\\n    Logarithmize the data matrix.\n\n    Computes :math:`X = \\\\log(X + 1)`,\n    where :math:`log` denotes the natural logarithm unless a different base is given.\n\n    Parameters\n    ----------\n    X\n        The (annotated) data matrix of shape `n_obs` \u00d7 `n_vars`.\n        Rows correspond to cells and columns to genes.\n    base\n        Base of the logarithm. Natural logarithm is used by default.\n    copy\n        If an :class:`~anndata.AnnData` is passed, determines whether a copy\n        is returned.\n    chunked\n        Process the data matrix in chunks, which will save memory.\n        Applies only to :class:`~anndata.AnnData`.\n    chunk_size\n        `n_obs` of the chunks to process the data in.\n    layer\n        Entry of layers to tranform.\n    obsm\n        Entry of obsm to transform.\n\n    Returns\n    -------\n    Returns or updates `data`, depending on `copy`.\n    \"\"\"\n    _check_array_function_arguments(\n        chunked=chunked, chunk_size=chunk_size, layer=layer, obsm=obsm\n    )\n    return log1p_array(X, copy=copy, base=base)", "idx": 355}
{"project": "Scanpy", "commit_id": "421_scanpy_1.9.0__simple.py_log1p_sparse.py", "target": 0, "func": "def log1p_sparse(X, *, base: Optional[Number] = None, copy: bool = False):\n    X = check_array(\n        X, accept_sparse=(\"csr\", \"csc\"), dtype=(np.float64, np.float32), copy=copy\n    )\n    X.data = log1p(X.data, copy=False, base=base)\n    return X", "idx": 356}
{"project": "Scanpy", "commit_id": "422_scanpy_1.9.0__simple.py_log1p_array.py", "target": 0, "func": "def log1p_array(X, *, base: Optional[Number] = None, copy: bool = False):\n    # Can force arrays to be np.ndarrays, but would be useful to not\n    # X = check_array(X, dtype=(np.float64, np.float32), ensure_2d=False, copy=copy)\n    if copy:\n        if not np.issubdtype(X.dtype, np.floating):\n            X = X.astype(float)\n        else:\n            X = X.copy()\n    elif not (np.issubdtype(X.dtype, np.floating) or np.issubdtype(X.dtype, complex)):\n        X = X.astype(float)\n    np.log1p(X, out=X)\n    if base is not None:\n        np.divide(X, np.log(base), out=X)\n    return X", "idx": 357}
{"project": "Scanpy", "commit_id": "423_scanpy_1.9.0__simple.py_log1p_anndata.py", "target": 0, "func": "def log1p_anndata(\n    adata,\n    *,\n    base: Optional[Number] = None,\n    copy: bool = False,\n    chunked: bool = False,\n    chunk_size: Optional[int] = None,\n    layer: Optional[str] = None,\n    obsm: Optional[str] = None,\n) -> Optional[AnnData]:\n    if \"log1p\" in adata.uns_keys():\n        logg.warning(\"adata.X seems to be already log-transformed.\")\n\n    adata = adata.copy() if copy else adata\n    view_to_actual(adata)\n\n    if chunked:\n        if (layer is not None) or (obsm is not None):\n            raise NotImplementedError(\n                \"Currently cannot perform chunked operations on arrays not stored in X.\"\n            )\n        for chunk, start, end in adata.chunked_X(chunk_size):\n            adata.X[start:end] = log1p(chunk, base=base, copy=False)\n    else:\n        X = _get_obs_rep(adata, layer=layer, obsm=obsm)\n        X = log1p(X, copy=False, base=base)\n        _set_obs_rep(adata, X, layer=layer, obsm=obsm)\n\n    adata.uns[\"log1p\"] = {\"base\": base}\n    if copy:\n        return adata", "idx": 358}
{"project": "Scanpy", "commit_id": "424_scanpy_1.9.0__simple.py_sqrt.py", "target": 0, "func": "def sqrt(\n    data: AnnData,\n    copy: bool = False,\n    chunked: bool = False,\n    chunk_size: Optional[int] = None,\n) -> Optional[AnnData]:\n    \"\"\"\\\n    Square root the data matrix.\n\n    Computes :math:`X = \\\\sqrt(X)`.\n\n    Parameters\n    ----------\n    data\n        The (annotated) data matrix of shape `n_obs` \u00d7 `n_vars`.\n        Rows correspond to cells and columns to genes.\n    copy\n        If an :class:`~anndata.AnnData` object is passed,\n        determines whether a copy is returned.\n    chunked\n        Process the data matrix in chunks, which will save memory.\n        Applies only to :class:`~anndata.AnnData`.\n    chunk_size\n        `n_obs` of the chunks to process the data in.\n\n    Returns\n    -------\n    Returns or updates `data`, depending on `copy`.\n    \"\"\"\n    if isinstance(data, AnnData):\n        adata = data.copy() if copy else data\n        if chunked:\n            for chunk, start, end in adata.chunked_X(chunk_size):\n                adata.X[start:end] = sqrt(chunk)\n        else:\n            adata.X = sqrt(data.X)\n        return adata if copy else None\n    X = data  # proceed with data matrix\n    if not issparse(X):\n        return np.sqrt(X)\n    else:\n        return X.sqrt()", "idx": 359}
{"project": "Scanpy", "commit_id": "425_scanpy_1.9.0__simple.py_normalize_per_cell.py", "target": 1, "func": "def normalize_per_cell(\n    data: Union[AnnData, np.ndarray, spmatrix],\n    counts_per_cell_after: Optional[float] = None,\n    counts_per_cell: Optional[np.ndarray] = None,\n    key_n_counts: str = 'n_counts',\n    copy: bool = False,\n    layers: Union[Literal['all'], Iterable[str]] = (),\n    use_rep: Optional[Literal['after', 'X']] = None,\n    min_counts: int = 1,\n) -> Optional[AnnData]:\n    \"\"\"\\\n    Normalize total counts per cell.\n\n    .. warning::\n        .. deprecated:: 1.3.7\n            Use :func:`~scanpy.pp.normalize_total` instead.\n            The new function is equivalent to the present\n            function, except that\n\n            * the new function doesn't filter cells based on `min_counts`,\n              use :func:`~scanpy.pp.filter_cells` if filtering is needed.\n            * some arguments were renamed\n            * `copy` is replaced by `inplace`\n\n    Normalize each cell by total counts over all genes, so that every cell has\n    the same total count after normalization.\n\n    Similar functions are used, for example, by Seurat [Satija15]_, Cell Ranger\n    [Zheng17]_ or SPRING [Weinreb17]_.\n\n    Parameters\n    ----------\n    data\n        The (annotated) data matrix of shape `n_obs` \u00d7 `n_vars`. Rows correspond\n        to cells and columns to genes.\n    counts_per_cell_after\n        If `None`, after normalization, each cell has a total count equal\n        to the median of the *counts_per_cell* before normalization.\n    counts_per_cell\n        Precomputed counts per cell.\n    key_n_counts\n        Name of the field in `adata.obs` where the total counts per cell are\n        stored.\n    copy\n        If an :class:`~anndata.AnnData` is passed, determines whether a copy\n        is returned.\n    min_counts\n        Cells with counts less than `min_counts` are filtered out during\n        normalization.\n\n    Returns\n    -------\n    Returns or updates `adata` with normalized version of the original\n    `adata.X`, depending on `copy`.\n\n    Examples\n    --------\n    >>> import scanpy as sc\n    >>> adata = AnnData(np.array([[1, 0], [3, 0], [5, 6]]))\n    >>> print(adata.X.sum(axis=1))\n    [  1.   3.  11.]\n    >>> sc.pp.normalize_per_cell(adata)\n    >>> print(adata.obs)\n    >>> print(adata.X.sum(axis=1))\n       n_counts\n    0       1.0\n    1       3.0\n    2      11.0\n    [ 3.  3.  3.]\n    >>> sc.pp.normalize_per_cell(\n    >>>     adata, counts_per_cell_after=1,\n    >>>     key_n_counts='n_counts2',\n    >>> )\n    >>> print(adata.obs)\n    >>> print(adata.X.sum(axis=1))\n       n_counts  n_counts2\n    0       1.0        3.0\n    1       3.0        3.0\n    2      11.0        3.0\n    [ 1.  1.  1.]\n    \"\"\"\n    if isinstance(data, AnnData):\n        start = logg.info('normalizing by total count per cell')\n        adata = data.copy() if copy else data\n        if counts_per_cell is None:\n            cell_subset, counts_per_cell = materialize_as_ndarray(\n                filter_cells(adata.X, min_counts=min_counts)\n            )\n            adata.obs[key_n_counts] = counts_per_cell\n            adata._inplace_subset_obs(cell_subset)\n            counts_per_cell = counts_per_cell[cell_subset]\n        normalize_per_cell(adata.X, counts_per_cell_after, counts_per_cell)\n\n        layers = adata.layers.keys() if layers == 'all' else layers\n        if use_rep == 'after':\n            after = counts_per_cell_after\n        elif use_rep == 'X':\n            after = np.median(counts_per_cell[cell_subset])\n        elif use_rep is None:\n            after = None\n        else:\n            raise ValueError('use_rep should be \"after\", \"X\" or None')\n        for layer in layers:\n            subset, counts = filter_cells(adata.layers[layer], min_counts=min_counts)\n            temp = normalize_per_cell(adata.layers[layer], after, counts, copy=True)\n            adata.layers[layer] = temp\n\n        logg.info(\n            '    finished ({time_passed}): normalized adata.X and added'\n            f'    {key_n_counts!r}, counts per cell before normalization (adata.obs)',\n            time=start,\n        )\n        return adata if copy else None\n    # proceed with data matrix\n    X = data.copy() if copy else data\n    if counts_per_cell is None:\n        if not copy:\n            raise ValueError('Can only be run with copy=True')\n        cell_subset, counts_per_cell = filter_cells(X, min_counts=min_counts)\n        X = X[cell_subset]\n        counts_per_cell = counts_per_cell[cell_subset]\n    if counts_per_cell_after is None:\n        counts_per_cell_after = np.median(counts_per_cell)\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n        counts_per_cell += counts_per_cell == 0\n        counts_per_cell /= counts_per_cell_after\n        if not issparse(X):\n            X /= materialize_as_ndarray(counts_per_cell[:, np.newaxis])\n        else:\n            sparsefuncs.inplace_row_scale(X, 1 / counts_per_cell)\n    return X if copy else None", "idx": 360}
{"project": "Scanpy", "commit_id": "426_scanpy_1.9.0__simple.py_regress_out.py", "target": 1, "func": "def regress_out(\n    adata: AnnData,\n    keys: Union[str, Sequence[str]],\n    n_jobs: Optional[int] = None,\n    copy: bool = False,\n) -> Optional[AnnData]:\n    \"\"\"\\\n    Regress out (mostly) unwanted sources of variation.\n\n    Uses simple linear regression. This is inspired by Seurat's `regressOut`\n    function in R [Satija15]. Note that this function tends to overcorrect\n    in certain circumstances as described in :issue:`526`.\n\n    Parameters\n    ----------\n    adata\n        The annotated data matrix.\n    keys\n        Keys for observation annotation on which to regress on.\n    n_jobs\n        Number of jobs for parallel computation.\n        `None` means using :attr:`scanpy._settings.ScanpyConfig.n_jobs`.\n    copy\n        Determines whether a copy of `adata` is returned.\n\n    Returns\n    -------\n    Depending on `copy` returns or updates `adata` with the corrected data matrix.\n    \"\"\"\n    start = logg.info(f'regressing out {keys}')\n    if issparse(adata.X):\n        logg.info('    sparse input is densified and may ' 'lead to high memory use')\n    adata = adata.copy() if copy else adata\n\n    sanitize_anndata(adata)\n\n    # TODO: This should throw an implicit modification warning\n    if adata.is_view:\n        adata._init_as_actual(adata.copy())\n\n    if isinstance(keys, str):\n        keys = [keys]\n\n    if issparse(adata.X):\n        adata.X = adata.X.toarray()\n\n    n_jobs = sett.n_jobs if n_jobs is None else n_jobs\n\n    # regress on a single categorical variable\n    variable_is_categorical = False\n    if keys[0] in adata.obs_keys() and is_categorical_dtype(adata.obs[keys[0]]):\n        if len(keys) > 1:\n            raise ValueError(\n                'If providing categorical variable, '\n                'only a single one is allowed. For this one '\n                'we regress on the mean for each category.'\n            )\n        logg.debug('... regressing on per-gene means within categories')\n        regressors = np.zeros(adata.X.shape, dtype='float32')\n        for category in adata.obs[keys[0]].cat.categories:\n            mask = (category == adata.obs[keys[0]]).values\n            for ix, x in enumerate(adata.X.T):\n                regressors[mask, ix] = x[mask].mean()\n        variable_is_categorical = True\n    # regress on one or several ordinal variables\n    else:\n        # create data frame with selected keys (if given)\n        if keys:\n            regressors = adata.obs[keys]\n        else:\n            regressors = adata.obs.copy()\n\n        # add column of ones at index 0 (first column)\n        regressors.insert(0, 'ones', 1.0)\n\n    len_chunk = np.ceil(min(1000, adata.X.shape[1]) / n_jobs).astype(int)\n    n_chunks = np.ceil(adata.X.shape[1] / len_chunk).astype(int)\n\n    tasks = []\n    # split the adata.X matrix by columns in chunks of size n_chunk\n    # (the last chunk could be of smaller size than the others)\n    chunk_list = np.array_split(adata.X, n_chunks, axis=1)\n    if variable_is_categorical:\n        regressors_chunk = np.array_split(regressors, n_chunks, axis=1)\n    for idx, data_chunk in enumerate(chunk_list):\n        # each task is a tuple of a data_chunk eg. (adata.X[:,0:100]) and\n        # the regressors. This data will be passed to each of the jobs.\n        if variable_is_categorical:\n            regres = regressors_chunk[idx]\n        else:\n            regres = regressors\n        tasks.append(tuple((data_chunk, regres, variable_is_categorical)))\n\n    from joblib import Parallel, delayed\n\n    # TODO: figure out how to test that this doesn't oversubscribe resources\n    res = Parallel(n_jobs=n_jobs)(delayed(_regress_out_chunk)(task) for task in tasks)\n\n    # res is a list of vectors (each corresponding to a regressed gene column).\n    # The transpose is needed to get the matrix in the shape needed\n    adata.X = np.vstack(res).T.astype(adata.X.dtype)\n    logg.info('    finished', time=start)\n    return adata if copy else None", "idx": 361}
{"project": "Scanpy", "commit_id": "427_scanpy_1.9.0__simple.py__regress_out_chunk.py", "target": 0, "func": "def _regress_out_chunk(data):\n    # data is a tuple containing the selected columns from adata.X\n    # and the regressors dataFrame\n    data_chunk = data[0]\n    regressors = data[1]\n    variable_is_categorical = data[2]\n\n    responses_chunk_list = []\n    import statsmodels.api as sm\n    from statsmodels.tools.sm_exceptions import PerfectSeparationError\n\n    for col_index in range(data_chunk.shape[1]):\n\n        # if all values are identical, the statsmodel.api.GLM throws an error;\n        # but then no regression is necessary anyways...\n        if not (data_chunk[:, col_index] != data_chunk[0, col_index]).any():\n            responses_chunk_list.append(data_chunk[:, col_index])\n            continue\n\n        if variable_is_categorical:\n            regres = np.c_[np.ones(regressors.shape[0]), regressors[:, col_index]]\n        else:\n            regres = regressors\n        try:\n            result = sm.GLM(\n                data_chunk[:, col_index], regres, family=sm.families.Gaussian()\n            ).fit()\n            new_column = result.resid_response\n        except PerfectSeparationError:  # this emulates R's behavior\n            logg.warning('Encountered PerfectSeparationError, setting to 0 as in R.')\n            new_column = np.zeros(data_chunk.shape[0])\n\n        responses_chunk_list.append(new_column)\n\n    return np.vstack(responses_chunk_list)", "idx": 362}
{"project": "Scanpy", "commit_id": "428_scanpy_1.9.0__simple.py_scale.py", "target": 0, "func": "def scale(\n    X: Union[AnnData, spmatrix, np.ndarray],\n    zero_center: bool = True,\n    max_value: Optional[float] = None,\n    copy: bool = False,\n    layer: Optional[str] = None,\n    obsm: Optional[str] = None,\n):\n    \"\"\"\\\n    Scale data to unit variance and zero mean.\n\n    .. note::\n        Variables (genes) that do not display any variation (are constant across\n        all observations) are retained and (for zero_center==True) set to 0\n        during this operation. In the future, they might be set to NaNs.\n\n    Parameters\n    ----------\n    X\n        The (annotated) data matrix of shape `n_obs` \u00d7 `n_vars`.\n        Rows correspond to cells and columns to genes.\n    zero_center\n        If `False`, omit zero-centering variables, which allows to handle sparse\n        input efficiently.\n    max_value\n        Clip (truncate) to this value after scaling. If `None`, do not clip.\n    copy\n        Whether this function should be performed inplace. If an AnnData object\n        is passed, this also determines if a copy is returned.\n    layer\n        If provided, which element of layers to scale.\n    obsm\n        If provided, which element of obsm to scale.\n\n    Returns\n    -------\n    Depending on `copy` returns or updates `adata` with a scaled `adata.X`,\n    annotated with `'mean'` and `'std'` in `adata.var`.\n    \"\"\"\n    _check_array_function_arguments(layer=layer, obsm=obsm)\n    if layer is not None:\n        raise ValueError(f\"`layer` argument inappropriate for value of type {type(X)}\")\n    if obsm is not None:\n        raise ValueError(f\"`obsm` argument inappropriate for value of type {type(X)}\")\n    return scale_array(X, zero_center=zero_center, max_value=max_value, copy=copy)", "idx": 363}
{"project": "Scanpy", "commit_id": "429_scanpy_1.9.0__simple.py_scale_array.py", "target": 1, "func": "def scale_array(\n    X,\n    *,\n    zero_center: bool = True,\n    max_value: Optional[float] = None,\n    copy: bool = False,\n    return_mean_std: bool = False,\n):\n    if copy:\n        X = X.copy()\n    if not zero_center and max_value is not None:\n        logg.info(  # Be careful of what? This should be more specific\n            \"... be careful when using `max_value` \" \"without `zero_center`.\"\n        )\n\n    if np.issubdtype(X.dtype, np.integer):\n        logg.info(\n            '... as scaling leads to float results, integer '\n            'input is cast to float, returning copy.'\n        )\n        X = X.astype(float)\n\n    mean, var = _get_mean_var(X)\n    std = np.sqrt(var)\n    std[std == 0] = 1\n    if issparse(X):\n        if zero_center:\n            raise ValueError(\"Cannot zero-center sparse matrix.\")\n        sparsefuncs.inplace_column_scale(X, 1 / std)\n    else:\n        if zero_center:\n            X -= mean\n        X /= std\n\n    # do the clipping\n    if max_value is not None:\n        logg.debug(f\"... clipping at max_value {max_value}\")\n        X[X > max_value] = max_value\n\n    if return_mean_std:\n        return X, mean, std\n    else:\n        return X", "idx": 364}
{"project": "Scanpy", "commit_id": "42_scanpy_1.9.0_logging.py__copy_docs_and_signature.py", "target": 0, "func": "def _copy_docs_and_signature(fn):\n    return partial(update_wrapper, wrapped=fn, assigned=['__doc__', '__annotations__'])", "idx": 365}
{"project": "Scanpy", "commit_id": "430_scanpy_1.9.0__simple.py_scale_sparse.py", "target": 0, "func": "def scale_sparse(\n    X,\n    *,\n    zero_center: bool = True,\n    max_value: Optional[float] = None,\n    copy: bool = False,\n    return_mean_std: bool = False,\n):\n    # need to add the following here to make inplace logic work\n    if zero_center:\n        logg.info(\n            \"... as `zero_center=True`, sparse input is \"\n            \"densified and may lead to large memory consumption\"\n        )\n        X = X.toarray()\n        copy = False  # Since the data has been copied\n    return scale_array(\n        X,\n        zero_center=zero_center,\n        copy=copy,\n        max_value=max_value,\n        return_mean_std=return_mean_std,", "idx": 366}
{"project": "Scanpy", "commit_id": "431_scanpy_1.9.0__simple.py_scale_anndata.py", "target": 1, "func": "def scale_anndata(\n    adata: AnnData,\n    *,\n    zero_center: bool = True,\n    max_value: Optional[float] = None,\n    copy: bool = False,\n    layer: Optional[str] = None,\n    obsm: Optional[str] = None,\n) -> Optional[AnnData]:\n    adata = adata.copy() if copy else adata\n    view_to_actual(adata)\n    X = _get_obs_rep(adata, layer=layer, obsm=obsm)\n    X, adata.var[\"mean\"], adata.var[\"std\"] = scale(\n        X,\n        zero_center=zero_center,\n        max_value=max_value,\n        copy=False,  # because a copy has already been made, if it were to be made\n        return_mean_std=True,\n    )\n    _set_obs_rep(adata, X, layer=layer, obsm=obsm)\n    if copy:\n        return adata", "idx": 367}
{"project": "Scanpy", "commit_id": "432_scanpy_1.9.0__simple.py_subsample.py", "target": 1, "func": "def subsample(\n    data: Union[AnnData, np.ndarray, spmatrix],\n    fraction: Optional[float] = None,\n    n_obs: Optional[int] = None,\n    random_state: AnyRandom = 0,\n    copy: bool = False,\n) -> Optional[AnnData]:\n    \"\"\"\\\n    Subsample to a fraction of the number of observations.\n\n    Parameters\n    ----------\n    data\n        The (annotated) data matrix of shape `n_obs` \u00d7 `n_vars`.\n        Rows correspond to cells and columns to genes.\n    fraction\n        Subsample to this `fraction` of the number of observations.\n    n_obs\n        Subsample to this number of observations.\n    random_state\n        Random seed to change subsampling.\n    copy\n        If an :class:`~anndata.AnnData` is passed,\n        determines whether a copy is returned.\n\n    Returns\n    -------\n    Returns `X[obs_indices], obs_indices` if data is array-like, otherwise\n    subsamples the passed :class:`~anndata.AnnData` (`copy == False`) or\n    returns a subsampled copy of it (`copy == True`).\n    \"\"\"\n    np.random.seed(random_state)\n    old_n_obs = data.n_obs if isinstance(data, AnnData) else data.shape[0]\n    if n_obs is not None:\n        new_n_obs = n_obs\n    elif fraction is not None:\n        if fraction > 1 or fraction < 0:\n            raise ValueError(f'`fraction` needs to be within [0, 1], not {fraction}')\n        new_n_obs = int(fraction * old_n_obs)\n        logg.debug(f'... subsampled to {new_n_obs} data points')\n    else:\n        raise ValueError('Either pass `n_obs` or `fraction`.')\n    obs_indices = np.random.choice(old_n_obs, size=new_n_obs, replace=False)\n    if isinstance(data, AnnData):\n        if copy:\n            return data[obs_indices].copy()\n        else:\n            data._inplace_subset_obs(obs_indices)\n    else:\n        X = data\n        return X[obs_indices], obs_indices", "idx": 368}
{"project": "Scanpy", "commit_id": "433_scanpy_1.9.0__simple.py_downsample_counts.py", "target": 0, "func": "def downsample_counts(\n    adata: AnnData,\n    counts_per_cell: Optional[Union[int, Collection[int]]] = None,\n    total_counts: Optional[int] = None,\n    *,\n    random_state: AnyRandom = 0,\n    replace: bool = False,\n    copy: bool = False,\n) -> Optional[AnnData]:\n    \"\"\"\\\n    Downsample counts from count matrix.\n\n    If `counts_per_cell` is specified, each cell will downsampled.\n    If `total_counts` is specified, expression matrix will be downsampled to\n    contain at most `total_counts`.\n\n    Parameters\n    ----------\n    adata\n        Annotated data matrix.\n    counts_per_cell\n        Target total counts per cell. If a cell has more than 'counts_per_cell',\n        it will be downsampled to this number. Resulting counts can be specified\n        on a per cell basis by passing an array.Should be an integer or integer\n        ndarray with same length as number of obs.\n    total_counts\n        Target total counts. If the count matrix has more than `total_counts`\n        it will be downsampled to have this number.\n    random_state\n        Random seed for subsampling.\n    replace\n        Whether to sample the counts with replacement.\n    copy\n        Determines whether a copy of `adata` is returned.\n\n    Returns\n    -------\n    Depending on `copy` returns or updates an `adata` with downsampled `.X`.\n    \"\"\"\n    # This logic is all dispatch\n    total_counts_call = total_counts is not None\n    counts_per_cell_call = counts_per_cell is not None\n    if total_counts_call is counts_per_cell_call:\n        raise ValueError(\n            \"Must specify exactly one of `total_counts` or `counts_per_cell`.\"\n        )\n    if copy:\n        adata = adata.copy()\n    if total_counts_call:\n        adata.X = _downsample_total_counts(adata.X, total_counts, random_state, replace)\n    elif counts_per_cell_call:\n        adata.X = _downsample_per_cell(adata.X, counts_per_cell, random_state, replace)\n    if copy:\n        return adata", "idx": 369}
{"project": "Scanpy", "commit_id": "434_scanpy_1.9.0__simple.py__downsample_per_cell.py", "target": 0, "func": "def _downsample_per_cell(X, counts_per_cell, random_state, replace):\n    n_obs = X.shape[0]\n    if isinstance(counts_per_cell, int):\n        counts_per_cell = np.full(n_obs, counts_per_cell)\n    else:\n        counts_per_cell = np.asarray(counts_per_cell)\n    # np.random.choice needs int arguments in numba code:\n    counts_per_cell = counts_per_cell.astype(np.int_, copy=False)\n    if not isinstance(counts_per_cell, np.ndarray) or len(counts_per_cell) != n_obs:\n        raise ValueError(\n            \"If provided, 'counts_per_cell' must be either an integer, or \"\n            \"coercible to an `np.ndarray` of length as number of observations\"\n            \" by `np.asarray(counts_per_cell)`.\"\n        )\n    if issparse(X):\n        original_type = type(X)\n        if not isspmatrix_csr(X):\n            X = csr_matrix(X)\n        totals = np.ravel(X.sum(axis=1))  # Faster for csr matrix\n        under_target = np.nonzero(totals > counts_per_cell)[0]\n        rows = np.split(X.data, X.indptr[1:-1])\n        for rowidx in under_target:\n            row = rows[rowidx]\n            _downsample_array(\n                row,\n                counts_per_cell[rowidx],\n                random_state=random_state,\n                replace=replace,\n                inplace=True,\n            )\n        X.eliminate_zeros()\n        if original_type is not csr_matrix:  # Put it back\n            X = original_type(X)\n    else:\n        totals = np.ravel(X.sum(axis=1))\n        under_target = np.nonzero(totals > counts_per_cell)[0]\n        for rowidx in under_target:\n            row = X[rowidx, :]\n            _downsample_array(\n                row,\n                counts_per_cell[rowidx],\n                random_state=random_state,\n                replace=replace,\n                inplace=True,\n            )\n    return X", "idx": 370}
{"project": "Scanpy", "commit_id": "435_scanpy_1.9.0__simple.py__downsample_total_counts.py", "target": 0, "func": "def _downsample_total_counts(X, total_counts, random_state, replace):\n    total_counts = int(total_counts)\n    total = X.sum()\n    if total < total_counts:\n        return X\n    if issparse(X):\n        original_type = type(X)\n        if not isspmatrix_csr(X):\n            X = csr_matrix(X)\n        _downsample_array(\n            X.data,\n            total_counts,\n            random_state=random_state,\n            replace=replace,\n            inplace=True,\n        )\n        X.eliminate_zeros()\n        if original_type is not csr_matrix:\n            X = original_type(X)\n    else:\n        v = X.reshape(np.multiply(*X.shape))\n        _downsample_array(v, total_counts, random_state, replace=replace, inplace=True)\n    return X", "idx": 371}
{"project": "Scanpy", "commit_id": "436_scanpy_1.9.0__simple.py__downsample_array.py", "target": 0, "func": "def _downsample_array(\n    col: np.ndarray,\n    target: int,\n    random_state: AnyRandom = 0,\n    replace: bool = True,\n    inplace: bool = False,\n):\n    \"\"\"\\\n    Evenly reduce counts in cell to target amount.\n\n    This is an internal function and has some restrictions:\n\n    * total counts in cell must be less than target\n    \"\"\"\n    np.random.seed(random_state)\n    cumcounts = col.cumsum()\n    if inplace:\n        col[:] = 0\n    else:\n        col = np.zeros_like(col)\n    total = np.int_(cumcounts[-1])\n    sample = np.random.choice(total, target, replace=replace)\n    sample.sort()\n    geneptr = 0\n    for count in sample:\n        while count >= cumcounts[geneptr]:\n            geneptr += 1\n        col[geneptr] += 1\n    return col", "idx": 372}
{"project": "Scanpy", "commit_id": "437_scanpy_1.9.0__simple.py__pca_fallback.py", "target": 0, "func": "def _pca_fallback(data, n_comps=2):\n    # mean center the data\n    data -= data.mean(axis=0)\n    # calculate the covariance matrix\n    C = np.cov(data, rowvar=False)\n    # calculate eigenvectors & eigenvalues of the covariance matrix\n    # use 'eigh' rather than 'eig' since C is symmetric,\n    # the performance gain is substantial\n    # evals, evecs = np.linalg.eigh(C)\n    evals, evecs = sp.sparse.linalg.eigsh(C, k=n_comps)\n    # sort eigenvalues in decreasing order\n    idcs = np.argsort(evals)[::-1]\n    evecs = evecs[:, idcs]\n    evals = evals[idcs]\n    # select the first n eigenvectors (n is desired dimension\n    # of rescaled data array, or n_comps)\n    evecs = evecs[:, :n_comps]\n    # project data points on eigenvectors\n    return np.dot(evecs.T, data.T).T", "idx": 373}
{"project": "Scanpy", "commit_id": "438_scanpy_1.9.0__utils.py__get_mean_var.py", "target": 1, "func": "def _get_mean_var(X, *, axis=0):\n    if sparse.issparse(X):\n        mean, var = sparse_mean_variance_axis(X, axis=axis)\n    else:\n        mean = np.mean(X, axis=axis, dtype=np.float64)\n        mean_sq = np.multiply(X, X).mean(axis=axis, dtype=np.float64)\n        var = mean_sq - mean**2\n    # enforce R convention (unbiased estimator) for variance\n    var *= X.shape[axis] / (X.shape[axis] - 1)\n    return mean, var", "idx": 374}
{"project": "Scanpy", "commit_id": "439_scanpy_1.9.0__utils.py_sparse_mean_variance_axis.py", "target": 0, "func": "def sparse_mean_variance_axis(mtx: sparse.spmatrix, axis: int):\n    \"\"\"\n    This code and internal functions are based on sklearns\n    `sparsefuncs.mean_variance_axis`.\n\n    Modifications:\n    * allow deciding on the output type, which can increase accuracy when calculating the mean and variance of 32bit floats.\n    * This doesn't currently implement support for null values, but could.\n    * Uses numba not cython\n    \"\"\"\n    assert axis in (0, 1)\n    if isinstance(mtx, sparse.csr_matrix):\n        ax_minor = 1\n        shape = mtx.shape\n    elif isinstance(mtx, sparse.csc_matrix):\n        ax_minor = 0\n        shape = mtx.shape[::-1]\n    else:\n        raise ValueError(\"This function only works on sparse csr and csc matrices\")\n    if axis == ax_minor:\n        return sparse_mean_var_major_axis(\n            mtx.data, mtx.indices, mtx.indptr, *shape, np.float64\n        )\n    else:\n        return sparse_mean_var_minor_axis(mtx.data, mtx.indices, *shape, np.float64)", "idx": 375}
{"project": "Scanpy", "commit_id": "43_scanpy_1.9.0_logging.py_error.py", "target": 0, "func": "def error(self, msg, *, time=None, deep=None, extra=None) -> datetime:\n        return self.log(ERROR, msg, time=time, deep=deep, extra=extra)", "idx": 376}
{"project": "Scanpy", "commit_id": "440_scanpy_1.9.0__utils.py_sparse_mean_var_minor_axis.py", "target": 0, "func": "def sparse_mean_var_minor_axis(data, indices, major_len, minor_len, dtype):\n    \"\"\"\n    Computes mean and variance for a sparse matrix for the minor axis.\n\n    Given arrays for a csr matrix, returns the means and variances for each\n    column back.\n    \"\"\"\n    non_zero = indices.shape[0]\n\n    means = np.zeros(minor_len, dtype=dtype)\n    variances = np.zeros_like(means, dtype=dtype)\n\n    counts = np.zeros(minor_len, dtype=np.int64)\n\n    for i in range(non_zero):\n        col_ind = indices[i]\n        means[col_ind] += data[i]\n\n    for i in range(minor_len):\n        means[i] /= major_len\n\n    for i in range(non_zero):\n        col_ind = indices[i]\n        diff = data[i] - means[col_ind]\n        variances[col_ind] += diff * diff\n        counts[col_ind] += 1\n\n    for i in range(minor_len):\n        variances[i] += (major_len - counts[i]) * means[i] ** 2\n        variances[i] /= major_len\n\n    return means, variances", "idx": 377}
{"project": "Scanpy", "commit_id": "441_scanpy_1.9.0__utils.py_sparse_mean_var_major_axis.py", "target": 0, "func": "def sparse_mean_var_major_axis(data, indices, indptr, major_len, minor_len, dtype):\n    \"\"\"\n    Computes mean and variance for a sparse array for the major axis.\n\n    Given arrays for a csr matrix, returns the means and variances for each\n    row back.\n    \"\"\"\n    means = np.zeros(major_len, dtype=dtype)\n    variances = np.zeros_like(means, dtype=dtype)\n\n    for i in range(major_len):\n        startptr = indptr[i]\n        endptr = indptr[i + 1]\n        counts = endptr - startptr\n\n        for j in range(startptr, endptr):\n            means[i] += data[j]\n        means[i] /= minor_len\n\n        for j in range(startptr, endptr):\n            diff = data[j] - means[i]\n            variances[i] += diff * diff\n\n        variances[i] += (minor_len - counts) * means[i] ** 2\n        variances[i] /= minor_len\n\n    return means, variances", "idx": 378}
{"project": "Scanpy", "commit_id": "442_scanpy_1.9.0_highly_variable_genes.py_filter_genes_dispersion.py", "target": 0, "func": "def filter_genes_dispersion(\n    data: AnnData,\n    flavor: Literal['seurat', 'cell_ranger'] = 'seurat',\n    min_disp: Optional[float] = None,\n    max_disp: Optional[float] = None,\n    min_mean: Optional[float] = None,\n    max_mean: Optional[float] = None,\n    n_bins: int = 20,\n    n_top_genes: Optional[int] = None,\n    log: bool = True,\n    subset: bool = True,\n    copy: bool = False,\n):\n    \"\"\"\\\n    Extract highly variable genes [Satija15]_ [Zheng17]_.\n\n    .. warning::\n        .. deprecated:: 1.3.6\n            Use :func:`~scanpy.pp.highly_variable_genes`\n            instead. The new function is equivalent to the present\n            function, except that\n\n            * the new function always expects logarithmized data\n            * `subset=False` in the new function, it suffices to\n              merely annotate the genes, tools like `pp.pca` will\n              detect the annotation\n            * you can now call: `sc.pl.highly_variable_genes(adata)`\n            * `copy` is replaced by `inplace`\n\n    If trying out parameters, pass the data matrix instead of AnnData.\n\n    Depending on `flavor`, this reproduces the R-implementations of Seurat\n    [Satija15]_ and Cell Ranger [Zheng17]_.\n\n    The normalized dispersion is obtained by scaling with the mean and standard\n    deviation of the dispersions for genes falling into a given bin for mean\n    expression of genes. This means that for each bin of mean expression, highly\n    variable genes are selected.\n\n    Use `flavor='cell_ranger'` with care and in the same way as in\n    :func:`~scanpy.pp.recipe_zheng17`.\n\n    Parameters\n    ----------\n    data\n        The (annotated) data matrix of shape `n_obs` \u00d7 `n_vars`. Rows correspond\n        to cells and columns to genes.\n    flavor\n        Choose the flavor for computing normalized dispersion. If choosing\n        'seurat', this expects non-logarithmized data \u2013 the logarithm of mean\n        and dispersion is taken internally when `log` is at its default value\n        `True`. For 'cell_ranger', this is usually called for logarithmized data\n        \u2013 in this case you should set `log` to `False`. In their default\n        workflows, Seurat passes the cutoffs whereas Cell Ranger passes\n        `n_top_genes`.\n    min_mean\n    max_mean\n    min_disp\n    max_disp\n        If `n_top_genes` unequals `None`, these cutoffs for the means and the\n        normalized dispersions are ignored.\n    n_bins\n        Number of bins for binning the mean gene expression. Normalization is\n        done with respect to each bin. If just a single gene falls into a bin,\n        the normalized dispersion is artificially set to 1. You'll be informed\n        about this if you set `settings.verbosity = 4`.\n    n_top_genes\n        Number of highly-variable genes to keep.\n    log\n        Use the logarithm of the mean to variance ratio.\n    subset\n        Keep highly-variable genes only (if True) else write a bool array for h\n        ighly-variable genes while keeping all genes\n    copy\n        If an :class:`~anndata.AnnData` is passed, determines whether a copy\n        is returned.\n\n    Returns\n    -------\n    If an AnnData `adata` is passed, returns or updates `adata` depending on\n    `copy`. It filters the `adata` and adds the annotations\n\n    **means** : adata.var\n        Means per gene. Logarithmized when `log` is `True`.\n    **dispersions** : adata.var\n        Dispersions per gene. Logarithmized when `log` is `True`.\n    **dispersions_norm** : adata.var\n        Normalized dispersions per gene. Logarithmized when `log` is `True`.\n\n    If a data matrix `X` is passed, the annotation is returned as `np.recarray`\n    with the same information stored in fields: `gene_subset`, `means`, `dispersions`, `dispersion_norm`.\n    \"\"\"\n    if n_top_genes is not None and not all(\n        x is None for x in [min_disp, max_disp, min_mean, max_mean]\n    ):\n        logg.info('If you pass `n_top_genes`, all cutoffs are ignored.')\n    if min_disp is None:\n        min_disp = 0.5\n    if min_mean is None:\n        min_mean = 0.0125\n    if max_mean is None:\n        max_mean = 3\n    if isinstance(data, AnnData):\n        adata = data.copy() if copy else data\n        result = filter_genes_dispersion(\n            adata.X,\n            log=log,\n            min_disp=min_disp,\n            max_disp=max_disp,\n            min_mean=min_mean,\n            max_mean=max_mean,\n            n_top_genes=n_top_genes,\n            flavor=flavor,\n        )\n        adata.var['means'] = result['means']\n        adata.var['dispersions'] = result['dispersions']\n        adata.var['dispersions_norm'] = result['dispersions_norm']\n        if subset:\n            adata._inplace_subset_var(result['gene_subset'])\n        else:\n            adata.var['highly_variable'] = result['gene_subset']\n        return adata if copy else None\n    start = logg.info('extracting highly variable genes')\n    X = data  # no copy necessary, X remains unchanged in the following\n    mean, var = materialize_as_ndarray(_get_mean_var(X))\n    # now actually compute the dispersion\n    mean[mean == 0] = 1e-12  # set entries equal to zero to small value\n    dispersion = var / mean\n    if log:  # logarithmized mean as in Seurat\n        dispersion[dispersion == 0] = np.nan\n        dispersion = np.log(dispersion)\n        mean = np.log1p(mean)\n    # all of the following quantities are \"per-gene\" here\n    df = pd.DataFrame()\n    df['mean'] = mean\n    df['dispersion'] = dispersion\n    if flavor == 'seurat':\n        df['mean_bin'] = pd.cut(df['mean'], bins=n_bins)\n        disp_grouped = df.groupby('mean_bin')['dispersion']\n        disp_mean_bin = disp_grouped.mean()\n        disp_std_bin = disp_grouped.std(ddof=1)\n        # retrieve those genes that have nan std, these are the ones where\n        # only a single gene fell in the bin and implicitly set them to have\n        # a normalized disperion of 1\n        one_gene_per_bin = disp_std_bin.isnull()\n        gen_indices = np.where(one_gene_per_bin[df['mean_bin'].values])[0].tolist()\n        if len(gen_indices) > 0:\n            logg.debug(\n                f'Gene indices {gen_indices} fell into a single bin: their '\n                'normalized dispersion was set to 1.\\n    '\n                'Decreasing `n_bins` will likely avoid this effect.'\n            )\n        # Circumvent pandas 0.23 bug. Both sides of the assignment have dtype==float32,\n        # but there\u2019s still a dtype error without \u201c.value\u201d.\n        disp_std_bin[one_gene_per_bin] = disp_mean_bin[one_gene_per_bin.values].values\n        disp_mean_bin[one_gene_per_bin] = 0\n        # actually do the normalization\n        df['dispersion_norm'] = (\n            # use values here as index differs\n            df['dispersion'].values\n            - disp_mean_bin[df['mean_bin'].values].values\n        ) / disp_std_bin[df['mean_bin'].values].values\n    elif flavor == 'cell_ranger':\n        from statsmodels import robust\n\n        df['mean_bin'] = pd.cut(\n            df['mean'],\n            np.r_[-np.inf, np.percentile(df['mean'], np.arange(10, 105, 5)), np.inf],\n        )\n        disp_grouped = df.groupby('mean_bin')['dispersion']\n        disp_median_bin = disp_grouped.median()\n        # the next line raises the warning: \"Mean of empty slice\"\n        with warnings.catch_warnings():\n            warnings.simplefilter('ignore')\n            disp_mad_bin = disp_grouped.apply(robust.mad)\n        df['dispersion_norm'] = (\n            np.abs(\n                df['dispersion'].values - disp_median_bin[df['mean_bin'].values].values\n            )\n            / disp_mad_bin[df['mean_bin'].values].values\n        )\n    else:\n        raise ValueError('`flavor` needs to be \"seurat\" or \"cell_ranger\"')\n    dispersion_norm = df['dispersion_norm'].values.astype('float32')\n    if n_top_genes is not None:\n        dispersion_norm = dispersion_norm[~np.isnan(dispersion_norm)]\n        dispersion_norm[\n            ::-1\n        ].sort()  # interestingly, np.argpartition is slightly slower\n        disp_cut_off = dispersion_norm[n_top_genes - 1]\n        gene_subset = df['dispersion_norm'].values >= disp_cut_off\n        logg.debug(\n            f'the {n_top_genes} top genes correspond to a '\n            f'normalized dispersion cutoff of {disp_cut_off}'\n        )\n    else:\n        max_disp = np.inf if max_disp is None else max_disp\n        dispersion_norm[np.isnan(dispersion_norm)] = 0  # similar to Seurat\n        gene_subset = np.logical_and.reduce(\n            (\n                mean > min_mean,\n                mean < max_mean,\n                dispersion_norm > min_disp,\n                dispersion_norm < max_disp,\n            )\n        )\n    logg.info('    finished', time=start)\n    return np.rec.fromarrays(\n        (\n            gene_subset,\n            df['mean'].values,\n            df['dispersion'].values,\n            df['dispersion_norm'].values.astype('float32', copy=False),\n        ),\n        dtype=[\n            ('gene_subset', bool),\n            ('means', 'float32'),\n            ('dispersions', 'float32'),\n            ('dispersions_norm', 'float32'),", "idx": 379}
{"project": "Scanpy", "commit_id": "443_scanpy_1.9.0_highly_variable_genes.py_filter_genes_cv_deprecated.py", "target": 0, "func": "def filter_genes_cv_deprecated(X, Ecutoff, cvFilter):\n    \"\"\"Filter genes by coefficient of variance and mean.\"\"\"\n    return _filter_genes(X, Ecutoff, cvFilter, np.std)", "idx": 380}
{"project": "Scanpy", "commit_id": "444_scanpy_1.9.0_highly_variable_genes.py_filter_genes_fano_deprecated.py", "target": 0, "func": "def filter_genes_fano_deprecated(X, Ecutoff, Vcutoff):\n    \"\"\"Filter genes by fano factor and mean.\"\"\"\n    return _filter_genes(X, Ecutoff, Vcutoff, np.var)", "idx": 381}
{"project": "Scanpy", "commit_id": "445_scanpy_1.9.0_highly_variable_genes.py__filter_genes.py", "target": 0, "func": "def _filter_genes(X, e_cutoff, v_cutoff, meth):\n    \"\"\"\\\n    See `filter_genes_dispersion`.\n\n    Reference: Weinreb et al. (2017).\"\"\"\n    if issparse(X):\n        raise ValueError('Not defined for sparse input. See `filter_genes_dispersion`.')\n    mean_filter = np.mean(X, axis=0) > e_cutoff\n    var_filter = meth(X, axis=0) / (np.mean(X, axis=0) + 0.0001) > v_cutoff\n    gene_subset = np.nonzero(np.all([mean_filter, var_filter], axis=0))[0]\n    return gene_subset", "idx": 382}
{"project": "Scanpy", "commit_id": "446_scanpy_1.9.0___init__.py_normalize_per_cell_weinreb16_deprecated.py", "target": 0, "func": "def normalize_per_cell_weinreb16_deprecated(\n    X: np.ndarray,\n    max_fraction: float = 1,\n    mult_with_mean: bool = False,\n) -> np.ndarray:\n    \"\"\"\\\n    Normalize each cell [Weinreb17]_.\n\n    This is a deprecated version. See `normalize_per_cell` instead.\n\n    Normalize each cell by UMI count, so that every cell has the same total\n    count.\n\n    Parameters\n    ----------\n    X\n        Expression matrix. Rows correspond to cells and columns to genes.\n    max_fraction\n        Only use genes that make up more than max_fraction of the total\n        reads in every cell.\n    mult_with_mean\n        Multiply the result with the mean of total counts.\n\n    Returns\n    -------\n    Normalized version of the original expression matrix.\n    \"\"\"\n    if max_fraction < 0 or max_fraction > 1:\n        raise ValueError('Choose max_fraction between 0 and 1.')\n\n    counts_per_cell = X.sum(1).A1 if issparse(X) else X.sum(1)\n    gene_subset = np.all(X <= counts_per_cell[:, None] * max_fraction, axis=0)\n    if issparse(X):\n        gene_subset = gene_subset.A1\n    tc_include = (\n        X[:, gene_subset].sum(1).A1 if issparse(X) else X[:, gene_subset].sum(1)\n    )\n\n    X_norm = (\n        X.multiply(csr_matrix(1 / tc_include[:, None]))\n        if issparse(X)\n        else X / tc_include[:, None]\n    )\n    if mult_with_mean:\n        X_norm *= np.mean(counts_per_cell)\n\n    return X_norm", "idx": 383}
{"project": "Scanpy", "commit_id": "447_scanpy_1.9.0___init__.py_zscore_deprecated.py", "target": 0, "func": "def zscore_deprecated(X: np.ndarray) -> np.ndarray:\n    \"\"\"\\\n    Z-score standardize each variable/gene in X.\n\n    Use `scale` instead.\n\n    Reference: Weinreb et al. (2017).\n\n    Parameters\n    ----------\n    X\n        Data matrix. Rows correspond to cells and columns to genes.\n\n    Returns\n    -------\n    Z-score standardized version of the data matrix.\n    \"\"\"\n    means = np.tile(np.mean(X, axis=0)[None, :], (X.shape[0], 1))\n    stds = np.tile(np.std(X, axis=0)[None, :], (X.shape[0], 1))\n    return (X - means) / (stds + 0.0001)", "idx": 384}
{"project": "Scanpy", "commit_id": "448_scanpy_1.9.0__queries.py_simple_query.py", "target": 0, "func": "def simple_query(\n    org: str,\n    attrs: Union[Iterable[str], str],\n    *,\n    filters: Optional[Dict[str, Any]] = None,\n    host: str = \"www.ensembl.org\",\n    use_cache: bool = False,\n) -> pd.DataFrame:\n    \"\"\"\\\n    A simple interface to biomart.\n\n    Params\n    ------\n    {doc_org}\n    attrs\n        What you want returned.\n    filters\n        What you want to pick out.\n    {doc_host}\n    {doc_use_cache}\n    \"\"\"\n    if isinstance(attrs, str):\n        attrs = [attrs]\n    elif isinstance(attrs, cabc.Iterable):\n        attrs = list(attrs)\n    else:\n        raise TypeError(f\"attrs must be of type list or str, was {type(attrs)}.\")\n    try:\n        from pybiomart import Server\n    except ImportError:\n        raise ImportError(\n            \"This method requires the `pybiomart` module to be installed.\"\n        )\n    server = Server(host, use_cache=use_cache)\n    dataset = server.marts[\"ENSEMBL_MART_ENSEMBL\"].datasets[\n        \"{}_gene_ensembl\".format(org)\n    ]\n    res = dataset.query(attributes=attrs, filters=filters, use_attr_names=True)\n    return res", "idx": 385}
{"project": "Scanpy", "commit_id": "449_scanpy_1.9.0__queries.py_biomart_annotations.py", "target": 0, "func": "def biomart_annotations(\n    org: str,\n    attrs: Iterable[str],\n    *,\n    host: str = \"www.ensembl.org\",\n    use_cache: bool = False,\n) -> pd.DataFrame:\n    \"\"\"\\\n    Retrieve gene annotations from ensembl biomart.\n\n    Parameters\n    ----------\n    {doc_org}\n    attrs\n        Attributes to query biomart for.\n    {doc_host}\n    {doc_use_cache}\n\n    Returns\n    -------\n    Dataframe containing annotations.\n\n    Examples\n    --------\n    Retrieve genes coordinates and chromosomes\n\n    >>> import scanpy as sc\n    >>> annot = sc.queries.biomart_annotations(\n            \"hsapiens\",\n            [\"ensembl_gene_id\", \"start_position\", \"end_position\", \"chromosome_name\"],\n        ).set_index(\"ensembl_gene_id\")\n    >>> adata.var[annot.columns] = annot\n    \"\"\"\n    return simple_query(org=org, attrs=attrs, host=host, use_cache=use_cache)", "idx": 386}
{"project": "Scanpy", "commit_id": "44_scanpy_1.9.0_logging.py_warning.py", "target": 0, "func": "def warning(self, msg, *, time=None, deep=None, extra=None) -> datetime:\n        return self.log(WARNING, msg, time=time, deep=deep, extra=extra)", "idx": 387}
{"project": "Scanpy", "commit_id": "450_scanpy_1.9.0__queries.py_gene_coordinates.py", "target": 0, "func": "def gene_coordinates(\n    org: str,\n    gene_name: str,\n    *,\n    gene_attr: str = \"external_gene_name\",\n    chr_exclude: Iterable[str] = (),\n    host: str = \"www.ensembl.org\",\n    use_cache: bool = False,\n) -> pd.DataFrame:\n    \"\"\"\\\n    Retrieve gene coordinates for specific organism through BioMart.\n\n    Parameters\n    ----------\n    {doc_org}\n    gene_name\n        The gene symbol (e.g. \"hgnc_symbol\" for human) for which to retrieve\n        coordinates.\n    gene_attr\n        The biomart attribute the gene symbol should show up for.\n    chr_exclude\n        A list of chromosomes to exclude from query.\n    {doc_host}\n    {doc_use_cache}\n\n    Returns\n    -------\n    Dataframe containing gene coordinates for the specified gene symbol.\n\n    Examples\n    --------\n    >>> import scanpy as sc\n    >>> sc.queries.gene_coordinates(\"hsapiens\", \"MT-TF\")\n    \"\"\"\n    res = simple_query(\n        org=org,\n        attrs=[\"chromosome_name\", \"start_position\", \"end_position\"],\n        filters={gene_attr: gene_name},\n        host=host,\n        use_cache=use_cache,\n    )\n    return res[~res[\"chromosome_name\"].isin(chr_exclude)]", "idx": 388}
{"project": "Scanpy", "commit_id": "451_scanpy_1.9.0__queries.py_mitochondrial_genes.py", "target": 0, "func": "def mitochondrial_genes(\n    org: str,\n    *,\n    attrname: str = \"external_gene_name\",\n    host: str = \"www.ensembl.org\",\n    use_cache: bool = False,\n    chromosome: str = \"MT\",\n) -> pd.DataFrame:\n    \"\"\"\\\n    Mitochondrial gene symbols for specific organism through BioMart.\n\n    Parameters\n    ----------\n    {doc_org}\n    attrname\n        Biomart attribute field to return. Possible values include\n        \"external_gene_name\", \"ensembl_gene_id\", \"hgnc_symbol\", \"mgi_symbol\",\n        and \"zfin_id_symbol\".\n    {doc_host}\n    {doc_use_cache}\n    chromosome\n        Mitochrondrial chromosome name used in BioMart for organism.\n\n    Returns\n    -------\n    Dataframe containing identifiers for mitochondrial genes.\n\n    Examples\n    --------\n    >>> import scanpy as sc\n    >>> mito_gene_names = sc.queries.mitochondrial_genes(\"hsapiens\")\n    >>> mito_ensembl_ids = sc.queries.mitochondrial_genes(\"hsapiens\", attrname=\"ensembl_gene_id\")\n    >>> mito_gene_names_fly = sc.queries.mitochondrial_genes(\"dmelanogaster\", chromosome=\"mitochondrion_genome\")\n    \"\"\"\n    return simple_query(\n        org,\n        attrs=[attrname],\n        filters={\"chromosome_name\": [chromosome]},\n        host=host,\n        use_cache=use_cache,", "idx": 389}
{"project": "Scanpy", "commit_id": "452_scanpy_1.9.0__queries.py_enrich.py", "target": 0, "func": "def enrich(\n    container: Union[Iterable[str], Mapping[str, Iterable[str]]],\n    *,\n    org: str = \"hsapiens\",\n    gprofiler_kwargs: Mapping[str, Any] = MappingProxyType({}),\n) -> pd.DataFrame:\n    \"\"\"\\\n    Get enrichment for DE results.\n\n    This is a thin convenience wrapper around the very useful gprofiler_.\n\n    This method dispatches on the first argument, leading to the following two\n    signatures::\n\n        enrich(container, ...)\n        enrich(adata: AnnData, group, key: str, ...)\n\n    Where::\n\n        enrich(adata, group, key, ...) = enrich(adata.uns[key][\"names\"][group], ...)\n\n    .. _gprofiler: https://pypi.org/project/gprofiler-official/#description\n\n    Parameters\n    ----------\n    container\n        Contains list of genes you'd like to search. If container is a `dict` all\n        enrichment queries are made at once.\n    adata\n        AnnData object whose group will be looked for.\n    group\n        The group whose genes should be used for enrichment.\n    key\n        Key in `uns` to find group under.\n    {doc_org}\n    gprofiler_kwargs\n        Keyword arguments to pass to `GProfiler.profile`, see gprofiler_. Some\n        useful options are `no_evidences=False` which reports gene intersections,\n        `sources=['GO:BP']` which limits gene sets to only GO biological processes and\n        `all_results=True` which returns all results including the non-significant ones.\n    **kwargs\n        All other keyword arguments are passed to `sc.get.rank_genes_groups_df`. E.g.\n        pval_cutoff, log2fc_min.\n\n    Returns\n    -------\n    Dataframe of enrichment results.\n\n    Examples\n    --------\n    Using `sc.queries.enrich` on a list of genes:\n\n    >>> import scanpy as sc\n    >>> sc.queries.enrich(['KLF4', 'PAX5', 'SOX2', 'NANOG'], org=\"hsapiens\")\n    >>> sc.queries.enrich({{'set1':['KLF4', 'PAX5'], 'set2':['SOX2', 'NANOG']}}, org=\"hsapiens\")\n\n    Using `sc.queries.enrich` on an :class:`anndata.AnnData` object:\n\n    >>> pbmcs = sc.datasets.pbmc68k_reduced()\n    >>> sc.tl.rank_genes_groups(pbmcs, \"bulk_labels\")\n    >>> sc.queries.enrich(pbmcs, \"CD34+\")\n    \"\"\"\n    try:\n        from gprofiler import GProfiler\n    except ImportError:\n        raise ImportError(\n            \"This method requires the `gprofiler-official` module to be installed.\"\n        )\n    gprofiler = GProfiler(user_agent=\"scanpy\", return_dataframe=True)\n    gprofiler_kwargs = dict(gprofiler_kwargs)\n    for k in [\"organism\"]:\n        if gprofiler_kwargs.get(k) is not None:\n            raise ValueError(\n                f\"Argument `{k}` should be passed directly through `enrich`, \"\n                \"not through `gprofiler_kwargs`\"\n            )\n    return gprofiler.profile(container, organism=org, **gprofiler_kwargs)", "idx": 390}
{"project": "Scanpy", "commit_id": "453_scanpy_1.9.0__queries.py__enrich_anndata.py", "target": 0, "func": "def _enrich_anndata(\n    adata: AnnData,\n    group: str,\n    *,\n    org: Optional[str] = \"hsapiens\",\n    key: str = \"rank_genes_groups\",\n    pval_cutoff: float = 0.05,\n    log2fc_min: Optional[float] = None,\n    log2fc_max: Optional[float] = None,\n    gene_symbols: Optional[str] = None,\n    gprofiler_kwargs: Mapping[str, Any] = MappingProxyType({}),\n) -> pd.DataFrame:\n    de = rank_genes_groups_df(\n        adata,\n        group=group,\n        key=key,\n        pval_cutoff=pval_cutoff,\n        log2fc_min=log2fc_min,\n        log2fc_max=log2fc_max,\n        gene_symbols=gene_symbols,\n    )\n    if gene_symbols is not None:\n        gene_list = list(de[gene_symbols].dropna())\n    else:\n        gene_list = list(de[\"names\"].dropna())\n    return enrich(gene_list, org=org, gprofiler_kwargs=gprofiler_kwargs)", "idx": 391}
{"project": "Scanpy", "commit_id": "454_scanpy_1.9.0_conftest.py_close_figures_on_teardown.py", "target": 0, "func": "def close_figures_on_teardown():\n    yield\n    pyplot.close(\"all\")", "idx": 392}
{"project": "Scanpy", "commit_id": "455_scanpy_1.9.0_conftest.py_clear_loggers.py", "target": 0, "func": "def clear_loggers():\n    \"\"\"Remove handlers from all loggers\n\n    Fixes: https://github.com/theislab/scanpy/issues/1736\n\n    Code from: https://github.com/pytest-dev/pytest/issues/5502#issuecomment-647157873\n    \"\"\"\n    import logging\n\n    loggers = [logging.getLogger()] + list(logging.Logger.manager.loggerDict.values())\n    for logger in loggers:\n        handlers = getattr(logger, 'handlers', [])\n        for handler in handlers:\n            logger.removeHandler(handler)", "idx": 393}
{"project": "Scanpy", "commit_id": "456_scanpy_1.9.0_conftest.py_close_logs_on_teardown.py", "target": 0, "func": "def close_logs_on_teardown(request):\n    request.addfinalizer(clear_loggers)", "idx": 394}
{"project": "Scanpy", "commit_id": "457_scanpy_1.9.0_conftest.py_imported_modules.py", "target": 0, "func": "def imported_modules():\n    return IMPORTED", "idx": 395}
{"project": "Scanpy", "commit_id": "458_scanpy_1.9.0_conftest.py_check_same_image.py", "target": 0, "func": "def check_same_image(add_nunit_attachment):\n    def _(pth1, pth2, *, tol: int, basename: str = \"\"):\n        def fmt_descr(descr):\n            if basename != \"\":\n                return f\"{descr} ({basename})\"\n            else:\n                return descr\n\n        pth1, pth2 = Path(pth1), Path(pth2)\n        try:\n            result = compare_images(str(pth1), str(pth2), tol=tol)\n            assert result is None, result\n        except Exception as e:\n            diff_pth = make_test_filename(pth2, 'failed-diff')\n            add_nunit_attachment(str(pth1), fmt_descr(\"Expected\"))\n            add_nunit_attachment(str(pth2), fmt_descr(\"Result\"))\n            if Path(diff_pth).is_file():\n                add_nunit_attachment(str(diff_pth), fmt_descr(\"Difference\"))\n            raise e\n\n    return _", "idx": 396}
{"project": "Scanpy", "commit_id": "459_scanpy_1.9.0_conftest.py_image_comparer.py", "target": 0, "func": "def image_comparer(check_same_image):\n    def make_comparer(path_expected: Path, path_actual: Path, *, tol: int):\n        def save_and_compare(basename, tol=tol):\n            path_actual.mkdir(parents=True, exist_ok=True)\n            out_path = path_actual / f'{basename}.png'\n            pyplot.savefig(out_path, dpi=40)\n            pyplot.close()\n            check_same_image(path_expected / f'{basename}.png', out_path, tol=tol)\n\n        return save_and_compare\n\n    return make_comparer", "idx": 397}
{"project": "Scanpy", "commit_id": "45_scanpy_1.9.0_logging.py_info.py", "target": 0, "func": "def info(self, msg, *, time=None, deep=None, extra=None) -> datetime:\n        return self.log(INFO, msg, time=time, deep=deep, extra=extra)", "idx": 398}
{"project": "Scanpy", "commit_id": "460_scanpy_1.9.0_conftest.py_plt.py", "target": 0, "func": "def plt():\n    return pyplot", "idx": 399}
{"project": "Scanpy", "commit_id": "461_scanpy_1.9.0_conftest.py__.py", "target": 0, "func": "def _(pth1, pth2, *, tol: int, basename: str = \"\"):\n        def fmt_descr(descr):\n            if basename != \"\":\n                return f\"{descr} ({basename})\"\n            else:\n                return descr\n\n        pth1, pth2 = Path(pth1), Path(pth2)\n        try:\n            result = compare_images(str(pth1), str(pth2), tol=tol)\n            assert result is None, result\n        except Exception as e:\n            diff_pth = make_test_filename(pth2, 'failed-diff')\n            add_nunit_attachment(str(pth1), fmt_descr(\"Expected\"))\n            add_nunit_attachment(str(pth2), fmt_descr(\"Result\"))\n            if Path(diff_pth).is_file():\n                add_nunit_attachment(str(diff_pth), fmt_descr(\"Difference\"))\n            raise e", "idx": 400}
{"project": "Scanpy", "commit_id": "462_scanpy_1.9.0_conftest.py_make_comparer.py", "target": 0, "func": "def make_comparer(path_expected: Path, path_actual: Path, *, tol: int):\n        def save_and_compare(basename, tol=tol):\n            path_actual.mkdir(parents=True, exist_ok=True)\n            out_path = path_actual / f'{basename}.png'\n            pyplot.savefig(out_path, dpi=40)\n            pyplot.close()\n            check_same_image(path_expected / f'{basename}.png', out_path, tol=tol)\n\n        return save_and_compare", "idx": 401}
{"project": "Scanpy", "commit_id": "463_scanpy_1.9.0_conftest.py_fmt_descr.py", "target": 0, "func": "def fmt_descr(descr):\n            if basename != \"\":\n                return f\"{descr} ({basename})\"\n            else:\n                return descr", "idx": 402}
{"project": "Scanpy", "commit_id": "464_scanpy_1.9.0_conftest.py_save_and_compare.py", "target": 0, "func": "def save_and_compare(basename, tol=tol):\n            path_actual.mkdir(parents=True, exist_ok=True)\n            out_path = path_actual / f'{basename}.png'\n            pyplot.savefig(out_path, dpi=40)\n            pyplot.close()\n            check_same_image(path_expected / f'{basename}.png', out_path, tol=tol)", "idx": 403}
{"project": "Scanpy", "commit_id": "465_scanpy_1.9.0_fixtures.py_array_type.py", "target": 0, "func": "def array_type(request):\n    \"\"\"Function which converts passed array to one of the common array types.\"\"\"\n    return request.param", "idx": 404}
{"project": "Scanpy", "commit_id": "466_scanpy_1.9.0_fixtures.py_float_dtype.py", "target": 0, "func": "def float_dtype(request):\n    return request.param", "idx": 405}
{"project": "Scanpy", "commit_id": "467_scanpy_1.9.0_helpers.py_check_rep_mutation.py", "target": 0, "func": "def check_rep_mutation(func, X, *, fields=[\"layer\", \"obsm\"], **kwargs):\n    \"\"\"Check that only the array meant to be modified is modified.\"\"\"\n    adata = sc.AnnData(X=X.copy(), dtype=X.dtype)\n    for field in fields:\n        sc.get._set_obs_rep(adata, X, **{field: field})\n    X_array = asarray(X)\n\n    adata_X = func(adata, copy=True, **kwargs)\n    adatas_proc = {\n        field: func(adata, copy=True, **{field: field}, **kwargs) for field in fields\n    }\n\n    # Modified fields\n    for field in fields:\n        result_array = asarray(\n            sc.get._get_obs_rep(adatas_proc[field], **{field: field})\n        )\n        np.testing.assert_array_equal(asarray(adata_X.X), result_array)\n\n    # Unmodified fields\n    for field in fields:\n        np.testing.assert_array_equal(X_array, asarray(adatas_proc[field].X))\n        np.testing.assert_array_equal(\n            X_array, asarray(sc.get._get_obs_rep(adata_X, **{field: field}))\n        )\n    for field_a, field_b in permutations(fields, 2):\n        result_array = asarray(\n            sc.get._get_obs_rep(adatas_proc[field_a], **{field_b: field_b})\n        )\n        np.testing.assert_array_equal(X_array, result_array)", "idx": 406}
{"project": "Scanpy", "commit_id": "468_scanpy_1.9.0_helpers.py_check_rep_results.py", "target": 0, "func": "def check_rep_results(func, X, *, fields=[\"layer\", \"obsm\"], **kwargs):\n    \"\"\"Checks that the results of a computation add values/ mutate the anndata object in a consistent way.\"\"\"\n    # Gen data\n    empty_X = np.zeros(shape=X.shape, dtype=X.dtype)\n    adata = sc.AnnData(\n        X=empty_X.copy(),\n        layers={\"layer\": empty_X.copy()},\n        obsm={\"obsm\": empty_X.copy()},\n    )\n\n    adata_X = adata.copy()\n    adata_X.X = X.copy()\n\n    adatas_proc = {}\n    for field in fields:\n        cur = adata.copy()\n        sc.get._set_obs_rep(cur, X.copy(), **{field: field})\n        adatas_proc[field] = cur\n\n    # Apply function\n    func(adata_X, **kwargs)\n    for field in fields:\n        func(adatas_proc[field], **{field: field}, **kwargs)\n\n    # Reset X\n    adata_X.X = empty_X.copy()\n    for field in fields:\n        sc.get._set_obs_rep(adatas_proc[field], empty_X.copy(), **{field: field})\n\n    for field_a, field_b in permutations(fields, 2):\n        assert_equal(adatas_proc[field_a], adatas_proc[field_b])\n    for field in fields:\n        assert_equal(adata_X, adatas_proc[field])", "idx": 407}
{"project": "Scanpy", "commit_id": "469_scanpy_1.9.0_helpers.py__prepare_pbmc_testdata.py", "target": 0, "func": "def _prepare_pbmc_testdata(sparsity_func, dtype, small=False):\n    \"\"\"Prepares 3k PBMC dataset with batch key `batch` and defined datatype/sparsity.\n\n    Params\n    ------\n    sparsity_func\n        sparsity function applied to adata.X (e.g. csr_matrix.toarray for dense or csr_matrix for sparse)\n    dtype\n        numpy dtype applied to adata.X (e.g. 'float32' or 'int64')\n    small\n        False (default) returns full data, True returns small subset of the data.\"\"\"\n\n    adata = pbmc3k().copy()\n\n    if small:\n        adata = adata[:1000, :500]\n        sc.pp.filter_cells(adata, min_genes=1)\n    np.random.seed(42)\n    adata.obs['batch'] = np.random.randint(0, 3, size=adata.shape[0])\n    sc.pp.filter_genes(adata, min_cells=1)\n    adata.X = sparsity_func(adata.X.astype(dtype))\n    return adata", "idx": 408}
{"project": "Scanpy", "commit_id": "46_scanpy_1.9.0_logging.py_hint.py", "target": 0, "func": "def hint(self, msg, *, time=None, deep=None, extra=None) -> datetime:\n        return self.log(HINT, msg, time=time, deep=deep, extra=extra)", "idx": 409}
{"project": "Scanpy", "commit_id": "470_scanpy_1.9.0_helpers.py__check_check_values_warnings.py", "target": 0, "func": "def _check_check_values_warnings(function, adata, expected_warning, kwargs={}):\n    '''Runs `function` on `adata` with provided arguments `kwargs` twice: once with `check_values=True` and once with `check_values=False`. Checks that the `expected_warning` is only raised whtn `check_values=True`.'''\n\n    # expecting 0 no-int warnings\n    with warnings.catch_warnings(record=True) as record:\n        function(adata.copy(), **kwargs, check_values=False)\n    warning_msgs = [w.message.args[0] for w in record]\n    assert expected_warning not in warning_msgs\n\n    # expecting 1 no-int warning\n    with warnings.catch_warnings(record=True) as record:\n        function(adata.copy(), **kwargs, check_values=True)\n    warning_msgs = [w.message.args[0] for w in record]\n    assert expected_warning in warning_msgs", "idx": 410}
{"project": "Scanpy", "commit_id": "471_scanpy_1.9.0_test_binary.py_set_path.py", "target": 0, "func": "def set_path(monkeypatch: MonkeyPatch) -> None:\n    monkeypatch.setenv('PATH', str(HERE / '_scripts'), prepend=os.pathsep)", "idx": 411}
{"project": "Scanpy", "commit_id": "472_scanpy_1.9.0_test_binary.py_test_builtin_settings.py", "target": 0, "func": "def test_builtin_settings(capsys: CaptureFixture):\n    main(['settings'])\n    captured = capsys.readouterr()\n    assert captured.out == f'{scanpy.settings}\\n'", "idx": 412}
{"project": "Scanpy", "commit_id": "473_scanpy_1.9.0_test_binary.py_test_help_displayed.py", "target": 0, "func": "def test_help_displayed(args: List[str], capsys: CaptureFixture):\n    try:  # -h raises it, no args doesn\u2019t. Maybe not ideal but meh.\n        main(args)\n    except SystemExit as se:\n        assert se.code == 0\n    captured = capsys.readouterr()\n    assert captured.out.startswith('usage: ')", "idx": 413}
{"project": "Scanpy", "commit_id": "474_scanpy_1.9.0_test_binary.py_test_help_output.py", "target": 0, "func": "def test_help_output(set_path: type(None), capsys: CaptureFixture):\n    with pytest.raises(SystemExit, match='^0$'):\n        main(['-h'])\n    captured = capsys.readouterr()\n    assert re.search(\n        r'^positional arguments:\\n\\s+\\{settings,[\\w,-]*testbin[\\w,-]*\\}$',\n        captured.out,\n        re.MULTILINE,", "idx": 414}
{"project": "Scanpy", "commit_id": "475_scanpy_1.9.0_test_binary.py_test_external.py", "target": 0, "func": "def test_external(set_path: type(None)):\n    # We need to capture the output manually, since subprocesses don\u2019t write to sys.stderr\n    cmdline = ['testbin', '-t', '--testarg', 'testpos']\n    cmd = main(cmdline, stdout=PIPE, encoding='utf-8', check=True)\n    assert cmd.stdout == 'test -t --testarg testpos\\n'", "idx": 415}
{"project": "Scanpy", "commit_id": "476_scanpy_1.9.0_test_binary.py_test_error_wrong_command.py", "target": 0, "func": "def test_error_wrong_command(capsys: CaptureFixture):\n    with pytest.raises(SystemExit, match='^2$'):\n        main(['idonotexist--'])\n    captured = capsys.readouterr()\n    assert 'No command \u201cidonotexist--\u201d. Choose from' in captured.err", "idx": 416}
{"project": "Scanpy", "commit_id": "477_scanpy_1.9.0_test_clustering.py_adata_neighbors.py", "target": 0, "func": "def adata_neighbors():\n    return pbmc68k_reduced()", "idx": 417}
{"project": "Scanpy", "commit_id": "478_scanpy_1.9.0_test_clustering.py_test_leiden_basic.py", "target": 0, "func": "def test_leiden_basic(adata_neighbors):\n    sc.tl.leiden(adata_neighbors)", "idx": 418}
{"project": "Scanpy", "commit_id": "479_scanpy_1.9.0_test_clustering.py_test_clustering_subset.py", "target": 0, "func": "def test_clustering_subset(adata_neighbors, clustering, key):\n    if clustering == sc.tl.louvain:\n        pytest.importorskip(\"louvain\")\n\n    clustering(adata_neighbors, key_added=key)\n\n    for c in adata_neighbors.obs[key].unique():\n        print('Analyzing cluster ', c)\n        cells_in_c = adata_neighbors.obs[key] == c\n        ncells_in_c = adata_neighbors.obs[key].value_counts().loc[c]\n        key_sub = str(key) + '_sub'\n        clustering(\n            adata_neighbors,\n            restrict_to=(key, [c]),\n            key_added=key_sub,\n        )\n        # Get new clustering labels\n        new_partition = adata_neighbors.obs[key_sub]\n\n        cat_counts = new_partition[cells_in_c].value_counts()\n\n        # Only original cluster's cells assigned to new categories\n        assert cat_counts.sum() == ncells_in_c\n\n        # Original category's cells assigned only to new categories\n        nonzero_cat = cat_counts[cat_counts > 0].index\n        common_cat = nonzero_cat.intersection(adata_neighbors.obs[key].cat.categories)\n        assert len(common_cat) == 0", "idx": 419}
{"project": "Scanpy", "commit_id": "47_scanpy_1.9.0_logging.py_debug.py", "target": 0, "func": "def debug(self, msg, *, time=None, deep=None, extra=None) -> datetime:\n        return self.log(DEBUG, msg, time=time, deep=deep, extra=extra)", "idx": 420}
{"project": "Scanpy", "commit_id": "480_scanpy_1.9.0_test_clustering.py_test_louvain_basic.py", "target": 0, "func": "def test_louvain_basic(adata_neighbors):\n    pytest.importorskip(\"louvain\")\n\n    sc.tl.louvain(adata_neighbors)\n    sc.tl.louvain(adata_neighbors, use_weights=True)\n    sc.tl.louvain(adata_neighbors, use_weights=True, flavor=\"igraph\")\n    sc.tl.louvain(adata_neighbors, flavor=\"igraph\")", "idx": 421}
{"project": "Scanpy", "commit_id": "481_scanpy_1.9.0_test_clustering.py_test_partition_type.py", "target": 0, "func": "def test_partition_type(adata_neighbors):\n    louvain = pytest.importorskip(\"louvain\")\n\n    sc.tl.louvain(adata_neighbors, partition_type=louvain.RBERVertexPartition)\n    sc.tl.louvain(adata_neighbors, partition_type=louvain.SurpriseVertexPartition)", "idx": 422}
{"project": "Scanpy", "commit_id": "482_scanpy_1.9.0_test_combat.py_test_norm.py", "target": 0, "func": "def test_norm():\n    # this test trivially checks whether mean normalisation worked\n\n    # load in data\n    adata = sc.datasets.blobs()\n    key = 'blobs'\n    data = pd.DataFrame(data=adata.X.T, index=adata.var_names, columns=adata.obs_names)\n\n    # construct a pandas series of the batch annotation\n    batch = pd.Series(adata.obs[key])\n    model = pd.DataFrame({'batch': batch})\n\n    # standardize the data\n    s_data, design, var_pooled, stand_mean = _standardize_data(model, data, 'batch')\n\n    assert np.allclose(s_data.mean(axis=1), np.zeros(s_data.shape[0]))", "idx": 423}
{"project": "Scanpy", "commit_id": "483_scanpy_1.9.0_test_combat.py_test_covariates.py", "target": 0, "func": "def test_covariates():\n    adata = sc.datasets.blobs()\n    key = 'blobs'\n\n    X1 = sc.pp.combat(adata, key=key, inplace=False)\n\n    np.random.seed(0)\n    adata.obs['cat1'] = np.random.binomial(3, 0.5, size=(adata.n_obs))\n    adata.obs['cat2'] = np.random.binomial(2, 0.1, size=(adata.n_obs))\n    adata.obs['num1'] = np.random.normal(size=(adata.n_obs))\n\n    X2 = sc.pp.combat(\n        adata, key=key, covariates=['cat1', 'cat2', 'num1'], inplace=False\n    )\n    sc.pp.combat(adata, key=key, covariates=['cat1', 'cat2', 'num1'], inplace=True)\n\n    assert X1.shape == X2.shape\n\n    df = adata.obs[['cat1', 'cat2', 'num1', key]]\n    batch_cats = adata.obs[key].cat.categories\n    design = _design_matrix(df, key, batch_cats)\n\n    assert len(design.columns) == 4 + len(batch_cats) - 1", "idx": 424}
{"project": "Scanpy", "commit_id": "484_scanpy_1.9.0_test_combat.py_test_combat_obs_names.py", "target": 0, "func": "def test_combat_obs_names():\n    # Test for fix to #1170\n    X = np.random.random((200, 100))\n    obs = pd.DataFrame(\n        {\"batch\": pd.Categorical(np.random.randint(0, 2, 200))},\n        index=np.repeat(np.arange(100), 2).astype(str),  # Non-unique index\n    )\n    a = sc.AnnData(X, obs)\n    b = a.copy()\n    b.obs_names_make_unique()\n\n    sc.pp.combat(a, \"batch\")\n    sc.pp.combat(b, \"batch\")\n\n    assert_equal(a.X, b.X)\n\n    a.obs_names_make_unique()\n    assert_equal(a, b)", "idx": 425}
{"project": "Scanpy", "commit_id": "485_scanpy_1.9.0_test_combat.py_test_silhouette.py", "target": 0, "func": "def test_silhouette():\n    # this test checks wether combat can align data from several gaussians\n    # it checks this by computing the silhouette coefficient in a pca embedding\n\n    # load in data\n    adata = sc.datasets.blobs()\n\n    # apply combat\n    sc.pp.combat(adata, 'blobs')\n\n    # compute pca\n    sc.tl.pca(adata)\n    X_pca = adata.obsm['X_pca']\n\n    # compute silhouette coefficient in pca\n    sh = silhouette_score(X_pca[:, :2], adata.obs['blobs'].values)\n\n    assert sh < 0.1", "idx": 426}
{"project": "Scanpy", "commit_id": "486_scanpy_1.9.0_test_datasets.py_tmp_dataset_dir.py", "target": 0, "func": "def tmp_dataset_dir(tmpdir_factory):\n    new_dir = Path(tmpdir_factory.mktemp(\"scanpy_data\"))\n    old_dir = sc.settings.datasetdir\n    sc.settings.datasetdir = new_dir  # Set up\n    yield sc.settings.datasetdir\n    sc.settings.datasetdir = old_dir  # Tear down", "idx": 427}
{"project": "Scanpy", "commit_id": "487_scanpy_1.9.0_test_datasets.py_test_burczynski06.py", "target": 0, "func": "def test_burczynski06(tmp_dataset_dir):\n    adata = sc.datasets.burczynski06()\n    assert adata.shape == (127, 22283)\n    assert not (adata.X == 0).any()", "idx": 428}
{"project": "Scanpy", "commit_id": "488_scanpy_1.9.0_test_datasets.py_test_moignard15.py", "target": 0, "func": "def test_moignard15(tmp_dataset_dir):\n    adata = sc.datasets.moignard15()\n    assert adata.shape == (3934, 42)", "idx": 429}
{"project": "Scanpy", "commit_id": "489_scanpy_1.9.0_test_datasets.py_test_paul15.py", "target": 0, "func": "def test_paul15(tmp_dataset_dir):\n    sc.datasets.paul15()", "idx": 430}
{"project": "Scanpy", "commit_id": "48_scanpy_1.9.0_logging.py___init__.py", "target": 0, "func": "def __init__(\n        self, fmt='{levelname}: {message}', datefmt='%Y-%m-%d %H:%M', style='{'\n    ):\n        super().__init__(fmt, datefmt, style)", "idx": 431}
{"project": "Scanpy", "commit_id": "490_scanpy_1.9.0_test_datasets.py_test_pbmc3k.py", "target": 0, "func": "def test_pbmc3k(tmp_dataset_dir):\n    adata = sc.datasets.pbmc3k()\n    assert adata.shape == (2700, 32738)\n    assert \"CD8A\" in adata.var_names", "idx": 432}
{"project": "Scanpy", "commit_id": "491_scanpy_1.9.0_test_datasets.py_test_pbmc3k_processed.py", "target": 0, "func": "def test_pbmc3k_processed(tmp_dataset_dir):\n    with pytest.warns(None) as records:\n        adata = sc.datasets.pbmc3k_processed()\n    assert adata.shape == (2638, 1838)\n    assert adata.raw.shape == (2638, 13714)\n\n    assert len(records) == 0", "idx": 433}
{"project": "Scanpy", "commit_id": "492_scanpy_1.9.0_test_datasets.py_test_ebi_expression_atlas.py", "target": 0, "func": "def test_ebi_expression_atlas(tmp_dataset_dir):\n    adata = sc.datasets.ebi_expression_atlas(\"E-MTAB-4888\")\n    assert adata.shape == (2315, 24051)  # This changes sometimes", "idx": 434}
{"project": "Scanpy", "commit_id": "493_scanpy_1.9.0_test_datasets.py_test_krumsiek11.py", "target": 0, "func": "def test_krumsiek11(tmp_dataset_dir):\n    adata = sc.datasets.krumsiek11()\n    assert adata.shape == (640, 11)\n    assert all(\n        np.unique(adata.obs[\"cell_type\"])\n        == np.array([\"Ery\", \"Mk\", \"Mo\", \"Neu\", \"progenitor\"])", "idx": 435}
{"project": "Scanpy", "commit_id": "494_scanpy_1.9.0_test_datasets.py_test_blobs.py", "target": 0, "func": "def test_blobs():\n    n_obs = np.random.randint(15, 30)\n    n_var = np.random.randint(500, 600)\n    adata = sc.datasets.blobs(n_variables=n_var, n_observations=n_obs)\n    assert adata.shape == (n_obs, n_var)", "idx": 436}
{"project": "Scanpy", "commit_id": "495_scanpy_1.9.0_test_datasets.py_test_toggleswitch.py", "target": 0, "func": "def test_toggleswitch():\n    sc.datasets.toggleswitch()", "idx": 437}
{"project": "Scanpy", "commit_id": "496_scanpy_1.9.0_test_datasets.py_test_pbmc68k_reduced.py", "target": 0, "func": "def test_pbmc68k_reduced():\n    with pytest.warns(None) as records:\n        sc.datasets.pbmc68k_reduced()\n    assert len(records) == 0  # Test that loading a dataset does not warn", "idx": 438}
{"project": "Scanpy", "commit_id": "497_scanpy_1.9.0_test_datasets.py_test_visium_datasets.py", "target": 0, "func": "def test_visium_datasets(tmp_dataset_dir, tmpdir):\n    # Tests that reading/ downloading works and is does not have global effects\n    hheart = sc.datasets.visium_sge(\"V1_Human_Heart\")\n    mbrain = sc.datasets.visium_sge(\"V1_Adult_Mouse_Brain\")\n    hheart_again = sc.datasets.visium_sge(\"V1_Human_Heart\")\n    assert_adata_equal(hheart, hheart_again)\n\n    # Test that changing the dataset dir doesn't break reading\n    sc.settings.datasetdir = Path(tmpdir)\n    mbrain_again = sc.datasets.visium_sge(\"V1_Adult_Mouse_Brain\")\n    assert_adata_equal(mbrain, mbrain_again)\n\n    # Test that downloading tissue image works\n    mbrain = sc.datasets.visium_sge(\"V1_Adult_Mouse_Brain\", include_hires_tiff=True)\n    expected_image_path = sc.settings.datasetdir / \"V1_Adult_Mouse_Brain\" / \"image.tif\"\n    image_path = Path(\n        mbrain.uns[\"spatial\"][\"V1_Adult_Mouse_Brain\"][\"metadata\"][\"source_image_path\"]\n    )\n    assert image_path == expected_image_path\n\n    # Test that tissue image exists and is a valid image file\n    assert image_path.exists()\n\n    # Test that tissue image is a tif image file (using `file`)\n    process = subprocess.run(\n        ['file', '--mime-type', image_path], stdout=subprocess.PIPE\n    )\n    output = process.stdout.strip().decode()  # make process output string\n    assert output == str(image_path) + ': image/tiff'", "idx": 439}
{"project": "Scanpy", "commit_id": "498_scanpy_1.9.0_test_datasets.py_test_download_failure.py", "target": 0, "func": "def test_download_failure():\n    from urllib.error import HTTPError\n\n    with pytest.raises(HTTPError):\n        sc.datasets.ebi_expression_atlas(\"not_a_real_accession\")", "idx": 440}
{"project": "Scanpy", "commit_id": "499_scanpy_1.9.0_test_dendrogram_key_added.py_adata.py", "target": 0, "func": "def adata():\n    return pbmc68k_reduced()", "idx": 441}
{"project": "Scanpy", "commit_id": "49_scanpy_1.9.0_logging.py_log.py", "target": 0, "func": "def log(\n        self,\n        level: int,\n        msg: str,\n        *,\n        extra: Optional[dict] = None,\n        time: datetime = None,\n        deep: Optional[str] = None,\n    ) -> datetime:\n        from . import settings\n\n        now = datetime.now(timezone.utc)\n        time_passed: timedelta = None if time is None else now - time\n        extra = {\n            **(extra or {}),\n            'deep': deep if settings.verbosity.level < level else None,\n            'time_passed': time_passed,\n        }\n        super().log(level, msg, extra=extra)\n        return now", "idx": 442}
{"project": "Scanpy", "commit_id": "4_scanpy_1.9.0_conftest.py_pbmc3k_normalized.py", "target": 0, "func": "def pbmc3k_normalized(_pbmc3k_normalized):\n    return _pbmc3k_normalized.copy()", "idx": 443}
{"project": "Scanpy", "commit_id": "500_scanpy_1.9.0_test_dendrogram_key_added.py_test_dendrogram_key_added.py", "target": 0, "func": "def test_dendrogram_key_added(adata, groupby, key_added):\n    sc.tl.dendrogram(adata, groupby=groupby, key_added=key_added, use_rep=\"X_pca\")\n    if isinstance(groupby, list):\n        dendrogram_key = f'dendrogram_{\"_\".join(groupby)}'\n    else:\n        dendrogram_key = f'dendrogram_{groupby}'\n\n    if key_added is None:\n        key_added = dendrogram_key\n    assert key_added in adata.uns", "idx": 444}
{"project": "Scanpy", "commit_id": "501_scanpy_1.9.0_test_deprecations.py_test_deprecate_multicore_tsne.py", "target": 0, "func": "def test_deprecate_multicore_tsne():\n    pbmc = pbmc68k_reduced()\n\n    with pytest.warns(\n        UserWarning, match=\"calling tsne with n_jobs > 1 would use MulticoreTSNE\"\n    ):\n        sc.tl.tsne(pbmc, n_jobs=2)\n\n    with pytest.warns(FutureWarning, match=\"Argument `use_fast_tsne` is deprecated\"):\n        sc.tl.tsne(pbmc, use_fast_tsne=True)\n\n    with pytest.warns(UserWarning, match=\"Falling back to scikit-learn\"):\n        sc.tl.tsne(pbmc, use_fast_tsne=True)", "idx": 445}
{"project": "Scanpy", "commit_id": "502_scanpy_1.9.0_test_embedding.py_test_tsne.py", "target": 0, "func": "def test_tsne():\n    pbmc = pbmc68k_reduced()\n\n    euclidean1 = sc.tl.tsne(pbmc, metric=\"euclidean\", copy=True)\n    with pytest.warns(UserWarning, match=\"In previous versions of scanpy\"):\n        euclidean2 = sc.tl.tsne(pbmc, metric=\"euclidean\", n_jobs=2, copy=True)\n    cosine = sc.tl.tsne(pbmc, metric=\"cosine\", copy=True)\n\n    # Reproducibility\n    np.testing.assert_equal(euclidean1.obsm[\"X_tsne\"], euclidean2.obsm[\"X_tsne\"])\n    # Metric has some effect\n    assert not np.array_equal(euclidean1.obsm[\"X_tsne\"], cosine.obsm[\"X_tsne\"])\n\n    # Params are recorded\n    assert euclidean1.uns[\"tsne\"][\"params\"][\"n_jobs\"] == 1\n    assert euclidean2.uns[\"tsne\"][\"params\"][\"n_jobs\"] == 2\n    assert cosine.uns[\"tsne\"][\"params\"][\"n_jobs\"] == 1\n    assert euclidean1.uns[\"tsne\"][\"params\"][\"metric\"] == \"euclidean\"\n    assert euclidean2.uns[\"tsne\"][\"params\"][\"metric\"] == \"euclidean\"\n    assert cosine.uns[\"tsne\"][\"params\"][\"metric\"] == \"cosine\"", "idx": 446}
{"project": "Scanpy", "commit_id": "503_scanpy_1.9.0_test_embedding.py_test_tsne_metric_warning.py", "target": 0, "func": "def test_tsne_metric_warning():\n    pbmc = pbmc68k_reduced()\n    import sklearn\n\n    with patch.object(sklearn, \"__version__\", \"0.23.0\"), pytest.warns(\n        UserWarning, match=\"Results for non-euclidean metrics changed\"\n    ):\n        sc.tl.tsne(pbmc, metric=\"cosine\")", "idx": 447}
{"project": "Scanpy", "commit_id": "504_scanpy_1.9.0_test_embedding.py_test_umap_init_dtype.py", "target": 0, "func": "def test_umap_init_dtype():\n    pbmc = pbmc68k_reduced()\n    pbmc = pbmc[:100, :].copy()\n    sc.tl.umap(pbmc, init_pos=pbmc.obsm[\"X_pca\"][:, :2].astype(np.float32))\n    embed1 = pbmc.obsm[\"X_umap\"].copy()\n    sc.tl.umap(pbmc, init_pos=pbmc.obsm[\"X_pca\"][:, :2].astype(np.float64))\n    embed2 = pbmc.obsm[\"X_umap\"].copy()\n    assert_array_almost_equal(embed1, embed2)\n    assert_array_almost_equal(embed1, embed2)", "idx": 448}
{"project": "Scanpy", "commit_id": "505_scanpy_1.9.0_test_embedding.py_test_umap_init_paga.py", "target": 0, "func": "def test_umap_init_paga(layout):\n    pbmc = pbmc68k_reduced()\n    pbmc = pbmc[:100, :].copy()\n    sc.tl.paga(pbmc)\n    sc.pl.paga(pbmc, layout=layout, show=False)\n    sc.tl.umap(pbmc, init_pos=\"paga\")", "idx": 449}
{"project": "Scanpy", "commit_id": "506_scanpy_1.9.0_test_embedding.py_test_diffmap.py", "target": 0, "func": "def test_diffmap():\n    pbmc = pbmc68k_reduced()\n\n    sc.tl.diffmap(pbmc)\n    d1 = pbmc.obsm['X_diffmap'].copy()\n    sc.tl.diffmap(pbmc)\n    d2 = pbmc.obsm['X_diffmap'].copy()\n    assert_array_equal(d1, d2)\n\n    # Checking if specifying random_state  works, arrays shouldn't be equal\n    sc.tl.diffmap(pbmc, random_state=1234)\n    d3 = pbmc.obsm['X_diffmap'].copy()\n    assert_raises(AssertionError, assert_array_equal, d1, d3)", "idx": 450}
{"project": "Scanpy", "commit_id": "507_scanpy_1.9.0_test_embedding_density.py_test_embedding_density.py", "target": 0, "func": "def test_embedding_density():\n    # Test that density values are scaled\n    # Test that the highest value is in the middle for a grid layout\n    test_data = AnnData(X=np.ones((9, 10)))\n    test_data.obsm['X_test'] = np.array([[x, y] for x in range(3) for y in range(3)])\n    sc.tl.embedding_density(test_data, 'test')\n\n    max_dens = np.max(test_data.obs['test_density'])\n    min_dens = np.min(test_data.obs['test_density'])\n    max_idx = test_data.obs['test_density'].idxmax()\n\n    assert max_idx == '4'\n    assert max_dens == 1\n    assert min_dens == 0", "idx": 451}
{"project": "Scanpy", "commit_id": "508_scanpy_1.9.0_test_embedding_density.py_test_embedding_density_plot.py", "target": 0, "func": "def test_embedding_density_plot():\n    # Test that sc.pl.embedding_density() runs without error\n    adata = pbmc68k_reduced()\n    sc.tl.embedding_density(adata, 'umap')\n    sc.pl.embedding_density(adata, 'umap', 'umap_density', show=False)", "idx": 452}
{"project": "Scanpy", "commit_id": "509_scanpy_1.9.0_test_embedding_plots.py_check_images.py", "target": 0, "func": "def check_images(pth1, pth2, *, tol):\n    result = compare_images(pth1, pth2, tol=tol)\n    assert result is None, result", "idx": 453}
{"project": "Scanpy", "commit_id": "50_scanpy_1.9.0_logging.py_critical.py", "target": 0, "func": "def critical(self, msg, *, time=None, deep=None, extra=None) -> datetime:\n        return self.log(CRITICAL, msg, time=time, deep=deep, extra=extra)", "idx": 454}
{"project": "Scanpy", "commit_id": "510_scanpy_1.9.0_test_embedding_plots.py_adata.py", "target": 0, "func": "def adata():\n    \"\"\"A bit cute.\"\"\"\n    from matplotlib.image import imread\n    from sklearn.datasets import make_blobs\n    from sklearn.cluster import DBSCAN\n\n    empty_pixel = np.array([1.0, 1.0, 1.0, 0]).reshape(1, 1, -1)\n    image = imread(\n        Path(sc.__file__).parent.parent / \"docs/_static/img/Scanpy_Logo_RGB.png\"\n    )\n    x, y = np.where(np.logical_and.reduce(~np.equal(image, empty_pixel), axis=2))\n\n    # Just using to calculate the hex coords\n    hexes = plt.hexbin(x, y, gridsize=(44, 100))\n    counts = hexes.get_array()\n    pixels = hexes.get_offsets()[counts != 0]\n    plt.close()\n\n    labels = DBSCAN(eps=20, min_samples=2).fit(pixels).labels_\n    order = np.argsort(labels)\n    adata = sc.AnnData(\n        make_blobs(\n            pd.value_counts(labels[order]).values,\n            n_features=20,\n            shuffle=False,\n            random_state=42,\n        )[0],\n        obs={\"label\": pd.Categorical(labels[order].astype(str))},\n        obsm={\"spatial\": pixels[order, ::-1]},\n        uns={\n            \"spatial\": {\n                \"scanpy_img\": {\n                    \"images\": {\"hires\": image},\n                    \"scalefactors\": {\n                        \"tissue_hires_scalef\": 1,\n                        \"spot_diameter_fullres\": 10,\n                    },\n                }\n            }\n        },\n    )\n    sc.pp.pca(adata)\n\n    # Adding some missing values\n    adata.obs[\"label_missing\"] = adata.obs[\"label\"].copy()\n    adata.obs[\"label_missing\"][::2] = np.nan\n\n    adata.obs[\"1_missing\"] = adata.obs_vector(\"1\")\n    adata.obs.loc[\n        adata.obsm[\"spatial\"][:, 0] < adata.obsm[\"spatial\"][:, 0].mean(), \"1_missing\"\n    ] = np.nan\n\n    return adata", "idx": 455}
{"project": "Scanpy", "commit_id": "511_scanpy_1.9.0_test_embedding_plots.py_fixture_request.py", "target": 0, "func": "def fixture_request(request):\n    \"\"\"Returns a Request object.\n\n    Allows you to access names of parameterized tests from within a test.\n    \"\"\"\n    return request", "idx": 456}
{"project": "Scanpy", "commit_id": "512_scanpy_1.9.0_test_embedding_plots.py_na_color.py", "target": 0, "func": "def na_color(request):\n    return request.param", "idx": 457}
{"project": "Scanpy", "commit_id": "513_scanpy_1.9.0_test_embedding_plots.py_na_in_legend.py", "target": 0, "func": "def na_in_legend(request):\n    return request.param", "idx": 458}
{"project": "Scanpy", "commit_id": "514_scanpy_1.9.0_test_embedding_plots.py_plotfunc.py", "target": 0, "func": "def plotfunc(request):\n    return request.param", "idx": 459}
{"project": "Scanpy", "commit_id": "515_scanpy_1.9.0_test_embedding_plots.py_legend_loc.py", "target": 0, "func": "def legend_loc(request):\n    return request.param", "idx": 460}
{"project": "Scanpy", "commit_id": "516_scanpy_1.9.0_test_embedding_plots.py_groupsfunc.py", "target": 0, "func": "def groupsfunc(request):\n    return request.param", "idx": 461}
{"project": "Scanpy", "commit_id": "517_scanpy_1.9.0_test_embedding_plots.py_vbounds.py", "target": 0, "func": "def vbounds(request):\n    return request.param", "idx": 462}
{"project": "Scanpy", "commit_id": "518_scanpy_1.9.0_test_embedding_plots.py_test_missing_values_categorical.py", "target": 0, "func": "def test_missing_values_categorical(\n    fixture_request,\n    image_comparer,\n    adata,\n    plotfunc,\n    na_color,\n    na_in_legend,\n    legend_loc,\n    groupsfunc,\n):\n    save_and_compare_images = image_comparer(\n        MISSING_VALUES_ROOT, MISSING_VALUES_FIGS, tol=15\n    )\n    base_name = fixture_request.node.name\n\n    # Passing through a dict so it's easier to use default values\n    kwargs = {}\n    kwargs[\"legend_loc\"] = legend_loc\n    kwargs[\"groups\"] = groupsfunc(adata.obs[\"label\"])\n    if na_color is not None:\n        kwargs[\"na_color\"] = na_color\n    kwargs[\"na_in_legend\"] = na_in_legend\n\n    plotfunc(adata, color=[\"label\", \"label_missing\"], **kwargs)\n\n    save_and_compare_images(base_name)", "idx": 463}
{"project": "Scanpy", "commit_id": "519_scanpy_1.9.0_test_embedding_plots.py_test_missing_values_continuous.py", "target": 0, "func": "def test_missing_values_continuous(\n    fixture_request, image_comparer, adata, plotfunc, na_color, legend_loc, vbounds\n):\n    save_and_compare_images = image_comparer(\n        MISSING_VALUES_ROOT, MISSING_VALUES_FIGS, tol=15\n    )\n    base_name = fixture_request.node.name\n\n    # Passing through a dict so it's easier to use default values\n    kwargs = {}\n    kwargs.update(vbounds)\n    kwargs[\"legend_loc\"] = legend_loc\n    if na_color is not None:\n        kwargs[\"na_color\"] = na_color\n\n    plotfunc(adata, color=[\"1\", \"1_missing\"], **kwargs)\n\n    save_and_compare_images(base_name)", "idx": 464}
{"project": "Scanpy", "commit_id": "51_scanpy_1.9.0_logging.py_format.py", "target": 0, "func": "def format(self, record: logging.LogRecord):\n        format_orig = self._style._fmt\n        if record.levelno == INFO:\n            self._style._fmt = '{message}'\n        elif record.levelno == HINT:\n            self._style._fmt = '--> {message}'\n        elif record.levelno == DEBUG:\n            self._style._fmt = '    {message}'\n        if record.time_passed:\n            # strip microseconds\n            if record.time_passed.microseconds:\n                record.time_passed = timedelta(\n                    seconds=int(record.time_passed.total_seconds())\n                )\n            if '{time_passed}' in record.msg:\n                record.msg = record.msg.replace(\n                    '{time_passed}', str(record.time_passed)\n                )\n            else:\n                self._style._fmt += ' ({time_passed})'\n        if record.deep:\n            record.msg = f'{record.msg}: {record.deep}'\n        result = logging.Formatter.format(self, record)\n        self._style._fmt = format_orig\n        return result", "idx": 465}
{"project": "Scanpy", "commit_id": "520_scanpy_1.9.0_test_embedding_plots.py_test_enumerated_palettes.py", "target": 0, "func": "def test_enumerated_palettes(fixture_request, adata, tmpdir, plotfunc):\n    tmpdir = Path(tmpdir)\n    base_name = fixture_request.node.name\n\n    categories = adata.obs[\"label\"].cat.categories\n    colors_rgb = dict(zip(categories, sns.color_palette(n_colors=12)))\n\n    dict_pth = tmpdir / f\"rgbdict_{base_name}.png\"\n    list_pth = tmpdir / f\"rgblist_{base_name}.png\"\n\n    # making a copy so colors aren't saved\n    plotfunc(adata.copy(), color=\"label\", palette=colors_rgb)\n    plt.savefig(dict_pth, dpi=40)\n    plt.close()\n    plotfunc(adata.copy(), color=\"label\", palette=[colors_rgb[c] for c in categories])\n    plt.savefig(list_pth, dpi=40)\n    plt.close()\n\n    check_images(dict_pth, list_pth, tol=15)", "idx": 466}
{"project": "Scanpy", "commit_id": "521_scanpy_1.9.0_test_embedding_plots.py_test_dimension_broadcasting.py", "target": 0, "func": "def test_dimension_broadcasting(adata, tmpdir, check_same_image):\n    tmpdir = Path(tmpdir)\n\n    with pytest.raises(ValueError):\n        sc.pl.pca(\n            adata, color=[\"label\", \"1_missing\"], dimensions=[(0, 1), (1, 2), (2, 3)]\n        )\n\n    dims_pth = tmpdir / \"broadcast_dims.png\"\n    color_pth = tmpdir / \"broadcast_colors.png\"\n\n    sc.pl.pca(adata, color=[\"label\", \"label\", \"label\"], dimensions=(2, 3), show=False)\n    plt.savefig(dims_pth, dpi=40)\n    plt.close()\n    sc.pl.pca(adata, color=\"label\", dimensions=[(2, 3), (2, 3), (2, 3)], show=False)\n    plt.savefig(color_pth, dpi=40)\n    plt.close()\n\n    check_same_image(dims_pth, color_pth, tol=5)", "idx": 467}
{"project": "Scanpy", "commit_id": "522_scanpy_1.9.0_test_embedding_plots.py_test_dimensions_same_as_components.py", "target": 0, "func": "def test_dimensions_same_as_components(adata, tmpdir, check_same_image):\n    tmpdir = Path(tmpdir)\n    adata = adata.copy()\n    adata.obs[\"mean\"] = np.ravel(adata.X.mean(axis=1))\n\n    comp_pth = tmpdir / \"components_plot.png\"\n    dims_pth = tmpdir / \"dimension_plot.png\"\n\n    # TODO: Deprecate components kwarg\n    # with pytest.warns(FutureWarning, match=r\"components .* deprecated\"):\n    sc.pl.pca(\n        adata,\n        color=[\"mean\", \"label\"],\n        components=[\"1,2\", \"2,3\"],\n        show=False,\n    )\n    plt.savefig(comp_pth, dpi=40)\n    plt.close()\n\n    sc.pl.pca(\n        adata,\n        color=[\"mean\", \"mean\", \"label\", \"label\"],\n        dimensions=[(0, 1), (1, 2), (0, 1), (1, 2)],\n        show=False,\n    )\n    plt.savefig(dims_pth, dpi=40)\n    plt.close()\n\n    check_same_image(dims_pth, comp_pth, tol=5)", "idx": 468}
{"project": "Scanpy", "commit_id": "523_scanpy_1.9.0_test_embedding_plots.py_test_embedding_colorbar_location.py", "target": 0, "func": "def test_embedding_colorbar_location(image_comparer):\n    save_and_compare_images = image_comparer(ROOT, FIGS, tol=15)\n    adata = datasets.pbmc3k_processed().raw.to_adata()\n\n    sc.pl.pca(adata, color=\"LDHB\", colorbar_loc=None)\n\n    save_and_compare_images(\"master_no_colorbar\")", "idx": 469}
{"project": "Scanpy", "commit_id": "524_scanpy_1.9.0_test_embedding_plots.py_test_visium_circles.py", "target": 0, "func": "def test_visium_circles(image_comparer):  # standard visium data\n    save_and_compare_images = image_comparer(ROOT, FIGS, tol=15)\n    adata = sc.read_visium(HERE / '_data' / 'visium_data' / '1.0.0')\n    adata.obs = adata.obs.astype({'array_row': 'str'})\n\n    sc.pl.spatial(\n        adata,\n        color=\"array_row\",\n        groups=[\"24\", \"33\"],\n        crop_coord=(100, 400, 400, 100),\n        alpha=0.5,\n        size=1.3,\n        show=False,\n    )\n\n    save_and_compare_images('master_spatial_visium')", "idx": 470}
{"project": "Scanpy", "commit_id": "525_scanpy_1.9.0_test_embedding_plots.py_test_visium_default.py", "target": 0, "func": "def test_visium_default(image_comparer):  # default values\n    save_and_compare_images = image_comparer(ROOT, FIGS, tol=15)\n    adata = sc.read_visium(HERE / '_data' / 'visium_data' / '1.0.0')\n    adata.obs = adata.obs.astype({'array_row': 'str'})\n\n    sc.pl.spatial(adata, show=False)\n\n    save_and_compare_images('master_spatial_visium_default')", "idx": 471}
{"project": "Scanpy", "commit_id": "526_scanpy_1.9.0_test_embedding_plots.py_test_visium_empty_img_key.py", "target": 0, "func": "def test_visium_empty_img_key(image_comparer):  # visium coordinates but image empty\n    save_and_compare_images = image_comparer(ROOT, FIGS, tol=15)\n    adata = sc.read_visium(HERE / '_data' / 'visium_data' / '1.0.0')\n    adata.obs = adata.obs.astype({'array_row': 'str'})\n\n    sc.pl.spatial(adata, img_key=None, color=\"array_row\", show=False)\n\n    save_and_compare_images('master_spatial_visium_empty_image')\n\n    sc.pl.embedding(adata, basis=\"spatial\", color=\"array_row\", show=False)\n    save_and_compare_images('master_spatial_visium_embedding')", "idx": 472}
{"project": "Scanpy", "commit_id": "527_scanpy_1.9.0_test_embedding_plots.py_test_spatial_general.py", "target": 0, "func": "def test_spatial_general(image_comparer):  # general coordinates\n    save_and_compare_images = image_comparer(ROOT, FIGS, tol=15)\n    adata = sc.read_visium(HERE / '_data' / 'visium_data' / '1.0.0')\n    adata.obs = adata.obs.astype({'array_row': 'str'})\n    spatial_metadata = adata.uns.pop(\n        \"spatial\"\n    )  # spatial data don't have imgs, so remove entry from uns\n    # Required argument for now\n    spot_size = list(spatial_metadata.values())[0][\"scalefactors\"][\n        \"spot_diameter_fullres\"\n    ]\n\n    sc.pl.spatial(adata, show=False, spot_size=spot_size)\n    save_and_compare_images('master_spatial_general_nocol')\n\n    # category\n    sc.pl.spatial(adata, show=False, spot_size=spot_size, color=\"array_row\")\n    save_and_compare_images('master_spatial_general_cat')\n\n    # continuous\n    sc.pl.spatial(adata, show=False, spot_size=spot_size, color=\"array_col\")\n    save_and_compare_images('master_spatial_general_cont')", "idx": 473}
{"project": "Scanpy", "commit_id": "528_scanpy_1.9.0_test_embedding_plots.py_test_spatial_external_img.py", "target": 0, "func": "def test_spatial_external_img(image_comparer):  # external image\n    save_and_compare_images = image_comparer(ROOT, FIGS, tol=15)\n    adata = sc.read_visium(HERE / '_data' / 'visium_data' / '1.0.0')\n    adata.obs = adata.obs.astype({'array_row': 'str'})\n\n    img = adata.uns[\"spatial\"][\"custom\"][\"images\"][\"hires\"]\n    scalef = adata.uns[\"spatial\"][\"custom\"][\"scalefactors\"][\"tissue_hires_scalef\"]\n    sc.pl.spatial(\n        adata,\n        color=\"array_row\",\n        scale_factor=scalef,\n        img=img,\n        basis=\"spatial\",\n        show=False,\n    )\n    save_and_compare_images('master_spatial_external_img')", "idx": 474}
{"project": "Scanpy", "commit_id": "529_scanpy_1.9.0_test_embedding_plots.py_equivalent_spatial_plotters.py", "target": 0, "func": "def equivalent_spatial_plotters(adata):\n    no_spatial = adata.copy()\n    del no_spatial.uns[\"spatial\"]\n\n    img_key = \"hires\"\n    library_id = list(adata.uns[\"spatial\"])[0]\n    spatial_data = adata.uns[\"spatial\"][library_id]\n    img = spatial_data[\"images\"][img_key]\n    scale_factor = spatial_data[\"scalefactors\"][f\"tissue_{img_key}_scalef\"]\n    spot_size = spatial_data[\"scalefactors\"][\"spot_diameter_fullres\"]\n\n    orig_plotter = partial(sc.pl.spatial, adata, color=\"1\", show=False)\n    removed_plotter = partial(\n        sc.pl.spatial,\n        no_spatial,\n        color=\"1\",\n        img=img,\n        scale_factor=scale_factor,\n        spot_size=spot_size,\n        show=False,\n    )\n\n    return (orig_plotter, removed_plotter)", "idx": 475}
{"project": "Scanpy", "commit_id": "52_scanpy_1.9.0_readwrite.py_read.py", "target": 0, "func": "def read(\n    filename: Union[Path, str],\n    backed: Optional[Literal['r', 'r+']] = None,\n    sheet: Optional[str] = None,\n    ext: Optional[str] = None,\n    delimiter: Optional[str] = None,\n    first_column_names: bool = False,\n    backup_url: Optional[str] = None,\n    cache: bool = False,\n    cache_compression: Union[Literal['gzip', 'lzf'], None, Empty] = _empty,\n    **kwargs,\n) -> AnnData:\n    \"\"\"\\\n    Read file and return :class:`~anndata.AnnData` object.\n\n    To speed up reading, consider passing ``cache=True``, which creates an hdf5\n    cache file.\n\n    Parameters\n    ----------\n    filename\n        If the filename has no file extension, it is interpreted as a key for\n        generating a filename via ``sc.settings.writedir / (filename +\n        sc.settings.file_format_data)``.  This is the same behavior as in\n        ``sc.read(filename, ...)``.\n    backed\n        If ``'r'``, load :class:`~anndata.AnnData` in ``backed`` mode instead\n        of fully loading it into memory (`memory` mode). If you want to modify\n        backed attributes of the AnnData object, you need to choose ``'r+'``.\n    sheet\n        Name of sheet/table in hdf5 or Excel file.\n    ext\n        Extension that indicates the file type. If ``None``, uses extension of\n        filename.\n    delimiter\n        Delimiter that separates data within text file. If ``None``, will split at\n        arbitrary number of white spaces, which is different from enforcing\n        splitting at any single white space ``' '``.\n    first_column_names\n        Assume the first column stores row names. This is only necessary if\n        these are not strings: strings in the first column are automatically\n        assumed to be row names.\n    backup_url\n        Retrieve the file from an URL if not present on disk.\n    cache\n        If `False`, read from source, if `True`, read from fast 'h5ad' cache.\n    cache_compression\n        See the h5py :ref:`dataset_compression`.\n        (Default: `settings.cache_compression`)\n    kwargs\n        Parameters passed to :func:`~anndata.read_loom`.\n\n    Returns\n    -------\n    An :class:`~anndata.AnnData` object\n    \"\"\"\n    filename = Path(filename)  # allow passing strings\n    if is_valid_filename(filename):\n        return _read(\n            filename,\n            backed=backed,\n            sheet=sheet,\n            ext=ext,\n            delimiter=delimiter,\n            first_column_names=first_column_names,\n            backup_url=backup_url,\n            cache=cache,\n            cache_compression=cache_compression,\n            **kwargs,\n        )\n    # generate filename and read to dict\n    filekey = str(filename)\n    filename = settings.writedir / (filekey + '.' + settings.file_format_data)\n    if not filename.exists():\n        raise ValueError(\n            f'Reading with filekey {filekey!r} failed, '\n            f'the inferred filename {filename!r} does not exist. '\n            'If you intended to provide a filename, either use a filename '\n            f'ending on one of the available extensions {avail_exts} '\n            'or pass the parameter `ext`.'\n        )\n    return read_h5ad(filename, backed=backed)", "idx": 476}
{"project": "Scanpy", "commit_id": "530_scanpy_1.9.0_test_embedding_plots.py_equivalent_spatial_plotters_no_img.py", "target": 0, "func": "def equivalent_spatial_plotters_no_img(equivalent_spatial_plotters):\n    orig, removed = equivalent_spatial_plotters\n    return (partial(orig, img_key=None), partial(removed, img=None, scale_factor=None))", "idx": 477}
{"project": "Scanpy", "commit_id": "531_scanpy_1.9.0_test_embedding_plots.py_spatial_kwargs.py", "target": 0, "func": "def spatial_kwargs(request):\n    return request.param", "idx": 478}
{"project": "Scanpy", "commit_id": "532_scanpy_1.9.0_test_embedding_plots.py_test_manual_equivalency.py", "target": 0, "func": "def test_manual_equivalency(equivalent_spatial_plotters, tmpdir, spatial_kwargs):\n    \"\"\"\n    Tests that manually passing values to sc.pl.spatial is similar to automatic extraction.\n    \"\"\"\n    orig, removed = equivalent_spatial_plotters\n\n    TESTDIR = Path(tmpdir)\n    orig_pth = TESTDIR / \"orig.png\"\n    removed_pth = TESTDIR / \"removed.png\"\n\n    orig(**spatial_kwargs)\n    plt.savefig(orig_pth, dpi=40)\n    plt.close()\n    removed(**spatial_kwargs)\n    plt.savefig(removed_pth, dpi=40)\n    plt.close()\n\n    check_images(orig_pth, removed_pth, tol=1)", "idx": 479}
{"project": "Scanpy", "commit_id": "533_scanpy_1.9.0_test_embedding_plots.py_test_manual_equivalency_no_img.py", "target": 0, "func": "def test_manual_equivalency_no_img(\n    equivalent_spatial_plotters_no_img, tmpdir, spatial_kwargs\n):\n    if \"bw\" in spatial_kwargs:\n        # Has no meaning when there is no image\n        pytest.skip()\n    orig, removed = equivalent_spatial_plotters_no_img\n\n    TESTDIR = Path(tmpdir)\n    orig_pth = TESTDIR / \"orig.png\"\n    removed_pth = TESTDIR / \"removed.png\"\n\n    orig(**spatial_kwargs)\n    plt.savefig(orig_pth, dpi=40)\n    plt.close()\n    removed(**spatial_kwargs)\n    plt.savefig(removed_pth, dpi=40)\n    plt.close()\n\n    check_images(orig_pth, removed_pth, tol=1)", "idx": 480}
{"project": "Scanpy", "commit_id": "534_scanpy_1.9.0_test_embedding_plots.py_test_white_background_vs_no_img.py", "target": 0, "func": "def test_white_background_vs_no_img(adata, tmpdir, spatial_kwargs):\n    if {\"bw\", \"img\", \"img_key\", \"na_color\"}.intersection(spatial_kwargs):\n        # These arguments don't make sense for this check\n        pytest.skip()\n\n    white_background = np.ones_like(\n        adata.uns[\"spatial\"][\"scanpy_img\"][\"images\"][\"hires\"]\n    )\n    TESTDIR = Path(tmpdir)\n    white_pth = TESTDIR / \"white_background.png\"\n    noimg_pth = TESTDIR / \"no_img.png\"\n\n    sc.pl.spatial(\n        adata,\n        color=\"2\",\n        img=white_background,\n        scale_factor=1.0,\n        show=False,\n        **spatial_kwargs,\n    )\n    plt.savefig(white_pth)\n    sc.pl.spatial(adata, color=\"2\", img_key=None, show=False, **spatial_kwargs)\n    plt.savefig(noimg_pth)\n\n    check_images(white_pth, noimg_pth, tol=1)", "idx": 481}
{"project": "Scanpy", "commit_id": "535_scanpy_1.9.0_test_embedding_plots.py_test_spatial_na_color.py", "target": 0, "func": "def test_spatial_na_color(adata, tmpdir):\n    \"\"\"\n    Check that na_color defaults to transparent when an image is present, light gray when not.\n    \"\"\"\n    white_background = np.ones_like(\n        adata.uns[\"spatial\"][\"scanpy_img\"][\"images\"][\"hires\"]\n    )\n    TESTDIR = Path(tmpdir)\n    lightgray_pth = TESTDIR / \"lightgray.png\"\n    transparent_pth = TESTDIR / \"transparent.png\"\n    noimg_pth = TESTDIR / \"noimg.png\"\n    whiteimg_pth = TESTDIR / \"whiteimg.png\"\n\n    def plot(pth, **kwargs):\n        sc.pl.spatial(adata, color=\"1_missing\", show=False, **kwargs)\n        plt.savefig(pth, dpi=40)\n        plt.close()\n\n    plot(lightgray_pth, na_color=\"lightgray\", img_key=None)\n    plot(transparent_pth, na_color=(0.0, 0.0, 0.0, 0.0), img_key=None)\n    plot(noimg_pth, img_key=None)\n    plot(whiteimg_pth, img=white_background, scale_factor=1.0)\n\n    check_images(lightgray_pth, noimg_pth, tol=1)\n    check_images(transparent_pth, whiteimg_pth, tol=1)\n    with pytest.raises(AssertionError):\n        check_images(lightgray_pth, transparent_pth, tol=1)", "idx": 482}
{"project": "Scanpy", "commit_id": "536_scanpy_1.9.0_test_embedding_plots.py_plot.py", "target": 0, "func": "def plot(pth, **kwargs):\n        sc.pl.spatial(adata, color=\"1_missing\", show=False, **kwargs)\n        plt.savefig(pth, dpi=40)\n        plt.close()", "idx": 483}
{"project": "Scanpy", "commit_id": "537_scanpy_1.9.0_test_filter_rank_genes_groups.py_test_filter_rank_genes_groups.py", "target": 0, "func": "def test_filter_rank_genes_groups():\n    adata = pbmc68k_reduced()\n\n    # fix filter defaults\n    args = {\n        'adata': adata,\n        'key_added': 'rank_genes_groups_filtered',\n        'min_in_group_fraction': 0.25,\n        'min_fold_change': 1,\n        'max_out_group_fraction': 0.5,\n    }\n\n    rank_genes_groups(\n        adata, 'bulk_labels', reference='Dendritic', method='wilcoxon', n_genes=5\n    )\n    filter_rank_genes_groups(**args)\n\n    assert np.array_equal(\n        names_reference,\n        np.array(adata.uns['rank_genes_groups_filtered']['names'].tolist()),\n    )\n\n    rank_genes_groups(adata, 'bulk_labels', method='wilcoxon', n_genes=5)\n    filter_rank_genes_groups(**args)\n\n    assert np.array_equal(\n        names_no_reference,\n        np.array(adata.uns['rank_genes_groups_filtered']['names'].tolist()),\n    )\n\n    rank_genes_groups(adata, 'bulk_labels', method='wilcoxon', pts=True, n_genes=5)\n    filter_rank_genes_groups(**args)\n\n    assert np.array_equal(\n        names_no_reference,\n        np.array(adata.uns['rank_genes_groups_filtered']['names'].tolist()),\n    )\n\n    # test compare_abs\n    rank_genes_groups(\n        adata, 'bulk_labels', method='wilcoxon', pts=True, rankby_abs=True, n_genes=5\n    )\n\n    filter_rank_genes_groups(\n        adata,\n        compare_abs=True,\n        min_in_group_fraction=-1,\n        max_out_group_fraction=1,\n        min_fold_change=3.1,\n    )\n\n    assert np.array_equal(\n        names_compare_abs,\n        np.array(adata.uns['rank_genes_groups_filtered']['names'].tolist()),", "idx": 484}
{"project": "Scanpy", "commit_id": "538_scanpy_1.9.0_test_get.py_adata.py", "target": 0, "func": "def adata():\n    \"\"\"\n    adata.X is np.ones((2, 2))\n    adata.layers['double'] is sparse np.ones((2,2)) * 2 to also test sparse matrices\n    \"\"\"\n    return AnnData(\n        X=np.ones((2, 2)),\n        obs=pd.DataFrame(\n            {\"obs1\": [0, 1], \"obs2\": [\"a\", \"b\"]}, index=[\"cell1\", \"cell2\"]\n        ),\n        var=pd.DataFrame(\n            {\"gene_symbols\": [\"genesymbol1\", \"genesymbol2\"]}, index=[\"gene1\", \"gene2\"]\n        ),\n        layers={\"double\": sparse.csr_matrix(np.ones((2, 2)), dtype=int) * 2},\n        dtype=int,", "idx": 485}
{"project": "Scanpy", "commit_id": "539_scanpy_1.9.0_test_get.py_test_obs_df.py", "target": 0, "func": "def test_obs_df(adata):\n    adata.obsm[\"eye\"] = np.eye(2, dtype=int)\n    adata.obsm[\"sparse\"] = sparse.csr_matrix(np.eye(2), dtype='float64')\n\n    # make raw with different genes than adata\n    adata.raw = AnnData(\n        X=np.array([[1, 2, 3], [2, 4, 6]]),\n        var=pd.DataFrame(\n            {\"gene_symbols\": [\"raw1\", \"raw2\", 'raw3']},\n            index=[\"gene2\", \"gene3\", \"gene4\"],\n        ),\n        dtype='float64',\n    )\n    pd.testing.assert_frame_equal(\n        sc.get.obs_df(\n            adata, keys=[\"gene2\", \"obs1\"], obsm_keys=[(\"eye\", 0), (\"sparse\", 1)]\n        ),\n        pd.DataFrame(\n            {\"gene2\": [1, 1], \"obs1\": [0, 1], \"eye-0\": [1, 0], \"sparse-1\": [0.0, 1.0]},\n            index=adata.obs_names,\n        ),\n    )\n    pd.testing.assert_frame_equal(\n        sc.get.obs_df(\n            adata,\n            keys=[\"genesymbol2\", \"obs1\"],\n            obsm_keys=[(\"eye\", 0), (\"sparse\", 1)],\n            gene_symbols=\"gene_symbols\",\n        ),\n        pd.DataFrame(\n            {\n                \"genesymbol2\": [1, 1],\n                \"obs1\": [0, 1],\n                \"eye-0\": [1, 0],\n                \"sparse-1\": [0.0, 1.0],\n            },\n            index=adata.obs_names,\n        ),\n    )\n    pd.testing.assert_frame_equal(\n        sc.get.obs_df(adata, keys=[\"gene2\", \"obs1\"], layer=\"double\"),\n        pd.DataFrame({\"gene2\": [2, 2], \"obs1\": [0, 1]}, index=adata.obs_names),\n    )\n\n    pd.testing.assert_frame_equal(\n        sc.get.obs_df(\n            adata,\n            keys=[\"raw2\", \"raw3\", \"obs1\"],\n            gene_symbols=\"gene_symbols\",\n            use_raw=True,\n        ),\n        pd.DataFrame(\n            {\"raw2\": [2.0, 4.0], \"raw3\": [3.0, 6.0], \"obs1\": [0, 1]},\n            index=adata.obs_names,\n        ),\n    )\n    # test only obs\n    pd.testing.assert_frame_equal(\n        sc.get.obs_df(adata, keys=[\"obs1\", \"obs2\"]),\n        pd.DataFrame({\"obs1\": [0, 1], \"obs2\": [\"a\", \"b\"]}, index=[\"cell1\", \"cell2\"]),\n    )\n    # test only var\n    pd.testing.assert_frame_equal(\n        sc.get.obs_df(adata, keys=[\"gene1\", \"gene2\"]),\n        pd.DataFrame({\"gene1\": [1, 1], \"gene2\": [1, 1]}, index=adata.obs_names),\n    )\n    pd.testing.assert_frame_equal(\n        sc.get.obs_df(adata, keys=[\"gene1\", \"gene2\"]),\n        pd.DataFrame({\"gene1\": [1, 1], \"gene2\": [1, 1]}, index=adata.obs_names),\n    )\n    # test handling of duplicated keys (in this case repeated gene names)\n    pd.testing.assert_frame_equal(\n        sc.get.obs_df(adata, keys=[\"gene1\", \"gene2\", \"gene1\", \"gene1\"]),\n        pd.DataFrame(\n            {\"gene1\": [1, 1], \"gene2\": [1, 1]},\n            index=adata.obs_names,\n        )[[\"gene1\", \"gene2\", \"gene1\", \"gene1\"]],\n    )\n\n    badkeys = [\"badkey1\", \"badkey2\"]\n    with pytest.raises(KeyError) as badkey_err:\n        sc.get.obs_df(adata, keys=badkeys)\n    with pytest.raises(AssertionError):\n        sc.get.obs_df(adata, keys=[\"gene1\"], use_raw=True, layer=\"double\")\n    assert all(badkey_err.match(k) for k in badkeys)\n\n    # test non unique index\n    adata = sc.AnnData(\n        np.arange(16).reshape(4, 4),\n        obs=pd.DataFrame(index=[\"a\", \"a\", \"b\", \"c\"]),\n        var=pd.DataFrame(index=[f\"gene{i}\" for i in range(4)]),\n    )\n    df = sc.get.obs_df(adata, [\"gene1\"])\n    pd.testing.assert_index_equal(df.index, adata.obs_names)", "idx": 486}
{"project": "Scanpy", "commit_id": "53_scanpy_1.9.0_readwrite.py_read_10x_h5.py", "target": 0, "func": "def read_10x_h5(\n    filename: Union[str, Path],\n    genome: Optional[str] = None,\n    gex_only: bool = True,\n    backup_url: Optional[str] = None,\n) -> AnnData:\n    \"\"\"\\\n    Read 10x-Genomics-formatted hdf5 file.\n\n    Parameters\n    ----------\n    filename\n        Path to a 10x hdf5 file.\n    genome\n        Filter expression to genes within this genome. For legacy 10x h5\n        files, this must be provided if the data contains more than one genome.\n    gex_only\n        Only keep 'Gene Expression' data and ignore other feature types,\n        e.g. 'Antibody Capture', 'CRISPR Guide Capture', or 'Custom'\n    backup_url\n        Retrieve the file from an URL if not present on disk.\n\n    Returns\n    -------\n    Annotated data matrix, where observations/cells are named by their\n    barcode and variables/genes by gene name. Stores the following information:\n\n    :attr:`~anndata.AnnData.X`\n        The data matrix is stored\n    :attr:`~anndata.AnnData.obs_names`\n        Cell names\n    :attr:`~anndata.AnnData.var_names`\n        Gene names\n    :attr:`~anndata.AnnData.var`\\\\ `['gene_ids']`\n        Gene IDs\n    :attr:`~anndata.AnnData.var`\\\\ `['feature_types']`\n        Feature types\n    \"\"\"\n    start = logg.info(f'reading {filename}')\n    is_present = _check_datafile_present_and_download(filename, backup_url=backup_url)\n    if not is_present:\n        logg.debug(f'... did not find original file {filename}')\n    with h5py.File(str(filename), 'r') as f:\n        v3 = '/matrix' in f\n    if v3:\n        adata = _read_v3_10x_h5(filename, start=start)\n        if genome:\n            if genome not in adata.var['genome'].values:\n                raise ValueError(\n                    f\"Could not find data corresponding to genome '{genome}' in '{filename}'. \"\n                    f'Available genomes are: {list(adata.var[\"genome\"].unique())}.'\n                )\n            adata = adata[:, adata.var['genome'] == genome]\n        if gex_only:\n            adata = adata[:, adata.var['feature_types'] == 'Gene Expression']\n        if adata.is_view:\n            adata = adata.copy()\n    else:\n        adata = _read_legacy_10x_h5(filename, genome=genome, start=start)\n    return adata", "idx": 487}
{"project": "Scanpy", "commit_id": "540_scanpy_1.9.0_test_get.py_test_repeated_gene_symbols.py", "target": 0, "func": "def test_repeated_gene_symbols():\n    \"\"\"\n    Gene symbols column allows repeats, but we can't unambiguously get data for these values.\n    \"\"\"\n    gene_symbols = [f\"symbol_{i}\" for i in [\"a\", \"b\", \"b\", \"c\"]]\n    var_names = pd.Index([f\"id_{i}\" for i in [\"a\", \"b.1\", \"b.2\", \"c\"]])\n    adata = sc.AnnData(\n        np.arange(3 * 4).reshape((3, 4)),\n        var=pd.DataFrame({\"gene_symbols\": gene_symbols}, index=var_names),\n    )\n\n    with pytest.raises(KeyError, match=\"symbol_b\"):\n        sc.get.obs_df(adata, [\"symbol_b\"], gene_symbols=\"gene_symbols\")\n\n    expected = pd.DataFrame(\n        np.arange(3 * 4).reshape((3, 4))[:, [0, 3]].astype(np.float32),\n        index=adata.obs_names,\n        columns=[\"symbol_a\", \"symbol_c\"],\n    )\n    result = sc.get.obs_df(adata, [\"symbol_a\", \"symbol_c\"], gene_symbols=\"gene_symbols\")\n\n    pd.testing.assert_frame_equal(expected, result)", "idx": 488}
{"project": "Scanpy", "commit_id": "541_scanpy_1.9.0_test_get.py_test_backed_vs_memory.py", "target": 0, "func": "def test_backed_vs_memory():\n    \"compares backed vs. memory\"\n    from pathlib import Path\n\n    # get location test h5ad file in datasets\n    HERE = Path(sc.__file__).parent\n    adata_file = HERE / \"datasets/10x_pbmc68k_reduced.h5ad\"\n    adata_backed = sc.read(adata_file, backed='r')\n    adata = sc.read_h5ad(adata_file)\n\n    # use non-sequential list of genes\n    genes = list(adata.var_names[20::-2])\n    obs_names = ['bulk_labels', 'n_genes']\n    pd.testing.assert_frame_equal(\n        sc.get.obs_df(adata, keys=genes + obs_names),\n        sc.get.obs_df(adata_backed, keys=genes + obs_names),\n    )\n\n    # use non-sequential list of cell indices\n    cell_indices = list(adata.obs_names[30::-2])\n    pd.testing.assert_frame_equal(\n        sc.get.var_df(adata, keys=cell_indices + [\"highly_variable\"]),\n        sc.get.var_df(adata_backed, keys=cell_indices + [\"highly_variable\"]),", "idx": 489}
{"project": "Scanpy", "commit_id": "542_scanpy_1.9.0_test_get.py_test_column_content.py", "target": 0, "func": "def test_column_content():\n    \"uses a larger dataset to test column order and content\"\n    adata = pbmc68k_reduced()\n\n    # test that columns content is correct for obs_df\n    query = ['CST3', 'NKG7', 'GNLY', 'louvain', 'n_counts', 'n_genes']\n    df = sc.get.obs_df(adata, query)\n    for col in query:\n        assert col in df\n        np.testing.assert_array_equal(query, df.columns)\n        np.testing.assert_array_equal(df[col].values, adata.obs_vector(col))\n\n    # test that columns content is correct for var_df\n    cell_ids = list(adata.obs.sample(5).index)\n    query = cell_ids + ['highly_variable', 'dispersions_norm', 'dispersions']\n    df = sc.get.var_df(adata, query)\n    np.testing.assert_array_equal(query, df.columns)\n    for col in query:\n        np.testing.assert_array_equal(df[col].values, adata.var_vector(col))", "idx": 490}
{"project": "Scanpy", "commit_id": "543_scanpy_1.9.0_test_get.py_test_var_df.py", "target": 0, "func": "def test_var_df(adata):\n    adata.varm[\"eye\"] = np.eye(2, dtype=int)\n    adata.varm[\"sparse\"] = sparse.csr_matrix(np.eye(2), dtype='float64')\n\n    pd.testing.assert_frame_equal(\n        sc.get.var_df(\n            adata,\n            keys=[\"cell2\", \"gene_symbols\"],\n            varm_keys=[(\"eye\", 0), (\"sparse\", 1)],\n        ),\n        pd.DataFrame(\n            {\n                \"cell2\": [1, 1],\n                \"gene_symbols\": [\"genesymbol1\", \"genesymbol2\"],\n                \"eye-0\": [1, 0],\n                \"sparse-1\": [0.0, 1.0],\n            },\n            index=adata.var_names,\n        ),\n    )\n    pd.testing.assert_frame_equal(\n        sc.get.var_df(adata, keys=[\"cell1\", \"gene_symbols\"], layer=\"double\"),\n        pd.DataFrame(\n            {\"cell1\": [2, 2], \"gene_symbols\": [\"genesymbol1\", \"genesymbol2\"]},\n            index=adata.var_names,\n        ),\n    )\n    # test only cells\n    pd.testing.assert_frame_equal(\n        sc.get.var_df(adata, keys=[\"cell1\", \"cell2\"]),\n        pd.DataFrame(\n            {\"cell1\": [1, 1], \"cell2\": [1, 1]},\n            index=adata.var_names,\n        ),\n    )\n    # test only var columns\n    pd.testing.assert_frame_equal(\n        sc.get.var_df(adata, keys=[\"gene_symbols\"]),\n        pd.DataFrame(\n            {\"gene_symbols\": [\"genesymbol1\", \"genesymbol2\"]},\n            index=adata.var_names,\n        ),\n    )\n\n    # test handling of duplicated keys (in this case repeated cell names)\n    pd.testing.assert_frame_equal(\n        sc.get.var_df(adata, keys=[\"cell1\", \"cell2\", \"cell2\", \"cell1\"]),\n        pd.DataFrame(\n            {\"cell1\": [1, 1], \"cell2\": [1, 1]},\n            index=adata.var_names,\n        )[[\"cell1\", \"cell2\", \"cell2\", \"cell1\"]],\n    )\n\n    badkeys = [\"badkey1\", \"badkey2\"]\n    with pytest.raises(KeyError) as badkey_err:\n        sc.get.var_df(adata, keys=badkeys)\n    assert all(badkey_err.match(k) for k in badkeys)", "idx": 491}
{"project": "Scanpy", "commit_id": "544_scanpy_1.9.0_test_get.py_test_just_mapping_keys.py", "target": 0, "func": "def test_just_mapping_keys(dim, transform, func):\n    # https://github.com/theislab/scanpy/issues/1634\n    # Test for error where just passing obsm_keys, but not keys, would cause error.\n    mapping_attr = f\"{dim}m\"\n    kwargs = {f\"{mapping_attr}_keys\": [(\"array\", 0), (\"array\", 1)]}\n\n    adata = transform(\n        sc.AnnData(\n            X=np.zeros((5, 5)),\n            obsm={\n                \"array\": np.arange(10).reshape((5, 2)),\n            },\n        )\n    )\n\n    expected = pd.DataFrame(\n        np.arange(10).reshape((5, 2)),\n        index=getattr(adata, f\"{dim}_names\"),\n        columns=[\"array-0\", \"array-1\"],\n    )\n    result = func(adata, **kwargs)\n\n    pd.testing.assert_frame_equal(expected, result)", "idx": 492}
{"project": "Scanpy", "commit_id": "545_scanpy_1.9.0_test_get.py_test_non_unique_cols_value_error.py", "target": 0, "func": "def test_non_unique_cols_value_error():\n    M, N = 5, 3\n    adata = sc.AnnData(\n        X=np.zeros((M, N)),\n        obs=pd.DataFrame(\n            np.arange(M * 2).reshape((M, 2)),\n            columns=[\"repeated_col\", \"repeated_col\"],\n            index=[f\"cell_{i}\" for i in range(M)],\n        ),\n        var=pd.DataFrame(\n            index=[f\"gene_{i}\" for i in range(N)],\n        ),\n    )\n    with pytest.raises(ValueError):\n        sc.get.obs_df(adata, [\"repeated_col\"])", "idx": 493}
{"project": "Scanpy", "commit_id": "546_scanpy_1.9.0_test_get.py_test_non_unique_var_index_value_error.py", "target": 0, "func": "def test_non_unique_var_index_value_error():\n    adata = sc.AnnData(\n        X=np.ones((2, 3)),\n        obs=pd.DataFrame(index=[\"cell-0\", \"cell-1\"]),\n        var=pd.DataFrame(index=[\"gene-0\", \"gene-0\", \"gene-1\"]),\n    )\n    with pytest.raises(ValueError):\n        sc.get.obs_df(adata, [\"gene-0\"])", "idx": 494}
{"project": "Scanpy", "commit_id": "547_scanpy_1.9.0_test_get.py_test_keys_in_both_obs_and_var_index_value_error.py", "target": 0, "func": "def test_keys_in_both_obs_and_var_index_value_error():\n    M, N = 5, 3\n    adata = sc.AnnData(\n        X=np.zeros((M, N)),\n        obs=pd.DataFrame(\n            np.arange(M),\n            columns=[\"var_id\"],\n            index=[f\"cell_{i}\" for i in range(M)],\n        ),\n        var=pd.DataFrame(\n            index=[\"var_id\"] + [f\"gene_{i}\" for i in range(N - 1)],\n        ),\n    )\n    with pytest.raises(KeyError, match=\"var_id\"):\n        sc.get.obs_df(adata, [\"var_id\"])", "idx": 495}
{"project": "Scanpy", "commit_id": "548_scanpy_1.9.0_test_get.py_test_repeated_cols.py", "target": 0, "func": "def test_repeated_cols(dim, transform, func):\n    adata = transform(\n        sc.AnnData(\n            np.ones((5, 10)),\n            obs=pd.DataFrame(\n                np.ones((5, 2)), columns=[\"a_column_name\", \"a_column_name\"]\n            ),\n            var=pd.DataFrame(index=[f\"gene-{i}\" for i in range(10)]),\n        )\n    )\n    # (?s) is inline re.DOTALL\n    with pytest.raises(ValueError, match=rf\"(?s)^adata\\.{dim}.*a_column_name.*$\"):\n        func(adata, [\"gene_5\"])", "idx": 496}
{"project": "Scanpy", "commit_id": "549_scanpy_1.9.0_test_get.py_test_repeated_index_vals.py", "target": 0, "func": "def test_repeated_index_vals(dim, transform, func):\n    # THis one could be reverted, see:\n    # https://github.com/theislab/scanpy/pull/1583#issuecomment-770641710\n    alt_dim = [\"obs\", \"var\"][dim == \"obs\"]\n    adata = transform(\n        sc.AnnData(\n            np.ones((5, 10)),\n            var=pd.DataFrame(\n                index=[\"repeated_id\"] * 2 + [f\"gene-{i}\" for i in range(8)]\n            ),\n        )\n    )\n\n    with pytest.raises(\n        ValueError,\n        match=rf\"(?s)adata\\.{alt_dim}_names.*{alt_dim}_names_make_unique\",\n    ):\n        func(adata, \"gene_5\")", "idx": 497}
{"project": "Scanpy", "commit_id": "54_scanpy_1.9.0_readwrite.py__read_legacy_10x_h5.py", "target": 0, "func": "def _read_legacy_10x_h5(filename, *, genome=None, start=None):\n    \"\"\"\n    Read hdf5 file from Cell Ranger v2 or earlier versions.\n    \"\"\"\n    with h5py.File(str(filename), 'r') as f:\n        try:\n            children = list(f.keys())\n            if not genome:\n                if len(children) > 1:\n                    raise ValueError(\n                        f\"'{filename}' contains more than one genome. For legacy 10x h5 \"\n                        \"files you must specify the genome if more than one is present. \"\n                        f\"Available genomes are: {children}\"\n                    )\n                genome = children[0]\n            elif genome not in children:\n                raise ValueError(\n                    f\"Could not find genome '{genome}' in '{filename}'. \"\n                    f'Available genomes are: {children}'\n                )\n\n            dsets = {}\n            _collect_datasets(dsets, f)\n\n            # AnnData works with csr matrices\n            # 10x stores the transposed data, so we do the transposition right away\n            from scipy.sparse import csr_matrix\n\n            M, N = dsets['shape']\n            data = dsets['data']\n            if dsets['data'].dtype == np.dtype('int32'):\n                data = dsets['data'].view('float32')\n                data[:] = dsets['data']\n            matrix = csr_matrix(\n                (data, dsets['indices'], dsets['indptr']),\n                shape=(N, M),\n            )\n            # the csc matrix is automatically the transposed csr matrix\n            # as scanpy expects it, so, no need for a further transpostion\n            adata = AnnData(\n                matrix,\n                obs=dict(obs_names=dsets['barcodes'].astype(str)),\n                var=dict(\n                    var_names=dsets['gene_names'].astype(str),\n                    gene_ids=dsets['genes'].astype(str),\n                ),\n            )\n            logg.info('', time=start)\n            return adata\n        except KeyError:\n            raise Exception('File is missing one or more required datasets.')", "idx": 498}
{"project": "Scanpy", "commit_id": "550_scanpy_1.9.0_test_get.py_shared_key_adata.py", "target": 0, "func": "def shared_key_adata(request):\n    kind = request.param\n    adata = sc.AnnData(\n        np.arange(50).reshape((5, 10)),\n        obs=pd.DataFrame(np.zeros((5, 1)), columns=[\"var_id\"]),\n        var=pd.DataFrame(index=[\"var_id\"] + [f\"gene_{i}\" for i in range(1, 10)]),\n    )\n    if kind == \"obs_df\":\n        return (\n            adata,\n            sc.get.obs_df,\n            r\"'var_id'.* adata\\.obs .* adata.var_names\",\n        )\n    elif kind == \"var_df\":\n        return (\n            adata.T,\n            sc.get.var_df,\n            r\"'var_id'.* adata\\.var .* adata.obs_names\",\n        )\n    elif kind == \"obs_df:use_raw\":\n        adata.raw = adata\n        adata.var_names = [f\"gene_{i}\" for i in range(10)]\n        return (\n            adata,\n            partial(sc.get.obs_df, use_raw=True),\n            r\"'var_id'.* adata\\.obs .* adata\\.raw\\.var_names\",\n        )\n    elif kind == \"obs_df:gene_symbols\":\n        adata.var[\"gene_symbols\"] = adata.var_names\n        adata.var_names = [f\"gene_{i}\" for i in range(10)]\n        return (\n            adata,\n            partial(sc.get.obs_df, gene_symbols=\"gene_symbols\"),\n            r\"'var_id'.* adata\\.obs .* adata\\.var\\['gene_symbols'\\]\",\n        )\n    elif kind == \"obs_df:gene_symbols,use_raw\":\n        base = adata.copy()\n        adata.var[\"gene_symbols\"] = adata.var_names\n        adata.var_names = [f\"gene_{i}\" for i in range(10)]\n        base.raw = adata\n        return (\n            base,\n            partial(\n                sc.get.obs_df,\n                gene_symbols=\"gene_symbols\",\n                use_raw=True,\n            ),\n            r\"'var_id'.* adata\\.obs .* adata\\.raw\\.var\\['gene_symbols'\\]\",\n        )\n    else:\n        assert False", "idx": 499}
{"project": "Scanpy", "commit_id": "551_scanpy_1.9.0_test_get.py_test_shared_key_errors.py", "target": 0, "func": "def test_shared_key_errors(shared_key_adata):\n    adata, func, regex = shared_key_adata\n\n    # This should error\n    with pytest.raises(KeyError, match=regex):\n        func(adata, keys=[\"var_id\"])\n\n    # This shouldn't error\n    _ = func(adata, keys=[\"gene_2\"])", "idx": 500}
{"project": "Scanpy", "commit_id": "552_scanpy_1.9.0_test_get.py_test_rank_genes_groups_df.py", "target": 0, "func": "def test_rank_genes_groups_df():\n    a = np.zeros((20, 3))\n    a[:10, 0] = 5\n    adata = AnnData(\n        a,\n        obs=pd.DataFrame(\n            {\"celltype\": list(chain(repeat(\"a\", 10), repeat(\"b\", 10)))},\n            index=[f\"cell{i}\" for i in range(a.shape[0])],\n        ),\n        var=pd.DataFrame(index=[f\"gene{i}\" for i in range(a.shape[1])]),\n    )\n    sc.tl.rank_genes_groups(adata, groupby=\"celltype\", method=\"wilcoxon\", pts=True)\n    dedf = sc.get.rank_genes_groups_df(adata, \"a\")\n    assert dedf[\"pvals\"].value_counts()[1.0] == 2\n    assert sc.get.rank_genes_groups_df(adata, \"a\", log2fc_max=0.1).shape[0] == 2\n    assert sc.get.rank_genes_groups_df(adata, \"a\", log2fc_min=0.1).shape[0] == 1\n    assert sc.get.rank_genes_groups_df(adata, \"a\", pval_cutoff=0.9).shape[0] == 1\n    del adata.uns[\"rank_genes_groups\"]\n    sc.tl.rank_genes_groups(\n        adata,\n        groupby=\"celltype\",\n        method=\"wilcoxon\",\n        key_added=\"different_key\",\n        pts=True,\n    )\n    with pytest.raises(KeyError):\n        sc.get.rank_genes_groups_df(adata, \"a\")\n    dedf2 = sc.get.rank_genes_groups_df(adata, \"a\", key=\"different_key\")\n    pd.testing.assert_frame_equal(dedf, dedf2)\n    assert 'pct_nz_group' in dedf2.columns\n    assert 'pct_nz_reference' in dedf2.columns\n\n    # get all groups\n    dedf3 = sc.get.rank_genes_groups_df(adata, group=None, key=\"different_key\")\n    assert 'a' in dedf3['group'].unique()\n    assert 'b' in dedf3['group'].unique()\n    adata.var_names.name = 'pr1388'\n    sc.get.rank_genes_groups_df(adata, group=None, key=\"different_key\")", "idx": 501}
{"project": "Scanpy", "commit_id": "553_scanpy_1.9.0_test_highly_variable_genes.py_test_highly_variable_genes_basic.py", "target": 1, "func": "def test_highly_variable_genes_basic():\n    adata = sc.datasets.blobs()\n    sc.pp.highly_variable_genes(adata)\n\n    adata = sc.datasets.blobs()\n    np.random.seed(0)\n    adata.obs['batch'] = np.random.binomial(3, 0.5, size=(adata.n_obs))\n    adata.obs['batch'] = adata.obs['batch'].astype('category')\n    sc.pp.highly_variable_genes(adata, batch_key='batch')\n    assert 'highly_variable_nbatches' in adata.var.columns\n    assert 'highly_variable_intersection' in adata.var.columns\n\n    adata = sc.datasets.blobs()\n    adata.obs['batch'] = np.random.binomial(4, 0.5, size=(adata.n_obs))\n    adata.obs['batch'] = adata.obs['batch'].astype('category')\n    sc.pp.highly_variable_genes(adata, batch_key='batch', n_top_genes=3)\n    assert 'highly_variable_nbatches' in adata.var.columns\n    assert adata.var['highly_variable'].sum() == 3\n\n    sc.pp.highly_variable_genes(adata)\n    no_batch_hvg = adata.var.highly_variable.copy()\n    assert no_batch_hvg.any()\n    adata.obs['batch'] = 'batch'\n    adata.obs['batch'] = adata.obs['batch'].astype('category')\n    sc.pp.highly_variable_genes(adata, batch_key='batch')\n    assert np.all(no_batch_hvg == adata.var.highly_variable)\n    assert np.all(adata.var.highly_variable_intersection == adata.var.highly_variable)\n\n    adata.obs[\"batch\"] = \"a\"\n    adata.obs.batch.loc[::2] = \"b\"\n    sc.pp.highly_variable_genes(adata, batch_key=\"batch\")\n    assert adata.var[\"highly_variable\"].any()\n\n    colnames = [\n        'means',\n        'dispersions',\n        'dispersions_norm',\n        'highly_variable_nbatches',\n        'highly_variable_intersection',\n        'highly_variable',\n    ]\n    hvg_df = sc.pp.highly_variable_genes(adata, batch_key=\"batch\", inplace=False)\n    assert np.all(np.isin(colnames, hvg_df.columns))", "idx": 502}
{"project": "Scanpy", "commit_id": "554_scanpy_1.9.0_test_highly_variable_genes.py__check_pearson_hvg_columns.py", "target": 0, "func": "def _check_pearson_hvg_columns(output_df, n_top_genes):\n\n    assert pd.api.types.is_float_dtype(output_df['residual_variances'].dtype)\n\n    assert output_df['highly_variable'].values.dtype is np.dtype('bool')\n    assert np.sum(output_df['highly_variable']) == n_top_genes\n\n    assert np.nanmax(output_df['highly_variable_rank'].values) <= n_top_genes - 1", "idx": 503}
{"project": "Scanpy", "commit_id": "555_scanpy_1.9.0_test_highly_variable_genes.py_test_highly_variable_genes_pearson_residuals_inputchecks.py", "target": 0, "func": "def test_highly_variable_genes_pearson_residuals_inputchecks(sparsity_func, dtype):\n\n    adata = _prepare_pbmc_testdata(sparsity_func, dtype, small=True)\n\n    # depending on check_values, warnings should be raised for non-integer data\n    if dtype == 'float32':\n\n        adata_noninteger = adata.copy()\n        x, y = np.nonzero(adata_noninteger.X)\n        adata_noninteger.X[x[0], y[0]] = 0.5\n\n        _check_check_values_warnings(\n            function=sc.experimental.pp.highly_variable_genes,\n            adata=adata_noninteger,\n            expected_warning=\"`flavor='pearson_residuals'` expects raw count data, but non-integers were found.\",\n            kwargs=dict(\n                flavor='pearson_residuals',\n                n_top_genes=100,\n            ),\n        )\n\n    # errors should be raised for invalid theta values\n    for theta in [0, -1]:\n\n        with pytest.raises(ValueError, match='Pearson residuals require theta > 0'):\n            sc.experimental.pp.highly_variable_genes(\n                adata.copy(), theta=theta, flavor='pearson_residuals', n_top_genes=100\n            )\n\n    with pytest.raises(\n        ValueError, match='Pearson residuals require `clip>=0` or `clip=None`.'\n    ):\n        sc.experimental.pp.highly_variable_genes(\n            adata.copy(), clip=-1, flavor='pearson_residuals', n_top_genes=100", "idx": 504}
{"project": "Scanpy", "commit_id": "556_scanpy_1.9.0_test_highly_variable_genes.py_test_highly_variable_genes_pearson_residuals_general.py", "target": 1, "func": "def test_highly_variable_genes_pearson_residuals_general(\n    subset, sparsity_func, dtype, clip, theta, n_top_genes\n):\n    adata = _prepare_pbmc_testdata(sparsity_func, dtype, small=True)\n    # cleanup var\n    del adata.var\n\n    # compute reference output\n    residuals_reference = sc.experimental.pp.normalize_pearson_residuals(\n        adata, clip=clip, theta=theta, inplace=False\n    )['X']\n    residual_variances_reference = np.var(residuals_reference, axis=0)\n\n    if subset:\n        # lazyly sort by residual variance and take top N\n        top_n_idx = np.argsort(-residual_variances_reference)[:n_top_genes]\n        # (results in sorted \"gene order\" in reference)\n        residual_variances_reference = residual_variances_reference[top_n_idx]\n\n    # compute output to be tested\n    output_df = sc.experimental.pp.highly_variable_genes(\n        adata,\n        flavor='pearson_residuals',\n        n_top_genes=n_top_genes,\n        subset=subset,\n        inplace=False,\n        clip=clip,\n        theta=theta,\n    )\n\n    sc.experimental.pp.highly_variable_genes(\n        adata,\n        flavor='pearson_residuals',\n        n_top_genes=n_top_genes,\n        subset=subset,\n        inplace=True,\n        clip=clip,\n        theta=theta,\n    )\n\n    # compare inplace=True and inplace=False output\n    pd.testing.assert_frame_equal(output_df, adata.var)\n\n    # check output is complete\n    for key in [\n        'highly_variable',\n        'means',\n        'variances',\n        'residual_variances',\n        'highly_variable_rank',\n    ]:\n        assert key in output_df.keys()\n\n    # check consistency with normalization method\n    if subset:\n        # sort values before comparing as reference is sorted as well for subset case\n        sort_output_idx = np.argsort(-output_df['residual_variances'].values)\n        assert np.allclose(\n            output_df['residual_variances'].values[sort_output_idx],\n            residual_variances_reference,\n        )\n    else:\n        assert np.allclose(\n            output_df['residual_variances'].values, residual_variances_reference\n        )\n\n    # check hvg flag\n    hvg_idx = np.where(output_df['highly_variable'])[0]\n    topn_idx = np.sort(\n        np.argsort(-output_df['residual_variances'].values)[:n_top_genes]\n    )\n    assert np.all(hvg_idx == topn_idx)\n\n    # check ranks\n    assert np.nanmin(output_df['highly_variable_rank'].values) == 0\n\n    # more general checks on ranks, hvg flag and residual variance\n    _check_pearson_hvg_columns(output_df, n_top_genes)", "idx": 505}
{"project": "Scanpy", "commit_id": "557_scanpy_1.9.0_test_highly_variable_genes.py_test_highly_variable_genes_pearson_residuals_batch.py", "target": 1, "func": "def test_highly_variable_genes_pearson_residuals_batch(\n    subset, n_top_genes, sparsity_func, dtype\n):\n    adata = _prepare_pbmc_testdata(sparsity_func, dtype, small=True)\n    # cleanup var\n    del adata.var\n    n_genes = adata.shape[1]\n\n    output_df = sc.experimental.pp.highly_variable_genes(\n        adata,\n        flavor='pearson_residuals',\n        n_top_genes=n_top_genes,\n        batch_key='batch',\n        subset=subset,\n        inplace=False,\n    )\n\n    sc.experimental.pp.highly_variable_genes(\n        adata,\n        flavor='pearson_residuals',\n        n_top_genes=n_top_genes,\n        batch_key='batch',\n        subset=subset,\n        inplace=True,\n    )\n\n    # compare inplace=True and inplace=False output\n    pd.testing.assert_frame_equal(output_df, adata.var)\n\n    # check output is complete\n    for key in [\n        'highly_variable',\n        'means',\n        'variances',\n        'residual_variances',\n        'highly_variable_rank',\n        'highly_variable_nbatches',\n        'highly_variable_intersection',\n    ]:\n        assert key in output_df.keys()\n\n    # general checks on ranks, hvg flag and residual variance\n    _check_pearson_hvg_columns(output_df, n_top_genes)\n\n    # check intersection flag\n    nbatches = len(np.unique(adata.obs['batch']))\n    assert output_df['highly_variable_intersection'].values.dtype is np.dtype('bool')\n    assert np.sum(output_df['highly_variable_intersection']) <= n_top_genes * nbatches\n    assert np.all(output_df['highly_variable'][output_df.highly_variable_intersection])\n\n    # check ranks (with batch_key these are the median of within-batch ranks)\n    assert pd.api.types.is_float_dtype(output_df['highly_variable_rank'].dtype)\n\n    # check nbatches\n    assert output_df['highly_variable_nbatches'].values.dtype is np.dtype('int')\n    assert np.min(output_df['highly_variable_nbatches'].values) >= 0\n    assert np.max(output_df['highly_variable_nbatches'].values) <= nbatches\n\n    # check subsetting\n    if subset:\n        assert len(output_df) == n_top_genes\n    else:\n        assert len(output_df) == n_genes", "idx": 506}
{"project": "Scanpy", "commit_id": "558_scanpy_1.9.0_test_highly_variable_genes.py_test_higly_variable_genes_compare_to_seurat.py", "target": 0, "func": "def test_higly_variable_genes_compare_to_seurat():\n    seurat_hvg_info = pd.read_csv(FILE, sep=' ')\n\n    pbmc = pbmc68k_reduced()\n    pbmc.X = pbmc.raw.X\n    pbmc.var_names_make_unique()\n\n    sc.pp.normalize_per_cell(pbmc, counts_per_cell_after=1e4)\n    sc.pp.log1p(pbmc)\n    sc.pp.highly_variable_genes(\n        pbmc, flavor='seurat', min_mean=0.0125, max_mean=3, min_disp=0.5, inplace=True\n    )\n\n    np.testing.assert_array_equal(\n        seurat_hvg_info['highly_variable'], pbmc.var['highly_variable']\n    )\n\n    # (still) Not equal to tolerance rtol=2e-05, atol=2e-05\n    # np.testing.assert_allclose(4, 3.9999, rtol=2e-05, atol=2e-05)\n    np.testing.assert_allclose(\n        seurat_hvg_info['means'],\n        pbmc.var['means'],\n        rtol=2e-05,\n        atol=2e-05,\n    )\n    np.testing.assert_allclose(\n        seurat_hvg_info['dispersions'],\n        pbmc.var['dispersions'],\n        rtol=2e-05,\n        atol=2e-05,\n    )\n    np.testing.assert_allclose(\n        seurat_hvg_info['dispersions_norm'],\n        pbmc.var['dispersions_norm'],\n        rtol=2e-05,\n        atol=2e-05,", "idx": 507}
{"project": "Scanpy", "commit_id": "559_scanpy_1.9.0_test_highly_variable_genes.py_test_higly_variable_genes_compare_to_seurat_v3.py", "target": 0, "func": "def test_higly_variable_genes_compare_to_seurat_v3():\n    seurat_hvg_info = pd.read_csv(\n        FILE_V3, sep=' ', dtype={\"variances_norm\": np.float64}\n    )\n\n    pbmc = pbmc3k()\n    pbmc.var_names_make_unique()\n\n    pbmc_dense = pbmc.copy()\n    pbmc_dense.X = pbmc_dense.X.toarray()\n\n    sc.pp.highly_variable_genes(pbmc, n_top_genes=1000, flavor='seurat_v3')\n    sc.pp.highly_variable_genes(pbmc_dense, n_top_genes=1000, flavor='seurat_v3')\n\n    np.testing.assert_array_equal(\n        seurat_hvg_info['highly_variable'], pbmc.var['highly_variable']\n    )\n    np.testing.assert_allclose(\n        seurat_hvg_info['variances'],\n        pbmc.var['variances'],\n        rtol=2e-05,\n        atol=2e-05,\n    )\n    np.testing.assert_allclose(\n        seurat_hvg_info['variances_norm'],\n        pbmc.var['variances_norm'],\n        rtol=2e-05,\n        atol=2e-05,\n    )\n    np.testing.assert_allclose(\n        pbmc_dense.var['variances_norm'],\n        pbmc.var['variances_norm'],\n        rtol=2e-05,\n        atol=2e-05,\n    )\n\n    batch = np.zeros((len(pbmc)), dtype=int)\n    batch[1500:] = 1\n    pbmc.obs[\"batch\"] = batch\n    df = sc.pp.highly_variable_genes(\n        pbmc, n_top_genes=4000, flavor='seurat_v3', batch_key=\"batch\", inplace=False\n    )\n    df.sort_values(\n        [\"highly_variable_nbatches\", \"highly_variable_rank\"],\n        ascending=[False, True],\n        na_position=\"last\",\n        inplace=True,\n    )\n    df = df.iloc[:4000]\n    seurat_hvg_info_batch = pd.read_csv(\n        FILE_V3_BATCH, sep=' ', dtype={\"variances_norm\": np.float64}\n    )\n\n    # ranks might be slightly different due to many genes having same normalized var\n    seu = pd.Index(seurat_hvg_info_batch['x'].values)\n    assert len(seu.intersection(df.index)) / 4000 > 0.95\n\n    sc.pp.log1p(pbmc)\n    with pytest.warns(\n        UserWarning,\n        match=\"`flavor='seurat_v3'` expects raw count data, but non-integers were found.\",\n    ):\n        sc.pp.highly_variable_genes(pbmc, n_top_genes=1000, flavor='seurat_v3')", "idx": 508}
{"project": "Scanpy", "commit_id": "55_scanpy_1.9.0_readwrite.py__collect_datasets.py", "target": 1, "func": "def _collect_datasets(dsets: dict, group: h5py.Group):\n    for k, v in group.items():\n        if isinstance(v, h5py.Dataset):\n            dsets[k] = v[:]\n        else:\n            _collect_datasets(dsets, v)", "idx": 509}
{"project": "Scanpy", "commit_id": "560_scanpy_1.9.0_test_highly_variable_genes.py_test_filter_genes_dispersion_compare_to_seurat.py", "target": 0, "func": "def test_filter_genes_dispersion_compare_to_seurat():\n    seurat_hvg_info = pd.read_csv(FILE, sep=' ')\n\n    pbmc = pbmc68k_reduced()\n    pbmc.X = pbmc.raw.X\n    pbmc.var_names_make_unique()\n\n    sc.pp.normalize_per_cell(pbmc, counts_per_cell_after=1e4)\n    sc.pp.filter_genes_dispersion(\n        pbmc,\n        flavor='seurat',\n        log=True,\n        subset=False,\n        min_mean=0.0125,\n        max_mean=3,\n        min_disp=0.5,\n    )\n\n    np.testing.assert_array_equal(\n        seurat_hvg_info['highly_variable'], pbmc.var['highly_variable']\n    )\n\n    # (still) Not equal to tolerance rtol=2e-05, atol=2e-05:\n    # np.testing.assert_allclose(4, 3.9999, rtol=2e-05, atol=2e-05)\n    np.testing.assert_allclose(\n        seurat_hvg_info['means'],\n        pbmc.var['means'],\n        rtol=2e-05,\n        atol=2e-05,\n    )\n    np.testing.assert_allclose(\n        seurat_hvg_info['dispersions'],\n        pbmc.var['dispersions'],\n        rtol=2e-05,\n        atol=2e-05,\n    )\n    np.testing.assert_allclose(\n        seurat_hvg_info['dispersions_norm'],\n        pbmc.var['dispersions_norm'],\n        rtol=2e-05,\n        atol=2e-05,", "idx": 510}
{"project": "Scanpy", "commit_id": "561_scanpy_1.9.0_test_highly_variable_genes.py_test_highly_variable_genes_batches.py", "target": 0, "func": "def test_highly_variable_genes_batches():\n    adata = pbmc68k_reduced()\n    adata[:100, :100].X = np.zeros((100, 100))\n\n    adata.obs['batch'] = ['0' if i < 100 else '1' for i in range(adata.n_obs)]\n    adata_1 = adata[adata.obs.batch.isin(['0']), :]\n    adata_2 = adata[adata.obs.batch.isin(['1']), :]\n\n    sc.pp.highly_variable_genes(\n        adata,\n        batch_key='batch',\n        flavor='cell_ranger',\n        n_top_genes=200,\n    )\n\n    sc.pp.filter_genes(adata_1, min_cells=1)\n    sc.pp.filter_genes(adata_2, min_cells=1)\n    hvg1 = sc.pp.highly_variable_genes(\n        adata_1, flavor='cell_ranger', n_top_genes=200, inplace=False\n    )\n    hvg2 = sc.pp.highly_variable_genes(\n        adata_2, flavor='cell_ranger', n_top_genes=200, inplace=False\n    )\n\n    assert np.isclose(\n        adata.var['dispersions_norm'][100],\n        0.5 * hvg1['dispersions_norm'][0] + 0.5 * hvg2['dispersions_norm'][100],\n    )\n    assert np.isclose(\n        adata.var['dispersions_norm'][101],\n        0.5 * hvg1['dispersions_norm'][1] + 0.5 * hvg2['dispersions_norm'][101],\n    )\n    assert np.isclose(\n        adata.var['dispersions_norm'][0], 0.5 * hvg2['dispersions_norm'][0]\n    )\n\n    colnames = [\n        'means',\n        'dispersions',\n        'dispersions_norm',\n        'highly_variable',\n    ]\n\n    assert np.all(np.isin(colnames, hvg1.columns))", "idx": 511}
{"project": "Scanpy", "commit_id": "562_scanpy_1.9.0_test_highly_variable_genes.py_test_seurat_v3_mean_var_output_with_batchkey.py", "target": 0, "func": "def test_seurat_v3_mean_var_output_with_batchkey():\n    pbmc = pbmc3k()\n    pbmc.var_names_make_unique()\n    n_cells = pbmc.shape[0]\n    batch = np.zeros((n_cells), dtype=int)\n    batch[1500:] = 1\n    pbmc.obs[\"batch\"] = batch\n\n    # true_mean, true_var = _get_mean_var(pbmc.X)\n    true_mean = np.mean(pbmc.X.toarray(), axis=0)\n    true_var = np.var(pbmc.X.toarray(), axis=0, dtype=np.float64, ddof=1)\n\n    result_df = sc.pp.highly_variable_genes(\n        pbmc, batch_key='batch', flavor='seurat_v3', n_top_genes=4000, inplace=False\n    )\n    np.testing.assert_allclose(true_mean, result_df['means'], rtol=2e-05, atol=2e-05)\n    np.testing.assert_allclose(true_var, result_df['variances'], rtol=2e-05, atol=2e-05)", "idx": 512}
{"project": "Scanpy", "commit_id": "563_scanpy_1.9.0_test_ingest.py_adatas.py", "target": 0, "func": "def adatas():\n    pbmc = pbmc68k_reduced()\n    n_split = 500\n    adata_ref = sc.AnnData(pbmc.X[:n_split, :], obs=pbmc.obs.iloc[:n_split])\n    adata_new = sc.AnnData(pbmc.X[n_split:, :])\n\n    sc.pp.pca(adata_ref)\n    sc.pp.neighbors(adata_ref)\n    sc.tl.umap(adata_ref)\n\n    return adata_ref, adata_new", "idx": 513}
{"project": "Scanpy", "commit_id": "564_scanpy_1.9.0_test_ingest.py_test_representation.py", "target": 0, "func": "def test_representation(adatas):\n    adata_ref = adatas[0].copy()\n    adata_new = adatas[1].copy()\n\n    ing = sc.tl.Ingest(adata_ref)\n    ing.fit(adata_new)\n\n    assert ing._use_rep == 'X_pca'\n    assert ing._obsm['rep'].shape == (adata_new.n_obs, settings.N_PCS)\n    assert ing._pca_centered\n\n    sc.pp.pca(adata_ref, n_comps=30, zero_center=False)\n    sc.pp.neighbors(adata_ref)\n\n    ing = sc.tl.Ingest(adata_ref)\n    ing.fit(adata_new)\n\n    assert ing._use_rep == 'X_pca'\n    assert ing._obsm['rep'].shape == (adata_new.n_obs, 30)\n    assert not ing._pca_centered\n\n    sc.pp.neighbors(adata_ref, use_rep='X')\n\n    ing = sc.tl.Ingest(adata_ref)\n    ing.fit(adata_new)\n\n    assert ing._use_rep == 'X'\n    assert ing._obsm['rep'] is adata_new.X", "idx": 514}
{"project": "Scanpy", "commit_id": "565_scanpy_1.9.0_test_ingest.py_test_neighbors.py", "target": 0, "func": "def test_neighbors(adatas):\n    adata_ref = adatas[0].copy()\n    adata_new = adatas[1].copy()\n\n    ing = sc.tl.Ingest(adata_ref)\n    ing.fit(adata_new)\n    ing.neighbors(k=10)\n    indices = ing._indices\n\n    tree = KDTree(adata_ref.obsm['X_pca'])\n    true_indices = tree.query(ing._obsm['rep'], 10, return_distance=False)\n\n    num_correct = 0.0\n    for i in range(adata_new.n_obs):\n        num_correct += np.sum(np.in1d(true_indices[i], indices[i]))\n    percent_correct = num_correct / (adata_new.n_obs * 10)\n\n    assert percent_correct > 0.99", "idx": 515}
{"project": "Scanpy", "commit_id": "566_scanpy_1.9.0_test_ingest.py_test_neighbors_defaults.py", "target": 0, "func": "def test_neighbors_defaults(adatas, n):\n    adata_ref = adatas[0].copy()\n    adata_new = adatas[1].copy()\n\n    sc.pp.neighbors(adata_ref, n_neighbors=n)\n\n    ing = sc.tl.Ingest(adata_ref)\n    ing.fit(adata_new)\n    ing.neighbors()\n    assert ing._indices.shape[1] == n", "idx": 516}
{"project": "Scanpy", "commit_id": "567_scanpy_1.9.0_test_ingest.py_test_ingest_function.py", "target": 0, "func": "def test_ingest_function(adatas):\n    adata_ref = adatas[0].copy()\n    adata_new = adatas[1].copy()\n\n    sc.tl.ingest(\n        adata_new,\n        adata_ref,\n        obs='bulk_labels',\n        embedding_method=['umap', 'pca'],\n        inplace=True,\n    )\n\n    assert 'bulk_labels' in adata_new.obs\n    assert 'X_umap' in adata_new.obsm\n    assert 'X_pca' in adata_new.obsm\n\n    ad = sc.tl.ingest(\n        adata_new,\n        adata_ref,\n        obs='bulk_labels',\n        embedding_method=['umap', 'pca'],\n        inplace=False,\n    )\n\n    assert 'bulk_labels' in ad.obs\n    assert 'X_umap' in ad.obsm\n    assert 'X_pca' in ad.obsm", "idx": 517}
{"project": "Scanpy", "commit_id": "568_scanpy_1.9.0_test_ingest.py_test_ingest_map_embedding_umap.py", "target": 0, "func": "def test_ingest_map_embedding_umap():\n    adata_ref = sc.AnnData(X)\n    adata_new = sc.AnnData(T)\n\n    sc.pp.neighbors(\n        adata_ref, method='umap', use_rep='X', n_neighbors=4, random_state=0\n    )\n    sc.tl.umap(adata_ref, random_state=0)\n\n    ing = sc.tl.Ingest(adata_ref)\n    ing.fit(adata_new)\n    ing.map_embedding(method='umap')\n\n    reducer = UMAP(min_dist=0.5, random_state=0, n_neighbors=4)\n    reducer.fit(X)\n    umap_transformed_t = reducer.transform(T)\n\n    assert np.allclose(ing._obsm['X_umap'], umap_transformed_t)", "idx": 518}
{"project": "Scanpy", "commit_id": "569_scanpy_1.9.0_test_logging.py_logging_state.py", "target": 0, "func": "def logging_state():\n    verbosity_orig = s.verbosity\n    yield\n    s.logfile = sys.stderr\n    s.verbosity = verbosity_orig", "idx": 519}
{"project": "Scanpy", "commit_id": "56_scanpy_1.9.0_readwrite.py__read_v3_10x_h5.py", "target": 1, "func": "def _read_v3_10x_h5(filename, *, start=None):\n    \"\"\"\n    Read hdf5 file from Cell Ranger v3 or later versions.\n    \"\"\"\n    with h5py.File(str(filename), 'r') as f:\n        try:\n            dsets = {}\n            _collect_datasets(dsets, f[\"matrix\"])\n\n            from scipy.sparse import csr_matrix\n\n            M, N = dsets['shape']\n            data = dsets['data']\n            if dsets['data'].dtype == np.dtype('int32'):\n                data = dsets['data'].view('float32')\n                data[:] = dsets['data']\n            matrix = csr_matrix(\n                (data, dsets['indices'], dsets['indptr']),\n                shape=(N, M),\n            )\n            adata = AnnData(\n                matrix,\n                obs=dict(obs_names=dsets['barcodes'].astype(str)),\n                var=dict(\n                    var_names=dsets['name'].astype(str),\n                    gene_ids=dsets['id'].astype(str),\n                    feature_types=dsets['feature_type'].astype(str),\n                    genome=dsets['genome'].astype(str),\n                ),\n            )\n            logg.info('', time=start)\n            return adata\n        except KeyError:\n            raise Exception('File is missing one or more required datasets.')", "idx": 520}
{"project": "Scanpy", "commit_id": "570_scanpy_1.9.0_test_logging.py_test_defaults.py", "target": 0, "func": "def test_defaults():\n    assert s.logpath is None", "idx": 521}
{"project": "Scanpy", "commit_id": "571_scanpy_1.9.0_test_logging.py_test_formats.py", "target": 0, "func": "def test_formats(capsys, logging_state):\n    s.logfile = sys.stderr\n    s.verbosity = Verbosity.debug\n    log.error('0')\n    assert capsys.readouterr().err == 'ERROR: 0\\n'\n    log.warning('1')\n    assert capsys.readouterr().err == 'WARNING: 1\\n'\n    log.info('2')\n    assert capsys.readouterr().err == '2\\n'\n    log.hint('3')\n    assert capsys.readouterr().err == '--> 3\\n'\n    log.debug('4')\n    assert capsys.readouterr().err == '    4\\n'", "idx": 522}
{"project": "Scanpy", "commit_id": "572_scanpy_1.9.0_test_logging.py_test_deep.py", "target": 0, "func": "def test_deep(capsys, logging_state):\n    s.logfile = sys.stderr\n    s.verbosity = Verbosity.hint\n    log.hint('0')\n    assert capsys.readouterr().err == '--> 0\\n'\n    log.hint('1', deep='1!')\n    assert capsys.readouterr().err == '--> 1\\n'\n    s.verbosity = Verbosity.debug\n    log.hint('2')\n    assert capsys.readouterr().err == '--> 2\\n'\n    log.hint('3', deep='3!')\n    assert capsys.readouterr().err == '--> 3: 3!\\n'", "idx": 523}
{"project": "Scanpy", "commit_id": "573_scanpy_1.9.0_test_logging.py_test_logfile.py", "target": 0, "func": "def test_logfile(tmp_path, logging_state):\n    s.verbosity = Verbosity.hint\n\n    io = StringIO()\n    s.logfile = io\n    assert s.logfile is io\n    assert s.logpath is None\n    log.error('test!')\n    assert io.getvalue() == 'ERROR: test!\\n'\n\n    p = tmp_path / 'test.log'\n    s.logpath = p\n    assert s.logpath == p\n    assert s.logfile.name == str(p)\n    log.hint('test2')\n    log.debug('invisible')\n    assert s.logpath.read_text() == '--> test2\\n'", "idx": 524}
{"project": "Scanpy", "commit_id": "574_scanpy_1.9.0_test_logging.py_test_timing.py", "target": 0, "func": "def test_timing(monkeypatch, capsys, logging_state):\n    s.logfile = sys.stderr\n    counter = 0\n\n    class IncTime:\n        @staticmethod\n        def now(tz):\n            nonlocal counter\n            counter += 1\n            return datetime(2000, 1, 1, second=counter, microsecond=counter, tzinfo=tz)\n\n    monkeypatch.setattr(log, 'datetime', IncTime)\n    s.verbosity = Verbosity.debug\n\n    log.hint('1')\n    assert counter == 1 and capsys.readouterr().err == '--> 1\\n'\n    start = log.info('2')\n    assert counter == 2 and capsys.readouterr().err == '2\\n'\n    log.hint('3')\n    assert counter == 3 and capsys.readouterr().err == '--> 3\\n'\n    log.info('4', time=start)\n    assert counter == 4 and capsys.readouterr().err == '4 (0:00:02)\\n'\n    log.info('5 {time_passed}', time=start)\n    assert counter == 5 and capsys.readouterr().err == '5 0:00:03\\n'", "idx": 525}
{"project": "Scanpy", "commit_id": "575_scanpy_1.9.0_test_logging.py_test_call_outputs.py", "target": 0, "func": "def test_call_outputs(func):\n    \"\"\"\n    Tests that these functions print to stdout and don't error.\n\n    Checks that https://github.com/theislab/scanpy/issues/1437 is fixed.\n    \"\"\"\n    output_io = StringIO()\n    with redirect_stdout(output_io):\n        func()\n    output = output_io.getvalue()\n    assert output != \"\"", "idx": 526}
{"project": "Scanpy", "commit_id": "576_scanpy_1.9.0_test_logging.py_now.py", "target": 0, "func": "def now(tz):\n            nonlocal counter\n            counter += 1\n            return datetime(2000, 1, 1, second=counter, microsecond=counter, tzinfo=tz)", "idx": 527}
{"project": "Scanpy", "commit_id": "577_scanpy_1.9.0_test_marker_gene_overlap.py_generate_test_data.py", "target": 0, "func": "def generate_test_data():\n    # Create an artificial data set\n    test_data = AnnData(X=np.ones((9, 10)))\n    test_data.uns['rank_genes_groups'] = dict()\n    test_data.uns['rank_genes_groups']['names'] = np.rec.fromarrays(\n        [['a', 'b', 'c', 'd', 'e'], ['a', 'f', 'g', 'h', 'i']], names='c0,c1'\n    )\n    test_data.uns['rank_genes_groups']['pvals_adj'] = np.rec.fromarrays(\n        [[0.001, 0.01, 0.02, 0.05, 0.6], [0.001, 0.01, 0.02, 0.05, 0.6]], names='c0,c1'\n    )\n\n    marker_genes = {'type 1': {'a', 'b', 'c'}, 'type 2': {'a', 'f', 'g'}}\n\n    return test_data, marker_genes", "idx": 528}
{"project": "Scanpy", "commit_id": "578_scanpy_1.9.0_test_marker_gene_overlap.py_test_marker_overlap_base.py", "target": 0, "func": "def test_marker_overlap_base():\n    # Test all overlap calculations on artificial data\n    test_data, marker_genes = generate_test_data()\n\n    t1 = sc.tl.marker_gene_overlap(test_data, marker_genes)\n\n    assert t1['c0']['type 1'] == 3.0\n    assert t1['c1']['type 2'] == 3.0", "idx": 529}
{"project": "Scanpy", "commit_id": "579_scanpy_1.9.0_test_marker_gene_overlap.py_test_marker_overlap_normalization.py", "target": 0, "func": "def test_marker_overlap_normalization():\n    test_data, marker_genes = generate_test_data()\n\n    t2 = sc.tl.marker_gene_overlap(test_data, marker_genes, normalize='reference')\n    t3 = sc.tl.marker_gene_overlap(test_data, marker_genes, normalize='data')\n\n    assert t2['c0']['type 1'] == 1.0\n    assert t3['c1']['type 2'] == 0.6", "idx": 530}
{"project": "Scanpy", "commit_id": "57_scanpy_1.9.0_readwrite.py_read_visium.py", "target": 1, "func": "def read_visium(\n    path: Union[str, Path],\n    genome: Optional[str] = None,\n    *,\n    count_file: str = \"filtered_feature_bc_matrix.h5\",\n    library_id: str = None,\n    load_images: Optional[bool] = True,\n    source_image_path: Optional[Union[str, Path]] = None,\n) -> AnnData:\n    \"\"\"\\\n    Read 10x-Genomics-formatted visum dataset.\n\n    In addition to reading regular 10x output,\n    this looks for the `spatial` folder and loads images,\n    coordinates and scale factors.\n    Based on the `Space Ranger output docs`_.\n\n    See :func:`~scanpy.pl.spatial` for a compatible plotting function.\n\n    .. _Space Ranger output docs: https://support.10xgenomics.com/spatial-gene-expression/software/pipelines/latest/output/overview\n\n    Parameters\n    ----------\n    path\n        Path to directory for visium datafiles.\n    genome\n        Filter expression to genes within this genome.\n    count_file\n        Which file in the passed directory to use as the count file. Typically would be one of:\n        'filtered_feature_bc_matrix.h5' or 'raw_feature_bc_matrix.h5'.\n    library_id\n        Identifier for the visium library. Can be modified when concatenating multiple adata objects.\n    source_image_path\n        Path to the high-resolution tissue image. Path will be included in\n        `.uns[\"spatial\"][library_id][\"metadata\"][\"source_image_path\"]`.\n\n    Returns\n    -------\n    Annotated data matrix, where observations/cells are named by their\n    barcode and variables/genes by gene name. Stores the following information:\n\n    :attr:`~anndata.AnnData.X`\n        The data matrix is stored\n    :attr:`~anndata.AnnData.obs_names`\n        Cell names\n    :attr:`~anndata.AnnData.var_names`\n        Gene names\n    :attr:`~anndata.AnnData.var`\\\\ `['gene_ids']`\n        Gene IDs\n    :attr:`~anndata.AnnData.var`\\\\ `['feature_types']`\n        Feature types\n    :attr:`~anndata.AnnData.uns`\\\\ `['spatial']`\n        Dict of spaceranger output files with 'library_id' as key\n    :attr:`~anndata.AnnData.uns`\\\\ `['spatial'][library_id]['images']`\n        Dict of images (`'hires'` and `'lowres'`)\n    :attr:`~anndata.AnnData.uns`\\\\ `['spatial'][library_id]['scalefactors']`\n        Scale factors for the spots\n    :attr:`~anndata.AnnData.uns`\\\\ `['spatial'][library_id]['metadata']`\n        Files metadata: 'chemistry_description', 'software_version', 'source_image_path'\n    :attr:`~anndata.AnnData.obsm`\\\\ `['spatial']`\n        Spatial spot coordinates, usable as `basis` by :func:`~scanpy.pl.embedding`.\n    \"\"\"\n    path = Path(path)\n    adata = read_10x_h5(path / count_file, genome=genome)\n\n    adata.uns[\"spatial\"] = dict()\n\n    from h5py import File\n\n    with File(path / count_file, mode=\"r\") as f:\n        attrs = dict(f.attrs)\n    if library_id is None:\n        library_id = str(attrs.pop(\"library_ids\")[0], \"utf-8\")\n\n    adata.uns[\"spatial\"][library_id] = dict()\n\n    if load_images:\n        files = dict(\n            tissue_positions_file=path / 'spatial/tissue_positions_list.csv',\n            scalefactors_json_file=path / 'spatial/scalefactors_json.json',\n            hires_image=path / 'spatial/tissue_hires_image.png',\n            lowres_image=path / 'spatial/tissue_lowres_image.png',\n        )\n\n        # check if files exists, continue if images are missing\n        for f in files.values():\n            if not f.exists():\n                if any(x in str(f) for x in [\"hires_image\", \"lowres_image\"]):\n                    logg.warning(\n                        f\"You seem to be missing an image file.\\n\"\n                        f\"Could not find '{f}'.\"\n                    )\n                else:\n                    raise OSError(f\"Could not find '{f}'\")\n\n        adata.uns[\"spatial\"][library_id]['images'] = dict()\n        for res in ['hires', 'lowres']:\n            try:\n                adata.uns[\"spatial\"][library_id]['images'][res] = imread(\n                    str(files[f'{res}_image'])\n                )\n            except Exception:\n                raise OSError(f\"Could not find '{res}_image'\")\n\n        # read json scalefactors\n        adata.uns[\"spatial\"][library_id]['scalefactors'] = json.loads(\n            files['scalefactors_json_file'].read_bytes()\n        )\n\n        adata.uns[\"spatial\"][library_id][\"metadata\"] = {\n            k: (str(attrs[k], \"utf-8\") if isinstance(attrs[k], bytes) else attrs[k])\n            for k in (\"chemistry_description\", \"software_version\")\n            if k in attrs\n        }\n\n        # read coordinates\n        positions = pd.read_csv(files['tissue_positions_file'], header=None)\n        positions.columns = [\n            'barcode',\n            'in_tissue',\n            'array_row',\n            'array_col',\n            'pxl_col_in_fullres',\n            'pxl_row_in_fullres',\n        ]\n        positions.index = positions['barcode']\n\n        adata.obs = adata.obs.join(positions, how=\"left\")\n\n        adata.obsm['spatial'] = adata.obs[\n            ['pxl_row_in_fullres', 'pxl_col_in_fullres']\n        ].to_numpy()\n        adata.obs.drop(\n            columns=['barcode', 'pxl_row_in_fullres', 'pxl_col_in_fullres'],\n            inplace=True,\n        )\n\n        # put image path in uns\n        if source_image_path is not None:\n            # get an absolute path\n            source_image_path = str(Path(source_image_path).resolve())\n            adata.uns[\"spatial\"][library_id][\"metadata\"][\"source_image_path\"] = str(\n                source_image_path\n            )\n\n    return adata", "idx": 531}
{"project": "Scanpy", "commit_id": "580_scanpy_1.9.0_test_marker_gene_overlap.py_test_marker_overlap_methods.py", "target": 0, "func": "def test_marker_overlap_methods():\n    test_data, marker_genes = generate_test_data()\n\n    t4 = sc.tl.marker_gene_overlap(test_data, marker_genes, method='overlap_coef')\n    t5 = sc.tl.marker_gene_overlap(test_data, marker_genes, method='jaccard')\n\n    assert t4['c0']['type 1'] == 1.0\n    assert t5['c0']['type 1'] == 0.6", "idx": 532}
{"project": "Scanpy", "commit_id": "581_scanpy_1.9.0_test_marker_gene_overlap.py_test_marker_overlap_subsetting.py", "target": 0, "func": "def test_marker_overlap_subsetting():\n    test_data, marker_genes = generate_test_data()\n\n    t6 = sc.tl.marker_gene_overlap(test_data, marker_genes, top_n_markers=2)\n    t7 = sc.tl.marker_gene_overlap(test_data, marker_genes, adj_pval_threshold=0.01)\n\n    assert t6['c0']['type 1'] == 2.0\n    assert t7['c0']['type 1'] == 1.0", "idx": 533}
{"project": "Scanpy", "commit_id": "582_scanpy_1.9.0_test_metrics.py_test_gearys_c_consistency.py", "target": 0, "func": "def test_gearys_c_consistency():\n    pbmc = pbmc68k_reduced()\n    pbmc.layers[\"raw\"] = pbmc.raw.X.copy()\n    g = pbmc.obsp[\"connectivities\"]\n\n    assert eq(\n        sc.metrics.gearys_c(g, pbmc.obs[\"percent_mito\"]),\n        sc.metrics.gearys_c(pbmc, vals=pbmc.obs[\"percent_mito\"]),\n    )\n\n    assert eq(  # Test that series and vectors return same value\n        sc.metrics.gearys_c(g, pbmc.obs[\"percent_mito\"]),\n        sc.metrics.gearys_c(g, pbmc.obs[\"percent_mito\"].values),\n    )\n\n    np.testing.assert_array_equal(\n        sc.metrics.gearys_c(pbmc, obsm=\"X_pca\"),\n        sc.metrics.gearys_c(g, pbmc.obsm[\"X_pca\"].T),\n    )\n\n    all_genes = sc.metrics.gearys_c(pbmc, layer=\"raw\")\n    first_gene = sc.metrics.gearys_c(\n        pbmc, vals=pbmc.obs_vector(pbmc.var_names[0], layer=\"raw\")\n    )\n\n    np.testing.assert_allclose(all_genes[0], first_gene)\n\n    # Test that results are similar for sparse and dense reps of same data\n    np.testing.assert_allclose(\n        sc.metrics.gearys_c(pbmc, layer=\"raw\"),\n        sc.metrics.gearys_c(pbmc, vals=pbmc.layers[\"raw\"].T.toarray()),", "idx": 534}
{"project": "Scanpy", "commit_id": "583_scanpy_1.9.0_test_metrics.py_test_gearys_c_correctness.py", "target": 0, "func": "def test_gearys_c_correctness():\n    # Test case with perfectly seperated groups\n    connected = np.zeros(100)\n    connected[np.random.choice(100, size=30, replace=False)] = 1\n    graph = np.zeros((100, 100))\n    graph[np.ix_(connected.astype(bool), connected.astype(bool))] = 1\n    graph[np.ix_(~connected.astype(bool), ~connected.astype(bool))] = 1\n    graph = sparse.csr_matrix(graph)\n\n    assert sc.metrics.gearys_c(graph, connected) == 0.0\n    assert eq(\n        sc.metrics.gearys_c(graph, connected),\n        sc.metrics.gearys_c(graph, sparse.csr_matrix(connected)),\n    )\n    # Check for anndata > 0.7\n    if hasattr(sc.AnnData, \"obsp\"):\n        # Checking that obsp works\n        adata = sc.AnnData(\n            sparse.csr_matrix((100, 100)), obsp={\"connectivities\": graph}\n        )\n        assert sc.metrics.gearys_c(adata, vals=connected) == 0.0", "idx": 535}
{"project": "Scanpy", "commit_id": "584_scanpy_1.9.0_test_metrics.py_test_morans_i_consistency.py", "target": 0, "func": "def test_morans_i_consistency():\n    pbmc = pbmc68k_reduced()\n    pbmc.layers[\"raw\"] = pbmc.raw.X.copy()\n    g = pbmc.obsp[\"connectivities\"]\n\n    assert eq(\n        sc.metrics.morans_i(g, pbmc.obs[\"percent_mito\"]),\n        sc.metrics.morans_i(pbmc, vals=pbmc.obs[\"percent_mito\"]),\n    )\n\n    assert eq(  # Test that series and vectors return same value\n        sc.metrics.morans_i(g, pbmc.obs[\"percent_mito\"]),\n        sc.metrics.morans_i(g, pbmc.obs[\"percent_mito\"].values),\n    )\n\n    np.testing.assert_array_equal(\n        sc.metrics.morans_i(pbmc, obsm=\"X_pca\"),\n        sc.metrics.morans_i(g, pbmc.obsm[\"X_pca\"].T),\n    )\n\n    all_genes = sc.metrics.morans_i(pbmc, layer=\"raw\")\n    first_gene = sc.metrics.morans_i(\n        pbmc, vals=pbmc.obs_vector(pbmc.var_names[0], layer=\"raw\")\n    )\n\n    np.testing.assert_allclose(all_genes[0], first_gene, rtol=1e-5)\n\n    # Test that results are similar for sparse and dense reps of same data\n    np.testing.assert_allclose(\n        sc.metrics.morans_i(pbmc, layer=\"raw\"),\n        sc.metrics.morans_i(pbmc, vals=pbmc.layers[\"raw\"].T.toarray()),", "idx": 536}
{"project": "Scanpy", "commit_id": "585_scanpy_1.9.0_test_metrics.py_test_morans_i_correctness.py", "target": 0, "func": "def test_morans_i_correctness():\n    # Test case with perfectly seperated groups\n    connected = np.zeros(100)\n    connected[np.random.choice(100, size=50, replace=False)] = 1\n    graph = np.zeros((100, 100))\n    graph[np.ix_(connected.astype(bool), connected.astype(bool))] = 1\n    graph[np.ix_(~connected.astype(bool), ~connected.astype(bool))] = 1\n    graph = sparse.csr_matrix(graph)\n\n    assert sc.metrics.morans_i(graph, connected) == 1.0\n    assert eq(\n        sc.metrics.morans_i(graph, connected),\n        sc.metrics.morans_i(graph, sparse.csr_matrix(connected)),\n    )\n    # Check for anndata > 0.7\n    if hasattr(sc.AnnData, \"obsp\"):\n        # Checking that obsp works\n        adata = sc.AnnData(\n            sparse.csr_matrix((100, 100)), obsp={\"connectivities\": graph}\n        )\n        assert sc.metrics.morans_i(adata, vals=connected) == 1.0", "idx": 537}
{"project": "Scanpy", "commit_id": "586_scanpy_1.9.0_test_metrics.py_test_graph_metrics_w_constant_values.py", "target": 0, "func": "def test_graph_metrics_w_constant_values(metric, array_type):\n    # https://github.com/theislab/scanpy/issues/1806\n    pbmc = pbmc68k_reduced()\n    XT = array_type(pbmc.raw.X.T.copy())\n    g = pbmc.obsp[\"connectivities\"].copy()\n\n    const_inds = np.random.choice(XT.shape[0], 10, replace=False)\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\", sparse.SparseEfficiencyWarning)\n        XT_zero_vals = XT.copy()\n        XT_zero_vals[const_inds, :] = 0\n        XT_const_vals = XT.copy()\n        XT_const_vals[const_inds, :] = 42\n\n    results_full = metric(g, XT)\n    # TODO: Check for warnings\n    with pytest.warns(\n        UserWarning, match=r\"10 variables were constant, will return nan for these\"\n    ):\n        results_const_zeros = metric(g, XT_zero_vals)\n    with pytest.warns(\n        UserWarning, match=r\"10 variables were constant, will return nan for these\"\n    ):\n        results_const_vals = metric(g, XT_const_vals)\n\n    assert not np.isnan(results_full).any()\n    np.testing.assert_array_equal(results_const_zeros, results_const_vals)\n    np.testing.assert_array_equal(np.nan, results_const_zeros[const_inds])\n    np.testing.assert_array_equal(np.nan, results_const_vals[const_inds])\n\n    non_const_mask = ~np.isin(np.arange(XT.shape[0]), const_inds)\n    np.testing.assert_array_equal(\n        results_full[non_const_mask], results_const_zeros[non_const_mask]", "idx": 538}
{"project": "Scanpy", "commit_id": "587_scanpy_1.9.0_test_metrics.py_test_confusion_matrix.py", "target": 0, "func": "def test_confusion_matrix():\n    mtx = sc.metrics.confusion_matrix([\"a\", \"b\"], [\"c\", \"d\"], normalize=False)\n    assert mtx.loc[\"a\", \"c\"] == 1\n    assert mtx.loc[\"a\", \"d\"] == 0\n    assert mtx.loc[\"b\", \"d\"] == 1\n    assert mtx.loc[\"b\", \"c\"] == 0\n\n    mtx = sc.metrics.confusion_matrix([\"a\", \"b\"], [\"c\", \"d\"], normalize=True)\n    assert mtx.loc[\"a\", \"c\"] == 1.0\n    assert mtx.loc[\"a\", \"d\"] == 0.0\n    assert mtx.loc[\"b\", \"d\"] == 1.0\n    assert mtx.loc[\"b\", \"c\"] == 0.0\n\n    mtx = sc.metrics.confusion_matrix(\n        [\"a\", \"a\", \"b\", \"b\"], [\"c\", \"d\", \"c\", \"d\"], normalize=True\n    )\n    assert np.all(mtx == 0.5)", "idx": 539}
{"project": "Scanpy", "commit_id": "588_scanpy_1.9.0_test_metrics.py_test_confusion_matrix_randomized.py", "target": 0, "func": "def test_confusion_matrix_randomized():\n    chars = np.array(list(ascii_letters))\n    pos = np.random.choice(len(chars), size=np.random.randint(50, 150))\n    a = chars[pos]\n    b = np.random.permutation(chars)[pos]\n    df = pd.DataFrame({\"a\": a, \"b\": b})\n\n    pd.testing.assert_frame_equal(\n        sc.metrics.confusion_matrix(\"a\", \"b\", df),\n        sc.metrics.confusion_matrix(df[\"a\"], df[\"b\"]),\n    )\n    pd.testing.assert_frame_equal(\n        sc.metrics.confusion_matrix(df[\"a\"].values, df[\"b\"].values),\n        sc.metrics.confusion_matrix(a, b),", "idx": 540}
{"project": "Scanpy", "commit_id": "589_scanpy_1.9.0_test_metrics.py_test_confusion_matrix_api.py", "target": 0, "func": "def test_confusion_matrix_api():\n    data = pd.DataFrame(\n        {\"a\": np.random.randint(5, size=100), \"b\": np.random.randint(5, size=100)}\n    )\n    expected = sc.metrics.confusion_matrix(data[\"a\"], data[\"b\"])\n\n    pd.testing.assert_frame_equal(expected, sc.metrics.confusion_matrix(\"a\", \"b\", data))\n\n    pd.testing.assert_frame_equal(\n        expected, sc.metrics.confusion_matrix(\"a\", data[\"b\"], data)\n    )\n\n    pd.testing.assert_frame_equal(\n        expected, sc.metrics.confusion_matrix(data[\"a\"], \"b\", data)", "idx": 541}
{"project": "Scanpy", "commit_id": "58_scanpy_1.9.0_readwrite.py_read_10x_mtx.py", "target": 1, "func": "def read_10x_mtx(\n    path: Union[Path, str],\n    var_names: Literal['gene_symbols', 'gene_ids'] = 'gene_symbols',\n    make_unique: bool = True,\n    cache: bool = False,\n    cache_compression: Union[Literal['gzip', 'lzf'], None, Empty] = _empty,\n    gex_only: bool = True,\n    *,\n    prefix: str = None,\n) -> AnnData:\n    \"\"\"\\\n    Read 10x-Genomics-formatted mtx directory.\n\n    Parameters\n    ----------\n    path\n        Path to directory for `.mtx` and `.tsv` files,\n        e.g. './filtered_gene_bc_matrices/hg19/'.\n    var_names\n        The variables index.\n    make_unique\n        Whether to make the variables index unique by appending '-1',\n        '-2' etc. or not.\n    cache\n        If `False`, read from source, if `True`, read from fast 'h5ad' cache.\n    cache_compression\n        See the h5py :ref:`dataset_compression`.\n        (Default: `settings.cache_compression`)\n    gex_only\n        Only keep 'Gene Expression' data and ignore other feature types,\n        e.g. 'Antibody Capture', 'CRISPR Guide Capture', or 'Custom'\n    prefix\n        Any prefix before `matrix.mtx`, `genes.tsv` and `barcodes.tsv`. For instance,\n        if the files are named `patientA_matrix.mtx`, `patientA_genes.tsv` and\n        `patientA_barcodes.tsv` the prefix is `patientA_`.\n        (Default: no prefix)\n\n    Returns\n    -------\n    An :class:`~anndata.AnnData` object\n    \"\"\"\n    path = Path(path)\n    prefix = \"\" if prefix is None else prefix\n    genefile_exists = (path / f'{prefix}genes.tsv').is_file()\n    read = _read_legacy_10x_mtx if genefile_exists else _read_v3_10x_mtx\n    adata = read(\n        str(path),\n        var_names=var_names,\n        make_unique=make_unique,\n        cache=cache,\n        cache_compression=cache_compression,\n        prefix=prefix,\n    )\n    if genefile_exists or not gex_only:\n        return adata\n    else:\n        gex_rows = list(\n            map(lambda x: x == 'Gene Expression', adata.var['feature_types'])\n        )\n        return adata[:, gex_rows].copy()", "idx": 542}
{"project": "Scanpy", "commit_id": "590_scanpy_1.9.0_test_neighbors.py_get_neighbors.py", "target": 0, "func": "def get_neighbors() -> Neighbors:\n    return Neighbors(AnnData(np.array(X)))", "idx": 543}
{"project": "Scanpy", "commit_id": "591_scanpy_1.9.0_test_neighbors.py_neigh.py", "target": 0, "func": "def neigh() -> Neighbors:\n    return get_neighbors()", "idx": 544}
{"project": "Scanpy", "commit_id": "592_scanpy_1.9.0_test_neighbors.py_test_umap_connectivities_euclidean.py", "target": 0, "func": "def test_umap_connectivities_euclidean(neigh):\n    neigh.compute_neighbors(method='umap', n_neighbors=n_neighbors)\n    assert np.allclose(neigh.distances.toarray(), distances_euclidean)\n    assert np.allclose(neigh.connectivities.toarray(), connectivities_umap)\n    neigh.compute_transitions()\n    assert np.allclose(neigh.transitions_sym.toarray(), transitions_sym_umap)\n    assert np.allclose(neigh.transitions.toarray(), transitions_umap)", "idx": 545}
{"project": "Scanpy", "commit_id": "593_scanpy_1.9.0_test_neighbors.py_test_gauss_noknn_connectivities_euclidean.py", "target": 0, "func": "def test_gauss_noknn_connectivities_euclidean(neigh):\n    neigh.compute_neighbors(method='gauss', knn=False, n_neighbors=3)\n    assert np.allclose(neigh.distances, distances_euclidean_all)\n    assert np.allclose(neigh.connectivities, connectivities_gauss_noknn)\n    neigh.compute_transitions()\n    assert np.allclose(neigh.transitions_sym, transitions_sym_gauss_noknn)\n    assert np.allclose(neigh.transitions, transitions_gauss_noknn)", "idx": 546}
{"project": "Scanpy", "commit_id": "594_scanpy_1.9.0_test_neighbors.py_test_gauss_connectivities_euclidean.py", "target": 0, "func": "def test_gauss_connectivities_euclidean(neigh):\n    neigh.compute_neighbors(method='gauss', n_neighbors=n_neighbors)\n    assert np.allclose(neigh.distances.toarray(), distances_euclidean)\n    assert np.allclose(neigh.connectivities.toarray(), connectivities_gauss_knn)\n    neigh.compute_transitions()\n    assert np.allclose(neigh.transitions_sym.toarray(), transitions_sym_gauss_knn)\n    assert np.allclose(neigh.transitions.toarray(), transitions_gauss_knn)", "idx": 547}
{"project": "Scanpy", "commit_id": "595_scanpy_1.9.0_test_neighbors.py_test_metrics_argument.py", "target": 0, "func": "def test_metrics_argument():\n    no_knn_euclidean = get_neighbors()\n    no_knn_euclidean.compute_neighbors(\n        method=\"gauss\", knn=False, n_neighbors=n_neighbors, metric=\"euclidean\"\n    )\n    no_knn_manhattan = get_neighbors()\n    no_knn_manhattan.compute_neighbors(\n        method=\"gauss\", knn=False, n_neighbors=n_neighbors, metric=\"manhattan\"\n    )\n    assert not np.allclose(no_knn_euclidean.distances, no_knn_manhattan.distances)", "idx": 548}
{"project": "Scanpy", "commit_id": "596_scanpy_1.9.0_test_neighbors.py_test_use_rep_argument.py", "target": 0, "func": "def test_use_rep_argument():\n    adata = AnnData(np.random.randn(30, 300))\n    sc.pp.pca(adata)\n    neigh_pca = Neighbors(adata)\n    neigh_pca.compute_neighbors(n_pcs=5, use_rep='X_pca')\n    neigh_none = Neighbors(adata)\n    neigh_none.compute_neighbors(n_pcs=5, use_rep=None)\n    assert np.allclose(neigh_pca.distances.toarray(), neigh_none.distances.toarray())", "idx": 549}
{"project": "Scanpy", "commit_id": "597_scanpy_1.9.0_test_neighbors.py_test_restore_n_neighbors.py", "target": 0, "func": "def test_restore_n_neighbors(neigh, conv):\n    neigh.compute_neighbors(method='gauss', n_neighbors=n_neighbors)\n\n    ad = AnnData(np.array(X))\n    # Allow deprecated usage for now\n    with warnings.catch_warnings():\n        warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"anndata\")\n        ad.uns['neighbors'] = dict(connectivities=conv(neigh.connectivities))\n    neigh_restored = Neighbors(ad)\n    assert neigh_restored.n_neighbors == 1", "idx": 550}
{"project": "Scanpy", "commit_id": "598_scanpy_1.9.0_test_neighbors_key_added.py_adata.py", "target": 0, "func": "def adata():\n    return sc.AnnData(pbmc68k_reduced().X)", "idx": 551}
{"project": "Scanpy", "commit_id": "599_scanpy_1.9.0_test_neighbors_key_added.py_test_neighbors_key_added.py", "target": 0, "func": "def test_neighbors_key_added(adata):\n    sc.pp.neighbors(adata, n_neighbors=n_neighbors, random_state=0)\n    sc.pp.neighbors(adata, n_neighbors=n_neighbors, random_state=0, key_added=key)\n\n    conns_key = adata.uns[key]['connectivities_key']\n    dists_key = adata.uns[key]['distances_key']\n\n    assert adata.uns['neighbors']['params'] == adata.uns[key]['params']\n    assert np.allclose(\n        adata.obsp['connectivities'].toarray(), adata.obsp[conns_key].toarray()\n    )\n    assert np.allclose(\n        adata.obsp['distances'].toarray(), adata.obsp[dists_key].toarray()", "idx": 552}
{"project": "Scanpy", "commit_id": "59_scanpy_1.9.0_readwrite.py__read_legacy_10x_mtx.py", "target": 0, "func": "def _read_legacy_10x_mtx(\n    path,\n    var_names='gene_symbols',\n    make_unique=True,\n    cache=False,\n    cache_compression=_empty,\n    *,\n    prefix=\"\",\n):\n    \"\"\"\n    Read mex from output from Cell Ranger v2 or earlier versions\n    \"\"\"\n    path = Path(path)\n    adata = read(\n        path / f'{prefix}matrix.mtx',\n        cache=cache,\n        cache_compression=cache_compression,\n    ).T  # transpose the data\n    genes = pd.read_csv(path / f'{prefix}genes.tsv', header=None, sep='\\t')\n    if var_names == 'gene_symbols':\n        var_names = genes[1].values\n        if make_unique:\n            var_names = anndata.utils.make_index_unique(pd.Index(var_names))\n        adata.var_names = var_names\n        adata.var['gene_ids'] = genes[0].values\n    elif var_names == 'gene_ids':\n        adata.var_names = genes[0].values\n        adata.var['gene_symbols'] = genes[1].values\n    else:\n        raise ValueError(\"`var_names` needs to be 'gene_symbols' or 'gene_ids'\")\n    adata.obs_names = pd.read_csv(path / f'{prefix}barcodes.tsv', header=None)[0].values\n    return adata", "idx": 553}
{"project": "Scanpy", "commit_id": "5_scanpy_1.9.0_conftest.py_add_nunit_attachment.py", "target": 0, "func": "def add_nunit_attachment(request):\n        def noop(file, description):\n            pass\n\n        return noop", "idx": 554}
{"project": "Scanpy", "commit_id": "600_scanpy_1.9.0_test_neighbors_key_added.py_test_neighbors_key_obsp.py", "target": 0, "func": "def test_neighbors_key_obsp(adata, field):\n    adata1 = adata.copy()\n\n    sc.pp.neighbors(adata, n_neighbors=n_neighbors, random_state=0)\n    sc.pp.neighbors(adata1, n_neighbors=n_neighbors, random_state=0, key_added=key)\n\n    if field == 'neighbors_key':\n        arg = {field: key}\n    else:\n        arg = {field: adata1.uns[key]['connectivities_key']}\n\n    sc.tl.draw_graph(adata, layout='fr', random_state=1)\n    sc.tl.draw_graph(adata1, layout='fr', random_state=1, **arg)\n\n    assert adata.uns['draw_graph']['params'] == adata1.uns['draw_graph']['params']\n    assert np.allclose(adata.obsm['X_draw_graph_fr'], adata1.obsm['X_draw_graph_fr'])\n\n    sc.tl.leiden(adata, random_state=0)\n    sc.tl.leiden(adata1, random_state=0, **arg)\n\n    assert adata.uns['leiden']['params'] == adata1.uns['leiden']['params']\n    assert np.all(adata.obs['leiden'] == adata1.obs['leiden'])\n\n    # no obsp in umap, paga\n    if field == 'neighbors_key':\n        sc.tl.umap(adata, random_state=0)\n        sc.tl.umap(adata1, random_state=0, neighbors_key=key)\n\n        assert adata.uns['umap']['params'] == adata1.uns['umap']['params']\n        assert np.allclose(adata.obsm['X_umap'], adata1.obsm['X_umap'])\n\n        sc.tl.paga(adata, groups='leiden')\n        sc.tl.paga(adata1, groups='leiden', neighbors_key=key)\n\n        assert np.allclose(\n            adata.uns['paga']['connectivities'].toarray(),\n            adata1.uns['paga']['connectivities'].toarray(),\n        )\n        assert np.allclose(\n            adata.uns['paga']['connectivities_tree'].toarray(),\n            adata1.uns['paga']['connectivities_tree'].toarray(),", "idx": 555}
{"project": "Scanpy", "commit_id": "601_scanpy_1.9.0_test_neighbors_key_added.py_test_neighbors_key_obsp_louvain.py", "target": 0, "func": "def test_neighbors_key_obsp_louvain(adata, field):\n    pytest.importorskip(\"louvain\")\n    adata1 = adata.copy()\n\n    sc.pp.neighbors(adata, n_neighbors=n_neighbors, random_state=0)\n    sc.pp.neighbors(adata1, n_neighbors=n_neighbors, random_state=0, key_added=key)\n\n    if field == 'neighbors_key':\n        arg = {field: key}\n    else:\n        arg = {field: adata1.uns[key]['connectivities_key']}\n\n    sc.tl.louvain(adata, random_state=0)\n    sc.tl.louvain(adata1, random_state=0, **arg)\n\n    assert adata.uns['louvain']['params'] == adata1.uns['louvain']['params']\n    assert np.all(adata.obs['louvain'] == adata1.obs['louvain'])", "idx": 556}
{"project": "Scanpy", "commit_id": "602_scanpy_1.9.0_test_normalization.py_test_normalize_total.py", "target": 0, "func": "def test_normalize_total(typ, dtype):\n    adata = AnnData(typ(X_total), dtype=dtype)\n    sc.pp.normalize_total(adata, key_added='n_counts')\n    assert np.allclose(np.ravel(adata.X.sum(axis=1)), [3.0, 3.0, 3.0])\n    sc.pp.normalize_total(adata, target_sum=1, key_added='n_counts2')\n    assert np.allclose(np.ravel(adata.X.sum(axis=1)), [1.0, 1.0, 1.0])\n\n    adata = AnnData(typ(X_frac), dtype=dtype)\n    sc.pp.normalize_total(adata, exclude_highly_expressed=True, max_fraction=0.7)\n    assert np.allclose(np.ravel(adata.X[:, 1:3].sum(axis=1)), [1.0, 1.0, 1.0])", "idx": 557}
{"project": "Scanpy", "commit_id": "603_scanpy_1.9.0_test_normalization.py_test_normalize_total_rep.py", "target": 0, "func": "def test_normalize_total_rep(typ, dtype):\n    # Test that layer kwarg works\n    X = typ(sparse.random(100, 50, format=\"csr\", density=0.2, dtype=dtype))\n    check_rep_mutation(sc.pp.normalize_total, X, fields=[\"layer\"])\n    check_rep_results(sc.pp.normalize_total, X, fields=[\"layer\"])", "idx": 558}
{"project": "Scanpy", "commit_id": "604_scanpy_1.9.0_test_normalization.py_test_normalize_total_layers.py", "target": 0, "func": "def test_normalize_total_layers(typ, dtype):\n    adata = AnnData(typ(X_total), dtype=dtype)\n    adata.layers[\"layer\"] = adata.X.copy()\n    with pytest.warns(FutureWarning, match=r\".*layers.*deprecated\"):\n        sc.pp.normalize_total(adata, layers=[\"layer\"])\n    assert np.allclose(adata.layers[\"layer\"].sum(axis=1), [3.0, 3.0, 3.0])", "idx": 559}
{"project": "Scanpy", "commit_id": "605_scanpy_1.9.0_test_normalization.py_test_normalize_total_view.py", "target": 0, "func": "def test_normalize_total_view(typ, dtype):\n    adata = AnnData(typ(X_total), dtype=dtype)\n    v = adata[:, :]\n\n    sc.pp.normalize_total(v)\n    sc.pp.normalize_total(adata)\n\n    assert not v.is_view\n    assert_equal(adata, v)", "idx": 560}
{"project": "Scanpy", "commit_id": "606_scanpy_1.9.0_test_normalization.py_test_normalize_pearson_residuals_inputchecks.py", "target": 0, "func": "def test_normalize_pearson_residuals_inputchecks(sparsity_func, dtype):\n\n    adata = _prepare_pbmc_testdata(sparsity_func, dtype)\n\n    # depending on check_values, warnings should be raised for non-integer data\n    if dtype == 'float32':\n\n        adata_noninteger = adata.copy()\n        x, y = np.nonzero(adata_noninteger.X)\n        adata_noninteger.X[x[0], y[0]] = 0.5\n\n        _check_check_values_warnings(\n            function=sc.experimental.pp.normalize_pearson_residuals,\n            adata=adata_noninteger,\n            expected_warning=\"`normalize_pearson_residuals()` expects raw count data, but non-integers were found.\",\n        )\n\n    # errors should be raised for invalid theta values\n    for theta in [0, -1]:\n\n        with pytest.raises(ValueError, match='Pearson residuals require theta > 0'):\n            sc.experimental.pp.normalize_pearson_residuals(adata.copy(), theta=theta)\n\n    with pytest.raises(\n        ValueError, match='Pearson residuals require `clip>=0` or `clip=None`.'\n    ):\n        sc.experimental.pp.normalize_pearson_residuals(adata.copy(), clip=-1)", "idx": 561}
{"project": "Scanpy", "commit_id": "607_scanpy_1.9.0_test_normalization.py_test_normalize_pearson_residuals_values.py", "target": 0, "func": "def test_normalize_pearson_residuals_values(sparsity_func, dtype, theta, clip):\n\n    # toy data\n    X = np.array([[3, 6], [2, 4], [1, 0]])\n    ns = np.sum(X, axis=1)\n    ps = np.sum(X, axis=0) / np.sum(X)\n    mu = np.outer(ns, ps)\n\n    # compute reference residuals\n    if np.isinf(theta):\n        # Poisson case\n        residuals_reference = (X - mu) / np.sqrt(mu)\n    else:\n        # NB case\n        residuals_reference = (X - mu) / np.sqrt(mu + mu**2 / theta)\n\n    # compute output to test\n    adata = AnnData(sparsity_func(X), dtype=dtype)\n    output = sc.experimental.pp.normalize_pearson_residuals(\n        adata, theta=theta, clip=clip, inplace=False\n    )\n    output_X = output['X']\n    sc.experimental.pp.normalize_pearson_residuals(\n        adata, theta=theta, clip=clip, inplace=True\n    )\n\n    # check for correct new `adata.uns` keys\n    assert np.all(np.isin(['pearson_residuals_normalization'], list(adata.uns.keys())))\n    assert np.all(\n        np.isin(\n            ['theta', 'clip', 'computed_on'],\n            list(adata.uns['pearson_residuals_normalization'].keys()),\n        )\n    )\n    # test against inplace\n    np.testing.assert_array_equal(adata.X, output_X)\n\n    if clip is None:\n        # default clipping: compare to sqrt(n) threshold\n        clipping_threshold = np.sqrt(adata.shape[0]).astype(np.float32)\n        assert np.max(output_X) <= clipping_threshold\n        assert np.min(output_X) >= -clipping_threshold\n    elif np.isinf(clip):\n        # no clipping: compare to raw residuals\n        assert np.allclose(output_X, residuals_reference)\n    else:\n        # custom clipping: compare to custom threshold\n        assert np.max(output_X) <= clip\n        assert np.min(output_X) >= -clip", "idx": 562}
{"project": "Scanpy", "commit_id": "608_scanpy_1.9.0_test_normalization.py__check_pearson_pca_fields.py", "target": 0, "func": "def _check_pearson_pca_fields(ad, n_cells, n_comps):\n    assert np.all(\n        np.isin(\n            ['pearson_residuals_normalization', 'pca'],\n            list(ad.uns.keys()),\n        )\n    ), (\n        \"\"\"Missing `.uns` keys. Expected `['pearson_residuals_normalization', 'pca']`, but only %s were found\"\"\"\n        % (list(ad.uns.keys()))\n    )\n    assert 'X_pca' in list(\n        ad.obsm.keys()\n    ), \"\"\"Missing `obsm` key `'X_pca'`, only %s were found\"\"\" % (list(ad.obsm.keys()))\n    assert 'PCs' in list(\n        ad.varm.keys()\n    ), \"\"\"Missing `varm` key `'PCs'`, only %s were found\"\"\" % (list(ad.varm.keys()))\n    assert ad.obsm['X_pca'].shape == (\n        n_cells,\n        n_comps,\n    ), 'Wrong shape of PCA output in `X_pca`'", "idx": 563}
{"project": "Scanpy", "commit_id": "609_scanpy_1.9.0_test_normalization.py_test_normalize_pearson_residuals_pca.py", "target": 0, "func": "def test_normalize_pearson_residuals_pca(sparsity_func, dtype, n_hvgs, n_comps):\n\n    adata = _prepare_pbmc_testdata(sparsity_func, dtype, small=True)\n    n_cells, n_genes = adata.shape\n\n    adata_with_hvgs = adata.copy()\n    sc.experimental.pp.highly_variable_genes(\n        adata_with_hvgs, flavor='pearson_residuals', n_top_genes=n_hvgs\n    )\n    adata_not_using_hvgs = adata_with_hvgs.copy()\n\n    ### inplace = False ###\n    # outputs the (potentially hvg-restricted) adata_pca object\n    # PCA on all genes (no HVGs present)\n    adata_pca = sc.experimental.pp.normalize_pearson_residuals_pca(\n        adata.copy(), inplace=False, n_comps=n_comps\n    )\n    # PCA on hvgs only (HVGs present, and by default, `use_highly_variable=True`)\n    adata_pca_with_hvgs = sc.experimental.pp.normalize_pearson_residuals_pca(\n        adata_with_hvgs.copy(), inplace=False, n_comps=n_comps\n    )\n    # PCA again on all genes (HVGs present, but hvg use supressed by `use_highly_variable=False`)\n    adata_pca_not_using_hvgs = sc.experimental.pp.normalize_pearson_residuals_pca(\n        adata_not_using_hvgs.copy(),\n        inplace=False,\n        n_comps=n_comps,\n        use_highly_variable=False,\n    )\n\n    # for all cases, check adata_pca keys are complete\n    for ad in [adata_pca, adata_pca_with_hvgs, adata_pca_not_using_hvgs]:\n        _check_pearson_pca_fields(ad, n_cells, n_comps)\n\n    # check adata shape to see if all genes or only HVGs are in the returned adata\n    assert adata_pca.shape == (n_cells, n_genes)\n    assert adata_pca_with_hvgs.shape == (n_cells, n_hvgs)  # only HVGs retained\n    assert adata_pca_not_using_hvgs.shape == (n_cells, n_genes)\n\n    # check PC shapes to see whether or not HVGs were used for PCA\n    assert adata_pca.varm['PCs'].shape == (n_genes, n_comps)\n    assert adata_pca_with_hvgs.varm['PCs'].shape == (\n        n_hvgs,\n        n_comps,\n    )\n    assert adata_pca_not_using_hvgs.varm['PCs'].shape == (n_genes, n_comps)\n\n    ### inplace = True ###\n    # modifies the input adata object\n    # PCA on all genes (no HVGs present)\n    sc.experimental.pp.normalize_pearson_residuals_pca(\n        adata, inplace=True, n_comps=n_comps\n    )\n    # PCA on hvgs only (HVGs present, and by default, `use_highly_variable=True`)\n    sc.experimental.pp.normalize_pearson_residuals_pca(\n        adata_with_hvgs, inplace=True, n_comps=n_comps\n    )\n    # PCA again on all genes (HVGs present, but hvg use supressed by `use_highly_variable=False`)\n    sc.experimental.pp.normalize_pearson_residuals_pca(\n        adata_not_using_hvgs,\n        inplace=True,\n        n_comps=n_comps,\n        use_highly_variable=False,\n    )\n\n    # for all cases, check adata_pca keys are complete\n    for ad in [adata, adata_with_hvgs, adata_not_using_hvgs]:\n        _check_pearson_pca_fields(ad, n_cells, n_comps)\n\n        # check shapes: inplace adata's should always retains original shape\n        assert ad.shape == (n_cells, n_genes)\n        assert ad.varm['PCs'].shape == (n_genes, n_comps)\n\n    # check if there are columns of all-zeros in the PCs shapes\n    # to see whether or not HVGs were used for PCA\n    # no all-zero-colums should exist\n    assert sum(np.sum(np.abs(adata.varm['PCs']), axis=1) == 0) == 0\n    # number of all-zero-colums should be number of non-hvgs\n    assert (\n        sum(np.sum(np.abs(adata_with_hvgs.varm['PCs']), axis=1) == 0)\n        == n_genes - n_hvgs\n    )\n    # no all-zero-colums should exist\n    assert sum(np.sum(np.abs(adata_not_using_hvgs.varm['PCs']), axis=1) == 0) == 0\n\n    # compare PCA results beteen inplace/outplace\n    for ad_inplace, ad_outplace in zip(\n        [adata_pca, adata_pca_with_hvgs, adata_pca_not_using_hvgs],\n        [adata, adata_with_hvgs, adata_not_using_hvgs],\n    ):\n        np.testing.assert_array_equal(\n            ad_inplace.obsm['X_pca'],\n            ad_outplace.obsm['X_pca'],", "idx": 564}
{"project": "Scanpy", "commit_id": "60_scanpy_1.9.0_readwrite.py__read_v3_10x_mtx.py", "target": 0, "func": "def _read_v3_10x_mtx(\n    path,\n    var_names='gene_symbols',\n    make_unique=True,\n    cache=False,\n    cache_compression=_empty,\n    *,\n    prefix=\"\",\n):\n    \"\"\"\n    Read mtx from output from Cell Ranger v3 or later versions\n    \"\"\"\n    path = Path(path)\n    adata = read(\n        path / f'{prefix}matrix.mtx.gz',\n        cache=cache,\n        cache_compression=cache_compression,\n    ).T  # transpose the data\n    genes = pd.read_csv(path / f'{prefix}features.tsv.gz', header=None, sep='\\t')\n    if var_names == 'gene_symbols':\n        var_names = genes[1].values\n        if make_unique:\n            var_names = anndata.utils.make_index_unique(pd.Index(var_names))\n        adata.var_names = var_names\n        adata.var['gene_ids'] = genes[0].values\n    elif var_names == 'gene_ids':\n        adata.var_names = genes[0].values\n        adata.var['gene_symbols'] = genes[1].values\n    else:\n        raise ValueError(\"`var_names` needs to be 'gene_symbols' or 'gene_ids'\")\n    adata.var['feature_types'] = genes[2].values\n    adata.obs_names = pd.read_csv(path / f'{prefix}barcodes.tsv.gz', header=None)[\n        0\n    ].values\n    return adata", "idx": 565}
{"project": "Scanpy", "commit_id": "610_scanpy_1.9.0_test_normalization.py_test_normalize_pearson_residuals_recipe.py", "target": 0, "func": "def test_normalize_pearson_residuals_recipe(sparsity_func, dtype, n_hvgs, n_comps):\n    adata = _prepare_pbmc_testdata(sparsity_func, dtype, small=True)\n    n_cells, n_genes = adata.shape\n\n    ### inplace = False ###\n    # outputs the (potentially hvg-restricted) adata_pca object\n    # PCA on all genes\n    adata_pca, hvg = sc.experimental.pp.recipe_pearson_residuals(\n        adata.copy(), inplace=False, n_comps=n_comps, n_top_genes=n_hvgs\n    )\n\n    # check PCA fields\n    _check_pearson_pca_fields(adata_pca, n_cells, n_comps)\n    # check adata output shape (only HVGs in output)\n    assert adata_pca.shape == (n_cells, n_hvgs)\n    # check PC shape (non-hvgs are removed, so only `n_hvgs` genes)\n    assert adata_pca.varm['PCs'].shape == (n_hvgs, n_comps)\n\n    # check hvg df\n    assert np.all(\n        np.isin(\n            [\n                'means',\n                'variances',\n                'residual_variances',\n                'highly_variable_rank',\n                'highly_variable',\n            ],\n            list(hvg.columns),\n        )\n    )\n    assert np.sum(hvg['highly_variable']) == n_hvgs\n    assert hvg.shape[0] == n_genes\n\n    ### inplace = True ###\n    # modifies the input adata object\n    # PCA on all genes\n    sc.experimental.pp.recipe_pearson_residuals(\n        adata, inplace=True, n_comps=n_comps, n_top_genes=n_hvgs\n    )\n\n    # check PCA fields and output shape\n    _check_pearson_pca_fields(adata, n_cells, n_comps)\n    # check adata shape (no change to input)\n    assert adata.shape == (n_cells, n_genes)\n    # check PC shape (non-hvgs are masked with 0s, so original number of genes)\n    assert adata.varm['PCs'].shape == (n_genes, n_comps)\n    # number of all-zero-colums should be number of non-hvgs\n    assert sum(np.sum(np.abs(adata.varm['PCs']), axis=1) == 0) == n_genes - n_hvgs", "idx": 566}
{"project": "Scanpy", "commit_id": "611_scanpy_1.9.0_test_package_structure.py_in_project_dir.py", "target": 0, "func": "def in_project_dir():\n    wd_orig = Path.cwd()\n    os.chdir(proj_dir)\n    try:\n        yield proj_dir\n    finally:\n        os.chdir(wd_orig)", "idx": 567}
{"project": "Scanpy", "commit_id": "612_scanpy_1.9.0_test_package_structure.py_test_function_headers.py", "target": 0, "func": "def test_function_headers(f):\n    name = f\"{f.__module__}.{f.__qualname__}\"\n    assert f.__doc__ is not None, f\"{name} has no docstring\"\n    lines = getattr(f, \"__orig_doc__\", f.__doc__).split(\"\\n\")\n    broken = [i for i, l in enumerate(lines) if l.strip() and not l.startswith(\"    \")]\n    if any(broken):\n        msg = f'''\\\nHeader of function `{name}`\u2019s docstring should start with one-line description\nand be consistently indented like this:\n\n\u2423\u2423\u2423\u2423\"\"\"\\\\\n\u2423\u2423\u2423\u2423My one-line\u2423description.\n\n\u2423\u2423\u2423\u2423\u2026\n\u2423\u2423\u2423\u2423\"\"\"\n\nThe displayed line is under-indented.\n'''\n        filename = inspect.getsourcefile(f)\n        _, lineno = inspect.getsourcelines(f)\n        text = f\">{lines[broken[0]]}<\"\n        raise SyntaxError(msg, (filename, lineno, 2, text))", "idx": 568}
{"project": "Scanpy", "commit_id": "613_scanpy_1.9.0_test_package_structure.py_test_metadata.py", "target": 0, "func": "def test_metadata(tmp_path, in_project_dir):\n    import flit_core.buildapi\n\n    flit_core.buildapi.prepare_metadata_for_build_wheel(tmp_path)\n\n    metadata_path = next(tmp_path.glob('*.dist-info')) / 'METADATA'\n    metadata = email.message_from_bytes(metadata_path.read_bytes())\n    assert not metadata.defects", "idx": 569}
{"project": "Scanpy", "commit_id": "614_scanpy_1.9.0_test_paga.py_pbmc.py", "target": 1, "func": "def pbmc():\n    pbmc = pbmc68k_reduced()\n    sc.tl.paga(pbmc, groups='bulk_labels')\n    pbmc.obs['cool_feature'] = pbmc[:, 'CST3'].X.squeeze()\n    return pbmc", "idx": 570}
{"project": "Scanpy", "commit_id": "615_scanpy_1.9.0_test_paga.py_test_paga_plots.py", "target": 0, "func": "def test_paga_plots(image_comparer, pbmc, test_id, func):\n    save_and_compare_images = image_comparer(ROOT, FIGS, tol=30)\n    common = dict(threshold=0.5, max_edge_width=1.0, random_state=0, show=False)\n\n    func(pbmc, **common)\n    save_and_compare_images(test_id)", "idx": 571}
{"project": "Scanpy", "commit_id": "616_scanpy_1.9.0_test_paga.py_test_paga_pie.py", "target": 0, "func": "def test_paga_pie(image_comparer, pbmc):\n    save_and_compare_images = image_comparer(ROOT, FIGS, tol=30)\n\n    colors = {\n        c: {cm.Set1(_): 0.33 for _ in range(3)}\n        for c in pbmc.obs[\"bulk_labels\"].cat.categories\n    }\n    colors[\"Dendritic\"] = {cm.Set2(_): 0.25 for _ in range(4)}\n\n    sc.pl.paga(pbmc, color=colors, colorbar=False)\n    save_and_compare_images('master_paga_pie')", "idx": 572}
{"project": "Scanpy", "commit_id": "617_scanpy_1.9.0_test_paga.py_test_paga_path.py", "target": 0, "func": "def test_paga_path(image_comparer, pbmc):\n    save_and_compare_images = image_comparer(ROOT, FIGS, tol=15)\n\n    pbmc.uns['iroot'] = 0\n    sc.tl.dpt(pbmc)\n    sc.pl.paga_path(\n        pbmc,\n        nodes=['Dendritic'],\n        keys=['HES4', 'SRM', 'CSTB'],\n        show=False,\n    )\n    save_and_compare_images('master_paga_path')", "idx": 573}
{"project": "Scanpy", "commit_id": "618_scanpy_1.9.0_test_paga.py_test_paga_compare.py", "target": 0, "func": "def test_paga_compare(image_comparer):\n    # Tests that https://github.com/theislab/scanpy/issues/1887 is fixed\n    save_and_compare_images = image_comparer(ROOT, FIGS, tol=15)\n\n    pbmc = pbmc3k_processed()\n    sc.tl.paga(pbmc, groups=\"louvain\")\n\n    sc.pl.paga_compare(pbmc, basis=\"umap\", show=False)\n\n    save_and_compare_images('master_paga_compare_pbmc3k')", "idx": 574}
{"project": "Scanpy", "commit_id": "619_scanpy_1.9.0_test_paga.py_test_paga_positions_reproducible.py", "target": 0, "func": "def test_paga_positions_reproducible():\n    \"\"\"Check exact reproducibility and effect of random_state on paga positions\"\"\"\n    # https://github.com/theislab/scanpy/issues/1859\n    pbmc = pbmc68k_reduced()\n    sc.tl.paga(pbmc, \"bulk_labels\")\n\n    a = pbmc.copy()\n    b = pbmc.copy()\n    c = pbmc.copy()\n\n    sc.pl.paga(a, show=False, random_state=42)\n    sc.pl.paga(b, show=False, random_state=42)\n    sc.pl.paga(c, show=False, random_state=13)\n\n    np.testing.assert_array_equal(a.uns[\"paga\"][\"pos\"], b.uns[\"paga\"][\"pos\"])\n    assert a.uns[\"paga\"][\"pos\"].tolist() != c.uns[\"paga\"][\"pos\"].tolist()", "idx": 575}
{"project": "Scanpy", "commit_id": "61_scanpy_1.9.0_readwrite.py_write.py", "target": 0, "func": "def write(\n    filename: Union[str, Path],\n    adata: AnnData,\n    ext: Optional[Literal['h5', 'csv', 'txt', 'npz']] = None,\n    compression: Optional[Literal['gzip', 'lzf']] = 'gzip',\n    compression_opts: Optional[int] = None,\n):\n    \"\"\"\\\n    Write :class:`~anndata.AnnData` objects to file.\n\n    Parameters\n    ----------\n    filename\n        If the filename has no file extension, it is interpreted as a key for\n        generating a filename via `sc.settings.writedir / (filename +\n        sc.settings.file_format_data)`. This is the same behavior as in\n        :func:`~scanpy.read`.\n    adata\n        Annotated data matrix.\n    ext\n        File extension from wich to infer file format. If `None`, defaults to\n        `sc.settings.file_format_data`.\n    compression\n        See http://docs.h5py.org/en/latest/high/dataset.html.\n    compression_opts\n        See http://docs.h5py.org/en/latest/high/dataset.html.\n    \"\"\"\n    filename = Path(filename)  # allow passing strings\n    if is_valid_filename(filename):\n        filename = filename\n        ext_ = is_valid_filename(filename, return_ext=True)\n        if ext is None:\n            ext = ext_\n        elif ext != ext_:\n            raise ValueError(\n                'It suffices to provide the file type by '\n                'providing a proper extension to the filename.'\n                'One of \"txt\", \"csv\", \"h5\" or \"npz\".'\n            )\n    else:\n        key = filename\n        ext = settings.file_format_data if ext is None else ext\n        filename = _get_filename_from_key(key, ext)\n    if ext == 'csv':\n        adata.write_csvs(filename)\n    else:\n        adata.write(\n            filename, compression=compression, compression_opts=compression_opts", "idx": 576}
{"project": "Scanpy", "commit_id": "620_scanpy_1.9.0_test_pca.py_test_pca_transform.py", "target": 0, "func": "def test_pca_transform(array_type):\n    A = array_type(A_list).astype('float32')\n    A_pca_abs = np.abs(A_pca)\n    A_svd_abs = np.abs(A_svd)\n\n    adata = AnnData(A)\n\n    sc.pp.pca(adata, n_comps=4, zero_center=True, svd_solver='arpack', dtype='float64')\n\n    assert np.linalg.norm(A_pca_abs[:, :4] - np.abs(adata.obsm['X_pca'])) < 2e-05\n\n    sc.pp.pca(\n        adata,\n        n_comps=5,\n        zero_center=True,\n        svd_solver='randomized',\n        dtype='float64',\n        random_state=14,\n    )\n    assert np.linalg.norm(A_pca_abs - np.abs(adata.obsm['X_pca'])) < 2e-05\n\n    sc.pp.pca(adata, n_comps=4, zero_center=False, dtype='float64', random_state=14)\n    assert np.linalg.norm(A_svd_abs[:, :4] - np.abs(adata.obsm['X_pca'])) < 2e-05", "idx": 577}
{"project": "Scanpy", "commit_id": "621_scanpy_1.9.0_test_pca.py_test_pca_shapes.py", "target": 0, "func": "def test_pca_shapes():\n    \"\"\"Tests that n_comps behaves correctly\"\"\"\n    # https://github.com/theislab/scanpy/issues/1051\n    adata = AnnData(np.random.randn(30, 20))\n    sc.pp.pca(adata)\n    assert adata.obsm[\"X_pca\"].shape == (30, 19)\n\n    adata = AnnData(np.random.randn(20, 30))\n    sc.pp.pca(adata)\n    assert adata.obsm[\"X_pca\"].shape == (20, 19)\n\n    with pytest.raises(ValueError):\n        sc.pp.pca(adata, n_comps=100)", "idx": 578}
{"project": "Scanpy", "commit_id": "622_scanpy_1.9.0_test_pca.py_test_pca_sparse.py", "target": 0, "func": "def test_pca_sparse(pbmc3k_normalized):\n    \"\"\"\n    Tests that implicitly centered pca on sparse arrays returns equivalent results to\n    explicit centering on dense arrays.\n    \"\"\"\n    pbmc = pbmc3k_normalized\n\n    pbmc_dense = pbmc.copy()\n    pbmc_dense.X = pbmc_dense.X.toarray()\n\n    implicit = sc.pp.pca(pbmc, dtype=np.float64, copy=True)\n    explicit = sc.pp.pca(pbmc_dense, dtype=np.float64, copy=True)\n\n    assert np.allclose(implicit.uns[\"pca\"][\"variance\"], explicit.uns[\"pca\"][\"variance\"])\n    assert np.allclose(\n        implicit.uns[\"pca\"][\"variance_ratio\"], explicit.uns[\"pca\"][\"variance_ratio\"]\n    )\n    assert np.allclose(implicit.obsm['X_pca'], explicit.obsm['X_pca'])\n    assert np.allclose(implicit.varm['PCs'], explicit.varm['PCs'])", "idx": 579}
{"project": "Scanpy", "commit_id": "623_scanpy_1.9.0_test_pca.py_test_pca_reproducible.py", "target": 0, "func": "def test_pca_reproducible(pbmc3k_normalized, array_type, float_dtype):\n    pbmc = pbmc3k_normalized\n    pbmc.X = array_type(pbmc.X)\n\n    a = sc.pp.pca(pbmc, copy=True, dtype=float_dtype, random_state=42)\n    b = sc.pp.pca(pbmc, copy=True, dtype=float_dtype, random_state=42)\n    c = sc.pp.pca(pbmc, copy=True, dtype=float_dtype, random_state=0)\n\n    assert_equal(a, b)\n    # Test that changing random seed changes result\n    assert not np.array_equal(a.obsm[\"X_pca\"], c.obsm[\"X_pca\"])", "idx": 580}
{"project": "Scanpy", "commit_id": "624_scanpy_1.9.0_test_pca.py_test_pca_chunked.py", "target": 0, "func": "def test_pca_chunked(pbmc3k_normalized):\n    # https://github.com/theislab/scanpy/issues/1590\n    # But also a more general test\n\n    # Subsetting for speed of test\n    pbmc = pbmc3k_normalized[::6].copy()\n    pbmc.X = pbmc.X.astype(np.float64)\n    chunked = sc.pp.pca(pbmc3k_normalized, chunked=True, copy=True)\n    default = sc.pp.pca(pbmc3k_normalized, copy=True)\n\n    # Taking absolute value since sometimes dimensions are flipped\n    np.testing.assert_allclose(\n        np.abs(chunked.obsm[\"X_pca\"]), np.abs(default.obsm[\"X_pca\"])\n    )\n    np.testing.assert_allclose(np.abs(chunked.varm[\"PCs\"]), np.abs(default.varm[\"PCs\"]))\n    np.testing.assert_allclose(\n        np.abs(chunked.uns[\"pca\"][\"variance\"]), np.abs(default.uns[\"pca\"][\"variance\"])\n    )\n    np.testing.assert_allclose(\n        np.abs(chunked.uns[\"pca\"][\"variance_ratio\"]),\n        np.abs(default.uns[\"pca\"][\"variance_ratio\"]),", "idx": 581}
{"project": "Scanpy", "commit_id": "625_scanpy_1.9.0_test_pca.py_test_pca_n_pcs.py", "target": 0, "func": "def test_pca_n_pcs(pbmc3k_normalized):\n    \"\"\"\n    Tests that the n_pcs parameter also works for\n    representations not called \"X_pca\"\n    \"\"\"\n    pbmc = pbmc3k_normalized\n    sc.pp.pca(pbmc, dtype=np.float64)\n    pbmc.obsm[\"X_pca_test\"] = pbmc.obsm[\"X_pca\"]\n    original = sc.pp.neighbors(pbmc, n_pcs=5, use_rep=\"X_pca\", copy=True)\n    renamed = sc.pp.neighbors(pbmc, n_pcs=5, use_rep=\"X_pca_test\", copy=True)\n\n    assert np.allclose(original.obsm[\"X_pca\"], renamed.obsm[\"X_pca_test\"])\n    assert np.allclose(\n        original.obsp[\"distances\"].toarray(), renamed.obsp[\"distances\"].toarray()", "idx": 582}
{"project": "Scanpy", "commit_id": "626_scanpy_1.9.0_test_performance.py_descend.py", "target": 0, "func": "def descend(profimp_data, modules, path):\n    module = profimp_data[\"module\"]\n    path = [*path, module]\n    if module in modules:\n        yield \" \u2192 \".join(e for e in path if e is not None)\n        modules.remove(module)\n    for child in profimp_data[\"children\"]:\n        yield from descend(child, modules, path)", "idx": 583}
{"project": "Scanpy", "commit_id": "627_scanpy_1.9.0_test_performance.py_get_import_paths.py", "target": 0, "func": "def get_import_paths(modules):\n    proc = run(\n        [sys.executable, \"-m\", \"profimp.main\", \"import scanpy\"],\n        capture_output=True,\n        check=True,\n    )\n    data = json.loads(proc.stdout)\n    return descend(data, set(modules), [])", "idx": 584}
{"project": "Scanpy", "commit_id": "628_scanpy_1.9.0_test_performance.py_test_deferred_imports.py", "target": 0, "func": "def test_deferred_imports(imported_modules):\n    slow_to_import = {\n        'umap',  # neighbors, tl.umap\n        'seaborn',  # plotting\n        'sklearn.metrics',  # neighbors\n        'networkx',  # diffmap, paga, plotting._utils\n        # TODO: 'matplotlib.pyplot',\n        # TODO (maybe): 'numba',\n    }\n    falsely_imported = slow_to_import & imported_modules\n\n    assert not falsely_imported, \"\\n\".join(get_import_paths(falsely_imported))", "idx": 585}
{"project": "Scanpy", "commit_id": "629_scanpy_1.9.0_test_plotting.py_test_heatmap.py", "target": 0, "func": "def test_heatmap(image_comparer):\n    save_and_compare_images = image_comparer(ROOT, FIGS, tol=15)\n\n    adata = krumsiek11()\n    sc.pl.heatmap(\n        adata, adata.var_names, 'cell_type', use_raw=False, show=False, dendrogram=True\n    )\n    save_and_compare_images('master_heatmap')\n\n    # test swap axes\n    sc.pl.heatmap(\n        adata,\n        adata.var_names,\n        'cell_type',\n        use_raw=False,\n        show=False,\n        dendrogram=True,\n        swap_axes=True,\n        figsize=(10, 3),\n        cmap='YlGnBu',\n    )\n    save_and_compare_images('master_heatmap_swap_axes')\n\n    # test heatmap numeric column():\n\n    # set as numeric column the vales for the first gene on the matrix\n    adata.obs['numeric_value'] = adata.X[:, 0]\n    sc.pl.heatmap(\n        adata,\n        adata.var_names,\n        'numeric_value',\n        use_raw=False,\n        num_categories=4,\n        figsize=(4.5, 5),\n        show=False,\n    )\n    save_and_compare_images('master_heatmap2')\n\n    # test var/obs standardization and layer\n    adata.layers['test'] = -1 * adata.X.copy()\n    sc.pl.heatmap(\n        adata,\n        adata.var_names,\n        'cell_type',\n        use_raw=False,\n        dendrogram=True,\n        show=False,\n        standard_scale='var',\n        layer='test',\n    )\n    save_and_compare_images('master_heatmap_std_scale_var')\n\n    # test standard_scale_obs\n    sc.pl.heatmap(\n        adata,\n        adata.var_names,\n        'cell_type',\n        use_raw=False,\n        dendrogram=True,\n        show=False,\n        standard_scale='obs',\n    )\n    save_and_compare_images('master_heatmap_std_scale_obs')\n\n    # test var_names as dict\n    pbmc = pbmc68k_reduced()\n    sc.tl.leiden(pbmc, key_added=\"clusters\", resolution=0.5)\n    # call umap to trigger colors for the clusters\n    sc.pl.umap(pbmc, color=\"clusters\")\n    marker_genes_dict = {\n        \"3\": [\"GNLY\", \"NKG7\"],\n        \"1\": [\"FCER1A\"],\n        \"2\": [\"CD3D\"],\n        \"0\": [\"FCGR3A\"],\n        \"4\": [\"CD79A\", \"MS4A1\"],\n    }\n    sc.pl.heatmap(\n        adata=pbmc,\n        var_names=marker_genes_dict,\n        groupby=\"clusters\",\n        vmin=-2,\n        vmax=2,\n        cmap=\"RdBu_r\",\n        dendrogram=True,\n        swap_axes=True,\n    )\n    save_and_compare_images('master_heatmap_var_as_dict')\n\n    # test that plot elements are well aligned\n    # small\n    a = AnnData(\n        np.array([[0, 0.3, 0.5], [1, 1.3, 1.5], [2, 2.3, 2.5]]),\n        obs={\"foo\": 'a b c'.split()},\n        var=pd.DataFrame({\"genes\": 'g1 g2 g3'.split()}).set_index('genes'),\n    )\n    a.obs['foo'] = a.obs['foo'].astype('category')\n    sc.pl.heatmap(\n        a, var_names=a.var_names, groupby='foo', swap_axes=True, figsize=(4, 4)\n    )\n    save_and_compare_images('master_heatmap_small_swap_alignment')\n\n    sc.pl.heatmap(\n        a, var_names=a.var_names, groupby='foo', swap_axes=False, figsize=(4, 4)\n    )\n    save_and_compare_images('master_heatmap_small_alignment')", "idx": 586}
{"project": "Scanpy", "commit_id": "62_scanpy_1.9.0_readwrite.py_read_params.py", "target": 0, "func": "def read_params(\n    filename: Union[Path, str], asheader: bool = False\n) -> Dict[str, Union[int, float, bool, str, None]]:\n    \"\"\"\\\n    Read parameter dictionary from text file.\n\n    Assumes that parameters are specified in the format::\n\n        par1 = value1\n        par2 = value2\n\n    Comments that start with '#' are allowed.\n\n    Parameters\n    ----------\n    filename\n        Filename of data file.\n    asheader\n        Read the dictionary from the header (comment section) of a file.\n\n    Returns\n    -------\n    Dictionary that stores parameters.\n    \"\"\"\n    filename = str(filename)  # allow passing pathlib.Path objects\n    from collections import OrderedDict\n\n    params = OrderedDict([])\n    for line in open(filename):\n        if '=' in line:\n            if not asheader or line.startswith('#'):\n                line = line[1:] if line.startswith('#') else line\n                key, val = line.split('=')\n                key = key.strip()\n                val = val.strip()\n                params[key] = convert_string(val)\n    return params", "idx": 587}
{"project": "Scanpy", "commit_id": "630_scanpy_1.9.0_test_plotting.py_test_clustermap.py", "target": 0, "func": "def test_clustermap(image_comparer, obs_keys, name):\n    save_and_compare_images = image_comparer(ROOT, FIGS, tol=15)\n    adata = krumsiek11()\n    sc.pl.clustermap(adata, obs_keys)\n    save_and_compare_images(name)", "idx": 588}
{"project": "Scanpy", "commit_id": "631_scanpy_1.9.0_test_plotting.py_test_dotplot_matrixplot_stacked_violin.py", "target": 0, "func": "def test_dotplot_matrixplot_stacked_violin(image_comparer, id, fn):\n    save_and_compare_images = image_comparer(ROOT, FIGS, tol=15)\n\n    adata = krumsiek11()\n    adata.obs['numeric_column'] = adata.X[:, 0]\n    adata.layers['test'] = -1 * adata.X.copy()\n    genes_dict = {\n        'group a': ['Gata2', 'Gata1'],\n        'group b': ['Fog1', 'EKLF', 'Fli1', 'SCL'],\n        'group c': ['Cebpa', 'Pu.1', 'cJun', 'EgrNab', 'Gfi1'],\n    }\n\n    if id.endswith(\"dict\"):\n        fn(adata, genes_dict, show=False)\n    else:\n        fn(adata, adata.var_names, show=False)\n    save_and_compare_images(f\"master_{id}\")", "idx": 589}
{"project": "Scanpy", "commit_id": "632_scanpy_1.9.0_test_plotting.py_test_dotplot_obj.py", "target": 0, "func": "def test_dotplot_obj(image_comparer):\n    save_and_compare_images = image_comparer(ROOT, FIGS, tol=15)\n    # test dotplot dot_min, dot_max, color_map, and var_groups\n    pbmc = pbmc68k_reduced()\n    genes = [\n        'CD79A',\n        'MS4A1',\n        'CD8A',\n        'CD8B',\n        'LYZ',\n        'LGALS3',\n        'S100A8',\n        'GNLY',\n        'NKG7',\n        'KLRB1',\n        'FCGR3A',\n        'FCER1A',\n        'CST3',\n    ]\n    # test layer, var standardization, smallest_dot,\n    # color title, size_title return_fig and dot_edge\n    pbmc.layers['test'] = pbmc.X * -1\n    plot = sc.pl.dotplot(\n        pbmc,\n        genes,\n        'bulk_labels',\n        layer='test',\n        dendrogram=True,\n        return_fig=True,\n        standard_scale='var',\n        smallest_dot=40,\n        colorbar_title='scaled column max',\n        size_title='Fraction of cells',\n    )\n    plot.style(dot_edge_color='black', dot_edge_lw=0.1, cmap='Reds').show()\n\n    save_and_compare_images('master_dotplot_std_scale_var')", "idx": 590}
{"project": "Scanpy", "commit_id": "633_scanpy_1.9.0_test_plotting.py_test_matrixplot_obj.py", "target": 0, "func": "def test_matrixplot_obj(image_comparer):\n    save_and_compare_images = image_comparer(ROOT, FIGS, tol=15)\n    adata = pbmc68k_reduced()\n    marker_genes_dict = {\n        \"3\": [\"GNLY\", \"NKG7\"],\n        \"1\": [\"FCER1A\"],\n        \"2\": [\"CD3D\"],\n        \"0\": [\"FCGR3A\"],\n        \"4\": [\"CD79A\", \"MS4A1\"],\n    }\n\n    plot = sc.pl.matrixplot(\n        adata,\n        marker_genes_dict,\n        'bulk_labels',\n        use_raw=False,\n        title='added totals',\n        return_fig=True,\n    )\n    plot.add_totals(sort='descending').style(edge_color='white', edge_lw=0.5).show()\n    save_and_compare_images('master_matrixplot_with_totals')\n\n    axes = plot.get_axes()\n    assert 'mainplot_ax' in axes, 'mainplot_ax not found in returned axes dict'", "idx": 591}
{"project": "Scanpy", "commit_id": "634_scanpy_1.9.0_test_plotting.py_test_stacked_violin_obj.py", "target": 0, "func": "def test_stacked_violin_obj(image_comparer, plt):\n    save_and_compare_images = image_comparer(ROOT, FIGS, tol=26)\n\n    pbmc = pbmc68k_reduced()\n    markers = {\n        'T-cell': ['CD3D', 'CD3E', 'IL32'],\n        'B-cell': ['CD79A', 'CD79B', 'MS4A1'],\n        'myeloid': ['CST3', 'LYZ'],\n    }\n    plot = sc.pl.stacked_violin(\n        pbmc,\n        markers,\n        'bulk_labels',\n        use_raw=False,\n        title=\"return_fig. add_totals\",\n        return_fig=True,\n    )\n    plot.add_totals().style(row_palette='tab20').show()\n    save_and_compare_images('master_stacked_violin_return_fig')", "idx": 592}
{"project": "Scanpy", "commit_id": "635_scanpy_1.9.0_test_plotting.py_test_tracksplot.py", "target": 0, "func": "def test_tracksplot(image_comparer):\n    save_and_compare_images = image_comparer(ROOT, FIGS, tol=15)\n\n    adata = krumsiek11()\n    sc.pl.tracksplot(\n        adata, adata.var_names, 'cell_type', dendrogram=True, use_raw=False\n    )\n    save_and_compare_images('master_tracksplot')", "idx": 593}
{"project": "Scanpy", "commit_id": "636_scanpy_1.9.0_test_plotting.py_test_multiple_plots.py", "target": 0, "func": "def test_multiple_plots(image_comparer):\n    # only testing stacked_violin, matrixplot and dotplot\n    save_and_compare_images = image_comparer(ROOT, FIGS, tol=15)\n\n    adata = pbmc68k_reduced()\n    markers = {\n        'T-cell': ['CD3D', 'CD3E', 'IL32'],\n        'B-cell': ['CD79A', 'CD79B', 'MS4A1'],\n        'myeloid': ['CST3', 'LYZ'],\n    }\n    fig, (ax1, ax2, ax3) = plt.subplots(\n        1, 3, figsize=(20, 5), gridspec_kw={'wspace': 0.7}\n    )\n    _ = sc.pl.stacked_violin(\n        adata,\n        markers,\n        groupby='bulk_labels',\n        ax=ax1,\n        title='stacked_violin',\n        dendrogram=True,\n        show=False,\n    )\n    _ = sc.pl.dotplot(\n        adata,\n        markers,\n        groupby='bulk_labels',\n        ax=ax2,\n        title='dotplot',\n        dendrogram=True,\n        show=False,\n    )\n    _ = sc.pl.matrixplot(\n        adata,\n        markers,\n        groupby='bulk_labels',\n        ax=ax3,\n        title='matrixplot',\n        dendrogram=True,\n        show=False,\n    )\n    save_and_compare_images('master_multiple_plots')", "idx": 594}
{"project": "Scanpy", "commit_id": "637_scanpy_1.9.0_test_plotting.py_test_violin.py", "target": 0, "func": "def test_violin(image_comparer):\n    save_and_compare_images = image_comparer(ROOT, FIGS, tol=40)\n\n    with plt.rc_context():\n        sc.pl.set_rcParams_defaults()\n        sc.set_figure_params(dpi=50, color_map='viridis')\n\n        pbmc = pbmc68k_reduced()\n        sc.pl.violin(\n            pbmc,\n            ['n_genes', 'percent_mito', 'n_counts'],\n            stripplot=True,\n            multi_panel=True,\n            jitter=True,\n            show=False,\n        )\n        save_and_compare_images('master_violin_multi_panel')\n\n        sc.pl.violin(\n            pbmc,\n            ['n_genes', 'percent_mito', 'n_counts'],\n            ylabel=[\"foo\", \"bar\", \"baz\"],\n            groupby='bulk_labels',\n            stripplot=True,\n            multi_panel=True,\n            jitter=True,\n            show=False,\n            rotation=90,\n        )\n        save_and_compare_images('master_violin_multi_panel_with_groupby')\n\n        # test use of layer\n        pbmc.layers['negative'] = pbmc.X * -1\n        sc.pl.violin(\n            pbmc,\n            'CST3',\n            groupby='bulk_labels',\n            stripplot=True,\n            multi_panel=True,\n            jitter=True,\n            show=False,\n            layer='negative',\n            use_raw=False,\n            rotation=90,\n        )\n        save_and_compare_images('master_violin_multi_panel_with_layer')", "idx": 595}
{"project": "Scanpy", "commit_id": "638_scanpy_1.9.0_test_plotting.py_test_violin_without_raw.py", "target": 0, "func": "def test_violin_without_raw(tmpdir):\n    # https://github.com/theislab/scanpy/issues/1546\n    TESTDIR = Path(tmpdir)\n\n    has_raw_pth = TESTDIR / \"has_raw.png\"\n    no_raw_pth = TESTDIR / \"no_raw.png\"\n\n    pbmc = pbmc68k_reduced()\n    pbmc_no_raw = pbmc.raw.to_adata().copy()\n\n    sc.pl.violin(pbmc, 'CST3', groupby=\"bulk_labels\", show=False, jitter=False)\n    plt.savefig(has_raw_pth)\n    plt.close()\n\n    sc.pl.violin(pbmc_no_raw, 'CST3', groupby=\"bulk_labels\", show=False, jitter=False)\n    plt.savefig(no_raw_pth)\n    plt.close()\n\n    assert compare_images(has_raw_pth, no_raw_pth, tol=5) is None", "idx": 596}
{"project": "Scanpy", "commit_id": "639_scanpy_1.9.0_test_plotting.py_test_dendrogram.py", "target": 0, "func": "def test_dendrogram(image_comparer):\n    save_and_compare_images = image_comparer(ROOT, FIGS, tol=10)\n\n    pbmc = pbmc68k_reduced()\n    sc.pl.dendrogram(pbmc, 'bulk_labels')\n    save_and_compare_images('dendrogram')", "idx": 597}
{"project": "Scanpy", "commit_id": "63_scanpy_1.9.0_readwrite.py_write_params.py", "target": 0, "func": "def write_params(path: Union[Path, str], *args, **maps):\n    \"\"\"\\\n    Write parameters to file, so that it's readable by read_params.\n\n    Uses INI file format.\n    \"\"\"\n    path = Path(path)\n    if not path.parent.is_dir():\n        path.parent.mkdir(parents=True)\n    if len(args) == 1:\n        maps[None] = args[0]\n    with path.open('w') as f:\n        for header, map in maps.items():\n            if header is not None:\n                f.write(f'[{header}]\\n')\n            for key, val in map.items():\n                f.write(f'{key} = {val}\\n')", "idx": 598}
{"project": "Scanpy", "commit_id": "640_scanpy_1.9.0_test_plotting.py_test_correlation.py", "target": 0, "func": "def test_correlation(image_comparer):\n    save_and_compare_images = image_comparer(ROOT, FIGS, tol=15)\n\n    pbmc = pbmc68k_reduced()\n    sc.pl.correlation_matrix(pbmc, 'bulk_labels')\n    save_and_compare_images('correlation')", "idx": 599}
{"project": "Scanpy", "commit_id": "641_scanpy_1.9.0_test_plotting.py_test_rank_genes_groups.py", "target": 0, "func": "def test_rank_genes_groups(image_comparer, name, fn):\n    save_and_compare_images = image_comparer(ROOT, FIGS, tol=15)\n\n    pbmc = pbmc68k_reduced()\n    sc.tl.rank_genes_groups(pbmc, 'louvain', n_genes=pbmc.raw.shape[1])\n\n    # add gene symbol\n    pbmc.var['symbol'] = pbmc.var.index + \"__\"\n\n    with plt.rc_context({\"axes.grid\": True, \"figure.figsize\": (4, 4)}):\n        fn(pbmc)\n        save_and_compare_images(f\"master_{name}\")\n        plt.close()", "idx": 600}
{"project": "Scanpy", "commit_id": "642_scanpy_1.9.0_test_plotting.py_gene_symbols_adatas.py", "target": 0, "func": "def gene_symbols_adatas():\n    \"\"\"Create two anndata objects which are equivalent except for var_names\n\n    Both have ensembl ids and hgnc symbols as columns in var. The first has ensembl\n    ids as var_names, the second has symbols.\n    \"\"\"\n    pbmc = pbmc3k_processed().raw.to_adata()\n    pbmc_counts = pbmc3k()\n\n    pbmc.layers[\"counts\"] = pbmc_counts[pbmc.obs_names, pbmc.var_names].X.copy()\n    pbmc.var[\"gene_symbol\"] = pbmc.var_names\n    pbmc.var[\"ensembl_id\"] = pbmc_counts.var[\"gene_ids\"].loc[pbmc.var_names]\n\n    pbmc.var = pbmc.var.set_index(\"ensembl_id\", drop=False)\n\n    # Cutting down on size for plotting, tracksplot and stacked_violin are slow\n    pbmc = pbmc[pbmc.obs[\"louvain\"].isin(pbmc.obs[\"louvain\"].cat.categories[:4])]\n    pbmc = pbmc[::3].copy()\n\n    # Creating variations\n    a = pbmc.copy()\n    b = pbmc.copy()\n    a.var = a.var.set_index(\"ensembl_id\")\n    b.var = b.var.set_index(\"gene_symbol\")\n\n    # Computing DE\n    sc.tl.rank_genes_groups(a, groupby=\"louvain\")\n    sc.tl.rank_genes_groups(b, groupby=\"louvain\")\n\n    return a, b", "idx": 601}
{"project": "Scanpy", "commit_id": "643_scanpy_1.9.0_test_plotting.py_test_plot_rank_genes_groups_gene_symbols.py", "target": 0, "func": "def test_plot_rank_genes_groups_gene_symbols(\n    gene_symbols_adatas, func, check_same_image\n):\n    a, b = gene_symbols_adatas\n\n    pth_1_a = FIGS / f\"{func.__name__}_equivalent_gene_symbols_1_a.png\"\n    pth_1_b = FIGS / f\"{func.__name__}_equivalent_gene_symbols_1_b.png\"\n\n    func(a, gene_symbols=\"gene_symbol\")\n    plt.savefig(pth_1_a)\n    plt.close()\n\n    func(b)\n    plt.savefig(pth_1_b)\n    pass\n\n    check_same_image(pth_1_a, pth_1_b, tol=1)\n\n    pth_2_a = FIGS / f\"{func.__name__}_equivalent_gene_symbols_2_a.png\"\n    pth_2_b = FIGS / f\"{func.__name__}_equivalent_gene_symbols_2_b.png\"\n\n    func(a)\n    plt.savefig(pth_2_a)\n    plt.close()\n\n    func(b, gene_symbols=\"ensembl_id\")\n    plt.savefig(pth_2_b)\n    plt.close()\n\n    check_same_image(pth_2_a, pth_2_b, tol=1)", "idx": 602}
{"project": "Scanpy", "commit_id": "644_scanpy_1.9.0_test_plotting.py_test_rank_genes_groups_plots_n_genes_vs_var_names.py", "target": 0, "func": "def test_rank_genes_groups_plots_n_genes_vs_var_names(tmpdir, func, check_same_image):\n    \"\"\"\\\n    Checks that passing a negative value for n_genes works, and that passing\n    var_names as a dict works.\n    \"\"\"\n    N = 3\n    pbmc = pbmc68k_reduced().raw.to_adata()\n    groups = pbmc.obs[\"louvain\"].cat.categories[:3]\n    pbmc = pbmc[pbmc.obs[\"louvain\"].isin(groups)][::3].copy()\n\n    sc.tl.rank_genes_groups(pbmc, groupby=\"louvain\")\n\n    top_genes = {}\n    bottom_genes = {}\n    for g, subdf in sc.get.rank_genes_groups_df(pbmc, group=groups).groupby(\"group\"):\n        top_genes[g] = list(subdf[\"names\"].head(N))\n        bottom_genes[g] = list(subdf[\"names\"].tail(N))\n\n    positive_n_pth = tmpdir / f\"{func.__name__}_positive_n.png\"\n    top_genes_pth = tmpdir / f\"{func.__name__}_top_genes.png\"\n    negative_n_pth = tmpdir / f\"{func.__name__}_negative_n.png\"\n    bottom_genes_pth = tmpdir / f\"{func.__name__}_bottom_genes.png\"\n\n    def wrapped(pth, **kwargs):\n        func(pbmc, groupby=\"louvain\", dendrogram=False, **kwargs)\n        plt.savefig(pth)\n        plt.close()\n\n    wrapped(positive_n_pth, n_genes=N)\n    wrapped(top_genes_pth, var_names=top_genes)\n\n    check_same_image(positive_n_pth, top_genes_pth, tol=1)\n\n    wrapped(negative_n_pth, n_genes=-N)\n    wrapped(bottom_genes_pth, var_names=bottom_genes)\n\n    check_same_image(negative_n_pth, bottom_genes_pth, tol=1)\n\n    # Shouldn't be able to pass these together\n    with pytest.raises(\n        ValueError, match=\"n_genes and var_names are mutually exclusive\"\n    ):\n        wrapped(tmpdir / \"not_written.png\", n_genes=N, var_names=top_genes)", "idx": 603}
{"project": "Scanpy", "commit_id": "645_scanpy_1.9.0_test_plotting.py_test_genes_symbols.py", "target": 0, "func": "def test_genes_symbols(image_comparer, id, fn):\n    save_and_compare_images = image_comparer(ROOT, FIGS, tol=15)\n\n    adata = krumsiek11()\n\n    # add a 'symbols' column\n    adata.var['symbols'] = adata.var.index.map(lambda x: \"symbol_{}\".format(x))\n    symbols = [\"symbol_{}\".format(x) for x in adata.var_names]\n\n    fn(adata, symbols, 'cell_type', dendrogram=True, gene_symbols='symbols', show=False)\n    save_and_compare_images(f\"master_{id}_gene_symbols\")", "idx": 604}
{"project": "Scanpy", "commit_id": "646_scanpy_1.9.0_test_plotting.py__pbmc_scatterplots.py", "target": 0, "func": "def _pbmc_scatterplots():\n    # Wrapped in another fixture to avoid mutation\n    pbmc = pbmc68k_reduced()\n    pbmc.layers[\"sparse\"] = pbmc.raw.X / 2\n    pbmc.layers[\"test\"] = pbmc.X.copy() + 100\n    pbmc.var[\"numbers\"] = [str(x) for x in range(pbmc.shape[1])]\n    sc.pp.neighbors(pbmc)\n    sc.tl.tsne(pbmc, random_state=0, n_pcs=30)\n    sc.tl.diffmap(pbmc)\n    return pbmc", "idx": 605}
{"project": "Scanpy", "commit_id": "647_scanpy_1.9.0_test_plotting.py_pbmc_scatterplots.py", "target": 0, "func": "def pbmc_scatterplots(_pbmc_scatterplots):\n    return _pbmc_scatterplots.copy()", "idx": 606}
{"project": "Scanpy", "commit_id": "648_scanpy_1.9.0_test_plotting.py_test_scatterplots.py", "target": 0, "func": "def test_scatterplots(image_comparer, pbmc_scatterplots, id, fn):\n    save_and_compare_images = image_comparer(ROOT, FIGS, tol=15)\n\n    # https://github.com/theislab/scanpy/issues/849\n    if id == \"3dprojection\" and version.parse(mpl.__version__) < version.parse(\"3.3.3\"):\n        with pytest.raises(ValueError, match=r\"known error with matplotlib 3d\"):\n            fn(pbmc_scatterplots, show=False)\n    else:\n        fn(pbmc_scatterplots, show=False)\n        save_and_compare_images(f\"master_{id}\")", "idx": 607}
{"project": "Scanpy", "commit_id": "649_scanpy_1.9.0_test_plotting.py_test_scatter_embedding_groups_and_size.py", "target": 0, "func": "def test_scatter_embedding_groups_and_size(image_comparer):\n    # test that the 'groups' parameter sorts\n    # cells, such that the cells belonging to the groups are\n    # plotted on top. This new ordering requires that the size\n    # vector is also ordered (if given).\n    save_and_compare_images = image_comparer(ROOT, FIGS, tol=15)\n    pbmc = pbmc68k_reduced()\n    sc.pl.embedding(\n        pbmc,\n        'umap',\n        color=['bulk_labels'],\n        groups=['CD14+ Monocyte', 'Dendritic'],\n        size=(np.arange(pbmc.shape[0]) / 40) ** 1.7,\n    )\n    save_and_compare_images('master_embedding_groups_size')", "idx": 608}
{"project": "Scanpy", "commit_id": "64_scanpy_1.9.0_readwrite.py__read.py", "target": 1, "func": "def _read(\n    filename: Path,\n    backed=None,\n    sheet=None,\n    ext=None,\n    delimiter=None,\n    first_column_names=None,\n    backup_url=None,\n    cache=False,\n    cache_compression=None,\n    suppress_cache_warning=False,\n    **kwargs,\n):\n    if ext is not None and ext not in avail_exts:\n        raise ValueError(\n            'Please provide one of the available extensions.\\n' f'{avail_exts}'\n        )\n    else:\n        ext = is_valid_filename(filename, return_ext=True)\n    is_present = _check_datafile_present_and_download(filename, backup_url=backup_url)\n    if not is_present:\n        logg.debug(f'... did not find original file {filename}')\n    # read hdf5 files\n    if ext in {'h5', 'h5ad'}:\n        if sheet is None:\n            return read_h5ad(filename, backed=backed)\n        else:\n            logg.debug(f'reading sheet {sheet} from file {filename}')\n            return read_hdf(filename, sheet)\n    # read other file types\n    path_cache = settings.cachedir / _slugify(filename).replace(\n        '.' + ext, '.h5ad'\n    )  # type: Path\n    if path_cache.suffix in {'.gz', '.bz2'}:\n        path_cache = path_cache.with_suffix('')\n    if cache and path_cache.is_file():\n        logg.info(f'... reading from cache file {path_cache}')\n        return read_h5ad(path_cache)\n\n    if not is_present:\n        raise FileNotFoundError(f'Did not find file {filename}.')\n    logg.debug(f'reading {filename}')\n    if not cache and not suppress_cache_warning:\n        logg.hint(\n            'This might be very slow. Consider passing `cache=True`, '\n            'which enables much faster reading from a cache file.'\n        )\n    # do the actual reading\n    if ext == 'xlsx' or ext == 'xls':\n        if sheet is None:\n            raise ValueError(\"Provide `sheet` parameter when reading '.xlsx' files.\")\n        else:\n            adata = read_excel(filename, sheet)\n    elif ext in {'mtx', 'mtx.gz'}:\n        adata = read_mtx(filename)\n    elif ext == 'csv':\n        adata = read_csv(filename, first_column_names=first_column_names)\n    elif ext in {'txt', 'tab', 'data', 'tsv'}:\n        if ext == 'data':\n            logg.hint(\n                \"... assuming '.data' means tab or white-space \" 'separated text file',\n            )\n            logg.hint('change this by passing `ext` to sc.read')\n        adata = read_text(filename, delimiter, first_column_names)\n    elif ext == 'soft.gz':\n        adata = _read_softgz(filename)\n    elif ext == 'loom':\n        adata = read_loom(filename=filename, **kwargs)\n    else:\n        raise ValueError(f'Unknown extension {ext}.')\n    if cache:\n        logg.info(\n            f'... writing an {settings.file_format_data} '\n            'cache file to speedup reading next time'\n        )\n        if cache_compression is _empty:\n            cache_compression = settings.cache_compression\n        if not path_cache.parent.is_dir():\n            path_cache.parent.mkdir(parents=True)\n        # write for faster reading when calling the next time\n        adata.write(path_cache, compression=cache_compression)\n    return adata", "idx": 609}
{"project": "Scanpy", "commit_id": "650_scanpy_1.9.0_test_plotting.py_test_scatter_embedding_add_outline_vmin_vmax_norm.py", "target": 0, "func": "def test_scatter_embedding_add_outline_vmin_vmax_norm(image_comparer, check_same_image):\n    save_and_compare_images = image_comparer(ROOT, FIGS, tol=15)\n    pbmc = pbmc68k_reduced()\n\n    sc.pl.embedding(\n        pbmc,\n        'X_umap',\n        color=['percent_mito', 'n_counts', 'bulk_labels', 'percent_mito'],\n        s=200,\n        frameon=False,\n        add_outline=True,\n        vmax=['p99.0', partial(np.percentile, q=90), None, 0.03],\n        vmin=0.01,\n        vcenter=[0.015, None, None, 0.025],\n        outline_color=('#555555', '0.9'),\n        outline_width=(0.5, 0.5),\n        cmap='viridis_r',\n        alpha=0.9,\n        wspace=0.5,\n    )\n    save_and_compare_images('master_embedding_outline_vmin_vmax')\n\n    import matplotlib as mpl\n    import matplotlib.pyplot as plt\n\n    norm = mpl.colors.LogNorm()\n    with pytest.raises(\n        ValueError, match=\"Passing both norm and vmin/vmax/vcenter is not allowed.\"\n    ):\n        sc.pl.embedding(\n            pbmc,\n            'X_umap',\n            color=['percent_mito', 'n_counts'],\n            norm=norm,\n            vmin=0,\n            vmax=1,\n            vcenter=0.5,\n            cmap='RdBu_r',\n        )\n\n    try:\n        from matplotlib.colors import TwoSlopeNorm as DivNorm\n    except ImportError:\n        # matplotlib<3.2\n        from matplotlib.colors import DivergingNorm as DivNorm\n\n    from matplotlib.colors import Normalize\n\n    norm = Normalize(0, 10000)\n    divnorm = DivNorm(200, 150, 6000)\n\n    # allowed\n    sc.pl.umap(\n        pbmc,\n        color=['n_counts', 'bulk_labels', 'percent_mito'],\n        frameon=False,\n        vmax=['p99.0', None, None],\n        vcenter=[0.015, None, None],\n        norm=[None, norm, norm],\n        wspace=0.5,\n    )\n\n    sc.pl.umap(\n        pbmc,\n        color=['n_counts', 'bulk_labels'],\n        frameon=False,\n        norm=norm,\n        wspace=0.5,\n    )\n    plt.savefig(FIGS / 'umap_norm_fig0.png')\n    plt.close()\n\n    sc.pl.umap(\n        pbmc,\n        color=['n_counts', 'bulk_labels'],\n        frameon=False,\n        norm=divnorm,\n        wspace=0.5,\n    )\n    plt.savefig(FIGS / 'umap_norm_fig1.png')\n    plt.close()\n\n    sc.pl.umap(\n        pbmc,\n        color=['n_counts', 'bulk_labels'],\n        frameon=False,\n        vcenter=200,\n        vmin=150,\n        vmax=6000,\n        wspace=0.5,\n    )\n    plt.savefig(FIGS / 'umap_norm_fig2.png')\n    plt.close()\n\n    check_same_image(FIGS / 'umap_norm_fig1.png', FIGS / 'umap_norm_fig2.png', tol=1)\n\n    with pytest.raises(AssertionError):\n        check_same_image(\n            FIGS / 'umap_norm_fig1.png', FIGS / 'umap_norm_fig0.png', tol=1", "idx": 610}
{"project": "Scanpy", "commit_id": "651_scanpy_1.9.0_test_plotting.py_test_timeseries.py", "target": 0, "func": "def test_timeseries():\n    adata = pbmc68k_reduced()\n    sc.pp.neighbors(adata, n_neighbors=5, method='gauss', knn=False)\n    sc.tl.diffmap(adata)\n    sc.tl.dpt(adata, n_branchings=1, n_dcs=10)\n    sc.pl.dpt_timeseries(adata, as_heatmap=True)", "idx": 611}
{"project": "Scanpy", "commit_id": "652_scanpy_1.9.0_test_plotting.py_test_scatter_raw.py", "target": 0, "func": "def test_scatter_raw(tmp_path):\n    pbmc = pbmc68k_reduced()[:100].copy()\n    raw_pth = tmp_path / \"raw.png\"\n    x_pth = tmp_path / \"X.png\"\n\n    sc.pl.scatter(pbmc, color=\"HES4\", basis=\"umap\", use_raw=True)\n    plt.savefig(raw_pth, dpi=60)\n    plt.close()\n\n    sc.pl.scatter(pbmc, color=\"HES4\", basis=\"umap\", use_raw=False)\n    plt.savefig(x_pth, dpi=60)\n    plt.close()\n\n    comp = compare_images(str(raw_pth), str(x_pth), tol=5)\n    assert \"Error\" in comp, \"Plots should change depending on use_raw.\"", "idx": 612}
{"project": "Scanpy", "commit_id": "653_scanpy_1.9.0_test_plotting.py_test_scatter_specify_layer_and_raw.py", "target": 0, "func": "def test_scatter_specify_layer_and_raw():\n    pbmc = pbmc68k_reduced()\n    pbmc.layers[\"layer\"] = pbmc.raw.X.copy()\n    with pytest.raises(ValueError):\n        sc.pl.umap(pbmc, color=\"HES4\", use_raw=True, layer=\"layer\")", "idx": 613}
{"project": "Scanpy", "commit_id": "654_scanpy_1.9.0_test_plotting.py_test_scatter_no_basis_per_obs.py", "target": 0, "func": "def test_scatter_no_basis_per_obs(image_comparer):\n    \"\"\"Test scatterplot of per-obs points with no basis\"\"\"\n    save_and_compare_images = image_comparer(ROOT, FIGS, tol=15)\n    pbmc = pbmc68k_reduced()\n    sc.pl.scatter(pbmc, x=\"HES4\", y=\"percent_mito\", color=\"n_genes\", use_raw=False)\n    save_and_compare_images(\"scatter_HES_percent_mito_n_genes\")", "idx": 614}
{"project": "Scanpy", "commit_id": "655_scanpy_1.9.0_test_plotting.py_test_scatter_no_basis_per_var.py", "target": 0, "func": "def test_scatter_no_basis_per_var(image_comparer):\n    \"\"\"Test scatterplot of per-var points with no basis\"\"\"\n    save_and_compare_images = image_comparer(ROOT, FIGS, tol=15)\n    pbmc = pbmc68k_reduced()\n    sc.pl.scatter(pbmc, x=\"AAAGCCTGGCTAAC-1\", y=\"AAATTCGATGCACA-1\", use_raw=False)\n    save_and_compare_images(\"scatter_AAAGCCTGGCTAAC-1_vs_AAATTCGATGCACA-1\")", "idx": 615}
{"project": "Scanpy", "commit_id": "656_scanpy_1.9.0_test_plotting.py_pbmc_filtered.py", "target": 0, "func": "def pbmc_filtered():\n    pbmc = pbmc68k_reduced()\n    sc.pp.filter_genes(pbmc, min_cells=10)\n    return pbmc", "idx": 616}
{"project": "Scanpy", "commit_id": "657_scanpy_1.9.0_test_plotting.py_test_scatter_no_basis_raw.py", "target": 0, "func": "def test_scatter_no_basis_raw(check_same_image, pbmc_filtered, tmpdir):\n    \"\"\"Test scatterplots of raw layer with no basis.\"\"\"\n    path1 = tmpdir / \"scatter_EGFL7_F12_FAM185A_rawNone.png\"\n    path2 = tmpdir / \"scatter_EGFL7_F12_FAM185A_rawTrue.png\"\n    path3 = tmpdir / \"scatter_EGFL7_F12_FAM185A_rawToAdata.png\"\n\n    sc.pl.scatter(pbmc_filtered, x='EGFL7', y='F12', color='FAM185A', use_raw=None)\n    plt.savefig(path1)\n    plt.close()\n\n    # is equivalent to:\n    sc.pl.scatter(pbmc_filtered, x='EGFL7', y='F12', color='FAM185A', use_raw=True)\n    plt.savefig(path2)\n    plt.close()\n\n    # and also to:\n    sc.pl.scatter(pbmc_filtered.raw.to_adata(), x='EGFL7', y='F12', color='FAM185A')\n    plt.savefig(path3)\n\n    check_same_image(path1, path2, tol=15)\n    check_same_image(path1, path3, tol=15)", "idx": 617}
{"project": "Scanpy", "commit_id": "658_scanpy_1.9.0_test_plotting.py_test_scatter_no_basis_value_error.py", "target": 0, "func": "def test_scatter_no_basis_value_error(pbmc_filtered, x, y, color, use_raw):\n    \"\"\"Test that `scatter()` raises `ValueError` where appropriate\n\n    If `sc.pl.scatter()` receives variable labels that either cannot be\n    found or are incompatible with one another, the function should\n    raise a `ValueError`. This test checks that this happens as\n    expected.\n    \"\"\"\n    with pytest.raises(ValueError):\n        sc.pl.scatter(pbmc_filtered, x=x, y=y, color=color, use_raw=use_raw)", "idx": 618}
{"project": "Scanpy", "commit_id": "659_scanpy_1.9.0_test_plotting.py_test_rankings.py", "target": 0, "func": "def test_rankings(image_comparer):\n    save_and_compare_images = image_comparer(ROOT, FIGS, tol=15)\n\n    pbmc = pbmc68k_reduced()\n    sc.pp.pca(pbmc)\n    sc.pl.pca_loadings(pbmc)\n    save_and_compare_images('master_pca_loadings')\n\n    sc.pl.pca_loadings(pbmc, components='1,2,3')\n    save_and_compare_images('master_pca_loadings')\n\n    sc.pl.pca_loadings(pbmc, components=[1, 2, 3])\n    save_and_compare_images('master_pca_loadings')\n\n    sc.pl.pca_loadings(pbmc, include_lowest=False)\n    save_and_compare_images('master_pca_loadings_without_lowest')\n\n    sc.pl.pca_loadings(pbmc, n_points=10)\n    save_and_compare_images('master_pca_loadings_10_points')", "idx": 619}
{"project": "Scanpy", "commit_id": "65_scanpy_1.9.0_readwrite.py__slugify.py", "target": 0, "func": "def _slugify(path: Union[str, PurePath]) -> str:\n    \"\"\"Make a path into a filename.\"\"\"\n    if not isinstance(path, PurePath):\n        path = PurePath(path)\n    parts = list(path.parts)\n    if parts[0] == '/':\n        parts.pop(0)\n    elif len(parts[0]) == 3 and parts[0][1:] == ':\\\\':\n        parts[0] = parts[0][0]  # C:\\ \u2192 C\n    filename = '-'.join(parts)\n    assert '/' not in filename, filename\n    assert not filename[1:].startswith(':'), filename\n    return filename", "idx": 620}
{"project": "Scanpy", "commit_id": "660_scanpy_1.9.0_test_plotting.py_test_scatter_rep.py", "target": 0, "func": "def test_scatter_rep(tmpdir):\n    \"\"\"\n    Test to make sure I can predict when scatter reps should be the same\n    \"\"\"\n    TESTDIR = Path(tmpdir)\n    rep_args = {\n        \"raw\": {\"use_raw\": True},\n        \"layer\": {\"layer\": \"layer\", \"use_raw\": False},\n        \"X\": {\"use_raw\": False},\n    }\n    states = pd.DataFrame.from_records(\n        zip(\n            list(chain.from_iterable(repeat(x, 3) for x in [\"X\", \"raw\", \"layer\"])),\n            list(chain.from_iterable(repeat(\"abc\", 3))),\n            [1, 2, 3, 3, 1, 2, 2, 3, 1],\n        ),\n        columns=[\"rep\", \"gene\", \"result\"],\n    )\n    states[\"outpth\"] = [\n        TESTDIR / f\"{state.gene}_{state.rep}_{state.result}.png\"\n        for state in states.itertuples()\n    ]\n    pattern = np.array(list(chain.from_iterable(repeat(i, 5) for i in range(3))))\n    coords = np.c_[np.arange(15) % 5, pattern]\n\n    adata = AnnData(\n        X=np.zeros((15, 3)),\n        layers={\"layer\": np.zeros((15, 3))},\n        obsm={\"X_pca\": coords},\n        var=pd.DataFrame(index=[x for x in list(\"abc\")]),\n        obs=pd.DataFrame(index=[f\"cell{i}\" for i in range(15)]),\n    )\n    adata.raw = adata.copy()\n    adata.X[np.arange(15), pattern] = 1\n    adata.raw.X[np.arange(15), (pattern + 1) % 3] = 1\n    adata.layers[\"layer\"][np.arange(15), (pattern + 2) % 3] = 1\n\n    for state in states.itertuples():\n        sc.pl.pca(adata, color=state.gene, **rep_args[state.rep], show=False)\n        plt.savefig(state.outpth, dpi=60)\n        plt.close()\n\n    for s1, s2 in combinations(states.itertuples(), 2):\n        comp = compare_images(str(s1.outpth), str(s2.outpth), tol=5)\n        if s1.result == s2.result:\n            assert comp is None, comp\n        else:\n            assert \"Error\" in comp, f\"{s1.outpth}, {s2.outpth} aren't supposed to match\"", "idx": 621}
{"project": "Scanpy", "commit_id": "661_scanpy_1.9.0_test_plotting.py_test_no_copy.py", "target": 0, "func": "def test_no_copy():\n    # https://github.com/theislab/scanpy/issues/1000\n    # Tests that plotting functions don't make a copy from a view unless they\n    # actually have to\n    actual = pbmc68k_reduced()\n    sc.pl.umap(actual, color=[\"bulk_labels\", \"louvain\"], show=False)  # Set colors\n\n    view = actual[np.random.choice(actual.obs_names, size=actual.shape[0] // 5), :]\n\n    sc.pl.umap(view, color=[\"bulk_labels\", \"louvain\"], show=False)\n    assert view.is_view\n\n    rank_genes_groups_plotting_funcs = [\n        sc.pl.rank_genes_groups,\n        sc.pl.rank_genes_groups_dotplot,\n        sc.pl.rank_genes_groups_heatmap,\n        sc.pl.rank_genes_groups_matrixplot,\n        sc.pl.rank_genes_groups_stacked_violin,\n        # TODO: raises ValueError about empty distance matrix \u2013 investigate\n        # sc.pl.rank_genes_groups_tracksplot,\n        sc.pl.rank_genes_groups_violin,\n    ]\n\n    # the pbmc68k was generated using rank_genes_groups with method='logreg'\n    # which does not generate 'logfoldchanges', although this field is\n    # required by `sc.get.rank_genes_groups_df`.\n    # After updating rank_genes_groups plots to use the latter function\n    # an error appears. Re-running rank_genes_groups with default method\n    # solves the problem.\n    sc.tl.rank_genes_groups(actual, 'bulk_labels')\n\n    # Only plotting one group at a time to avoid generating dendrogram\n    # TODO: Generating a dendrogram modifies the object, this should be\n    # optional and also maybe not modify the object.\n    for plotfunc in rank_genes_groups_plotting_funcs:\n        view = actual[actual.obs[\"bulk_labels\"] == \"Dendritic\"]\n        plotfunc(view, [\"Dendritic\"], show=False)\n        assert view.is_view", "idx": 622}
{"project": "Scanpy", "commit_id": "662_scanpy_1.9.0_test_plotting.py_test_groupby_index.py", "target": 0, "func": "def test_groupby_index(image_comparer):\n    save_and_compare_images = image_comparer(ROOT, FIGS, tol=15)\n    pbmc = pbmc68k_reduced()\n\n    genes = [\n        'CD79A',\n        'MS4A1',\n        'CD8A',\n        'CD8B',\n        'LYZ',\n        'LGALS3',\n        'S100A8',\n        'GNLY',\n        'NKG7',\n        'KLRB1',\n        'FCGR3A',\n        'FCER1A',\n        'CST3',\n    ]\n    pbmc_subset = pbmc[:10].copy()\n    sc.pl.dotplot(pbmc_subset, genes, groupby='index')\n    save_and_compare_images('master_dotplot_groupby_index')", "idx": 623}
{"project": "Scanpy", "commit_id": "663_scanpy_1.9.0_test_plotting.py_test_groupby_list.py", "target": 0, "func": "def test_groupby_list(image_comparer):\n    save_and_compare_images = image_comparer(ROOT, FIGS, tol=30)\n    adata = krumsiek11()\n\n    np.random.seed(1)\n\n    cat_val = adata.obs.cell_type.tolist()\n    np.random.shuffle(cat_val)\n    cats = adata.obs.cell_type.cat.categories.tolist()\n    np.random.shuffle(cats)\n    adata.obs['rand_cat'] = pd.Categorical(cat_val, categories=cats)\n\n    with mpl.rc_context({\"figure.subplot.bottom\": 0.5}):\n        sc.pl.dotplot(\n            adata, ['Gata1', 'Gata2'], groupby=['rand_cat', 'cell_type'], swap_axes=True\n        )\n        save_and_compare_images('master_dotplot_groupby_list_catorder')", "idx": 624}
{"project": "Scanpy", "commit_id": "664_scanpy_1.9.0_test_plotting.py_test_color_cycler.py", "target": 0, "func": "def test_color_cycler(caplog):\n    # https://github.com/theislab/scanpy/issues/1885\n    import logging\n\n    pbmc = pbmc68k_reduced()\n    colors = sns.color_palette(\"deep\")\n    cyl = sns.rcmod.cycler('color', sns.color_palette(\"deep\"))\n\n    with caplog.at_level(logging.WARNING):\n        with plt.rc_context({'axes.prop_cycle': cyl, \"patch.facecolor\": colors[0]}):\n            sc.pl.umap(pbmc, color=\"phase\")\n            plt.show()\n            plt.close()\n\n    assert caplog.text == \"\"", "idx": 625}
{"project": "Scanpy", "commit_id": "665_scanpy_1.9.0_test_plotting.py_test_repeated_colors_w_missing_value.py", "target": 0, "func": "def test_repeated_colors_w_missing_value():\n    # https://github.com/theislab/scanpy/issues/2133\n    v = pd.Series(np.arange(10).astype(str))\n    v[0] = np.nan\n    v = v.astype(\"category\")\n\n    ad = sc.AnnData(obs=pd.DataFrame(v, columns=[\"value\"]))\n    ad.obsm[\"X_umap\"] = np.random.normal(size=(ad.n_obs, 2))\n\n    sc.pl.umap(ad, color=\"value\")\n\n    ad.uns['value_colors'][1] = ad.uns['value_colors'][0]\n\n    sc.pl.umap(ad, color=\"value\")", "idx": 626}
{"project": "Scanpy", "commit_id": "666_scanpy_1.9.0_test_plotting.py_test_filter_rank_genes_groups_plots.py", "target": 0, "func": "def test_filter_rank_genes_groups_plots(tmpdir, plot, check_same_image):\n    TESTDIR = Path(tmpdir)\n    N_GENES = 4\n\n    adata = pbmc68k_reduced()\n\n    sc.tl.rank_genes_groups(adata, 'bulk_labels', method='wilcoxon', pts=True)\n\n    sc.tl.filter_rank_genes_groups(\n        adata,\n        key_added='rank_genes_groups_filtered',\n        min_in_group_fraction=0.25,\n        min_fold_change=1,\n        max_out_group_fraction=0.5,\n    )\n\n    conditions = 'logfoldchanges >= 1 & pct_nz_group >= .25 & pct_nz_reference < .5'\n    df = sc.get.rank_genes_groups_df(adata, group=None, key=\"rank_genes_groups\")\n    df = df.query(conditions)[[\"group\", \"names\"]]\n\n    var_names = {k: v.head(N_GENES).tolist() for k, v in df.groupby(\"group\")[\"names\"]}\n\n    pth_a = TESTDIR / f\"{plot.__name__}_filter_a.png\"\n    pth_b = TESTDIR / f\"{plot.__name__}_filter_b.png\"\n\n    plot(adata, key='rank_genes_groups_filtered', n_genes=N_GENES)\n    plt.savefig(pth_a)\n    plt.close()\n\n    plot(adata, key='rank_genes_groups', var_names=var_names)\n    plt.savefig(pth_b)\n    plt.close()\n\n    check_same_image(pth_a, pth_b, tol=1)", "idx": 627}
{"project": "Scanpy", "commit_id": "667_scanpy_1.9.0_test_plotting.py_test_scrublet_plots.py", "target": 0, "func": "def test_scrublet_plots(image_comparer, plt):\n    save_and_compare_images = image_comparer(ROOT, FIGS, tol=30)\n\n    adata = pbmc3k()\n    sc.external.pp.scrublet(adata, use_approx_neighbors=False)\n\n    sc.external.pl.scrublet_score_distribution(adata, return_fig=True)\n    save_and_compare_images('scrublet')\n\n    del adata.uns['scrublet']['threshold']\n    adata.obs['predicted_doublet'] = False\n\n    sc.external.pl.scrublet_score_distribution(adata, return_fig=True)\n    save_and_compare_images('scrublet_no_threshold')\n\n    adata.obs['batch'] = 1350 * ['a'] + 1350 * ['b']\n    sc.external.pp.scrublet(adata, use_approx_neighbors=False, batch_key='batch')\n\n    sc.external.pl.scrublet_score_distribution(adata, return_fig=True)\n    save_and_compare_images('scrublet_with_batches')", "idx": 628}
{"project": "Scanpy", "commit_id": "668_scanpy_1.9.0_test_plotting.py_wrapped.py", "target": 0, "func": "def wrapped(pth, **kwargs):\n        func(pbmc, groupby=\"louvain\", dendrogram=False, **kwargs)\n        plt.savefig(pth)\n        plt.close()", "idx": 629}
{"project": "Scanpy", "commit_id": "669_scanpy_1.9.0_test_preprocessing.py_test_log1p.py", "target": 1, "func": "def test_log1p(tmp_path):\n    A = np.random.rand(200, 10)\n    A_l = np.log1p(A)\n    ad = AnnData(A)\n    ad2 = AnnData(A)\n    ad3 = AnnData(A)\n    ad3.filename = tmp_path / 'test.h5ad'\n    sc.pp.log1p(ad)\n    assert np.allclose(ad.X, A_l)\n    sc.pp.log1p(ad2, chunked=True)\n    assert np.allclose(ad2.X, ad.X)\n    sc.pp.log1p(ad3, chunked=True)\n    assert np.allclose(ad3.X, ad.X)\n\n    # Test base\n    ad4 = AnnData(A)\n    sc.pp.log1p(ad4, base=2)\n    assert np.allclose(ad4.X, A_l / np.log(2))", "idx": 630}
{"project": "Scanpy", "commit_id": "66_scanpy_1.9.0_readwrite.py__read_softgz.py", "target": 0, "func": "def _read_softgz(filename: Union[str, bytes, Path, BinaryIO]) -> AnnData:\n    \"\"\"\\\n    Read a SOFT format data file.\n\n    The SOFT format is documented here\n    http://www.ncbi.nlm.nih.gov/geo/info/soft2.html.\n\n    Notes\n    -----\n    The function is based on a script by Kerby Shedden.\n    http://dept.stat.lsa.umich.edu/~kshedden/Python-Workshop/gene_expression_comparison.html\n    \"\"\"\n    import gzip\n\n    with gzip.open(filename, mode='rt') as file:\n        # The header part of the file contains information about the\n        # samples. Read that information first.\n        samples_info = {}\n        for line in file:\n            if line.startswith(\"!dataset_table_begin\"):\n                break\n            elif line.startswith(\"!subset_description\"):\n                subset_description = line.split(\"=\")[1].strip()\n            elif line.startswith(\"!subset_sample_id\"):\n                subset_ids = line.split(\"=\")[1].split(\",\")\n                subset_ids = [x.strip() for x in subset_ids]\n                for k in subset_ids:\n                    samples_info[k] = subset_description\n        # Next line is the column headers (sample id's)\n        sample_names = file.readline().strip().split(\"\\t\")\n        # The column indices that contain gene expression data\n        indices = [i for i, x in enumerate(sample_names) if x.startswith(\"GSM\")]\n        # Restrict the column headers to those that we keep\n        sample_names = [sample_names[i] for i in indices]\n        # Get a list of sample labels\n        groups = [samples_info[k] for k in sample_names]\n        # Read the gene expression data as a list of lists, also get the gene\n        # identifiers\n        gene_names, X = [], []\n        for line in file:\n            # This is what signals the end of the gene expression data\n            # section in the file\n            if line.startswith(\"!dataset_table_end\"):\n                break\n            V = line.split(\"\\t\")\n            # Extract the values that correspond to gene expression measures\n            # and convert the strings to numbers\n            x = [float(V[i]) for i in indices]\n            X.append(x)\n            gene_names.append(V[1])\n    # Convert the Python list of lists to a Numpy array and transpose to match\n    # the Scanpy convention of storing samples in rows and variables in colums.\n    X = np.array(X).T\n    obs = pd.DataFrame({\"groups\": groups}, index=sample_names)\n    var = pd.DataFrame(index=gene_names)\n    return AnnData(X=X, obs=obs, var=var, dtype=X.dtype)", "idx": 631}
{"project": "Scanpy", "commit_id": "670_scanpy_1.9.0_test_preprocessing.py_base.py", "target": 0, "func": "def base(request):\n    return request.param", "idx": 632}
{"project": "Scanpy", "commit_id": "671_scanpy_1.9.0_test_preprocessing.py_test_log1p_rep.py", "target": 0, "func": "def test_log1p_rep(count_matrix_format, base, dtype):\n    X = count_matrix_format(\n        np.abs(sp.random(100, 200, density=0.3, dtype=dtype)).toarray()\n    )\n    check_rep_mutation(sc.pp.log1p, X, base=base)\n    check_rep_results(sc.pp.log1p, X, base=base)", "idx": 633}
{"project": "Scanpy", "commit_id": "672_scanpy_1.9.0_test_preprocessing.py_test_mean_var_sparse.py", "target": 0, "func": "def test_mean_var_sparse():\n    from sklearn.utils.sparsefuncs import mean_variance_axis\n\n    csr64 = sp.random(10000, 1000, format=\"csr\", dtype=np.float64)\n    csc64 = csr64.tocsc()\n\n    # Test that we're equivalent for 64 bit\n    for mtx, ax in product((csr64, csc64), (0, 1)):\n        scm, scv = sc.pp._utils._get_mean_var(mtx, axis=ax)\n        skm, skv = mean_variance_axis(mtx, ax)\n        skv *= mtx.shape[ax] / (mtx.shape[ax] - 1)\n\n        assert np.allclose(scm, skm)\n        assert np.allclose(scv, skv)\n\n    csr32 = csr64.astype(np.float32)\n    csc32 = csc64.astype(np.float32)\n\n    # Test whether ours is more accurate for 32 bit\n    for mtx32, mtx64 in [(csc32, csc64), (csr32, csr64)]:\n        scm32, scv32 = sc.pp._utils._get_mean_var(mtx32)\n        scm64, scv64 = sc.pp._utils._get_mean_var(mtx64)\n        skm32, skv32 = mean_variance_axis(mtx32, 0)\n        skm64, skv64 = mean_variance_axis(mtx64, 0)\n        skv32 *= mtx.shape[0] / (mtx.shape[0] - 1)\n        skv64 *= mtx.shape[0] / (mtx.shape[0] - 1)\n\n        m_resid_sc = np.mean(np.abs(scm64 - scm32))\n        m_resid_sk = np.mean(np.abs(skm64 - skm32))\n        v_resid_sc = np.mean(np.abs(scv64 - scv32))\n        v_resid_sk = np.mean(np.abs(skv64 - skv32))\n\n        assert m_resid_sc < m_resid_sk\n        assert v_resid_sc < v_resid_sk", "idx": 634}
{"project": "Scanpy", "commit_id": "673_scanpy_1.9.0_test_preprocessing.py_test_normalize_per_cell.py", "target": 0, "func": "def test_normalize_per_cell():\n    adata = AnnData(np.array([[1, 0], [3, 0], [5, 6]]))\n    sc.pp.normalize_per_cell(adata, counts_per_cell_after=1, key_n_counts='n_counts2')\n    assert adata.X.sum(axis=1).tolist() == [1.0, 1.0, 1.0]\n    # now with copy option\n    adata = AnnData(np.array([[1, 0], [3, 0], [5, 6]]))\n    # note that sc.pp.normalize_per_cell is also used in\n    # pl.highest_expr_genes with parameter counts_per_cell_after=100\n    adata_copy = sc.pp.normalize_per_cell(adata, counts_per_cell_after=1, copy=True)\n    assert adata_copy.X.sum(axis=1).tolist() == [1.0, 1.0, 1.0]\n    # now sparse\n    adata = AnnData(np.array([[1, 0], [3, 0], [5, 6]]))\n    adata_sparse = AnnData(sp.csr_matrix([[1, 0], [3, 0], [5, 6]]))\n    sc.pp.normalize_per_cell(adata)\n    sc.pp.normalize_per_cell(adata_sparse)\n    assert adata.X.sum(axis=1).tolist() == adata_sparse.X.sum(axis=1).A1.tolist()", "idx": 635}
{"project": "Scanpy", "commit_id": "674_scanpy_1.9.0_test_preprocessing.py_test_subsample.py", "target": 0, "func": "def test_subsample():\n    adata = AnnData(np.ones((200, 10)))\n    sc.pp.subsample(adata, n_obs=40)\n    assert adata.n_obs == 40\n    sc.pp.subsample(adata, fraction=0.1)\n    assert adata.n_obs == 4", "idx": 636}
{"project": "Scanpy", "commit_id": "675_scanpy_1.9.0_test_preprocessing.py_test_subsample_copy.py", "target": 0, "func": "def test_subsample_copy():\n    adata = AnnData(np.ones((200, 10)))\n    assert sc.pp.subsample(adata, n_obs=40, copy=True).shape == (40, 10)\n    assert sc.pp.subsample(adata, fraction=0.1, copy=True).shape == (20, 10)", "idx": 637}
{"project": "Scanpy", "commit_id": "676_scanpy_1.9.0_test_preprocessing.py_test_scale.py", "target": 0, "func": "def test_scale():\n    adata = pbmc68k_reduced()\n    adata.X = adata.raw.X\n    v = adata[:, 0 : adata.shape[1] // 2]\n    # Should turn view to copy https://github.com/theislab/anndata/issues/171#issuecomment-508689965\n    assert v.is_view\n    with pytest.warns(Warning, match=\"view\"):\n        sc.pp.scale(v)\n    assert not v.is_view\n    assert_allclose(v.X.var(axis=0), np.ones(v.shape[1]), atol=0.01)\n    assert_allclose(v.X.mean(axis=0), np.zeros(v.shape[1]), atol=0.00001)", "idx": 638}
{"project": "Scanpy", "commit_id": "677_scanpy_1.9.0_test_preprocessing.py_zero_center.py", "target": 0, "func": "def zero_center(request):\n    return request.param", "idx": 639}
{"project": "Scanpy", "commit_id": "678_scanpy_1.9.0_test_preprocessing.py_test_scale_rep.py", "target": 0, "func": "def test_scale_rep(count_matrix_format, zero_center):\n    \"\"\"\n    Test that it doesn't matter where the array being scaled is in the anndata object.\n    \"\"\"\n    X = count_matrix_format(sp.random(100, 200, density=0.3).toarray())\n    check_rep_mutation(sc.pp.scale, X, zero_center=zero_center)\n    check_rep_results(sc.pp.scale, X, zero_center=zero_center)", "idx": 640}
{"project": "Scanpy", "commit_id": "679_scanpy_1.9.0_test_preprocessing.py_test_scale_array.py", "target": 0, "func": "def test_scale_array(count_matrix_format, zero_center):\n    \"\"\"\n    Test that running sc.pp.scale on an anndata object and an array returns the same results.\n    \"\"\"\n    X = count_matrix_format(sp.random(100, 200, density=0.3).toarray())\n    adata = sc.AnnData(X=X.copy(), dtype=np.float64)\n\n    sc.pp.scale(adata, zero_center=zero_center)\n    scaled_X = sc.pp.scale(X, zero_center=zero_center, copy=True)\n    assert np.array_equal(asarray(scaled_X), asarray(adata.X))", "idx": 641}
{"project": "Scanpy", "commit_id": "67_scanpy_1.9.0_readwrite.py_is_float.py", "target": 0, "func": "def is_float(string: str) -> float:\n    \"\"\"Check whether string is float.\n\n    See also\n    --------\n    http://stackoverflow.com/questions/736043/checking-if-a-string-can-be-converted-to-float-in-python\n    \"\"\"\n    try:\n        float(string)\n        return True\n    except ValueError:\n        return False", "idx": 642}
{"project": "Scanpy", "commit_id": "680_scanpy_1.9.0_test_preprocessing.py_test_recipe_plotting.py", "target": 0, "func": "def test_recipe_plotting():\n    sc.settings.autoshow = False\n    adata = AnnData(np.random.randint(0, 1000, (1000, 1000)))\n    # These shouldn't throw an error\n    sc.pp.recipe_seurat(adata.copy(), plot=True)\n    sc.pp.recipe_zheng17(adata.copy(), plot=True)", "idx": 643}
{"project": "Scanpy", "commit_id": "681_scanpy_1.9.0_test_preprocessing.py_test_regress_out_ordinal.py", "target": 0, "func": "def test_regress_out_ordinal():\n    from scipy.sparse import random\n\n    adata = AnnData(random(1000, 100, density=0.6, format='csr'))\n    adata.obs['percent_mito'] = np.random.rand(adata.X.shape[0])\n    adata.obs['n_counts'] = adata.X.sum(axis=1)\n\n    # results using only one processor\n    single = sc.pp.regress_out(\n        adata, keys=['n_counts', 'percent_mito'], n_jobs=1, copy=True\n    )\n    assert adata.X.shape == single.X.shape\n\n    # results using 8 processors\n    multi = sc.pp.regress_out(\n        adata, keys=['n_counts', 'percent_mito'], n_jobs=8, copy=True\n    )\n\n    np.testing.assert_array_equal(single.X, multi.X)", "idx": 644}
{"project": "Scanpy", "commit_id": "682_scanpy_1.9.0_test_preprocessing.py_test_regress_out_view.py", "target": 0, "func": "def test_regress_out_view():\n    from scipy.sparse import random\n\n    adata = AnnData(random(500, 1100, density=0.2, format='csr'))\n    adata.obs['percent_mito'] = np.random.rand(adata.X.shape[0])\n    adata.obs['n_counts'] = adata.X.sum(axis=1)\n    subset_adata = adata[:, :1050]\n    subset_adata_copy = subset_adata.copy()\n\n    sc.pp.regress_out(subset_adata, keys=['n_counts', 'percent_mito'])\n    sc.pp.regress_out(subset_adata_copy, keys=['n_counts', 'percent_mito'])\n    assert_equal(subset_adata, subset_adata_copy)\n    assert not subset_adata.is_view", "idx": 645}
{"project": "Scanpy", "commit_id": "683_scanpy_1.9.0_test_preprocessing.py_test_regress_out_categorical.py", "target": 0, "func": "def test_regress_out_categorical():\n    from scipy.sparse import random\n    import pandas as pd\n\n    adata = AnnData(random(1000, 100, density=0.6, format='csr'))\n    # create a categorical column\n    adata.obs['batch'] = pd.Categorical(np.random.randint(1, 4, size=adata.X.shape[0]))\n\n    multi = sc.pp.regress_out(adata, keys='batch', n_jobs=8, copy=True)\n    assert adata.X.shape == multi.X.shape", "idx": 646}
{"project": "Scanpy", "commit_id": "684_scanpy_1.9.0_test_preprocessing.py_test_regress_out_constants.py", "target": 0, "func": "def test_regress_out_constants():\n    adata = AnnData(np.hstack((np.full((10, 1), 0.0), np.full((10, 1), 1.0))))\n    adata.obs['percent_mito'] = np.random.rand(adata.X.shape[0])\n    adata.obs['n_counts'] = adata.X.sum(axis=1)\n    adata_copy = adata.copy()\n\n    sc.pp.regress_out(adata, keys=['n_counts', 'percent_mito'])\n    assert_equal(adata, adata_copy)", "idx": 647}
{"project": "Scanpy", "commit_id": "685_scanpy_1.9.0_test_preprocessing.py_test_regress_out_constants_equivalent.py", "target": 0, "func": "def test_regress_out_constants_equivalent():\n    # Tests that constant values don't change results\n    # (since support for constant values is implemented by us)\n    from sklearn.datasets import make_blobs\n\n    X, cat = make_blobs(100, 20)\n    a = sc.AnnData(np.hstack([X, np.zeros((100, 5))]), obs={\"cat\": pd.Categorical(cat)})\n    b = sc.AnnData(X, obs={\"cat\": pd.Categorical(cat)})\n\n    sc.pp.regress_out(a, \"cat\")\n    sc.pp.regress_out(b, \"cat\")\n\n    np.testing.assert_equal(a[:, b.var_names].X, b.X)", "idx": 648}
{"project": "Scanpy", "commit_id": "686_scanpy_1.9.0_test_preprocessing.py_count_matrix_format.py", "target": 0, "func": "def count_matrix_format(request):\n    return request.param", "idx": 649}
{"project": "Scanpy", "commit_id": "687_scanpy_1.9.0_test_preprocessing.py_replace.py", "target": 0, "func": "def replace(request):\n    return request.param", "idx": 650}
{"project": "Scanpy", "commit_id": "688_scanpy_1.9.0_test_preprocessing.py_dtype.py", "target": 0, "func": "def dtype(request):\n    return request.param", "idx": 651}
{"project": "Scanpy", "commit_id": "689_scanpy_1.9.0_test_preprocessing.py_test_downsample_counts_per_cell.py", "target": 0, "func": "def test_downsample_counts_per_cell(count_matrix_format, replace, dtype):\n    TARGET = 1000\n    X = np.random.randint(0, 100, (1000, 100)) * np.random.binomial(1, 0.3, (1000, 100))\n    X = X.astype(dtype)\n    adata = AnnData(X=count_matrix_format(X), dtype=dtype)\n    with pytest.raises(ValueError):\n        sc.pp.downsample_counts(\n            adata, counts_per_cell=TARGET, total_counts=TARGET, replace=replace\n        )\n    with pytest.raises(ValueError):\n        sc.pp.downsample_counts(adata, replace=replace)\n    initial_totals = np.ravel(adata.X.sum(axis=1))\n    adata = sc.pp.downsample_counts(\n        adata, counts_per_cell=TARGET, replace=replace, copy=True\n    )\n    new_totals = np.ravel(adata.X.sum(axis=1))\n    if sp.issparse(adata.X):\n        assert all(adata.X.toarray()[X == 0] == 0)\n    else:\n        assert all(adata.X[X == 0] == 0)\n    assert all(new_totals <= TARGET)\n    assert all(initial_totals >= new_totals)\n    assert all(\n        initial_totals[initial_totals <= TARGET] == new_totals[initial_totals <= TARGET]\n    )\n    if not replace:\n        assert np.all(X >= adata.X)\n    assert X.dtype == adata.X.dtype", "idx": 652}
{"project": "Scanpy", "commit_id": "68_scanpy_1.9.0_readwrite.py_is_int.py", "target": 0, "func": "def is_int(string: str) -> bool:\n    \"\"\"Check whether string is integer.\"\"\"\n    try:\n        int(string)\n        return True\n    except ValueError:\n        return False", "idx": 653}
{"project": "Scanpy", "commit_id": "690_scanpy_1.9.0_test_preprocessing.py_test_downsample_counts_per_cell_multiple_targets.py", "target": 0, "func": "def test_downsample_counts_per_cell_multiple_targets(\n    count_matrix_format, replace, dtype\n):\n    TARGETS = np.random.randint(500, 1500, 1000)\n    X = np.random.randint(0, 100, (1000, 100)) * np.random.binomial(1, 0.3, (1000, 100))\n    X = X.astype(dtype)\n    adata = AnnData(X=count_matrix_format(X), dtype=dtype)\n    initial_totals = np.ravel(adata.X.sum(axis=1))\n    with pytest.raises(ValueError):\n        sc.pp.downsample_counts(adata, counts_per_cell=[40, 10], replace=replace)\n    adata = sc.pp.downsample_counts(\n        adata, counts_per_cell=TARGETS, replace=replace, copy=True\n    )\n    new_totals = np.ravel(adata.X.sum(axis=1))\n    if sp.issparse(adata.X):\n        assert all(adata.X.toarray()[X == 0] == 0)\n    else:\n        assert all(adata.X[X == 0] == 0)\n    assert all(new_totals <= TARGETS)\n    assert all(initial_totals >= new_totals)\n    assert all(\n        initial_totals[initial_totals <= TARGETS]\n        == new_totals[initial_totals <= TARGETS]\n    )\n    if not replace:\n        assert np.all(X >= adata.X)\n    assert X.dtype == adata.X.dtype", "idx": 654}
{"project": "Scanpy", "commit_id": "691_scanpy_1.9.0_test_preprocessing.py_test_downsample_total_counts.py", "target": 0, "func": "def test_downsample_total_counts(count_matrix_format, replace, dtype):\n    X = np.random.randint(0, 100, (1000, 100)) * np.random.binomial(1, 0.3, (1000, 100))\n    X = X.astype(dtype)\n    adata_orig = AnnData(X=count_matrix_format(X), dtype=dtype)\n    total = X.sum()\n    target = np.floor_divide(total, 10)\n    initial_totals = np.ravel(adata_orig.X.sum(axis=1))\n    adata = sc.pp.downsample_counts(\n        adata_orig, total_counts=target, replace=replace, copy=True\n    )\n    new_totals = np.ravel(adata.X.sum(axis=1))\n    if sp.issparse(adata.X):\n        assert all(adata.X.toarray()[X == 0] == 0)\n    else:\n        assert all(adata.X[X == 0] == 0)\n    assert adata.X.sum() == target\n    assert all(initial_totals >= new_totals)\n    if not replace:\n        assert np.all(X >= adata.X)\n        adata = sc.pp.downsample_counts(\n            adata_orig, total_counts=total + 10, replace=False, copy=True\n        )\n        assert (adata.X == X).all()\n    assert X.dtype == adata.X.dtype", "idx": 655}
{"project": "Scanpy", "commit_id": "692_scanpy_1.9.0_test_preprocessing.py_test_recipe_weinreb.py", "target": 0, "func": "def test_recipe_weinreb():\n    # Just tests for failure for now\n    adata = pbmc68k_reduced().raw.to_adata()\n    adata.X = adata.X.toarray()\n\n    orig = adata.copy()\n    sc.pp.recipe_weinreb17(adata, log=False, copy=True)\n    assert_equal(orig, adata)", "idx": 656}
{"project": "Scanpy", "commit_id": "693_scanpy_1.9.0_test_preprocessing_distributed.py_adata.py", "target": 0, "func": "def adata(self):\n        a = ad.read_zarr(input_file)  # regular anndata\n        a.X = a.X[:]  # convert to numpy array\n        return a", "idx": 657}
{"project": "Scanpy", "commit_id": "694_scanpy_1.9.0_test_preprocessing_distributed.py_adata_dist.py", "target": 0, "func": "def adata_dist(self, request):\n        # regular anndata except for X, which we replace on the next line\n        a = ad.read_zarr(input_file)\n        input_file_X = input_file + \"/X\"\n        if request.param == \"direct\":\n            import zappy.direct\n\n            a.X = zappy.direct.from_zarr(input_file_X)\n            yield a\n        elif request.param == \"dask\":\n            import dask.array as da\n\n            a.X = da.from_zarr(input_file_X)\n            yield a", "idx": 658}
{"project": "Scanpy", "commit_id": "695_scanpy_1.9.0_test_preprocessing_distributed.py_test_log1p.py", "target": 0, "func": "def test_log1p(self, adata, adata_dist):\n        log1p(adata_dist)\n        result = materialize_as_ndarray(adata_dist.X)\n        log1p(adata)\n        assert result.shape == adata.shape\n        assert result.shape == (adata.n_obs, adata.n_vars)\n        npt.assert_allclose(result, adata.X)", "idx": 659}
{"project": "Scanpy", "commit_id": "696_scanpy_1.9.0_test_preprocessing_distributed.py_test_normalize_per_cell.py", "target": 0, "func": "def test_normalize_per_cell(self, adata, adata_dist):\n        normalize_per_cell(adata_dist)\n        result = materialize_as_ndarray(adata_dist.X)\n        normalize_per_cell(adata)\n        assert result.shape == adata.shape\n        assert result.shape == (adata.n_obs, adata.n_vars)\n        npt.assert_allclose(result, adata.X)", "idx": 660}
{"project": "Scanpy", "commit_id": "697_scanpy_1.9.0_test_preprocessing_distributed.py_test_normalize_total.py", "target": 0, "func": "def test_normalize_total(self, adata, adata_dist):\n        normalize_total(adata_dist)\n        result = materialize_as_ndarray(adata_dist.X)\n        normalize_total(adata)\n        assert result.shape == adata.shape\n        assert result.shape == (adata.n_obs, adata.n_vars)\n        npt.assert_allclose(result, adata.X)", "idx": 661}
{"project": "Scanpy", "commit_id": "698_scanpy_1.9.0_test_preprocessing_distributed.py_test_filter_cells.py", "target": 0, "func": "def test_filter_cells(self, adata, adata_dist):\n        filter_cells(adata_dist, min_genes=3)\n        result = materialize_as_ndarray(adata_dist.X)\n        filter_cells(adata, min_genes=3)\n        assert result.shape == adata.shape\n        assert result.shape == (adata.n_obs, adata.n_vars)\n        npt.assert_allclose(result, adata.X)", "idx": 662}
{"project": "Scanpy", "commit_id": "699_scanpy_1.9.0_test_preprocessing_distributed.py_test_filter_genes.py", "target": 0, "func": "def test_filter_genes(self, adata, adata_dist):\n        filter_genes(adata_dist, min_cells=2)\n        result = materialize_as_ndarray(adata_dist.X)\n        filter_genes(adata, min_cells=2)\n        assert result.shape == adata.shape\n        assert result.shape == (adata.n_obs, adata.n_vars)\n        npt.assert_allclose(result, adata.X)", "idx": 663}
{"project": "Scanpy", "commit_id": "69_scanpy_1.9.0_readwrite.py_convert_bool.py", "target": 0, "func": "def convert_bool(string: str) -> Tuple[bool, bool]:\n    \"\"\"Check whether string is boolean.\"\"\"\n    if string == 'True':\n        return True, True\n    elif string == 'False':\n        return True, False\n    else:\n        return False, False", "idx": 664}
{"project": "Scanpy", "commit_id": "6_scanpy_1.9.0_conftest.py_noop.py", "target": 0, "func": "def noop(file, description):\n            pass", "idx": 665}
{"project": "Scanpy", "commit_id": "700_scanpy_1.9.0_test_preprocessing_distributed.py_test_write_zarr.py", "target": 0, "func": "def test_write_zarr(self, adata, adata_dist):\n        import dask.array as da\n        import zarr\n\n        log1p(adata_dist)\n        temp_store = zarr.TempStore()\n        chunks = adata_dist.X.chunks\n        if isinstance(chunks[0], tuple):\n            chunks = (chunks[0][0],) + chunks[1]\n        # write metadata using regular anndata\n        adata.write_zarr(temp_store, chunks)\n        if isinstance(adata_dist.X, da.Array):\n            adata_dist.X.to_zarr(temp_store.dir_path(\"X\"), overwrite=True)\n        else:\n            adata_dist.X.to_zarr(temp_store.dir_path(\"X\"), chunks)\n        # read back as zarr directly and check it is the same as adata.X\n        adata_log1p = ad.read_zarr(temp_store)\n        log1p(adata)\n        npt.assert_allclose(adata_log1p.X, adata.X)", "idx": 666}
{"project": "Scanpy", "commit_id": "701_scanpy_1.9.0_test_qc_metrics.py_anndata.py", "target": 0, "func": "def anndata():\n    a = np.random.binomial(100, 0.005, (1000, 1000))\n    adata = AnnData(\n        sparse.csr_matrix(a),\n        obs=pd.DataFrame(index=[f\"cell{i}\" for i in range(a.shape[0])]),\n        var=pd.DataFrame(index=[f\"gene{i}\" for i in range(a.shape[1])]),\n    )\n    return adata", "idx": 667}
{"project": "Scanpy", "commit_id": "702_scanpy_1.9.0_test_qc_metrics.py_test_proportions.py", "target": 0, "func": "def test_proportions(a):\n    prop = top_proportions(a, 100)\n    assert (prop[:, -1] == 1).all()\n    assert np.array_equal(np.sort(prop, axis=1), prop)\n    assert np.apply_along_axis(lambda x: len(np.unique(x)) == 1, 0, prop).all()\n    assert (prop[:, 49] == 0.5).all()", "idx": 668}
{"project": "Scanpy", "commit_id": "703_scanpy_1.9.0_test_qc_metrics.py_test_segments_binary.py", "target": 0, "func": "def test_segments_binary():\n    a = np.concatenate([np.zeros((300, 50)), np.ones((300, 50))], 1)\n    a = np.apply_along_axis(np.random.permutation, 1, a)\n    seg = top_segment_proportions(a, [25, 50, 100])\n    assert (seg[:, 0] == 0.5).all()\n    assert (top_segment_proportions(a, [25]) == 0.5).all()\n    assert (seg[:, 1] == 1.0).all()\n    assert (seg[:, 2] == 1.0).all()\n    segfull = top_segment_proportions(a, np.arange(100) + 1)\n    propfull = top_proportions(a, 100)\n    assert (segfull == propfull).all()", "idx": 669}
{"project": "Scanpy", "commit_id": "704_scanpy_1.9.0_test_qc_metrics.py_test_top_segments.py", "target": 0, "func": "def test_top_segments(cls):\n    a = cls(np.ones((300, 100)))\n    seg = top_segment_proportions(a, [50, 100])\n    assert (seg[:, 0] == 0.5).all()\n    assert (seg[:, 1] == 1.0).all()\n    segfull = top_segment_proportions(a, np.arange(100) + 1)\n    propfull = top_proportions(a, 100)\n    assert (segfull == propfull).all()", "idx": 670}
{"project": "Scanpy", "commit_id": "705_scanpy_1.9.0_test_qc_metrics.py_test_qc_metrics.py", "target": 0, "func": "def test_qc_metrics():\n    adata = AnnData(X=sparse.csr_matrix(np.random.binomial(100, 0.005, (1000, 1000))))\n    adata.var[\"mito\"] = np.concatenate(\n        (np.ones(100, dtype=bool), np.zeros(900, dtype=bool))\n    )\n    adata.var[\"negative\"] = False\n    sc.pp.calculate_qc_metrics(adata, qc_vars=[\"mito\", \"negative\"], inplace=True)\n    assert (adata.obs[\"n_genes_by_counts\"] < adata.shape[1]).all()\n    assert (\n        adata.obs[\"n_genes_by_counts\"] >= adata.obs[\"log1p_n_genes_by_counts\"]\n    ).all()\n    assert (adata.obs[\"total_counts\"] == np.ravel(adata.X.sum(axis=1))).all()\n    assert (adata.obs[\"total_counts\"] >= adata.obs[\"log1p_total_counts\"]).all()\n    assert (\n        adata.obs[\"total_counts_mito\"] >= adata.obs[\"log1p_total_counts_mito\"]\n    ).all()\n    assert (adata.obs[\"total_counts_negative\"] == 0).all()\n    assert (\n        adata.obs[\"pct_counts_in_top_50_genes\"]\n        <= adata.obs[\"pct_counts_in_top_100_genes\"]\n    ).all()\n    for col in filter(lambda x: \"negative\" not in x, adata.obs.columns):\n        assert (adata.obs[col] >= 0).all()  # Values should be positive or zero\n        assert (adata.obs[col] != 0).any().all()  # Nothing should be all zeros\n        if col.startswith(\"pct_counts_in_top\"):\n            assert (adata.obs[col] <= 100).all()\n            assert (adata.obs[col] >= 0).all()\n    for col in adata.var.columns:\n        assert (adata.var[col] >= 0).all()\n    assert (adata.var[\"mean_counts\"] < np.ravel(adata.X.max(axis=0).todense())).all()\n    assert (adata.var[\"mean_counts\"] >= adata.var[\"log1p_mean_counts\"]).all()\n    assert (adata.var[\"total_counts\"] >= adata.var[\"log1p_total_counts\"]).all()\n    # Should return the same thing if run again\n    old_obs, old_var = adata.obs.copy(), adata.var.copy()\n    sc.pp.calculate_qc_metrics(adata, qc_vars=[\"mito\", \"negative\"], inplace=True)\n    assert set(adata.obs.columns) == set(old_obs.columns)\n    assert set(adata.var.columns) == set(old_var.columns)\n    for col in adata.obs:\n        assert np.allclose(adata.obs[col], old_obs[col])\n    for col in adata.var:\n        assert np.allclose(adata.var[col], old_var[col])\n    # with log1p=False\n    adata = AnnData(X=sparse.csr_matrix(np.random.binomial(100, 0.005, (1000, 1000))))\n    adata.var[\"mito\"] = np.concatenate(\n        (np.ones(100, dtype=bool), np.zeros(900, dtype=bool))\n    )\n    adata.var[\"negative\"] = False\n    sc.pp.calculate_qc_metrics(\n        adata, qc_vars=[\"mito\", \"negative\"], log1p=False, inplace=True\n    )\n    assert not np.any(adata.obs.columns.str.startswith(\"log1p_\"))\n    assert not np.any(adata.var.columns.str.startswith(\"log1p_\"))", "idx": 671}
{"project": "Scanpy", "commit_id": "706_scanpy_1.9.0_test_qc_metrics.py_adata_mito.py", "target": 0, "func": "def adata_mito():\n    a = np.random.binomial(100, 0.005, (1000, 1000))\n    init_var = pd.DataFrame(\n        dict(mito=np.concatenate((np.ones(100, dtype=bool), np.zeros(900, dtype=bool))))\n    )\n    adata_dense = AnnData(X=a, var=init_var.copy())\n    return adata_dense, init_var", "idx": 672}
{"project": "Scanpy", "commit_id": "707_scanpy_1.9.0_test_qc_metrics.py_test_qc_metrics_format.py", "target": 0, "func": "def test_qc_metrics_format(cls):\n    adata_dense, init_var = adata_mito()\n    sc.pp.calculate_qc_metrics(adata_dense, qc_vars=[\"mito\"], inplace=True)\n    adata = AnnData(X=cls(adata_dense.X), var=init_var.copy())\n    sc.pp.calculate_qc_metrics(adata, qc_vars=[\"mito\"], inplace=True)\n    assert np.allclose(adata.obs, adata_dense.obs)\n    for col in adata.var:  # np.allclose doesn't like mix of types\n        assert np.allclose(adata.var[col], adata_dense.var[col])", "idx": 673}
{"project": "Scanpy", "commit_id": "708_scanpy_1.9.0_test_qc_metrics.py_test_qc_metrics_percentage.py", "target": 0, "func": "def test_qc_metrics_percentage():  # In response to #421\n    adata_dense, init_var = adata_mito()\n    sc.pp.calculate_qc_metrics(adata_dense, percent_top=[])\n    sc.pp.calculate_qc_metrics(adata_dense, percent_top=())\n    sc.pp.calculate_qc_metrics(adata_dense, percent_top=None)\n    sc.pp.calculate_qc_metrics(adata_dense, percent_top=[1, 2, 3, 10])\n    sc.pp.calculate_qc_metrics(adata_dense, percent_top=[1])\n    with pytest.raises(IndexError):\n        sc.pp.calculate_qc_metrics(adata_dense, percent_top=[1, 2, 3, -5])\n    with pytest.raises(IndexError):\n        sc.pp.calculate_qc_metrics(adata_dense, percent_top=[20, 30, 1001])", "idx": 674}
{"project": "Scanpy", "commit_id": "709_scanpy_1.9.0_test_qc_metrics.py_test_layer_raw.py", "target": 0, "func": "def test_layer_raw(anndata):\n    adata = anndata.copy()\n    adata.raw = adata.copy()\n    adata.layers[\"counts\"] = adata.X.copy()\n    obs_orig, var_orig = sc.pp.calculate_qc_metrics(adata)\n    sc.pp.log1p(adata)  # To be sure they aren't reusing it\n    obs_layer, var_layer = sc.pp.calculate_qc_metrics(adata, layer=\"counts\")\n    obs_raw, var_raw = sc.pp.calculate_qc_metrics(adata, use_raw=True)\n    assert np.allclose(obs_orig, obs_layer)\n    assert np.allclose(obs_orig, obs_raw)\n    assert np.allclose(var_orig, var_layer)\n    assert np.allclose(var_orig, var_raw)", "idx": 675}
{"project": "Scanpy", "commit_id": "70_scanpy_1.9.0_readwrite.py_convert_string.py", "target": 0, "func": "def convert_string(string: str) -> Union[int, float, bool, str, None]:\n    \"\"\"Convert string to int, float or bool.\"\"\"\n    if is_int(string):\n        return int(string)\n    elif is_float(string):\n        return float(string)\n    elif convert_bool(string)[0]:\n        return convert_bool(string)[1]\n    elif string == 'None':\n        return None\n    else:\n        return string", "idx": 676}
{"project": "Scanpy", "commit_id": "710_scanpy_1.9.0_test_qc_metrics.py_test_inner_methods.py", "target": 0, "func": "def test_inner_methods(anndata):\n    adata = anndata.copy()\n    full_inplace = adata.copy()\n    partial_inplace = adata.copy()\n    obs_orig, var_orig = sc.pp.calculate_qc_metrics(adata)\n    assert np.all(obs_orig == describe_obs(adata))\n    assert np.all(var_orig == describe_var(adata))\n    sc.pp.calculate_qc_metrics(full_inplace, inplace=True)\n    describe_obs(partial_inplace, inplace=True)\n    describe_var(partial_inplace, inplace=True)\n    assert np.all(full_inplace.obs == partial_inplace.obs)\n    assert np.all(full_inplace.var == partial_inplace.var)\n    assert np.all(partial_inplace.obs[obs_orig.columns] == obs_orig)\n    assert np.all(partial_inplace.var[var_orig.columns] == var_orig)", "idx": 677}
{"project": "Scanpy", "commit_id": "711_scanpy_1.9.0_test_queries.py_test_enrich.py", "target": 0, "func": "def test_enrich():\n    pbmc = pbmc68k_reduced()\n    sc.tl.rank_genes_groups(pbmc, \"louvain\", n_genes=pbmc.shape[1])\n    enrich_anndata = sc.queries.enrich(pbmc, \"1\")\n    de = pd.DataFrame()\n    for k in [\"pvals_adj\", \"names\"]:\n        de[k] = pbmc.uns[\"rank_genes_groups\"][k][\"1\"]\n    de_genes = de.loc[lambda x: x[\"pvals_adj\"] < 0.05, \"names\"]\n    enrich_list = sc.queries.enrich(list(de_genes))\n    assert (enrich_anndata == enrich_list).all().all()\n\n    # theislab/scanpy/#1043\n    sc.tl.filter_rank_genes_groups(pbmc, min_fold_change=1)\n    sc.queries.enrich(pbmc, \"1\")\n\n    gene_dict = {'set1': ['KLF4', 'PAX5'], 'set2': ['SOX2', 'NANOG']}\n    enrich_list = sc.queries.enrich(\n        gene_dict, org=\"hsapiens\", gprofiler_kwargs=dict(sources=['GO:BP'])\n    )\n    assert 'set1' in enrich_list['query'].unique()\n    assert 'set2' in enrich_list['query'].unique()", "idx": 678}
{"project": "Scanpy", "commit_id": "712_scanpy_1.9.0_test_queries.py_test_mito_genes.py", "target": 0, "func": "def test_mito_genes():\n    pbmc = pbmc68k_reduced()\n    mt_genes = sc.queries.mitochondrial_genes(\"hsapiens\")\n    assert (\n        pbmc.var_names.isin(mt_genes[\"external_gene_name\"]).sum() == 1", "idx": 679}
{"project": "Scanpy", "commit_id": "713_scanpy_1.9.0_test_rank_genes_groups.py_get_example_data.py", "target": 1, "func": "def get_example_data(*, sparse=False):\n    # create test object\n    adata = AnnData(\n        np.multiply(binomial(1, 0.15, (100, 20)), negative_binomial(2, 0.25, (100, 20)))\n    )\n    # adapt marker_genes for cluster (so as to have some form of reasonable input\n    adata.X[0:10, 0:5] = np.multiply(\n        binomial(1, 0.9, (10, 5)), negative_binomial(1, 0.5, (10, 5))\n    )\n\n    # The following construction is inefficient, but makes sure that the same data is used in the sparse case\n    if sparse:\n        adata.X = sp.csr_matrix(adata.X)\n\n    # Create cluster according to groups\n    adata.obs['true_groups'] = pd.Categorical(\n        np.concatenate(\n            (\n                np.zeros((10,), dtype=int),\n                np.ones((90,), dtype=int),\n            )\n        )\n    )\n\n    return adata", "idx": 680}
{"project": "Scanpy", "commit_id": "714_scanpy_1.9.0_test_rank_genes_groups.py_get_true_scores.py", "target": 0, "func": "def get_true_scores():\n    with Path(HERE, 'objs_t_test.pkl').open('rb') as f:\n        true_scores_t_test, true_names_t_test = pickle.load(f)\n    with Path(HERE, 'objs_wilcoxon.pkl').open('rb') as f:\n        true_scores_wilcoxon, true_names_wilcoxon = pickle.load(f)\n\n    return (\n        true_names_t_test,\n        true_names_wilcoxon,\n        true_scores_t_test,\n        true_scores_wilcoxon,", "idx": 681}
{"project": "Scanpy", "commit_id": "715_scanpy_1.9.0_test_rank_genes_groups.py_test_results_dense.py", "target": 0, "func": "def test_results_dense():\n    seed(1234)\n\n    adata = get_example_data()\n    assert adata.raw is None  # Assumption for later checks\n\n    (\n        true_names_t_test,\n        true_names_wilcoxon,\n        true_scores_t_test,\n        true_scores_wilcoxon,\n    ) = get_true_scores()\n\n    rank_genes_groups(adata, 'true_groups', n_genes=20, method='t-test')\n\n    adata.uns['rank_genes_groups']['names'] = adata.uns['rank_genes_groups'][\n        'names'\n    ].astype(true_names_t_test.dtype)\n\n    for name in true_scores_t_test.dtype.names:\n        assert np.allclose(\n            true_scores_t_test[name], adata.uns['rank_genes_groups']['scores'][name]\n        )\n    assert np.array_equal(true_names_t_test, adata.uns['rank_genes_groups']['names'])\n    assert adata.uns[\"rank_genes_groups\"][\"params\"][\"use_raw\"] is False\n\n    rank_genes_groups(adata, 'true_groups', n_genes=20, method='wilcoxon')\n\n    adata.uns['rank_genes_groups']['names'] = adata.uns['rank_genes_groups'][\n        'names'\n    ].astype(true_names_wilcoxon.dtype)\n\n    for name in true_scores_t_test.dtype.names:\n        assert np.allclose(\n            true_scores_wilcoxon[name][:7],\n            adata.uns['rank_genes_groups']['scores'][name][:7],\n        )\n    assert np.array_equal(\n        true_names_wilcoxon[:7], adata.uns['rank_genes_groups']['names'][:7]\n    )\n    assert adata.uns[\"rank_genes_groups\"][\"params\"][\"use_raw\"] is False", "idx": 682}
{"project": "Scanpy", "commit_id": "716_scanpy_1.9.0_test_rank_genes_groups.py_test_results_sparse.py", "target": 0, "func": "def test_results_sparse():\n    seed(1234)\n\n    adata = get_example_data(sparse=True)\n\n    (\n        true_names_t_test,\n        true_names_wilcoxon,\n        true_scores_t_test,\n        true_scores_wilcoxon,\n    ) = get_true_scores()\n\n    rank_genes_groups(adata, 'true_groups', n_genes=20, method='t-test')\n\n    adata.uns['rank_genes_groups']['names'] = adata.uns['rank_genes_groups'][\n        'names'\n    ].astype(true_names_t_test.dtype)\n\n    for name in true_scores_t_test.dtype.names:\n        assert np.allclose(\n            true_scores_t_test[name], adata.uns['rank_genes_groups']['scores'][name]\n        )\n    assert np.array_equal(true_names_t_test, adata.uns['rank_genes_groups']['names'])\n    assert adata.uns[\"rank_genes_groups\"][\"params\"][\"use_raw\"] is False\n\n    rank_genes_groups(adata, 'true_groups', n_genes=20, method='wilcoxon')\n\n    adata.uns['rank_genes_groups']['names'] = adata.uns['rank_genes_groups'][\n        'names'\n    ].astype(true_names_wilcoxon.dtype)\n\n    for name in true_scores_t_test.dtype.names:\n        assert np.allclose(\n            true_scores_wilcoxon[name][:7],\n            adata.uns['rank_genes_groups']['scores'][name][:7],\n        )\n    assert np.array_equal(\n        true_names_wilcoxon[:7], adata.uns['rank_genes_groups']['names'][:7]\n    )\n    assert adata.uns[\"rank_genes_groups\"][\"params\"][\"use_raw\"] is False", "idx": 683}
{"project": "Scanpy", "commit_id": "717_scanpy_1.9.0_test_rank_genes_groups.py_test_results_layers.py", "target": 0, "func": "def test_results_layers():\n    seed(1234)\n\n    adata = get_example_data(sparse=False)\n    adata.layers[\"to_test\"] = adata.X.copy()\n    adata.X = adata.X * np.random.randint(0, 2, adata.shape, dtype=bool)\n\n    (\n        true_names_t_test,\n        true_names_wilcoxon,\n        true_scores_t_test,\n        true_scores_wilcoxon,\n    ) = get_true_scores()\n\n    # Wilcoxon\n    rank_genes_groups(\n        adata,\n        'true_groups',\n        method='wilcoxon',\n        layer=\"to_test\",\n        n_genes=20,\n    )\n    assert adata.uns[\"rank_genes_groups\"][\"params\"][\"use_raw\"] is False\n    for name in true_scores_t_test.dtype.names:\n        assert np.allclose(\n            true_scores_wilcoxon[name][:7],\n            adata.uns['rank_genes_groups']['scores'][name][:7],\n        )\n\n    rank_genes_groups(adata, 'true_groups', method='wilcoxon', n_genes=20)\n    for name in true_scores_t_test.dtype.names:\n        assert not np.allclose(\n            true_scores_wilcoxon[name][:7],\n            adata.uns['rank_genes_groups']['scores'][name][:7],\n        )\n\n    # t-test\n    rank_genes_groups(\n        adata,\n        'true_groups',\n        method='t-test',\n        layer=\"to_test\",\n        use_raw=False,\n        n_genes=20,\n    )\n    for name in true_scores_t_test.dtype.names:\n        assert np.allclose(\n            true_scores_t_test[name][:7],\n            adata.uns['rank_genes_groups']['scores'][name][:7],\n        )\n\n    rank_genes_groups(adata, 'true_groups', method='t-test', n_genes=20)\n    for name in true_scores_t_test.dtype.names:\n        assert not np.allclose(\n            true_scores_t_test[name][:7],\n            adata.uns['rank_genes_groups']['scores'][name][:7],", "idx": 684}
{"project": "Scanpy", "commit_id": "718_scanpy_1.9.0_test_rank_genes_groups.py_test_rank_genes_groups_use_raw.py", "target": 0, "func": "def test_rank_genes_groups_use_raw():\n    # https://github.com/theislab/scanpy/issues/1929\n    pbmc = pbmc68k_reduced()\n    assert pbmc.raw is not None\n\n    sc.tl.rank_genes_groups(pbmc, groupby=\"bulk_labels\", use_raw=True)\n\n    pbmc = pbmc68k_reduced()\n    del pbmc.raw\n    assert pbmc.raw is None\n\n    with pytest.raises(\n        ValueError, match=\"Received `use_raw=True`, but `adata.raw` is empty\"\n    ):\n        sc.tl.rank_genes_groups(pbmc, groupby=\"bulk_labels\", use_raw=True)", "idx": 685}
{"project": "Scanpy", "commit_id": "719_scanpy_1.9.0_test_rank_genes_groups.py_test_singlets.py", "target": 0, "func": "def test_singlets():\n    pbmc = pbmc68k_reduced()\n    pbmc.obs['louvain'] = pbmc.obs['louvain'].cat.add_categories(['11'])\n    pbmc.obs['louvain'][0] = '11'\n\n    with pytest.raises(ValueError, match=rf\"Could not calculate statistics.*{'11'}\"):\n        rank_genes_groups(pbmc, groupby='louvain')", "idx": 686}
{"project": "Scanpy", "commit_id": "71_scanpy_1.9.0_readwrite.py_get_used_files.py", "target": 0, "func": "def get_used_files():\n    \"\"\"Get files used by processes with name scanpy.\"\"\"\n    import psutil\n\n    loop_over_scanpy_processes = (\n        proc for proc in psutil.process_iter() if proc.name() == 'scanpy'\n    )\n    filenames = []\n    for proc in loop_over_scanpy_processes:\n        try:\n            flist = proc.open_files()\n            for nt in flist:\n                filenames.append(nt.path)\n        # This catches a race condition where a process ends\n        # before we can examine its files\n        except psutil.NoSuchProcess:\n            pass\n    return set(filenames)", "idx": 687}
{"project": "Scanpy", "commit_id": "720_scanpy_1.9.0_test_rank_genes_groups.py_test_emptycat.py", "target": 0, "func": "def test_emptycat():\n    pbmc = pbmc68k_reduced()\n    pbmc.obs['louvain'] = pbmc.obs['louvain'].cat.add_categories(['11'])\n\n    with pytest.raises(ValueError, match=rf\"Could not calculate statistics.*{'11'}\"):\n        rank_genes_groups(pbmc, groupby='louvain')", "idx": 688}
{"project": "Scanpy", "commit_id": "721_scanpy_1.9.0_test_rank_genes_groups.py_test_wilcoxon_symmetry.py", "target": 0, "func": "def test_wilcoxon_symmetry():\n    pbmc = pbmc68k_reduced()\n\n    rank_genes_groups(\n        pbmc,\n        groupby=\"bulk_labels\",\n        groups=[\"CD14+ Monocyte\", \"Dendritic\"],\n        reference=\"Dendritic\",\n        method='wilcoxon',\n        rankby_abs=True,\n    )\n    assert pbmc.uns[\"rank_genes_groups\"][\"params\"][\"use_raw\"] is True\n\n    stats_mono = (\n        rank_genes_groups_df(pbmc, group=\"CD14+ Monocyte\")\n        .drop(columns=\"names\")\n        .to_numpy()\n    )\n\n    rank_genes_groups(\n        pbmc,\n        groupby=\"bulk_labels\",\n        groups=[\"CD14+ Monocyte\", \"Dendritic\"],\n        reference=\"CD14+ Monocyte\",\n        method='wilcoxon',\n        rankby_abs=True,\n    )\n\n    stats_dend = (\n        rank_genes_groups_df(pbmc, group=\"Dendritic\").drop(columns=\"names\").to_numpy()\n    )\n\n    assert np.allclose(np.abs(stats_mono), np.abs(stats_dend))", "idx": 689}
{"project": "Scanpy", "commit_id": "722_scanpy_1.9.0_test_rank_genes_groups.py_test_wilcoxon_tie_correction.py", "target": 0, "func": "def test_wilcoxon_tie_correction(reference):\n    pbmc = pbmc68k_reduced()\n\n    groups = ['CD14+ Monocyte', 'Dendritic']\n    groupby = 'bulk_labels'\n\n    _, groups_masks = select_groups(pbmc, groups, groupby)\n\n    X = pbmc.raw.X[groups_masks[0]].toarray()\n\n    mask_rest = groups_masks[1] if reference else ~groups_masks[0]\n    Y = pbmc.raw.X[mask_rest].toarray()\n\n    # Handle scipy versions\n    if version.parse(scipy.__version__) >= version.parse(\"1.7.0\"):\n        pvals = mannwhitneyu(X, Y, use_continuity=False, alternative='two-sided').pvalue\n        pvals[np.isnan(pvals)] = 1.0\n    else:\n        # Backwards compat, to drop once we drop scipy < 1.7\n        n_genes = X.shape[1]\n        pvals = np.zeros(n_genes)\n\n        for i in range(n_genes):\n            try:\n                _, pvals[i] = mannwhitneyu(\n                    X[:, i], Y[:, i], use_continuity=False, alternative='two-sided'\n                )\n            except ValueError:\n                pvals[i] = 1\n\n    if reference:\n        ref = groups[1]\n    else:\n        ref = 'rest'\n        groups = groups[:1]\n\n    test_obj = _RankGenes(pbmc, groups, groupby, reference=ref)\n    test_obj.compute_statistics('wilcoxon', tie_correct=True)\n\n    np.testing.assert_allclose(test_obj.stats[groups[0]]['pvals'], pvals)", "idx": 690}
{"project": "Scanpy", "commit_id": "723_scanpy_1.9.0_test_rank_genes_groups_logreg.py_test_rank_genes_groups_with_renamed_categories.py", "target": 0, "func": "def test_rank_genes_groups_with_renamed_categories(method):\n    adata = sc.datasets.blobs(n_variables=4, n_centers=3, n_observations=200)\n    assert np.allclose(adata.X[1], [9.214668, -2.6487126, 4.2020774, 0.51076424])\n\n    # for method in ['logreg', 't-test']:\n\n    sc.tl.rank_genes_groups(adata, 'blobs', method=method)\n    assert adata.uns['rank_genes_groups']['names'].dtype.names == ('0', '1', '2')\n    assert adata.uns['rank_genes_groups']['names'][0].tolist() == ('1', '3', '0')\n\n    adata.rename_categories('blobs', ['Zero', 'One', 'Two'])\n    assert adata.uns['rank_genes_groups']['names'][0].tolist() == ('1', '3', '0')\n\n    sc.tl.rank_genes_groups(adata, 'blobs', method=method)\n    assert adata.uns['rank_genes_groups']['names'][0].tolist() == ('1', '3', '0')\n    assert adata.uns['rank_genes_groups']['names'].dtype.names == ('Zero', 'One', 'Two')", "idx": 691}
{"project": "Scanpy", "commit_id": "724_scanpy_1.9.0_test_rank_genes_groups_logreg.py_test_rank_genes_groups_with_renamed_categories_use_rep.py", "target": 0, "func": "def test_rank_genes_groups_with_renamed_categories_use_rep():\n    adata = sc.datasets.blobs(n_variables=4, n_centers=3, n_observations=200)\n    assert np.allclose(adata.X[1], [9.214668, -2.6487126, 4.2020774, 0.51076424])\n\n    adata.layers[\"to_test\"] = adata.X.copy()\n    adata.X = adata.X[::-1, :]\n\n    sc.tl.rank_genes_groups(\n        adata, 'blobs', method='logreg', layer=\"to_test\", use_raw=False\n    )\n    assert adata.uns['rank_genes_groups']['names'].dtype.names == ('0', '1', '2')\n    assert adata.uns['rank_genes_groups']['names'][0].tolist() == ('1', '3', '0')\n\n    sc.tl.rank_genes_groups(adata, 'blobs', method=\"logreg\")\n    assert not adata.uns['rank_genes_groups']['names'][0].tolist() == ('3', '1', '0')", "idx": 692}
{"project": "Scanpy", "commit_id": "725_scanpy_1.9.0_test_readwrite.py_test_slugify.py", "target": 0, "func": "def test_slugify(path):\n    assert _slugify(path) == 'C-foo-bar'", "idx": 693}
{"project": "Scanpy", "commit_id": "726_scanpy_1.9.0_test_read_10x.py_assert_anndata_equal.py", "target": 0, "func": "def assert_anndata_equal(a1, a2):\n    assert a1.shape == a2.shape\n    assert (a1.obs == a2.obs).all(axis=None)\n    assert (a1.var == a2.var).all(axis=None)\n    assert np.allclose(a1.X.todense(), a2.X.todense())", "idx": 694}
{"project": "Scanpy", "commit_id": "727_scanpy_1.9.0_test_read_10x.py_test_read_10x.py", "target": 0, "func": "def test_read_10x(tmp_path, mtx_path, h5_path, prefix):\n    if prefix is not None:\n        # Build files named \"prefix_XXX.xxx\" in a temporary directory.\n        mtx_path_orig = mtx_path\n        mtx_path = tmp_path / \"filtered_gene_bc_matrices_prefix\"\n        mtx_path.mkdir()\n        for item in mtx_path_orig.iterdir():\n            if item.is_file():\n                shutil.copyfile(item, mtx_path / f\"{prefix}{item.name}\")\n\n    mtx = sc.read_10x_mtx(mtx_path, var_names=\"gene_symbols\", prefix=prefix)\n    h5 = sc.read_10x_h5(h5_path)\n\n    # Drop genome column for comparing v3\n    if \"3.0.0\" in str(h5_path):\n        h5.var.drop(columns=\"genome\", inplace=True)\n\n    # Check equivalence\n    assert_anndata_equal(mtx, h5)\n\n    # Test that it can be written:\n    from_mtx_pth = tmp_path / \"from_mtx.h5ad\"\n    from_h5_pth = tmp_path / \"from_h5.h5ad\"\n\n    mtx.write(from_mtx_pth)\n    h5.write(from_h5_pth)\n\n    assert_anndata_equal(sc.read_h5ad(from_mtx_pth), sc.read_h5ad(from_h5_pth))", "idx": 695}
{"project": "Scanpy", "commit_id": "728_scanpy_1.9.0_test_read_10x.py_test_read_10x_h5_v1.py", "target": 0, "func": "def test_read_10x_h5_v1():\n    spec_genome_v1 = sc.read_10x_h5(\n        ROOT / '1.2.0' / 'filtered_gene_bc_matrices_h5.h5',\n        genome='hg19_chr21',\n    )\n    nospec_genome_v1 = sc.read_10x_h5(\n        ROOT / '1.2.0' / 'filtered_gene_bc_matrices_h5.h5'\n    )\n    assert_anndata_equal(spec_genome_v1, nospec_genome_v1)", "idx": 696}
{"project": "Scanpy", "commit_id": "729_scanpy_1.9.0_test_read_10x.py_test_read_10x_h5.py", "target": 0, "func": "def test_read_10x_h5():\n    spec_genome_v3 = sc.read_10x_h5(\n        ROOT / '3.0.0' / 'filtered_feature_bc_matrix.h5',\n        genome='GRCh38_chr21',\n    )\n    nospec_genome_v3 = sc.read_10x_h5(ROOT / '3.0.0' / 'filtered_feature_bc_matrix.h5')\n    assert_anndata_equal(spec_genome_v3, nospec_genome_v3)", "idx": 697}
{"project": "Scanpy", "commit_id": "72_scanpy_1.9.0_readwrite.py__get_filename_from_key.py", "target": 0, "func": "def _get_filename_from_key(key, ext=None) -> Path:\n    ext = settings.file_format_data if ext is None else ext\n    return settings.writedir / f'{key}.{ext}'", "idx": 698}
{"project": "Scanpy", "commit_id": "730_scanpy_1.9.0_test_read_10x.py_test_error_10x_h5_legacy.py", "target": 0, "func": "def test_error_10x_h5_legacy(tmp_path):\n    onepth = ROOT / '1.2.0' / 'filtered_gene_bc_matrices_h5.h5'\n    twopth = tmp_path / \"two_genomes.h5\"\n    with h5py.File(onepth, \"r\") as one, h5py.File(twopth, \"w\") as two:\n        one.copy(\"hg19_chr21\", two)\n        one.copy(\"hg19_chr21\", two, name=\"hg19_chr21_copy\")\n    with pytest.raises(ValueError):\n        sc.read_10x_h5(twopth)\n    sc.read_10x_h5(twopth, genome=\"hg19_chr21_copy\")", "idx": 699}
{"project": "Scanpy", "commit_id": "731_scanpy_1.9.0_test_read_10x.py_test_error_missing_genome.py", "target": 0, "func": "def test_error_missing_genome():\n    legacy_pth = ROOT / '1.2.0' / 'filtered_gene_bc_matrices_h5.h5'\n    v3_pth = ROOT / '3.0.0' / 'filtered_feature_bc_matrix.h5'\n    with pytest.raises(ValueError, match=r\".*hg19_chr21.*\"):\n        sc.read_10x_h5(legacy_pth, genome=\"not a genome\")\n    with pytest.raises(ValueError, match=r\".*GRCh38_chr21.*\"):\n        sc.read_10x_h5(v3_pth, genome=\"not a genome\")", "idx": 700}
{"project": "Scanpy", "commit_id": "732_scanpy_1.9.0_test_read_10x.py_test_read_visium_counts.py", "target": 0, "func": "def test_read_visium_counts():\n    # Test that checks the read_visium function\n    visium_pth = ROOT / '../visium_data/1.0.0'\n    spec_genome_v3 = sc.read_visium(visium_pth, genome='GRCh38')\n    nospec_genome_v3 = sc.read_visium(visium_pth)\n    assert_anndata_equal(spec_genome_v3, nospec_genome_v3)", "idx": 701}
{"project": "Scanpy", "commit_id": "733_scanpy_1.9.0_test_read_10x.py_test_10x_h5_gex.py", "target": 0, "func": "def test_10x_h5_gex():\n    # Tests that gex option doesn't, say, make the function return None\n    h5_pth = ROOT / '3.0.0' / 'filtered_feature_bc_matrix.h5'\n    assert_anndata_equal(\n        sc.read_10x_h5(h5_pth, gex_only=True), sc.read_10x_h5(h5_pth, gex_only=False)", "idx": 702}
{"project": "Scanpy", "commit_id": "734_scanpy_1.9.0_test_scaling.py_test_scale.py", "target": 0, "func": "def test_scale(typ, dtype):\n    # test AnnData arguments\n    # test scaling with default zero_center == True\n    adata0 = AnnData(typ(X), dtype=dtype)\n    sc.pp.scale(adata0)\n    assert np.allclose(csr_matrix(adata0.X).toarray(), X_centered)\n    # test scaling with explicit zero_center == True\n    adata1 = AnnData(typ(X), dtype=dtype)\n    sc.pp.scale(adata1, zero_center=True)\n    assert np.allclose(csr_matrix(adata1.X).toarray(), X_centered)\n    # test scaling with explicit zero_center == False\n    adata2 = AnnData(typ(X), dtype=dtype)\n    sc.pp.scale(adata2, zero_center=False)\n    assert np.allclose(csr_matrix(adata2.X).toarray(), X_scaled)\n    # test bare count arguments, for simplicity only with explicit copy=True\n    # test scaling with default zero_center == True\n    data0 = typ(X, dtype=dtype)\n    cdata0 = sc.pp.scale(data0, copy=True)\n    assert np.allclose(csr_matrix(cdata0).toarray(), X_centered)\n    # test scaling with explicit zero_center == True\n    data1 = typ(X, dtype=dtype)\n    cdata1 = sc.pp.scale(data1, zero_center=True, copy=True)\n    assert np.allclose(csr_matrix(cdata1).toarray(), X_centered)\n    # test scaling with explicit zero_center == False\n    data2 = typ(X, dtype=dtype)\n    cdata2 = sc.pp.scale(data2, zero_center=False, copy=True)\n    assert np.allclose(csr_matrix(cdata2).toarray(), X_scaled)", "idx": 703}
{"project": "Scanpy", "commit_id": "735_scanpy_1.9.0_test_score_genes.py__create_random_gene_names.py", "target": 0, "func": "def _create_random_gene_names(n_genes, name_length):\n    \"\"\"\n    creates a bunch of random gene names (just CAPS letters)\n    \"\"\"\n    return np.array(\n        [\n            ''.join(map(chr, np.random.randint(65, 90, name_length)))\n            for _ in range(n_genes)", "idx": 704}
{"project": "Scanpy", "commit_id": "736_scanpy_1.9.0_test_score_genes.py__create_sparse_nan_matrix.py", "target": 0, "func": "def _create_sparse_nan_matrix(rows, cols, percent_zero, percent_nan):\n    \"\"\"\n    creates a sparse matrix, with certain amounts of NaN and Zeros\n    \"\"\"\n    A = np.random.randint(0, 1000, rows * cols).reshape((rows, cols)).astype('float32')\n    maskzero = np.random.rand(rows, cols) < percent_zero\n    masknan = np.random.rand(rows, cols) < percent_nan\n    if np.any(maskzero):\n        A[maskzero] = 0\n    if np.any(masknan):\n        A[masknan] = np.nan\n    S = csr_matrix(A)\n    return S", "idx": 705}
{"project": "Scanpy", "commit_id": "737_scanpy_1.9.0_test_score_genes.py__create_adata.py", "target": 0, "func": "def _create_adata(n_obs, n_var, p_zero, p_nan):\n    \"\"\"\n    creates an AnnData with random data, sparseness and some NaN values\n    \"\"\"\n    X = _create_sparse_nan_matrix(n_obs, n_var, p_zero, p_nan)\n    adata = AnnData(X)\n    gene_names = _create_random_gene_names(n_var, name_length=6)\n    adata.var_names = gene_names\n    return adata", "idx": 706}
{"project": "Scanpy", "commit_id": "738_scanpy_1.9.0_test_score_genes.py_test_score_with_reference.py", "target": 1, "func": "def test_score_with_reference():\n    \"\"\"\n    Checks if score_genes output agrees with pre-computed reference values.\n    The reference values had been generated using the same code\n    and stored as a pickle object in ./data\n    \"\"\"\n\n    adata = paul15()\n    sc.pp.normalize_per_cell(adata, counts_per_cell_after=10000)\n    sc.pp.scale(adata)\n\n    sc.tl.score_genes(adata, gene_list=adata.var_names[:100], score_name='Test')\n    with Path(HERE, 'score_genes_reference_paul2015.pkl').open('rb') as file:\n        reference = pickle.load(file)\n    # assert np.allclose(reference, adata.obs.Test.values)\n    assert np.array_equal(reference, adata.obs.Test.values)", "idx": 707}
{"project": "Scanpy", "commit_id": "739_scanpy_1.9.0_test_score_genes.py_test_add_score.py", "target": 0, "func": "def test_add_score():\n    \"\"\"\n    check the dtype of the scores\n    check that non-existing genes get ignored\n    \"\"\"\n    # TODO: write a test that costs less resources and is more meaningful\n    adata = _create_adata(100, 1000, p_zero=0, p_nan=0)\n\n    sc.pp.normalize_per_cell(adata, counts_per_cell_after=1e4)\n    sc.pp.log1p(adata)\n\n    # the actual genes names are all 6letters\n    # create some non-estinsting names with 7 letters:\n    non_existing_genes = _create_random_gene_names(n_genes=3, name_length=7)\n    some_genes = np.r_[\n        np.unique(np.random.choice(adata.var_names, 10)), np.unique(non_existing_genes)\n    ]\n    sc.tl.score_genes(adata, some_genes, score_name='Test')\n    assert adata.obs['Test'].dtype == 'float64'", "idx": 708}
{"project": "Scanpy", "commit_id": "73_scanpy_1.9.0_readwrite.py__download.py", "target": 0, "func": "def _download(url: str, path: Path):\n    try:\n        import ipywidgets\n        from tqdm.auto import tqdm\n    except ImportError:\n        from tqdm import tqdm\n\n    from urllib.request import urlopen, Request\n    from urllib.error import URLError\n\n    blocksize = 1024 * 8\n    blocknum = 0\n\n    try:\n        req = Request(url, headers={\"User-agent\": \"scanpy-user\"})\n\n        try:\n            open_url = urlopen(req)\n        except URLError:\n            logg.warning(\n                'Failed to open the url with default certificates, trying with certifi.'\n            )\n\n            from certifi import where\n            from ssl import create_default_context\n\n            open_url = urlopen(req, context=create_default_context(cafile=where()))\n\n        with open_url as resp:\n            total = resp.info().get(\"content-length\", None)\n            with tqdm(\n                unit=\"B\",\n                unit_scale=True,\n                miniters=1,\n                unit_divisor=1024,\n                total=total if total is None else int(total),\n            ) as t, path.open(\"wb\") as f:\n                block = resp.read(blocksize)\n                while block:\n                    f.write(block)\n                    blocknum += 1\n                    t.update(len(block))\n                    block = resp.read(blocksize)\n\n    except (KeyboardInterrupt, Exception):\n        # Make sure file doesn\u2019t exist half-downloaded\n        if path.is_file():\n            path.unlink()\n        raise", "idx": 709}
{"project": "Scanpy", "commit_id": "740_scanpy_1.9.0_test_score_genes.py_test_sparse_nanmean.py", "target": 0, "func": "def test_sparse_nanmean():\n    \"\"\"\n    check that _sparse_nanmean() is equivalent to np.nanmean()\n    \"\"\"\n    from scanpy.tools._score_genes import _sparse_nanmean\n\n    R, C = 60, 50\n\n    # sparse matrix, no NaN\n    S = _create_sparse_nan_matrix(R, C, percent_zero=0.3, percent_nan=0)\n    # col/col sum\n    np.testing.assert_allclose(S.A.mean(0), np.array(_sparse_nanmean(S, 0)).flatten())\n    np.testing.assert_allclose(S.A.mean(1), np.array(_sparse_nanmean(S, 1)).flatten())\n\n    # sparse matrix with nan\n    S = _create_sparse_nan_matrix(R, C, percent_zero=0.3, percent_nan=0.3)\n    np.testing.assert_allclose(\n        np.nanmean(S.A, 1), np.array(_sparse_nanmean(S, 1)).flatten()\n    )\n    np.testing.assert_allclose(\n        np.nanmean(S.A, 0), np.array(_sparse_nanmean(S, 0)).flatten()\n    )\n\n    # edge case of only NaNs per row\n    A = np.full((10, 1), np.nan)\n\n    meanA = np.array(_sparse_nanmean(csr_matrix(A), 0)).flatten()\n    np.testing.assert_allclose(np.nanmean(A, 0), meanA)", "idx": 710}
{"project": "Scanpy", "commit_id": "741_scanpy_1.9.0_test_score_genes.py_test_sparse_nanmean_on_dense_matrix.py", "target": 0, "func": "def test_sparse_nanmean_on_dense_matrix():\n    \"\"\"\n    TypeError must be thrown when calling _sparse_nanmean with a dense matrix\n    \"\"\"\n    from scanpy.tools._score_genes import _sparse_nanmean\n\n    with pytest.raises(TypeError):\n        _sparse_nanmean(np.random.rand(4, 5), 0)", "idx": 711}
{"project": "Scanpy", "commit_id": "742_scanpy_1.9.0_test_score_genes.py_test_score_genes_sparse_vs_dense.py", "target": 0, "func": "def test_score_genes_sparse_vs_dense():\n    \"\"\"\n    score_genes() should give the same result for dense and sparse matrices\n    \"\"\"\n    adata_sparse = _create_adata(100, 1000, p_zero=0.3, p_nan=0.3)\n\n    adata_dense = adata_sparse.copy()\n    adata_dense.X = adata_dense.X.A\n\n    gene_set = adata_dense.var_names[:10]\n\n    sc.tl.score_genes(adata_sparse, gene_list=gene_set, score_name='Test')\n    sc.tl.score_genes(adata_dense, gene_list=gene_set, score_name='Test')\n\n    np.testing.assert_allclose(\n        adata_sparse.obs['Test'].values, adata_dense.obs['Test'].values", "idx": 712}
{"project": "Scanpy", "commit_id": "743_scanpy_1.9.0_test_score_genes.py_test_score_genes_deplete.py", "target": 0, "func": "def test_score_genes_deplete():\n    \"\"\"\n    deplete some cells from a set of genes.\n    their score should be <0 since the sum of markers is 0 and\n    the sum of random genes is >=0\n\n    check that for both sparse and dense matrices\n    \"\"\"\n    adata_sparse = _create_adata(100, 1000, p_zero=0.3, p_nan=0.3)\n\n    adata_dense = adata_sparse.copy()\n    adata_dense.X = adata_dense.X.A\n\n    # here's an arbitary gene set\n    gene_set = adata_dense.var_names[:10]\n\n    for adata in [adata_sparse, adata_dense]:\n        # deplete these genes in 50 cells,\n        ix_obs = np.random.choice(adata.shape[0], 50)\n        adata[ix_obs][:, gene_set].X = 0\n\n        sc.tl.score_genes(adata, gene_list=gene_set, score_name='Test')\n        scores = adata.obs['Test'].values\n\n        np.testing.assert_array_less(scores[ix_obs], 0)", "idx": 713}
{"project": "Scanpy", "commit_id": "744_scanpy_1.9.0_test_score_genes.py_test_npnanmean_vs_sparsemean.py", "target": 0, "func": "def test_npnanmean_vs_sparsemean(monkeypatch):\n    \"\"\"\n    another check that _sparsemean behaves like np.nanmean!\n\n    monkeypatch the _score_genes._sparse_nanmean function to np.nanmean\n    and check that the result is the same as the non-patched (i.e. sparse_nanmean)\n    function\n    \"\"\"\n\n    adata = _create_adata(100, 1000, p_zero=0.3, p_nan=0.3)\n    gene_set = adata.var_names[:10]\n\n    # the unpatched, i.e. _sparse_nanmean version\n    sc.tl.score_genes(adata, gene_list=gene_set, score_name='Test')\n    sparse_scores = adata.obs['Test'].values.tolist()\n\n    # now patch _sparse_nanmean by np.nanmean inside sc.tools\n    def mock_fn(x, axis):\n        return np.nanmean(x.A, axis, dtype='float64')\n\n    monkeypatch.setattr(sc.tools._score_genes, '_sparse_nanmean', mock_fn)\n    sc.tl.score_genes(adata, gene_list=gene_set, score_name='Test')\n    dense_scores = adata.obs['Test'].values\n\n    np.testing.assert_allclose(sparse_scores, dense_scores)", "idx": 714}
{"project": "Scanpy", "commit_id": "745_scanpy_1.9.0_test_score_genes.py_test_missing_genes.py", "target": 0, "func": "def test_missing_genes():\n    adata = _create_adata(100, 1000, p_zero=0, p_nan=0)\n    # These genes have a different length of name\n    non_extant_genes = _create_random_gene_names(n_genes=3, name_length=7)\n\n    with pytest.raises(ValueError):\n        sc.tl.score_genes(adata, non_extant_genes)", "idx": 715}
{"project": "Scanpy", "commit_id": "746_scanpy_1.9.0_test_score_genes.py_test_one_gene.py", "target": 0, "func": "def test_one_gene():\n    # https://github.com/theislab/scanpy/issues/1395\n    adata = _create_adata(100, 1000, p_zero=0, p_nan=0)\n    sc.tl.score_genes(adata, [adata.var_names[0]])", "idx": 716}
{"project": "Scanpy", "commit_id": "747_scanpy_1.9.0_test_score_genes.py_test_use_raw_None.py", "target": 0, "func": "def test_use_raw_None():\n    adata = _create_adata(100, 1000, p_zero=0, p_nan=0)\n    adata_raw = adata.copy()\n    adata_raw.var_names = [str(i) for i in range(adata_raw.n_vars)]\n    adata.raw = adata_raw\n\n    sc.tl.score_genes(adata, adata_raw.var_names[:3], use_raw=None)", "idx": 717}
{"project": "Scanpy", "commit_id": "748_scanpy_1.9.0_test_score_genes.py_test_invalid_gene_pool.py", "target": 0, "func": "def test_invalid_gene_pool(gene_pool):\n    adata = _create_adata(100, 1000, p_zero=0, p_nan=0)\n\n    with pytest.raises(ValueError, match=\"reference set\"):\n        sc.tl.score_genes(adata, adata.var_names[:3], gene_pool=gene_pool)", "idx": 718}
{"project": "Scanpy", "commit_id": "749_scanpy_1.9.0_test_score_genes.py_mock_fn.py", "target": 0, "func": "def mock_fn(x, axis):\n        return np.nanmean(x.A, axis, dtype='float64')", "idx": 719}
{"project": "Scanpy", "commit_id": "74_scanpy_1.9.0_readwrite.py__check_datafile_present_and_download.py", "target": 0, "func": "def _check_datafile_present_and_download(path, backup_url=None):\n    \"\"\"Check whether the file is present, otherwise download.\"\"\"\n    path = Path(path)\n    if path.is_file():\n        return True\n    if backup_url is None:\n        return False\n    logg.info(\n        f'try downloading from url\\n{backup_url}\\n'\n        '... this may take a while but only happens once'\n    )\n    if not path.parent.is_dir():\n        logg.info(f'creating directory {path.parent}/ for saving data')\n        path.parent.mkdir(parents=True)\n\n    _download(backup_url, path)\n    return True", "idx": 720}
{"project": "Scanpy", "commit_id": "750_scanpy_1.9.0_test_sim.py_test_sim_toggleswitch.py", "target": 0, "func": "def test_sim_toggleswitch():\n    adata = sc.tl.sim('toggleswitch')\n    np.allclose(adata.X, sc.datasets.toggleswitch().X, np.finfo(np.float32).eps)", "idx": 721}
{"project": "Scanpy", "commit_id": "751_scanpy_1.9.0_test_utils.py_test_descend_classes_and_funcs.py", "target": 0, "func": "def test_descend_classes_and_funcs():\n    # create module hierarchy\n    a = ModuleType('a')\n    a.b = ModuleType('a.b')\n\n    # populate with classes\n    a.A = type('A', (), {})\n    a.A.__module__ = a.__name__\n    a.b.B = type('B', (), {})\n    a.b.B.__module__ = a.b.__name__\n\n    # create a loop to check if that gets caught\n    a.b.a = a\n\n    assert {a.A, a.b.B} == set(descend_classes_and_funcs(a, 'a'))", "idx": 722}
{"project": "Scanpy", "commit_id": "752_scanpy_1.9.0_test_utils.py_test_check_nonnegative_integers.py", "target": 0, "func": "def test_check_nonnegative_integers():\n\n    X = np.random.poisson(size=(100, 100)).astype(np.float64)\n    assert check_nonnegative_integers(X) is True\n    assert check_nonnegative_integers(-X) is False\n\n    X_ = X + np.random.normal(size=(100, 100))\n    assert check_nonnegative_integers(X_) is False\n\n    X = csr_matrix(X)\n    assert check_nonnegative_integers(X) is True\n    assert check_nonnegative_integers(-X) is False\n\n    X_ = csr_matrix(X_)\n    assert check_nonnegative_integers(X_) is False", "idx": 723}
{"project": "Scanpy", "commit_id": "753_scanpy_1.9.0_test_utils.py_test_is_constant.py", "target": 0, "func": "def test_is_constant(array_type):\n    from scanpy._utils import is_constant\n\n    constant_inds = [1, 3]\n    A = np.arange(20).reshape(5, 4)\n    A[constant_inds, :] = 10\n    A = array_type(A)\n    AT = array_type(A.T)\n\n    assert not is_constant(A)\n    assert not np.any(is_constant(A, axis=0))\n    np.testing.assert_array_equal(\n        [False, True, False, True, False], is_constant(A, axis=1)\n    )\n\n    assert not is_constant(AT)\n    assert not np.any(is_constant(AT, axis=1))\n    np.testing.assert_array_equal(\n        [False, True, False, True, False], is_constant(AT, axis=0)", "idx": 724}
{"project": "Scanpy", "commit_id": "754_scanpy_1.9.0_test_harmony_integrate.py_test_harmony_integrate.py", "target": 0, "func": "def test_harmony_integrate():\n    \"\"\"\n    Test that Harmony integrate works.\n\n    This is a very simple test that just checks to see if the Harmony\n    integrate wrapper succesfully added a new field to ``adata.obsm``\n    and makes sure it has the same dimensions as the original PCA table.\n    \"\"\"\n    adata = sc.datasets.pbmc3k()\n    sc.pp.recipe_zheng17(adata)\n    sc.tl.pca(adata)\n    adata.obs['batch'] = 1350 * ['a'] + 1350 * ['b']\n    sce.pp.harmony_integrate(adata, 'batch')\n    assert adata.obsm['X_pca_harmony'].shape == adata.obsm['X_pca'].shape", "idx": 725}
{"project": "Scanpy", "commit_id": "755_scanpy_1.9.0_test_harmony_timeseries.py_test_load_timepoints_from_anndata_list.py", "target": 0, "func": "def test_load_timepoints_from_anndata_list():\n    adata_ref = sc.datasets.pbmc3k()\n    start = [596, 615, 1682, 1663, 1409, 1432]\n    adata = AnnData.concatenate(\n        *(adata_ref[i : i + 1000] for i in start),\n        join=\"outer\",\n        batch_key=\"sample\",\n        batch_categories=[f\"sa{i}_Rep{j}\" for i, j in product((1, 2, 3), (1, 2))],\n    )\n    adata.obs[\"time_points\"] = adata.obs[\"sample\"].str.split(\"_\", expand=True)[0]\n    adata.obs[\"time_points\"] = adata.obs[\"time_points\"].astype(\"category\")\n    sc.pp.normalize_total(adata, target_sum=10000)\n    sc.pp.log1p(adata)\n    sc.pp.highly_variable_genes(adata, n_top_genes=1000, subset=True)\n\n    sce.tl.harmony_timeseries(adata=adata, tp=\"time_points\", n_components=None)\n    assert all(\n        [adata.obsp['harmony_aff'].shape[0], adata.obsp['harmony_aff_aug'].shape[0]]\n    ), \"harmony_timeseries augmented affinity matrix Error!\"", "idx": 726}
{"project": "Scanpy", "commit_id": "756_scanpy_1.9.0_test_hashsolo.py_test_cell_demultiplexing.py", "target": 1, "func": "def test_cell_demultiplexing():\n    from scipy import stats\n    import random\n\n    random.seed(52)\n    signal = stats.poisson.rvs(1000, 1, 990)\n    doublet_signal = stats.poisson.rvs(1000, 1, 10)\n    x = np.reshape(stats.poisson.rvs(500, 1, 10000), (1000, 10))\n    for idx, signal_count in enumerate(signal):\n        col_pos = idx % 10\n        x[idx, col_pos] = signal_count\n\n    for idx, signal_count in enumerate(doublet_signal):\n        col_pos = (idx % 10) - 1\n        x[idx, col_pos] = signal_count\n\n    test_data = AnnData(np.random.randint(0, 100, size=x.shape), obs=x)\n    sce.pp.hashsolo(test_data, test_data.obs.columns)\n\n    doublets = [\"Doublet\"] * 10\n    classes = list(\n        np.repeat(np.arange(10), 98).reshape(98, 10, order=\"F\").ravel().astype(str)\n    )\n    negatives = [\"Negative\"] * 10\n    classification = doublets + classes + negatives\n    assert all(test_data.obs[\"Classification\"].astype(str) == classification)", "idx": 727}
{"project": "Scanpy", "commit_id": "757_scanpy_1.9.0_test_magic.py_test_magic_default.py", "target": 0, "func": "def test_magic_default():\n    A = np.array(A_list, dtype='float32')\n    adata = AnnData(A)\n    sc.external.pp.magic(adata, knn=1)\n    # check raw unchanged\n    np.testing.assert_array_equal(adata.raw.X, A)\n    # check .X changed\n    assert not np.all(adata.X == A)\n    # check .X shape unchanged\n    assert adata.X.shape == A.shape", "idx": 728}
{"project": "Scanpy", "commit_id": "758_scanpy_1.9.0_test_magic.py_test_magic_pca_only.py", "target": 0, "func": "def test_magic_pca_only():\n    A = np.array(A_list, dtype='float32')\n    # pca only\n    adata = AnnData(A)\n    n_pca = 3\n    sc.external.pp.magic(adata, knn=1, name_list='pca_only', n_pca=n_pca)\n    # check raw unchanged\n    np.testing.assert_array_equal(adata.X, A)\n    # check .X shape consistent with n_pca\n    assert adata.obsm['X_magic'].shape == (A.shape[0], n_pca)", "idx": 729}
{"project": "Scanpy", "commit_id": "759_scanpy_1.9.0_test_magic.py_test_magic_copy.py", "target": 0, "func": "def test_magic_copy():\n    A = np.array(A_list, dtype='float32')\n    adata = AnnData(A)\n    adata_copy = sc.external.pp.magic(adata, knn=1, copy=True)\n    # check adata unchanged\n    np.testing.assert_array_equal(adata.X, A)\n    # check copy raw unchanged\n    np.testing.assert_array_equal(adata_copy.raw.X, A)\n    # check .X changed\n    assert not np.all(adata_copy.X == A)\n    # check .X shape unchanged\n    assert adata_copy.X.shape == A.shape", "idx": 730}
{"project": "Scanpy", "commit_id": "75_scanpy_1.9.0_readwrite.py_is_valid_filename.py", "target": 0, "func": "def is_valid_filename(filename: Path, return_ext=False):\n    \"\"\"Check whether the argument is a filename.\"\"\"\n    ext = filename.suffixes\n\n    if len(ext) > 2:\n        logg.warning(\n            f'Your filename has more than two extensions: {ext}.\\n'\n            f'Only considering the two last: {ext[-2:]}.'\n        )\n        ext = ext[-2:]\n\n    # cases for gzipped/bzipped text files\n    if len(ext) == 2 and ext[0][1:] in text_exts and ext[1][1:] in ('gz', 'bz2'):\n        return ext[0][1:] if return_ext else True\n    elif ext and ext[-1][1:] in avail_exts:\n        return ext[-1][1:] if return_ext else True\n    elif ''.join(ext) == '.soft.gz':\n        return 'soft.gz' if return_ext else True\n    elif ''.join(ext) == '.mtx.gz':\n        return 'mtx.gz' if return_ext else True\n    elif not return_ext:\n        return False\n    raise ValueError(\n        f'''\\\n{filename!r} does not end on a valid extension.\nPlease, provide one of the available extensions.\n{avail_exts}", "idx": 731}
{"project": "Scanpy", "commit_id": "760_scanpy_1.9.0_test_palantir.py_test_palantir_core.py", "target": 0, "func": "def test_palantir_core():\n    adata = sc.datasets.pbmc3k_processed()\n\n    sce.tl.palantir(adata=adata, n_components=5, knn=30)\n    assert adata.layers['palantir_imp'].shape[0], \"palantir_imp matrix Error!\"", "idx": 732}
{"project": "Scanpy", "commit_id": "761_scanpy_1.9.0_test_phenograph.py_test_phenograph.py", "target": 0, "func": "def test_phenograph():\n    df = np.random.rand(1000, 40)\n    dframe = pd.DataFrame(df)\n    dframe.index, dframe.columns = (map(str, dframe.index), map(str, dframe.columns))\n    adata = AnnData(dframe)\n    sc.tl.pca(adata, n_comps=20)\n    sce.tl.phenograph(adata, clustering_algo=\"leiden\", k=50)\n    assert adata.obs['pheno_leiden'].shape[0], \"phenograph_Community Detection Error!\"", "idx": 733}
{"project": "Scanpy", "commit_id": "762_scanpy_1.9.0_test_sam.py_test_sam.py", "target": 0, "func": "def test_sam():\n    adata_ref = sc.datasets.pbmc3k()\n    ix = np.random.choice(adata_ref.shape[0], size=200, replace=False)\n    adata = adata_ref[ix, :].copy()\n    sc.pp.normalize_total(adata, target_sum=10000)\n    sc.pp.log1p(adata)\n    sce.tl.sam(adata, inplace=True)\n    uns_keys = list(adata.uns.keys())\n    obsm_keys = list(adata.obsm.keys())\n    assert all(['sam' in uns_keys, 'X_umap' in obsm_keys, 'neighbors' in uns_keys])", "idx": 734}
{"project": "Scanpy", "commit_id": "763_scanpy_1.9.0_test_scanorama_integrate.py_test_scanorama_integrate.py", "target": 0, "func": "def test_scanorama_integrate():\n    \"\"\"\n    Test that Scanorama integration works.\n\n    This is a very simple test that just checks to see if the Scanorama\n    integrate wrapper succesfully added a new field to ``adata.obsm``\n    and makes sure it has the same dimensions as the original PCA table.\n    \"\"\"\n    adata = sc.datasets.pbmc68k_reduced()\n    sc.tl.pca(adata)\n    adata.obs['batch'] = 350 * ['a'] + 350 * ['b']\n    sce.pp.scanorama_integrate(adata, 'batch', approx=False)\n    assert adata.obsm['X_scanorama'].shape == adata.obsm['X_pca'].shape", "idx": 735}
{"project": "Scanpy", "commit_id": "764_scanpy_1.9.0_test_scrublet.py_test_scrublet.py", "target": 0, "func": "def test_scrublet():\n    \"\"\"\n    Test that Scrublet run works.\n\n    Check that scrublet runs and detects some doublets.\n    \"\"\"\n    pytest.importorskip(\"scrublet\")\n\n    adata = sc.datasets.pbmc3k()\n    sce.pp.scrublet(adata, use_approx_neighbors=False)\n\n    # replace assertions by conditions\n    assert \"predicted_doublet\" in adata.obs.columns\n    assert \"doublet_score\" in adata.obs.columns\n\n    assert adata.obs[\"predicted_doublet\"].any(), \"Expect some doublets to be identified\"", "idx": 736}
{"project": "Scanpy", "commit_id": "765_scanpy_1.9.0_test_scrublet.py_test_scrublet_batched.py", "target": 0, "func": "def test_scrublet_batched():\n    \"\"\"\n    Test that Scrublet run works with batched data.\n\n    Check that scrublet runs and detects some doublets.\n    \"\"\"\n    pytest.importorskip(\"scrublet\")\n\n    adata = sc.datasets.pbmc3k()\n    adata.obs['batch'] = 1350 * ['a'] + 1350 * ['b']\n    split = [adata[adata.obs[\"batch\"] == x].copy() for x in (\"a\", \"b\")]\n\n    sce.pp.scrublet(adata, use_approx_neighbors=False, batch_key='batch')\n\n    # replace assertions by conditions\n    assert \"predicted_doublet\" in adata.obs.columns\n    assert \"doublet_score\" in adata.obs.columns\n\n    assert adata.obs[\"predicted_doublet\"].any(), \"Expect some doublets to be identified\"\n    assert (\n        'batches' in adata.uns['scrublet'].keys()\n    ), \"Expect .uns to contain batch info\"\n\n    # Check that results are independent\n    for s in split:\n        sce.pp.scrublet(s, use_approx_neighbors=False)\n    merged = sc.concat(split)\n\n    pd.testing.assert_frame_equal(adata.obs[merged.obs.columns], merged.obs)", "idx": 737}
{"project": "Scanpy", "commit_id": "766_scanpy_1.9.0_test_scrublet.py_test_scrublet_data.py", "target": 0, "func": "def test_scrublet_data():\n    \"\"\"\n    Test that Scrublet processing is arranged correctly.\n\n    Check that simulations run on raw data.\n    \"\"\"\n    pytest.importorskip(\"scrublet\")\n\n    random_state = 1234\n\n    # Run Scrublet and let the main function run simulations\n    adata_scrublet_auto_sim = sce.pp.scrublet(\n        sc.datasets.pbmc3k(),\n        use_approx_neighbors=False,\n        copy=True,\n        random_state=random_state,\n    )\n\n    # Now make our own simulated data so we can check the result from function\n    # is the same, and by inference that the processing steps have not been\n    # broken\n\n    # Replicate the preprocessing steps used by the main function\n\n    def preprocess_for_scrublet(adata):\n\n        adata_pp = adata.copy()\n        pp.filter_genes(adata_pp, min_cells=3)\n        pp.filter_cells(adata_pp, min_genes=3)\n        adata_pp.layers['raw'] = adata_pp.X.copy()\n        pp.normalize_total(adata_pp)\n        logged = pp.log1p(adata_pp, copy=True)\n        pp.highly_variable_genes(logged)\n        adata_pp = adata_pp[:, logged.var['highly_variable']]\n\n        return adata_pp\n\n    # Simulate doublets using the same parents\n\n    def create_sim_from_parents(adata, parents):\n\n        # Now simulate doublets based on the randomly selected parents used\n        # previously\n\n        N_sim = parents.shape[0]\n        I = sparse.coo_matrix(\n            (\n                np.ones(2 * N_sim),\n                (np.repeat(np.arange(N_sim), 2), parents.flat),\n            ),\n            (N_sim, adata_obs.n_obs),\n        )\n        X = I @ adata_obs.layers['raw']\n        return ad.AnnData(\n            X,\n            var=pd.DataFrame(index=adata_obs.var_names),\n            obs=pd.DataFrame({\"total_counts\": np.ravel(X.sum(axis=1))}),\n            obsm={\"doublet_parents\": parents.copy()},\n        )\n\n    # Preprocess the data and make the simulated doublets\n\n    adata_obs = preprocess_for_scrublet(sc.datasets.pbmc3k())\n    adata_sim = create_sim_from_parents(\n        adata_obs, adata_scrublet_auto_sim.uns['scrublet']['doublet_parents']\n    )\n\n    # Apply the same post-normalisation the Scrublet function would\n\n    pp.normalize_total(adata_obs, target_sum=1e6)\n    pp.normalize_total(adata_sim, target_sum=1e6)\n\n    adata_scrublet_manual_sim = sce.pp.scrublet(\n        adata_obs,\n        adata_sim=adata_sim,\n        use_approx_neighbors=False,\n        copy=True,\n        random_state=random_state,\n    )\n\n    # Require that the doublet scores are the same whether simulation is via\n    # the main function or manually provided\n\n    assert (\n        adata_scrublet_manual_sim.obs['doublet_score']\n        == adata_scrublet_auto_sim.obs['doublet_score']", "idx": 738}
{"project": "Scanpy", "commit_id": "767_scanpy_1.9.0_test_scrublet.py_test_scrublet_dense.py", "target": 0, "func": "def test_scrublet_dense():\n    \"\"\"\n    Test that Scrublet works for dense matrices.\n\n    Check that scrublet runs and detects some doublets when a dense matrix is supplied.\n    \"\"\"\n    pytest.importorskip(\"scrublet\")\n\n    adata = sc.datasets.paul15()[:500].copy()\n    sce.pp.scrublet(adata, use_approx_neighbors=False)\n\n    # replace assertions by conditions\n    assert \"predicted_doublet\" in adata.obs.columns\n    assert \"doublet_score\" in adata.obs.columns\n\n    assert adata.obs[\"predicted_doublet\"].any(), \"Expect some doublets to be identified\"", "idx": 739}
{"project": "Scanpy", "commit_id": "768_scanpy_1.9.0_test_scrublet.py_test_scrublet_params.py", "target": 0, "func": "def test_scrublet_params():\n    \"\"\"\n    Test that Scrublet args are passed.\n\n    Check that changes to parameters change scrublet results.\n    \"\"\"\n    pytest.importorskip(\"scrublet\")\n\n    # Reduce size of input for faster test\n    adata = sc.datasets.pbmc3k()[:500].copy()\n    sc.pp.filter_genes(adata, min_counts=100)\n\n    # Get the default output\n\n    default = sce.pp.scrublet(adata, use_approx_neighbors=False, copy=True)\n\n    test_params = {\n        'expected_doublet_rate': 0.1,\n        'synthetic_doublet_umi_subsampling': 0.8,\n        'knn_dist_metric': 'manhattan',\n        'normalize_variance': False,\n        'log_transform': True,\n        'mean_center': False,\n        'n_prin_comps': 10,\n        'n_neighbors': 2,\n        'threshold': 0.1,\n    }\n\n    # Test each parameter and make sure something changes\n\n    for param in test_params.keys():\n        test_args = {\n            'adata': adata,\n            'use_approx_neighbors': False,\n            'copy': True,\n            param: test_params[param],\n        }\n        curr = sc.external.pp.scrublet(**test_args)\n        with pytest.raises(AssertionError):\n            assert_equal(default, curr)", "idx": 740}
{"project": "Scanpy", "commit_id": "769_scanpy_1.9.0_test_scrublet.py_test_scrublet_simulate_doublets.py", "target": 0, "func": "def test_scrublet_simulate_doublets():\n    \"\"\"\n    Test that standalone Scrublet doublet simulation works.\n\n    Check that doublet simulation runs and simulates some doublets..\n    \"\"\"\n    pytest.importorskip(\"scrublet\")\n\n    adata_obs = sc.datasets.pbmc3k()\n    sc.pp.filter_genes(adata_obs, min_cells=3)\n    sc.pp.filter_cells(adata_obs, min_genes=3)\n    adata_obs.layers['raw'] = adata_obs.X\n    sc.pp.normalize_total(adata_obs)\n    logged = sc.pp.log1p(adata_obs, copy=True)\n\n    _ = sc.pp.highly_variable_genes(logged)\n    adata_obs = adata_obs[:, logged.var['highly_variable']]\n\n    adata_sim = sce.pp.scrublet_simulate_doublets(adata_obs, layer='raw')\n\n    assert 'doublet_parents' in adata_sim.obsm.keys()", "idx": 741}
{"project": "Scanpy", "commit_id": "76_scanpy_1.9.0__compat.py_pkg_metadata.py", "target": 0, "func": "def pkg_metadata(package):\n    try:\n        from importlib.metadata import metadata as m\n    except ImportError:  # < Python 3.8: Use backport module\n        from importlib_metadata import metadata as m\n    return m(package)", "idx": 742}
{"project": "Scanpy", "commit_id": "770_scanpy_1.9.0_test_scrublet.py_preprocess_for_scrublet.py", "target": 0, "func": "def preprocess_for_scrublet(adata):\n\n        adata_pp = adata.copy()\n        pp.filter_genes(adata_pp, min_cells=3)\n        pp.filter_cells(adata_pp, min_genes=3)\n        adata_pp.layers['raw'] = adata_pp.X.copy()\n        pp.normalize_total(adata_pp)\n        logged = pp.log1p(adata_pp, copy=True)\n        pp.highly_variable_genes(logged)\n        adata_pp = adata_pp[:, logged.var['highly_variable']]\n\n        return adata_pp", "idx": 743}
{"project": "Scanpy", "commit_id": "771_scanpy_1.9.0_test_scrublet.py_create_sim_from_parents.py", "target": 0, "func": "def create_sim_from_parents(adata, parents):\n\n        # Now simulate doublets based on the randomly selected parents used\n        # previously\n\n        N_sim = parents.shape[0]\n        I = sparse.coo_matrix(\n            (\n                np.ones(2 * N_sim),\n                (np.repeat(np.arange(N_sim), 2), parents.flat),\n            ),\n            (N_sim, adata_obs.n_obs),\n        )\n        X = I @ adata_obs.layers['raw']\n        return ad.AnnData(\n            X,\n            var=pd.DataFrame(index=adata_obs.var_names),\n            obs=pd.DataFrame({\"total_counts\": np.ravel(X.sum(axis=1))}),\n            obsm={\"doublet_parents\": parents.copy()},", "idx": 744}
{"project": "Scanpy", "commit_id": "772_scanpy_1.9.0_test_wishbone.py_test_run_wishbone.py", "target": 0, "func": "def test_run_wishbone():\n    adata = sc.datasets.pbmc3k()\n    sc.pp.normalize_per_cell(adata)\n    sc.pp.neighbors(adata, n_pcs=15, n_neighbors=10)\n    sc.pp.pca(adata)\n    sc.tl.tsne(adata=adata, n_pcs=5, perplexity=30)\n    sc.tl.diffmap(adata, n_comps=10)\n\n    sce.tl.wishbone(\n        adata=adata,\n        start_cell='ACAAGAGACTTATC-1',\n        components=[2, 3],\n        num_waypoints=150,\n    )\n    assert all(\n        [k in adata.obs for k in ['trajectory_wishbone', 'branch_wishbone']]\n    ), \"Run Wishbone Error!\"", "idx": 745}
{"project": "Scanpy", "commit_id": "773_scanpy_1.9.0_test_paga_paul15_subsampled.py_test_paga_paul15_subsampled.py", "target": 0, "func": "def test_paga_paul15_subsampled(image_comparer, plt):\n    save_and_compare_images = image_comparer(ROOT, FIGS, tol=25)\n\n    adata = sc.datasets.paul15()\n    sc.pp.subsample(adata, n_obs=200)\n    del adata.uns['iroot']\n    adata.X = adata.X.astype('float64')\n\n    # Preprocessing and Visualization\n    sc.pp.recipe_zheng17(adata)\n    sc.tl.pca(adata, svd_solver='arpack')\n    sc.pp.neighbors(adata, n_neighbors=4, n_pcs=20)\n    sc.tl.draw_graph(adata)\n    sc.pl.draw_graph(adata, color='paul15_clusters', legend_loc='on data')\n\n    sc.tl.diffmap(adata)\n    sc.tl.diffmap(adata)  # See #1262\n    sc.pp.neighbors(adata, n_neighbors=10, use_rep='X_diffmap')\n    sc.tl.draw_graph(adata)\n\n    sc.pl.draw_graph(adata, color='paul15_clusters', legend_loc='on data')\n\n    # TODO: skip if louvain isn't installed, needs major rework\n    pytest.importorskip(\"louvain\")\n\n    # Clustering and PAGA\n    sc.tl.louvain(adata, resolution=1.0)\n    sc.tl.paga(adata, groups='louvain')\n    # sc.pl.paga(adata, color=['louvain', 'Hba-a2', 'Elane', 'Irf8'])\n    # sc.pl.paga(adata, color=['louvain', 'Itga2b', 'Prss34'])\n\n    adata.obs['louvain_anno'] = adata.obs['louvain']\n    sc.tl.paga(adata, groups='louvain_anno')\n\n    PAGA_CONNECTIVITIES = np.array(\n        [\n            [0.0, 0.128553, 0.0, 0.07825, 0.0, 0.0, 0.238741, 0.0, 0.0, 0.657049],\n            [\n                0.128553,\n                0.0,\n                0.480676,\n                0.257505,\n                0.533036,\n                0.043871,\n                0.0,\n                0.032903,\n                0.0,\n                0.087743,\n            ],\n        ]\n    )\n\n    assert np.allclose(\n        adata.uns['paga']['connectivities'].toarray()[:2],\n        PAGA_CONNECTIVITIES,\n        atol=1e-4,\n    )\n\n    sc.pl.paga(adata, threshold=0.03)\n\n    # !!!! no clue why it doesn't produce images with the same shape\n    # save_and_compare_images('paga')\n\n    sc.tl.draw_graph(adata, init_pos='paga')\n    sc.pl.paga_compare(\n        adata,\n        threshold=0.03,\n        title='',\n        right_margin=0.2,\n        size=10,\n        edge_width_scale=0.5,\n        legend_fontsize=12,\n        fontsize=12,\n        frameon=False,\n        edges=True,\n    )\n\n    # slight deviations because of graph drawing\n    # save_and_compare_images('paga_compare')\n\n    adata.uns['iroot'] = np.flatnonzero(adata.obs['louvain_anno'] == '3')[0]\n    sc.tl.dpt(adata)\n    gene_names = [\n        'Gata2',\n        'Gata1',\n        'Klf1',\n        'Hba-a2',  # erythroid\n        'Elane',\n        'Cebpe',  # neutrophil\n        'Irf8',\n    ]  # monocyte\n\n    paths = [\n        ('erythrocytes', [3, 9, 0, 6]),\n        ('neutrophils', [3, 1, 2]),\n        ('monocytes', [3, 1, 4, 5]),\n    ]\n\n    adata.obs['distance'] = adata.obs['dpt_pseudotime']\n\n    _, axs = plt.subplots(\n        ncols=3, figsize=(6, 2.5), gridspec_kw={'wspace': 0.05, 'left': 0.12}\n    )\n    plt.subplots_adjust(left=0.05, right=0.98, top=0.82, bottom=0.2)\n    for ipath, (descr, path) in enumerate(paths):\n        _, data = sc.pl.paga_path(\n            adata,\n            path,\n            gene_names,\n            show_node_names=False,\n            ax=axs[ipath],\n            ytick_fontsize=12,\n            left_margin=0.15,\n            n_avg=50,\n            annotations=['distance'],\n            show_yticks=True if ipath == 0 else False,\n            show_colorbar=False,\n            color_map='Greys',\n            color_maps_annotations={'distance': 'viridis'},\n            title='{} path'.format(descr),\n            return_data=True,\n            show=False,\n        )\n        # add a test for this at some point\n        # data.to_csv('./write/paga_path_{}.csv'.format(descr))\n\n    save_and_compare_images('paga_path')", "idx": 746}
{"project": "Scanpy", "commit_id": "774_scanpy_1.9.0_test_pbmc3k.py_test_pbmc3k.py", "target": 0, "func": "def test_pbmc3k(image_comparer):\n    save_and_compare_images = image_comparer(ROOT, FIGS, tol=20)\n\n    adata = sc.read(\n        './data/pbmc3k_raw.h5ad', backup_url='http://falexwolf.de/data/pbmc3k_raw.h5ad'\n    )\n\n    # Preprocessing\n\n    sc.pl.highest_expr_genes(adata, n_top=20, show=False)\n    save_and_compare_images('highest_expr_genes')\n\n    sc.pp.filter_cells(adata, min_genes=200)\n    sc.pp.filter_genes(adata, min_cells=3)\n\n    mito_genes = [name for name in adata.var_names if name.startswith('MT-')]\n    # for each cell compute fraction of counts in mito genes vs. all genes\n    # the `.A1` is only necessary as X is sparse to transform to a dense array after summing\n    adata.obs['percent_mito'] = (\n        np.sum(adata[:, mito_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1\n    )\n    # add the total counts per cell as observations-annotation to adata\n    adata.obs['n_counts'] = adata.X.sum(axis=1).A1\n\n    sc.pl.violin(\n        adata,\n        ['n_genes', 'n_counts', 'percent_mito'],\n        jitter=False,\n        multi_panel=True,\n        show=False,\n    )\n    save_and_compare_images('violin')\n\n    sc.pl.scatter(adata, x='n_counts', y='percent_mito', show=False)\n    save_and_compare_images('scatter_1')\n    sc.pl.scatter(adata, x='n_counts', y='n_genes', show=False)\n    save_and_compare_images('scatter_2')\n\n    adata = adata[adata.obs['n_genes'] < 2500, :]\n    adata = adata[adata.obs['percent_mito'] < 0.05, :]\n\n    adata.raw = sc.pp.log1p(adata, copy=True)\n\n    sc.pp.normalize_per_cell(adata, counts_per_cell_after=1e4)\n\n    filter_result = sc.pp.filter_genes_dispersion(\n        adata.X,\n        min_mean=0.0125,\n        max_mean=3,\n        min_disp=0.5,\n    )\n    sc.pl.filter_genes_dispersion(filter_result, show=False)\n    save_and_compare_images('filter_genes_dispersion')\n\n    adata = adata[:, filter_result.gene_subset]\n    sc.pp.log1p(adata)\n    sc.pp.regress_out(adata, ['n_counts', 'percent_mito'])\n    sc.pp.scale(adata, max_value=10)\n\n    # PCA\n\n    sc.tl.pca(adata, svd_solver='arpack')\n    sc.pl.pca(adata, color='CST3', show=False)\n    save_and_compare_images('pca')\n\n    sc.pl.pca_variance_ratio(adata, log=True, show=False)\n    save_and_compare_images('pca_variance_ratio')\n\n    # UMAP\n\n    sc.pp.neighbors(adata, n_neighbors=10, n_pcs=40)\n    # sc.tl.umap(adata)  # umaps lead to slight variations\n\n    # sc.pl.umap(adata, color=['CST3', 'NKG7', 'PPBP'], use_raw=False, show=False)\n    # save_and_compare_images('umap_1')\n\n    # Clustering the graph\n\n    sc.tl.leiden(adata)\n    # sc.pl.umap(adata, color=['leiden', 'CST3', 'NKG7'], show=False)\n    # save_and_compare_images('umap_2')\n    sc.pl.scatter(adata, 'CST3', 'NKG7', color='leiden', show=False)\n    save_and_compare_images('scatter_3')\n\n    # Finding marker genes\n\n    sc.tl.rank_genes_groups(adata, 'leiden')\n    sc.pl.rank_genes_groups(adata, n_genes=20, sharey=False, show=False)\n    save_and_compare_images('rank_genes_groups_1')\n\n    sc.tl.rank_genes_groups(adata, 'leiden', method='logreg')\n    sc.pl.rank_genes_groups(adata, n_genes=20, sharey=False, show=False)\n    save_and_compare_images('rank_genes_groups_2')\n\n    sc.tl.rank_genes_groups(adata, 'leiden', groups=['0'], reference='1')\n    sc.pl.rank_genes_groups(adata, groups='0', n_genes=20, show=False)\n    save_and_compare_images('rank_genes_groups_3')\n\n    # gives a strange error, probably due to jitter or something\n    # sc.pl.rank_genes_groups_violin(adata, groups='0', n_genes=8)\n    # save_and_compare_images('rank_genes_groups_4')\n\n    new_cluster_names = [\n        'CD4 T cells',\n        'CD14+ Monocytes',\n        'B cells',\n        'CD8 T cells',\n        'NK cells',\n        'FCGR3A+ Monocytes',\n        'Dendritic cells',\n        'Megakaryocytes',\n    ]\n    adata.rename_categories('leiden', new_cluster_names)\n\n    # sc.pl.umap(adata, color='leiden', legend_loc='on data', title='', frameon=False, show=False)\n    # save_and_compare_images('umap_3')\n\n    sc.pl.violin(\n        adata, ['CST3', 'NKG7', 'PPBP'], groupby='leiden', rotation=90, show=False\n    )\n    save_and_compare_images('violin_2')", "idx": 747}
{"project": "Scanpy", "commit_id": "775_scanpy_1.9.0__cached_datasets.py_cached_dataset.py", "target": 0, "func": "def cached_dataset(func):\n    store = []\n\n    @wraps(func)\n    def wrapper():\n        if len(store) < 1:\n            store.append(func())\n        return store[0].copy()\n\n    return wrapper", "idx": 748}
{"project": "Scanpy", "commit_id": "776_scanpy_1.9.0__cached_datasets.py_wrapper.py", "target": 0, "func": "def wrapper():\n        if len(store) < 1:\n            store.append(func())\n        return store[0].copy()", "idx": 749}
{"project": "Scanpy", "commit_id": "777_scanpy_1.9.0__dendrogram.py_dendrogram.py", "target": 1, "func": "def dendrogram(\n    adata: AnnData,\n    groupby: str,\n    n_pcs: Optional[int] = None,\n    use_rep: Optional[str] = None,\n    var_names: Optional[Sequence[str]] = None,\n    use_raw: Optional[bool] = None,\n    cor_method: str = 'pearson',\n    linkage_method: str = 'complete',\n    optimal_ordering: bool = False,\n    key_added: Optional[str] = None,\n    inplace: bool = True,\n) -> Optional[Dict[str, Any]]:\n    \"\"\"\\\n    Computes a hierarchical clustering for the given `groupby` categories.\n\n    By default, the PCA representation is used unless `.X`\n    has less than 50 variables.\n\n    Alternatively, a list of `var_names` (e.g. genes) can be given.\n\n    Average values of either `var_names` or components are used\n    to compute a correlation matrix.\n\n    The hierarchical clustering can be visualized using\n    :func:`scanpy.pl.dendrogram` or multiple other visualizations that can\n    include a dendrogram: :func:`~scanpy.pl.matrixplot`,\n    :func:`~scanpy.pl.heatmap`, :func:`~scanpy.pl.dotplot`,\n    and :func:`~scanpy.pl.stacked_violin`.\n\n    .. note::\n        The computation of the hierarchical clustering is based on predefined\n        groups and not per cell. The correlation matrix is computed using by\n        default pearson but other methods are available.\n\n    Parameters\n    ----------\n    adata\n        Annotated data matrix\n    {n_pcs}\n    {use_rep}\n    var_names\n        List of var_names to use for computing the hierarchical clustering.\n        If `var_names` is given, then `use_rep` and `n_pcs` is ignored.\n    use_raw\n        Only when `var_names` is not None.\n        Use `raw` attribute of `adata` if present.\n    cor_method\n        correlation method to use.\n        Options are 'pearson', 'kendall', and 'spearman'\n    linkage_method\n        linkage method to use. See :func:`scipy.cluster.hierarchy.linkage`\n        for more information.\n    optimal_ordering\n        Same as the optimal_ordering argument of :func:`scipy.cluster.hierarchy.linkage`\n        which reorders the linkage matrix so that the distance between successive\n        leaves is minimal.\n    key_added\n        By default, the dendrogram information is added to\n        `.uns[f'dendrogram_{{groupby}}']`.\n        Notice that the `groupby` information is added to the dendrogram.\n    inplace\n        If `True`, adds dendrogram information to `adata.uns[key_added]`,\n        else this function returns the information.\n\n    Returns\n    -------\n    If `inplace=False`, returns dendrogram information,\n    else `adata.uns[key_added]` is updated with it.\n\n    Examples\n    --------\n    >>> import scanpy as sc\n    >>> adata = sc.datasets.pbmc68k_reduced()\n    >>> sc.tl.dendrogram(adata, groupby='bulk_labels')\n    >>> sc.pl.dendrogram(adata)\n    >>> markers = ['C1QA', 'PSAP', 'CD79A', 'CD79B', 'CST3', 'LYZ']\n    >>> sc.pl.dotplot(adata, markers, groupby='bulk_labels', dendrogram=True)\n    \"\"\"\n    if isinstance(groupby, str):\n        # if not a list, turn into a list\n        groupby = [groupby]\n    for group in groupby:\n        if group not in adata.obs_keys():\n            raise ValueError(\n                'groupby has to be a valid observation. '\n                f'Given value: {group}, valid observations: {adata.obs_keys()}'\n            )\n        if not is_categorical_dtype(adata.obs[group]):\n            raise ValueError(\n                'groupby has to be a categorical observation. '\n                f'Given value: {group}, Column type: {adata.obs[group].dtype}'\n            )\n\n    if var_names is None:\n        rep_df = pd.DataFrame(\n            _choose_representation(adata, use_rep=use_rep, n_pcs=n_pcs)\n        )\n        categorical = adata.obs[groupby[0]]\n        if len(groupby) > 1:\n            for group in groupby[1:]:\n                # create new category by merging the given groupby categories\n                categorical = (\n                    categorical.astype(str) + \"_\" + adata.obs[group].astype(str)\n                ).astype('category')\n        categorical.name = \"_\".join(groupby)\n\n        rep_df.set_index(categorical, inplace=True)\n        categories = rep_df.index.categories\n    else:\n        gene_names = adata.raw.var_names if use_raw else adata.var_names\n        from ..plotting._anndata import _prepare_dataframe\n\n        categories, rep_df = _prepare_dataframe(adata, gene_names, groupby, use_raw)\n\n    # aggregate values within categories using 'mean'\n    mean_df = rep_df.groupby(level=0).mean()\n\n    import scipy.cluster.hierarchy as sch\n    from scipy.spatial import distance\n\n    corr_matrix = mean_df.T.corr(method=cor_method)\n    corr_condensed = distance.squareform(1 - corr_matrix)\n    z_var = sch.linkage(\n        corr_condensed, method=linkage_method, optimal_ordering=optimal_ordering\n    )\n    dendro_info = sch.dendrogram(z_var, labels=list(categories), no_plot=True)\n\n    dat = dict(\n        linkage=z_var,\n        groupby=groupby,\n        use_rep=use_rep,\n        cor_method=cor_method,\n        linkage_method=linkage_method,\n        categories_ordered=dendro_info['ivl'],\n        categories_idx_ordered=dendro_info['leaves'],\n        dendrogram_info=dendro_info,\n        correlation_matrix=corr_matrix.values,\n    )\n\n    if inplace:\n        if key_added is None:\n            key_added = f'dendrogram_{\"_\".join(groupby)}'\n        logg.info(f'Storing dendrogram info using `.uns[{key_added!r}]`')\n        adata.uns[key_added] = dat\n    else:\n        return dat", "idx": 750}
{"project": "Scanpy", "commit_id": "778_scanpy_1.9.0__diffmap.py_diffmap.py", "target": 0, "func": "def diffmap(\n    adata: AnnData,\n    n_comps: int = 15,\n    neighbors_key: Optional[str] = None,\n    random_state: AnyRandom = 0,\n    copy: bool = False,\n):\n    \"\"\"\\\n    Diffusion Maps [Coifman05]_ [Haghverdi15]_ [Wolf18]_.\n\n    Diffusion maps [Coifman05]_ has been proposed for visualizing single-cell\n    data by [Haghverdi15]_. The tool uses the adapted Gaussian kernel suggested\n    by [Haghverdi16]_ in the implementation of [Wolf18]_.\n\n    The width (\"sigma\") of the connectivity kernel is implicitly determined by\n    the number of neighbors used to compute the single-cell graph in\n    :func:`~scanpy.pp.neighbors`. To reproduce the original implementation\n    using a Gaussian kernel, use `method=='gauss'` in\n    :func:`~scanpy.pp.neighbors`. To use an exponential kernel, use the default\n    `method=='umap'`. Differences between these options shouldn't usually be\n    dramatic.\n\n    Parameters\n    ----------\n    adata\n        Annotated data matrix.\n    n_comps\n        The number of dimensions of the representation.\n    neighbors_key\n        If not specified, diffmap looks .uns['neighbors'] for neighbors settings\n        and .obsp['connectivities'], .obsp['distances'] for connectivities and\n        distances respectively (default storage places for pp.neighbors).\n        If specified, diffmap looks .uns[neighbors_key] for neighbors settings and\n        .obsp[.uns[neighbors_key]['connectivities_key']],\n        .obsp[.uns[neighbors_key]['distances_key']] for connectivities and distances\n        respectively.\n    random_state\n        A numpy random seed\n    copy\n        Return a copy instead of writing to adata.\n\n    Returns\n    -------\n    Depending on `copy`, returns or updates `adata` with the following fields.\n\n    `X_diffmap` : :class:`numpy.ndarray` (`adata.obsm`)\n        Diffusion map representation of data, which is the right eigen basis of\n        the transition matrix with eigenvectors as columns.\n    `diffmap_evals` : :class:`numpy.ndarray` (`adata.uns`)\n        Array of size (number of eigen vectors).\n        Eigenvalues of transition matrix.\n\n    Notes\n    -----\n    The 0-th column in `adata.obsm[\"X_diffmap\"]` is the steady-state solution,\n    which is non-informative in diffusion maps.\n    Therefore, the first diffusion component is at index 1,\n    e.g. `adata.obsm[\"X_diffmap\"][:,1]`\n    \"\"\"\n    if neighbors_key is None:\n        neighbors_key = 'neighbors'\n\n    if neighbors_key not in adata.uns:\n        raise ValueError(\n            'You need to run `pp.neighbors` first to compute a neighborhood graph.'\n        )\n    if n_comps <= 2:\n        raise ValueError('Provide any value greater than 2 for `n_comps`. ')\n    adata = adata.copy() if copy else adata\n    _diffmap(\n        adata, n_comps=n_comps, neighbors_key=neighbors_key, random_state=random_state\n    )\n    return adata if copy else None", "idx": 751}
{"project": "Scanpy", "commit_id": "779_scanpy_1.9.0__dpt.py__diffmap.py", "target": 0, "func": "def _diffmap(adata, n_comps=15, neighbors_key=None, random_state=0):\n    start = logg.info(f'computing Diffusion Maps using n_comps={n_comps}(=n_dcs)')\n    dpt = DPT(adata, neighbors_key=neighbors_key)\n    dpt.compute_transitions()\n    dpt.compute_eigen(n_comps=n_comps, random_state=random_state)\n    adata.obsm['X_diffmap'] = dpt.eigen_basis\n    adata.uns['diffmap_evals'] = dpt.eigen_values\n    logg.info(\n        '    finished',\n        time=start,\n        deep=(\n            'added\\n'", "idx": 752}
{"project": "Scanpy", "commit_id": "77_scanpy_1.9.0__compat.py_pkg_version.py", "target": 0, "func": "def pkg_version(package):\n    try:\n        from importlib.metadata import version as v\n    except ImportError:  # < Python 3.8: Use backport module\n        from importlib_metadata import version as v\n    return version.parse(v(package))", "idx": 753}
{"project": "Scanpy", "commit_id": "780_scanpy_1.9.0__dpt.py_dpt.py", "target": 0, "func": "def dpt(\n    adata: AnnData,\n    n_dcs: int = 10,\n    n_branchings: int = 0,\n    min_group_size: float = 0.01,\n    allow_kendall_tau_shift: bool = True,\n    neighbors_key: Optional[str] = None,\n    copy: bool = False,\n) -> Optional[AnnData]:\n    \"\"\"\\\n    Infer progression of cells through geodesic distance along the graph\n    [Haghverdi16]_ [Wolf19]_.\n\n    Reconstruct the progression of a biological process from snapshot\n    data. `Diffusion Pseudotime` has been introduced by [Haghverdi16]_ and\n    implemented within Scanpy [Wolf18]_. Here, we use a further developed\n    version, which is able to deal with disconnected graphs [Wolf19]_ and can\n    be run in a `hierarchical` mode by setting the parameter\n    `n_branchings>1`. We recommend, however, to only use\n    :func:`~scanpy.tl.dpt` for computing pseudotime (`n_branchings=0`) and\n    to detect branchings via :func:`~scanpy.tl.paga`. For pseudotime, you need\n    to annotate your data with a root cell. For instance::\n\n        adata.uns['iroot'] = np.flatnonzero(adata.obs['cell_types'] == 'Stem')[0]\n\n    This requires to run :func:`~scanpy.pp.neighbors`, first. In order to\n    reproduce the original implementation of DPT, use `method=='gauss'` in\n    this. Using the default `method=='umap'` only leads to minor quantitative\n    differences, though.\n\n    .. versionadded:: 1.1\n\n    :func:`~scanpy.tl.dpt` also requires to run\n    :func:`~scanpy.tl.diffmap` first. As previously,\n    :func:`~scanpy.tl.dpt` came with a default parameter of ``n_dcs=10`` but\n    :func:`~scanpy.tl.diffmap` has a default parameter of ``n_comps=15``,\n    you need to pass ``n_comps=10`` in :func:`~scanpy.tl.diffmap` in order\n    to exactly reproduce previous :func:`~scanpy.tl.dpt` results.\n\n    Parameters\n    ----------\n    adata\n        Annotated data matrix.\n    n_dcs\n        The number of diffusion components to use.\n    n_branchings\n        Number of branchings to detect.\n    min_group_size\n        During recursive splitting of branches ('dpt groups') for `n_branchings`\n        > 1, do not consider groups that contain less than `min_group_size` data\n        points. If a float, `min_group_size` refers to a fraction of the total\n        number of data points.\n    allow_kendall_tau_shift\n        If a very small branch is detected upon splitting, shift away from\n        maximum correlation in Kendall tau criterion of [Haghverdi16]_ to\n        stabilize the splitting.\n    neighbors_key\n        If not specified, dpt looks .uns['neighbors'] for neighbors settings\n        and .obsp['connectivities'], .obsp['distances'] for connectivities and\n        distances respectively (default storage places for pp.neighbors).\n        If specified, dpt looks .uns[neighbors_key] for neighbors settings and\n        .obsp[.uns[neighbors_key]['connectivities_key']],\n        .obsp[.uns[neighbors_key]['distances_key']] for connectivities and distances\n        respectively.\n    copy\n        Copy instance before computation and return a copy.\n        Otherwise, perform computation inplace and return `None`.\n\n    Returns\n    -------\n    Depending on `copy`, returns or updates `adata` with the following fields.\n\n    If `n_branchings==0`, no field `dpt_groups` will be written.\n\n    `dpt_pseudotime` : :class:`pandas.Series` (`adata.obs`, dtype `float`)\n        Array of dim (number of samples) that stores the pseudotime of each\n        cell, that is, the DPT distance with respect to the root cell.\n    `dpt_groups` : :class:`pandas.Series` (`adata.obs`, dtype `category`)\n        Array of dim (number of samples) that stores the subgroup id ('0',\n        '1', ...) for each cell. The groups  typically correspond to\n        'progenitor cells', 'undecided cells' or 'branches' of a process.\n\n    Notes\n    -----\n    The tool is similar to the R package `destiny` of [Angerer16]_.\n    \"\"\"\n    # standard errors, warnings etc.\n    adata = adata.copy() if copy else adata\n\n    if neighbors_key is None:\n        neighbors_key = 'neighbors'\n    if neighbors_key not in adata.uns:\n        raise ValueError('You need to run `pp.neighbors` and `tl.diffmap` first.')\n    if 'iroot' not in adata.uns and 'xroot' not in adata.var:\n        logg.warning(\n            'No root cell found. To compute pseudotime, pass the index or '\n            'expression vector of a root cell, one of:\\n'\n            '    adata.uns[\\'iroot\\'] = root_cell_index\\n'\n            '    adata.var[\\'xroot\\'] = adata[root_cell_name, :].X'\n        )\n    if 'X_diffmap' not in adata.obsm.keys():\n        logg.warning(\n            'Trying to run `tl.dpt` without prior call of `tl.diffmap`. '\n            'Falling back to `tl.diffmap` with default parameters.'\n        )\n        _diffmap(adata, neighbors_key=neighbors_key)\n    # start with the actual computation\n    dpt = DPT(\n        adata,\n        n_dcs=n_dcs,\n        min_group_size=min_group_size,\n        n_branchings=n_branchings,\n        allow_kendall_tau_shift=allow_kendall_tau_shift,\n        neighbors_key=neighbors_key,\n    )\n    start = logg.info(f'computing Diffusion Pseudotime using n_dcs={n_dcs}')\n    if n_branchings > 1:\n        logg.info('    this uses a hierarchical implementation')\n    if dpt.iroot is not None:\n        dpt._set_pseudotime()  # pseudotimes are distances from root point\n        adata.uns[\n            'iroot'\n        ] = dpt.iroot  # update iroot, might have changed when subsampling, for example\n        adata.obs['dpt_pseudotime'] = dpt.pseudotime\n    # detect branchings and partition the data into segments\n    if n_branchings > 0:\n        dpt.branchings_segments()\n        adata.obs['dpt_groups'] = pd.Categorical(\n            values=dpt.segs_names.astype('U'),\n            categories=natsorted(np.array(dpt.segs_names_unique).astype('U')),\n        )\n        # the \"change points\" separate segments in the ordering above\n        adata.uns['dpt_changepoints'] = dpt.changepoints\n        # the tip points of segments\n        adata.uns['dpt_grouptips'] = dpt.segs_tips\n        # the ordering according to segments and pseudotime\n        ordering_id = np.zeros(adata.n_obs, dtype=int)\n        for count, idx in enumerate(dpt.indices):\n            ordering_id[idx] = count\n        adata.obs['dpt_order'] = ordering_id\n        adata.obs['dpt_order_indices'] = dpt.indices\n    logg.info(\n        '    finished',\n        time=start,\n        deep=(\n            'added\\n'\n            + (\n                \"    'dpt_pseudotime', the pseudotime (adata.obs)\"\n                if dpt.iroot is not None\n                else ''\n            )\n            + (\n                \"\\n    'dpt_groups', the branching subgroups of dpt (adata.obs)\"\n                \"\\n    'dpt_order', cell order (adata.obs)\"\n                if n_branchings > 0\n                else ''\n            )\n        ),\n    )\n    return adata if copy else None", "idx": 754}
{"project": "Scanpy", "commit_id": "781_scanpy_1.9.0__dpt.py___init__.py", "target": 0, "func": "def __init__(\n        self,\n        adata,\n        n_dcs=None,\n        min_group_size=0.01,\n        n_branchings=0,\n        allow_kendall_tau_shift=False,\n        neighbors_key=None,\n    ):\n        super().__init__(adata, n_dcs=n_dcs, neighbors_key=neighbors_key)\n        self.flavor = 'haghverdi16'\n        self.n_branchings = n_branchings\n        self.min_group_size = (\n            min_group_size\n            if min_group_size >= 1\n            else int(min_group_size * self._adata.shape[0])\n        )\n        self.passed_adata = adata  # just for debugging purposes\n        self.choose_largest_segment = False\n        self.allow_kendall_tau_shift = allow_kendall_tau_shift", "idx": 755}
{"project": "Scanpy", "commit_id": "782_scanpy_1.9.0__dpt.py_branchings_segments.py", "target": 0, "func": "def branchings_segments(self):\n        \"\"\"\\\n        Detect branchings and partition the data into corresponding segments.\n\n        Detect all branchings up to `n_branchings`.\n\n        Writes\n        ------\n        segs : np.ndarray\n            Array of dimension (number of segments) \u00d7 (number of data\n            points). Each row stores a mask array that defines a segment.\n        segs_tips : np.ndarray\n            Array of dimension (number of segments) \u00d7 2. Each row stores the\n            indices of the two tip points of each segment.\n        segs_names : np.ndarray\n            Array of dimension (number of data points). Stores an integer label\n            for each segment.\n        \"\"\"\n        self.detect_branchings()\n        self.postprocess_segments()\n        self.set_segs_names()\n        self.order_pseudotime()", "idx": 756}
{"project": "Scanpy", "commit_id": "783_scanpy_1.9.0__dpt.py_detect_branchings.py", "target": 0, "func": "def detect_branchings(self):\n        \"\"\"\\\n        Detect all branchings up to `n_branchings`.\n\n        Writes Attributes\n        -----------------\n        segs : np.ndarray\n            List of integer index arrays.\n        segs_tips : np.ndarray\n            List of indices of the tips of segments.\n        \"\"\"\n        logg.debug(\n            f'    detect {self.n_branchings} '\n            f'branching{\"\" if self.n_branchings == 1 else \"s\"}',\n        )\n        # a segment is a subset of points of the data set (defined by the\n        # indices of the points in the segment)\n        # initialize the search for branchings with a single segment,\n        # that is, get the indices of the whole data set\n        indices_all = np.arange(self._adata.shape[0], dtype=int)\n        # let's keep a list of segments, the first segment to add is the\n        # whole data set\n        segs = [indices_all]\n        # a segment can as well be defined by the two points that have maximal\n        # distance in the segment, the \"tips\" of the segment\n        #\n        # the rest of the points in the segment is then defined by demanding\n        # them to \"be close to the line segment that connects the tips\", that\n        # is, for such a point, the normalized added distance to both tips is\n        # smaller than one:\n        #     (D[tips[0],i] + D[tips[1],i])/D[tips[0],tips[1] < 1\n        # of course, this condition is fulfilled by the full cylindrical\n        # subspace surrounding that line segment, where the radius of the\n        # cylinder can be infinite\n        #\n        # if D denotes a euclidian distance matrix, a line segment is a linear\n        # object, and the name \"line\" is justified. if we take the\n        # diffusion-based distance matrix Dchosen, which approximates geodesic\n        # distance, with \"line\", we mean the shortest path between two points,\n        # which can be highly non-linear in the original space\n        #\n        # let us define the tips of the whole data set\n        if False:  # this is safe, but not compatible with on-the-fly computation\n            tips_all = np.array(\n                np.unravel_index(\n                    np.argmax(self.distances_dpt), self.distances_dpt.shape\n                )\n            )\n        else:\n            if self.iroot is not None:\n                tip_0 = np.argmax(self.distances_dpt[self.iroot])\n            else:\n                tip_0 = np.argmax(self.distances_dpt[0])\n            tips_all = np.array([tip_0, np.argmax(self.distances_dpt[tip_0])])\n        # we keep a list of the tips of each segment\n        segs_tips = [tips_all]\n        segs_connects = [[]]\n        segs_undecided = [True]\n        segs_adjacency = [[]]\n        logg.debug(\n            '    do not consider groups with less than '\n            f'{self.min_group_size} points for splitting'\n        )\n        for ibranch in range(self.n_branchings):\n            iseg, tips3 = self.select_segment(segs, segs_tips, segs_undecided)\n            if iseg == -1:\n                logg.debug('    partitioning converged')\n                break\n            logg.debug(\n                f'    branching {ibranch + 1}: split group {iseg}',\n            )  # [third start end]\n            # detect branching and update segs and segs_tips\n            self.detect_branching(\n                segs,\n                segs_tips,\n                segs_connects,\n                segs_undecided,\n                segs_adjacency,\n                iseg,\n                tips3,\n            )\n        # store as class members\n        self.segs = segs\n        self.segs_tips = segs_tips\n        self.segs_undecided = segs_undecided\n        # the following is a bit too much, but this allows easy storage\n        self.segs_adjacency = sp.sparse.lil_matrix((len(segs), len(segs)), dtype=float)\n        self.segs_connects = sp.sparse.lil_matrix((len(segs), len(segs)), dtype=int)\n        for i, seg_adjacency in enumerate(segs_adjacency):\n            self.segs_connects[i, seg_adjacency] = segs_connects[i]\n        for i in range(len(segs)):\n            for j in range(len(segs)):\n                self.segs_adjacency[i, j] = self.distances_dpt[\n                    self.segs_connects[i, j], self.segs_connects[j, i]\n                ]\n        self.segs_adjacency = self.segs_adjacency.tocsr()\n        self.segs_connects = self.segs_connects.tocsr()", "idx": 757}
{"project": "Scanpy", "commit_id": "784_scanpy_1.9.0__dpt.py_check_adjacency.py", "target": 0, "func": "def check_adjacency(self):\n        n_edges_per_seg = np.sum(self.segs_adjacency > 0, axis=1).A1\n        for n_edges in range(1, np.max(n_edges_per_seg) + 1):\n            for iseg in range(self.segs_adjacency.shape[0]):\n                if n_edges_per_seg[iseg] == n_edges:\n                    neighbor_segs = (  # noqa: F841  TODO Evaluate whether to assign the variable or not\n                        self.segs_adjacency[iseg].todense().A1\n                    )\n                    closest_points_other_segs = [\n                        seg[np.argmin(self.distances_dpt[self.segs_tips[iseg][0], seg])]\n                        for seg in self.segs\n                    ]\n                    seg = self.segs[iseg]\n                    closest_points_in_segs = [\n                        seg[np.argmin(self.distances_dpt[tips[0], seg])]\n                        for tips in self.segs_tips\n                    ]\n                    distance_segs = [\n                        self.distances_dpt[closest_points_other_segs[ipoint], point]\n                        for ipoint, point in enumerate(closest_points_in_segs)\n                    ]\n                    # exclude the first point, the segment itself\n                    closest_segs = np.argsort(distance_segs)[1 : n_edges + 1]\n                    # update adjacency matrix within the loop!\n                    # self.segs_adjacency[iseg, neighbor_segs > 0] = 0\n                    # self.segs_adjacency[iseg, closest_segs] = np.array(distance_segs)[closest_segs]\n                    # self.segs_adjacency[neighbor_segs > 0, iseg] = 0\n                    # self.segs_adjacency[closest_segs, iseg] = np.array(distance_segs)[closest_segs].reshape(len(closest_segs), 1)\n                    # n_edges_per_seg = np.sum(self.segs_adjacency > 0, axis=1).A1\n                    print(iseg, distance_segs, closest_segs)", "idx": 758}
{"project": "Scanpy", "commit_id": "785_scanpy_1.9.0__dpt.py_select_segment.py", "target": 0, "func": "def select_segment(self, segs, segs_tips, segs_undecided) -> Tuple[int, int]:\n        \"\"\"\\\n        Out of a list of line segments, choose segment that has the most\n        distant second data point.\n\n        Assume the distance matrix Ddiff is sorted according to seg_idcs.\n        Compute all the distances.\n\n        Returns\n        -------\n        iseg\n            Index identifying the position within the list of line segments.\n        tips3\n            Positions of tips within chosen segment.\n        \"\"\"\n        scores_tips = np.zeros((len(segs), 4))\n        allindices = np.arange(self._adata.shape[0], dtype=int)\n        for iseg, seg in enumerate(segs):\n            # do not consider too small segments\n            if segs_tips[iseg][0] == -1:\n                continue\n            # restrict distance matrix to points in segment\n            if not isinstance(self.distances_dpt, OnFlySymMatrix):\n                Dseg = self.distances_dpt[np.ix_(seg, seg)]\n            else:\n                Dseg = self.distances_dpt.restrict(seg)\n            third_maximizer = None\n            if segs_undecided[iseg]:\n                # check that none of our tips \"connects\" with a tip of the\n                # other segments\n                for jseg in range(len(segs)):\n                    if jseg != iseg:\n                        # take the inner tip, the \"second tip\" of the segment\n                        for itip in range(2):\n                            if (\n                                self.distances_dpt[\n                                    segs_tips[jseg][1], segs_tips[iseg][itip]\n                                ]\n                                < 0.5\n                                * self.distances_dpt[\n                                    segs_tips[iseg][~itip], segs_tips[iseg][itip]\n                                ]\n                            ):\n                                # logg.debug(\n                                #     '    group', iseg, 'with tip', segs_tips[iseg][itip],\n                                #     'connects with', jseg, 'with tip', segs_tips[jseg][1],\n                                # )\n                                # logg.debug('    do not use the tip for \"triangulation\"')\n                                third_maximizer = itip\n            # map the global position to the position within the segment\n            tips = [np.where(allindices[seg] == tip)[0][0] for tip in segs_tips[iseg]]\n            # find the third point on the segment that has maximal\n            # added distance from the two tip points\n            dseg = Dseg[tips[0]] + Dseg[tips[1]]\n            if not np.isfinite(dseg).any():\n                continue\n            # add this point to tips, it's a third tip, we store it at the first\n            # position in an array called tips3\n            third_tip = np.argmax(dseg)\n            if third_maximizer is not None:\n                # find a fourth point that has maximal distance to all three\n                dseg += Dseg[third_tip]\n                fourth_tip = np.argmax(dseg)\n                if fourth_tip != tips[0] and fourth_tip != third_tip:\n                    tips[1] = fourth_tip\n                    dseg -= Dseg[tips[1]]\n                else:\n                    dseg -= Dseg[third_tip]\n            tips3 = np.append(tips, third_tip)\n            # compute the score as ratio of the added distance to the third tip,\n            # to what it would be if it were on the straight line between the\n            # two first tips, given by Dseg[tips[:2]]\n            # if we did not normalize, there would be a danger of simply\n            # assigning the highest score to the longest segment\n            score = dseg[tips3[2]] / Dseg[tips3[0], tips3[1]]\n            score = (\n                len(seg) if self.choose_largest_segment else score\n            )  # simply the number of points\n            logg.debug(\n                f'    group {iseg} score {score} n_points {len(seg)} ' + '(too small)'\n                if len(seg) < self.min_group_size\n                else '',\n            )\n            if len(seg) <= self.min_group_size:\n                score = 0\n            # write result\n            scores_tips[iseg, 0] = score\n            scores_tips[iseg, 1:] = tips3\n        iseg = np.argmax(scores_tips[:, 0])\n        if scores_tips[iseg, 0] == 0:\n            return -1, None\n        tips3 = scores_tips[iseg, 1:].astype(int)\n        return iseg, tips3", "idx": 759}
{"project": "Scanpy", "commit_id": "786_scanpy_1.9.0__dpt.py_postprocess_segments.py", "target": 0, "func": "def postprocess_segments(self):\n        \"\"\"Convert the format of the segment class members.\"\"\"\n        # make segs a list of mask arrays, it's easier to store\n        # as there is a hdf5 equivalent\n        for iseg, seg in enumerate(self.segs):\n            mask = np.zeros(self._adata.shape[0], dtype=bool)\n            mask[seg] = True\n            self.segs[iseg] = mask\n        # convert to arrays\n        self.segs = np.array(self.segs)\n        self.segs_tips = np.array(self.segs_tips)", "idx": 760}
{"project": "Scanpy", "commit_id": "787_scanpy_1.9.0__dpt.py_set_segs_names.py", "target": 0, "func": "def set_segs_names(self):\n        \"\"\"Return a single array that stores integer segment labels.\"\"\"\n        segs_names = np.zeros(self._adata.shape[0], dtype=np.int8)\n        self.segs_names_unique = []\n        for iseg, seg in enumerate(self.segs):\n            segs_names[seg] = iseg\n            self.segs_names_unique.append(iseg)\n        self.segs_names = segs_names", "idx": 761}
{"project": "Scanpy", "commit_id": "788_scanpy_1.9.0__dpt.py_order_pseudotime.py", "target": 0, "func": "def order_pseudotime(self):\n        \"\"\"\\\n        Define indices that reflect segment and pseudotime order.\n\n        Writes\n        ------\n        indices : np.ndarray\n            Index array of shape n, which stores an ordering of the data points\n            with respect to increasing segment index and increasing pseudotime.\n        changepoints : np.ndarray\n            Index array of shape len(ssegs)-1, which stores the indices of\n            points where the segment index changes, with respect to the ordering\n            of indices.\n        \"\"\"\n        # within segs_tips, order tips according to pseudotime\n        if self.iroot is not None:\n            for itips, tips in enumerate(self.segs_tips):\n                if tips[0] != -1:\n                    indices = np.argsort(self.pseudotime[tips])\n                    self.segs_tips[itips] = self.segs_tips[itips][indices]\n                else:\n                    logg.debug(f'    group {itips} is very small')\n        # sort indices according to segments\n        indices = np.argsort(self.segs_names)\n        segs_names = self.segs_names[indices]\n        # find changepoints of segments\n        changepoints = np.arange(indices.size - 1)[np.diff(segs_names) == 1] + 1\n        if self.iroot is not None:\n            pseudotime = self.pseudotime[indices]\n            for iseg, seg in enumerate(self.segs):\n                # only consider one segment, it's already ordered by segment\n                seg_sorted = seg[indices]\n                # consider the pseudotime on this segment and sort them\n                seg_indices = np.argsort(pseudotime[seg_sorted])\n                # within the segment, order indices according to increasing pseudotime\n                indices[seg_sorted] = indices[seg_sorted][seg_indices]\n        # define class members\n        self.indices = indices\n        self.changepoints = changepoints", "idx": 762}
{"project": "Scanpy", "commit_id": "789_scanpy_1.9.0__dpt.py_detect_branching.py", "target": 0, "func": "def detect_branching(\n        self,\n        segs: Sequence[np.ndarray],\n        segs_tips: Sequence[np.ndarray],\n        segs_connects,\n        segs_undecided,\n        segs_adjacency,\n        iseg: int,\n        tips3: np.ndarray,\n    ):\n        \"\"\"\\\n        Detect branching on given segment.\n\n        Updates all list parameters inplace.\n\n        Call function _detect_branching and perform bookkeeping on segs and\n        segs_tips.\n\n        Parameters\n        ----------\n        segs\n            Dchosen distance matrix restricted to segment.\n        segs_tips\n            Stores all tip points for the segments in segs.\n        iseg\n            Position of segment under study in segs.\n        tips3\n            The three tip points. They form a 'triangle' that contains the data.\n        \"\"\"\n        seg = segs[iseg]\n        # restrict distance matrix to points in segment\n        if not isinstance(self.distances_dpt, OnFlySymMatrix):\n            Dseg = self.distances_dpt[np.ix_(seg, seg)]\n        else:\n            Dseg = self.distances_dpt.restrict(seg)\n        # given the three tip points and the distance matrix detect the\n        # branching on the segment, return the list ssegs of segments that\n        # are defined by splitting this segment\n        result = self._detect_branching(Dseg, tips3, seg)\n        ssegs, ssegs_tips, ssegs_adjacency, ssegs_connects, trunk = result\n        # map back to global indices\n        for iseg_new, seg_new in enumerate(ssegs):\n            ssegs[iseg_new] = seg[seg_new]\n            ssegs_tips[iseg_new] = seg[ssegs_tips[iseg_new]]\n            ssegs_connects[iseg_new] = list(seg[ssegs_connects[iseg_new]])\n        # remove previous segment\n        segs.pop(iseg)\n        segs_tips.pop(iseg)\n        # insert trunk/undecided_cells at same position\n        segs.insert(iseg, ssegs[trunk])\n        segs_tips.insert(iseg, ssegs_tips[trunk])\n        # append other segments\n        segs += [seg for iseg, seg in enumerate(ssegs) if iseg != trunk]\n        segs_tips += [\n            seg_tips for iseg, seg_tips in enumerate(ssegs_tips) if iseg != trunk\n        ]\n        if len(ssegs) == 4:\n            # insert undecided cells at same position\n            segs_undecided.pop(iseg)\n            segs_undecided.insert(iseg, True)\n        # correct edges in adjacency matrix\n        n_add = len(ssegs) - 1\n        prev_connecting_segments = segs_adjacency[iseg].copy()\n        if self.flavor == 'haghverdi16':\n            segs_adjacency += [[iseg] for i in range(n_add)]\n            segs_connects += [\n                seg_connects\n                for iseg, seg_connects in enumerate(ssegs_connects)\n                if iseg != trunk\n            ]\n            prev_connecting_points = segs_connects[  # noqa: F841  TODO Evaluate whether to assign the variable or not\n                iseg\n            ]\n            for jseg_cnt, jseg in enumerate(prev_connecting_segments):\n                iseg_cnt = 0\n                for iseg_new, seg_new in enumerate(ssegs):\n                    if iseg_new != trunk:\n                        pos = segs_adjacency[jseg].index(iseg)\n                        connection_to_iseg = segs_connects[jseg][pos]\n                        if connection_to_iseg in seg_new:\n                            kseg = len(segs) - n_add + iseg_cnt\n                            segs_adjacency[jseg][pos] = kseg\n                            pos_2 = segs_adjacency[iseg].index(jseg)\n                            segs_adjacency[iseg].pop(pos_2)\n                            idx = segs_connects[iseg].pop(pos_2)\n                            segs_adjacency[kseg].append(jseg)\n                            segs_connects[kseg].append(idx)\n                            break\n                        iseg_cnt += 1\n            segs_adjacency[iseg] += list(\n                range(len(segs_adjacency) - n_add, len(segs_adjacency))\n            )\n            segs_connects[iseg] += ssegs_connects[trunk]\n        else:\n            import networkx as nx\n\n            segs_adjacency += [[] for i in range(n_add)]\n            segs_connects += [[] for i in range(n_add)]\n            kseg_list = [iseg] + list(range(len(segs) - n_add, len(segs)))\n            for jseg in prev_connecting_segments:\n                pos = segs_adjacency[jseg].index(iseg)\n                distances = []\n                closest_points_in_jseg = []\n                closest_points_in_kseg = []\n                for kseg in kseg_list:\n                    reference_point_in_k = segs_tips[kseg][0]\n                    closest_points_in_jseg.append(\n                        segs[jseg][\n                            np.argmin(\n                                self.distances_dpt[reference_point_in_k, segs[jseg]]\n                            )\n                        ]\n                    )\n                    # do not use the tip in the large segment j, instead, use the closest point\n                    reference_point_in_j = closest_points_in_jseg[\n                        -1\n                    ]  # segs_tips[jseg][0]\n                    closest_points_in_kseg.append(\n                        segs[kseg][\n                            np.argmin(\n                                self.distances_dpt[reference_point_in_j, segs[kseg]]\n                            )\n                        ]\n                    )\n                    distances.append(\n                        self.distances_dpt[\n                            closest_points_in_jseg[-1], closest_points_in_kseg[-1]\n                        ]\n                    )\n                    # print(jseg, '(', segs_tips[jseg][0], closest_points_in_jseg[-1], ')',\n                    #       kseg, '(', segs_tips[kseg][0], closest_points_in_kseg[-1], ') :', distances[-1])\n                idx = np.argmin(distances)\n                kseg_min = kseg_list[idx]\n                segs_adjacency[jseg][pos] = kseg_min\n                segs_connects[jseg][pos] = closest_points_in_kseg[idx]\n                pos_2 = segs_adjacency[iseg].index(jseg)\n                segs_adjacency[iseg].pop(pos_2)\n                segs_connects[iseg].pop(pos_2)\n                segs_adjacency[kseg_min].append(jseg)\n                segs_connects[kseg_min].append(closest_points_in_jseg[idx])\n            # if we split two clusters, we need to check whether the new segments connect to any of the other\n            # old segments\n            # if not, we add a link between the new segments, if yes, we add two links to connect them at the\n            # correct old segments\n            do_not_attach_kseg = False\n            for kseg in kseg_list:\n                distances = []\n                closest_points_in_jseg = []\n                closest_points_in_kseg = []\n                jseg_list = [\n                    jseg\n                    for jseg in range(len(segs))\n                    if jseg != kseg and jseg not in prev_connecting_segments\n                ]\n                for jseg in jseg_list:\n                    reference_point_in_k = segs_tips[kseg][0]\n                    closest_points_in_jseg.append(\n                        segs[jseg][\n                            np.argmin(\n                                self.distances_dpt[reference_point_in_k, segs[jseg]]\n                            )\n                        ]\n                    )\n                    # do not use the tip in the large segment j, instead, use the closest point\n                    reference_point_in_j = closest_points_in_jseg[\n                        -1\n                    ]  # segs_tips[jseg][0]\n                    closest_points_in_kseg.append(\n                        segs[kseg][\n                            np.argmin(\n                                self.distances_dpt[reference_point_in_j, segs[kseg]]\n                            )\n                        ]\n                    )\n                    distances.append(\n                        self.distances_dpt[\n                            closest_points_in_jseg[-1], closest_points_in_kseg[-1]\n                        ]\n                    )\n                idx = np.argmin(distances)\n                jseg_min = jseg_list[idx]\n                if jseg_min not in kseg_list:\n                    segs_adjacency_sparse = sp.sparse.lil_matrix(\n                        (len(segs), len(segs)), dtype=float\n                    )\n                    for i, seg_adjacency in enumerate(segs_adjacency):\n                        segs_adjacency_sparse[i, seg_adjacency] = 1\n                    G = nx.Graph(segs_adjacency_sparse)\n                    paths_all = nx.single_source_dijkstra_path(G, source=kseg)\n                    if jseg_min not in paths_all:\n                        segs_adjacency[jseg_min].append(kseg)\n                        segs_connects[jseg_min].append(closest_points_in_kseg[idx])\n                        segs_adjacency[kseg].append(jseg_min)\n                        segs_connects[kseg].append(closest_points_in_jseg[idx])\n                        logg.debug(f'    attaching new segment {kseg} at {jseg_min}')\n                        # if we split the cluster, we should not attach kseg\n                        do_not_attach_kseg = True\n                    else:\n                        logg.debug(\n                            f'    cannot attach new segment {kseg} at {jseg_min} '\n                            '(would produce cycle)'\n                        )\n                        if kseg != kseg_list[-1]:\n                            logg.debug('        continue')\n                            continue\n                        else:\n                            logg.debug('        do not add another link')\n                            break\n                if jseg_min in kseg_list and not do_not_attach_kseg:\n                    segs_adjacency[jseg_min].append(kseg)\n                    segs_connects[jseg_min].append(closest_points_in_kseg[idx])\n                    segs_adjacency[kseg].append(jseg_min)\n                    segs_connects[kseg].append(closest_points_in_jseg[idx])\n                    break\n        segs_undecided += [False for i in range(n_add)]", "idx": 763}
{"project": "Scanpy", "commit_id": "78_scanpy_1.9.0__compat.py___getitem__.py", "target": 0, "func": "def __getitem__(cls, values):\n                if not isinstance(values, tuple):\n                    values = (values,)\n                return type('Literal_', (Literal,), dict(__args__=values))", "idx": 764}
{"project": "Scanpy", "commit_id": "790_scanpy_1.9.0__dpt.py__detect_branching.py", "target": 0, "func": "def _detect_branching(\n        self,\n        Dseg: np.ndarray,\n        tips: np.ndarray,\n        seg_reference=None,\n    ) -> Tuple[\n        List[np.ndarray],\n        List[np.ndarray],\n        List[List[int]],\n        List[List[int]],\n        int,\n    ]:\n        \"\"\"\\\n        Detect branching on given segment.\n\n        Call function __detect_branching three times for all three orderings of\n        tips. Points that do not belong to the same segment in all three\n        orderings are assigned to a fourth segment. The latter is, by Haghverdi\n        et al. (2016) referred to as 'undecided cells'.\n\n        Parameters\n        ----------\n        Dseg\n            Dchosen distance matrix restricted to segment.\n        tips\n            The three tip points. They form a 'triangle' that contains the data.\n\n        Returns\n        -------\n        ssegs\n            List of segments obtained from splitting the single segment defined\n            via the first two tip cells.\n        ssegs_tips\n            List of tips of segments in ssegs.\n        ssegs_adjacency\n            ?\n        ssegs_connects\n            ?\n        trunk\n            ?\n        \"\"\"\n        if self.flavor == 'haghverdi16':\n            ssegs = self._detect_branching_single_haghverdi16(Dseg, tips)\n        elif self.flavor == 'wolf17_tri':\n            ssegs = self._detect_branching_single_wolf17_tri(Dseg, tips)\n        elif self.flavor == 'wolf17_bi' or self.flavor == 'wolf17_bi_un':\n            ssegs = self._detect_branching_single_wolf17_bi(Dseg, tips)\n        else:\n            raise ValueError(\n                '`flavor` needs to be in {\"haghverdi16\", \"wolf17_tri\", \"wolf17_bi\"}.'\n            )\n        # make sure that each data point has a unique association with a segment\n        masks = np.zeros((len(ssegs), Dseg.shape[0]), dtype=bool)\n        for iseg, seg in enumerate(ssegs):\n            masks[iseg][seg] = True\n        nonunique = np.sum(masks, axis=0) > 1\n        ssegs = []\n        for iseg, mask in enumerate(masks):\n            mask[nonunique] = False\n            ssegs.append(np.arange(Dseg.shape[0], dtype=int)[mask])\n        # compute new tips within new segments\n        ssegs_tips = []\n        for inewseg, newseg in enumerate(ssegs):\n            if len(np.flatnonzero(newseg)) <= 1:\n                logg.warning(f'detected group with only {np.flatnonzero(newseg)} cells')\n            secondtip = newseg[np.argmax(Dseg[tips[inewseg]][newseg])]\n            ssegs_tips.append([tips[inewseg], secondtip])\n        undecided_cells = np.arange(Dseg.shape[0], dtype=int)[nonunique]\n        if len(undecided_cells) > 0:\n            ssegs.append(undecided_cells)\n            # establish the connecting points with the other segments\n            ssegs_connects = [[], [], [], []]\n            for inewseg, newseg_tips in enumerate(ssegs_tips):\n                reference_point = newseg_tips[0]\n                # closest cell to the new segment within undecided cells\n                closest_cell = undecided_cells[\n                    np.argmin(Dseg[reference_point][undecided_cells])\n                ]\n                ssegs_connects[inewseg].append(closest_cell)\n                # closest cell to the undecided cells within new segment\n                closest_cell = ssegs[inewseg][\n                    np.argmin(Dseg[closest_cell][ssegs[inewseg]])\n                ]\n                ssegs_connects[-1].append(closest_cell)\n            # also compute tips for the undecided cells\n            tip_0 = undecided_cells[\n                np.argmax(Dseg[undecided_cells[0]][undecided_cells])\n            ]\n            tip_1 = undecided_cells[np.argmax(Dseg[tip_0][undecided_cells])]\n            ssegs_tips.append([tip_0, tip_1])\n            ssegs_adjacency = [[3], [3], [3], [0, 1, 2]]\n            trunk = 3\n        elif len(ssegs) == 3:\n            reference_point = np.zeros(3, dtype=int)\n            reference_point[0] = ssegs_tips[0][0]\n            reference_point[1] = ssegs_tips[1][0]\n            reference_point[2] = ssegs_tips[2][0]\n            closest_points = np.zeros((3, 3), dtype=int)\n            # this is another strategy than for the undecided_cells\n            # here it's possible to use the more symmetric procedure\n            # shouldn't make much of a difference\n            closest_points[0, 1] = ssegs[1][\n                np.argmin(Dseg[reference_point[0]][ssegs[1]])\n            ]\n            closest_points[1, 0] = ssegs[0][\n                np.argmin(Dseg[reference_point[1]][ssegs[0]])\n            ]\n            closest_points[0, 2] = ssegs[2][\n                np.argmin(Dseg[reference_point[0]][ssegs[2]])\n            ]\n            closest_points[2, 0] = ssegs[0][\n                np.argmin(Dseg[reference_point[2]][ssegs[0]])\n            ]\n            closest_points[1, 2] = ssegs[2][\n                np.argmin(Dseg[reference_point[1]][ssegs[2]])\n            ]\n            closest_points[2, 1] = ssegs[1][\n                np.argmin(Dseg[reference_point[2]][ssegs[1]])\n            ]\n            added_dist = np.zeros(3)\n            added_dist[0] = (\n                Dseg[closest_points[1, 0], closest_points[0, 1]]\n                + Dseg[closest_points[2, 0], closest_points[0, 2]]\n            )\n            added_dist[1] = (\n                Dseg[closest_points[0, 1], closest_points[1, 0]]\n                + Dseg[closest_points[2, 1], closest_points[1, 2]]\n            )\n            added_dist[2] = (\n                Dseg[closest_points[1, 2], closest_points[2, 1]]\n                + Dseg[closest_points[0, 2], closest_points[2, 0]]\n            )\n            trunk = np.argmin(added_dist)\n            ssegs_adjacency = [\n                [trunk] if i != trunk else [j for j in range(3) if j != trunk]\n                for i in range(3)\n            ]\n            ssegs_connects = [\n                [closest_points[i, trunk]]\n                if i != trunk\n                else [closest_points[trunk, j] for j in range(3) if j != trunk]\n                for i in range(3)\n            ]\n        else:\n            trunk = 0\n            ssegs_adjacency = [[1], [0]]\n            reference_point_in_0 = ssegs_tips[0][0]\n            closest_point_in_1 = ssegs[1][\n                np.argmin(Dseg[reference_point_in_0][ssegs[1]])\n            ]\n            reference_point_in_1 = closest_point_in_1  # ssegs_tips[1][0]\n            closest_point_in_0 = ssegs[0][\n                np.argmin(Dseg[reference_point_in_1][ssegs[0]])\n            ]\n            ssegs_connects = [[closest_point_in_1], [closest_point_in_0]]\n        return ssegs, ssegs_tips, ssegs_adjacency, ssegs_connects, trunk", "idx": 765}
{"project": "Scanpy", "commit_id": "791_scanpy_1.9.0__dpt.py__detect_branching_single_haghverdi16.py", "target": 0, "func": "def _detect_branching_single_haghverdi16(self, Dseg, tips):\n        \"\"\"Detect branching on given segment.\"\"\"\n        # compute branchings using different starting points the first index of\n        # tips is the starting point for the other two, the order does not\n        # matter\n        ssegs = []\n        # permutations of tip cells\n        ps = [\n            [0, 1, 2],  # start by computing distances from the first tip\n            [1, 2, 0],  #             -\"-                       second tip\n            [2, 0, 1],  #             -\"-                       third tip\n        ]\n        for i, p in enumerate(ps):\n            ssegs.append(self.__detect_branching_haghverdi16(Dseg, tips[p]))\n        return ssegs", "idx": 766}
{"project": "Scanpy", "commit_id": "792_scanpy_1.9.0__dpt.py__detect_branching_single_wolf17_tri.py", "target": 0, "func": "def _detect_branching_single_wolf17_tri(self, Dseg, tips):\n        # all pairwise distances\n        dist_from_0 = Dseg[tips[0]]\n        dist_from_1 = Dseg[tips[1]]\n        dist_from_2 = Dseg[tips[2]]\n        closer_to_0_than_to_1 = dist_from_0 < dist_from_1\n        closer_to_0_than_to_2 = dist_from_0 < dist_from_2\n        closer_to_1_than_to_2 = dist_from_1 < dist_from_2\n        masks = np.zeros((2, Dseg.shape[0]), dtype=bool)\n        masks[0] = closer_to_0_than_to_1\n        masks[1] = closer_to_0_than_to_2\n        segment_0 = np.sum(masks, axis=0) == 2\n        masks = np.zeros((2, Dseg.shape[0]), dtype=bool)\n        masks[0] = ~closer_to_0_than_to_1\n        masks[1] = closer_to_1_than_to_2\n        segment_1 = np.sum(masks, axis=0) == 2\n        masks = np.zeros((2, Dseg.shape[0]), dtype=bool)\n        masks[0] = ~closer_to_0_than_to_2\n        masks[1] = ~closer_to_1_than_to_2\n        segment_2 = np.sum(masks, axis=0) == 2\n        ssegs = [segment_0, segment_1, segment_2]\n        return ssegs", "idx": 767}
{"project": "Scanpy", "commit_id": "793_scanpy_1.9.0__dpt.py__detect_branching_single_wolf17_bi.py", "target": 0, "func": "def _detect_branching_single_wolf17_bi(self, Dseg, tips):\n        dist_from_0 = Dseg[tips[0]]\n        dist_from_1 = Dseg[tips[1]]\n        closer_to_0_than_to_1 = dist_from_0 < dist_from_1\n        ssegs = [closer_to_0_than_to_1, ~closer_to_0_than_to_1]\n        return ssegs", "idx": 768}
{"project": "Scanpy", "commit_id": "794_scanpy_1.9.0__dpt.py___detect_branching_haghverdi16.py", "target": 0, "func": "def __detect_branching_haghverdi16(\n        self, Dseg: np.ndarray, tips: np.ndarray\n    ) -> np.ndarray:\n        \"\"\"\\\n        Detect branching on given segment.\n\n        Compute point that maximizes kendall tau correlation of the sequences of\n        distances to the second and the third tip, respectively, when 'moving\n        away' from the first tip: tips[0]. 'Moving away' means moving in the\n        direction of increasing distance from the first tip.\n\n        Parameters\n        ----------\n        Dseg\n            Dchosen distance matrix restricted to segment.\n        tips\n            The three tip points. They form a 'triangle' that contains the data.\n\n        Returns\n        -------\n        Segments obtained from \"splitting away the first tip cell\".\n        \"\"\"\n        # sort distance from first tip point\n        # then the sequence of distances Dseg[tips[0]][idcs] increases\n        idcs = np.argsort(Dseg[tips[0]])\n        # consider now the sequence of distances from the other\n        # two tip points, which only increase when being close to `tips[0]`\n        # where they become correlated\n        # at the point where this happens, we define a branching point\n        if True:\n            imax = self.kendall_tau_split(\n                Dseg[tips[1]][idcs],\n                Dseg[tips[2]][idcs],\n            )\n        if False:\n            # if we were in euclidian space, the following should work\n            # as well, but here, it doesn't because the scales in Dseg are\n            # highly different, one would need to write the following equation\n            # in terms of an ordering, such as exploited by the kendall\n            # correlation method above\n            imax = np.argmin(\n                Dseg[tips[0]][idcs] + Dseg[tips[1]][idcs] + Dseg[tips[2]][idcs]\n            )\n        # init list to store new segments\n        ssegs = []  # noqa: F841  # TODO Look into this\n        # first new segment: all points until, but excluding the branching point\n        # increasing the following slightly from imax is a more conservative choice\n        # as the criterion based on normalized distances, which follows below,\n        # is less stable\n        if imax > 0.95 * len(idcs) and self.allow_kendall_tau_shift:\n            # if \"everything\" is correlated (very large value of imax), a more\n            # conservative choice amounts to reducing this\n            logg.warning(\n                'shifting branching point away from maximal kendall-tau '\n                'correlation (suppress this with `allow_kendall_tau_shift=False`)'\n            )\n            ibranch = int(0.95 * imax)\n        else:\n            # otherwise, a more conservative choice is the following\n            ibranch = imax + 1\n        return idcs[:ibranch]", "idx": 769}
{"project": "Scanpy", "commit_id": "795_scanpy_1.9.0__dpt.py_kendall_tau_split.py", "target": 0, "func": "def kendall_tau_split(self, a, b) -> int:\n        \"\"\"Return splitting index that maximizes correlation in the sequences.\n\n        Compute difference in Kendall tau for all splitted sequences.\n\n        For each splitting index i, compute the difference of the two\n        correlation measures kendalltau(a[:i], b[:i]) and\n        kendalltau(a[i:], b[i:]).\n\n        Returns the splitting index that maximizes\n            kendalltau(a[:i], b[:i]) - kendalltau(a[i:], b[i:])\n\n        Parameters\n        ----------\n        a, b : np.ndarray\n            One dimensional sequences.\n\n        Returns\n        -------\n        Splitting index according to above description.\n        \"\"\"\n        if a.size != b.size:\n            raise ValueError('a and b need to have the same size')\n        if a.ndim != b.ndim != 1:\n            raise ValueError('a and b need to be one-dimensional arrays')\n        import scipy as sp\n\n        min_length = 5\n        n = a.size\n        idx_range = np.arange(min_length, a.size - min_length - 1, dtype=int)\n        corr_coeff = np.zeros(idx_range.size)\n        pos_old = sp.stats.kendalltau(a[:min_length], b[:min_length])[0]\n        neg_old = sp.stats.kendalltau(a[min_length:], b[min_length:])[0]\n        for ii, i in enumerate(idx_range):\n            if True:\n                # compute differences in concordance when adding a[i] and b[i]\n                # to the first subsequence, and removing these elements from\n                # the second subsequence\n                diff_pos, diff_neg = self._kendall_tau_diff(a, b, i)\n                pos = pos_old + self._kendall_tau_add(i, diff_pos, pos_old)\n                neg = neg_old + self._kendall_tau_subtract(n - i, diff_neg, neg_old)\n                pos_old = pos\n                neg_old = neg\n            if False:\n                # computation using sp.stats.kendalltau, takes much longer!\n                # just for debugging purposes\n                pos = sp.stats.kendalltau(a[: i + 1], b[: i + 1])[0]\n                neg = sp.stats.kendalltau(a[i + 1 :], b[i + 1 :])[0]\n            if False:\n                # the following is much slower than using sp.stats.kendalltau,\n                # it is only good for debugging because it allows to compute the\n                # tau-a version, which does not account for ties, whereas\n                # sp.stats.kendalltau computes tau-b version, which accounts for\n                # ties\n                pos = sp.stats.mstats.kendalltau(a[:i], b[:i], use_ties=False)[0]\n                neg = sp.stats.mstats.kendalltau(a[i:], b[i:], use_ties=False)[0]\n            corr_coeff[ii] = pos - neg\n        iimax = np.argmax(corr_coeff)\n        imax = min_length + iimax\n        corr_coeff_max = corr_coeff[iimax]\n        if corr_coeff_max < 0.3:\n            logg.debug('    is root itself, never obtain significant correlation')\n        return imax", "idx": 770}
{"project": "Scanpy", "commit_id": "796_scanpy_1.9.0__dpt.py__kendall_tau_add.py", "target": 0, "func": "def _kendall_tau_add(self, len_old: int, diff_pos: int, tau_old: float):\n        \"\"\"Compute Kendall tau delta.\n\n        The new sequence has length len_old + 1.\n\n        Parameters\n        ----------\n        len_old\n            The length of the old sequence, used to compute tau_old.\n        diff_pos\n            Difference between concordant and non-concordant pairs.\n        tau_old\n            Kendall rank correlation of the old sequence.\n        \"\"\"\n        return 2.0 / (len_old + 1) * (float(diff_pos) / len_old - tau_old)", "idx": 771}
{"project": "Scanpy", "commit_id": "797_scanpy_1.9.0__dpt.py__kendall_tau_subtract.py", "target": 0, "func": "def _kendall_tau_subtract(self, len_old: int, diff_neg: int, tau_old: float):\n        \"\"\"Compute Kendall tau delta.\n\n        The new sequence has length len_old - 1.\n\n        Parameters\n        ----------\n        len_old\n            The length of the old sequence, used to compute tau_old.\n        diff_neg\n            Difference between concordant and non-concordant pairs.\n        tau_old\n            Kendall rank correlation of the old sequence.\n        \"\"\"\n        return 2.0 / (len_old - 2) * (-float(diff_neg) / (len_old - 1) + tau_old)", "idx": 772}
{"project": "Scanpy", "commit_id": "798_scanpy_1.9.0__dpt.py__kendall_tau_diff.py", "target": 0, "func": "def _kendall_tau_diff(self, a: np.ndarray, b: np.ndarray, i) -> Tuple[int, int]:\n        \"\"\"Compute difference in concordance of pairs in split sequences.\n\n        Consider splitting a and b at index i.\n\n        Parameters\n        ----------\n        a\n            ?\n        b\n            ?\n\n        Returns\n        -------\n        diff_pos\n            Difference between concordant pairs for both subsequences.\n        diff_neg\n            Difference between non-concordant pairs for both subsequences.\n        \"\"\"\n        # compute ordering relation of the single points a[i] and b[i]\n        # with all previous points of the sequences a and b, respectively\n        a_pos = np.zeros(a[:i].size, dtype=int)\n        a_pos[a[:i] > a[i]] = 1\n        a_pos[a[:i] < a[i]] = -1\n        b_pos = np.zeros(b[:i].size, dtype=int)\n        b_pos[b[:i] > b[i]] = 1\n        b_pos[b[:i] < b[i]] = -1\n        diff_pos = np.dot(a_pos, b_pos).astype(float)\n\n        # compute ordering relation of the single points a[i] and b[i]\n        # with all later points of the sequences\n        a_neg = np.zeros(a[i:].size, dtype=int)\n        a_neg[a[i:] > a[i]] = 1\n        a_neg[a[i:] < a[i]] = -1\n        b_neg = np.zeros(b[i:].size, dtype=int)\n        b_neg[b[i:] > b[i]] = 1\n        b_neg[b[i:] < b[i]] = -1\n        diff_neg = np.dot(a_neg, b_neg)\n\n        return diff_pos, diff_neg", "idx": 773}
{"project": "Scanpy", "commit_id": "799_scanpy_1.9.0__draw_graph.py_draw_graph.py", "target": 0, "func": "def draw_graph(\n    adata: AnnData,\n    layout: _Layout = 'fa',\n    init_pos: Union[str, bool, None] = None,\n    root: Optional[int] = None,\n    random_state: AnyRandom = 0,\n    n_jobs: Optional[int] = None,\n    adjacency: Optional[spmatrix] = None,\n    key_added_ext: Optional[str] = None,\n    neighbors_key: Optional[str] = None,\n    obsp: Optional[str] = None,\n    copy: bool = False,\n    **kwds,\n):\n    \"\"\"\\\n    Force-directed graph drawing [Islam11]_ [Jacomy14]_ [Chippada18]_.\n\n    An alternative to tSNE that often preserves the topology of the data\n    better. This requires to run :func:`~scanpy.pp.neighbors`, first.\n\n    The default layout ('fa', `ForceAtlas2`) [Jacomy14]_ uses the package |fa2|_\n    [Chippada18]_, which can be installed via `pip install fa2`.\n\n    `Force-directed graph drawing`_ describes a class of long-established\n    algorithms for visualizing graphs.\n    It has been suggested for visualizing single-cell data by [Islam11]_.\n    Many other layouts as implemented in igraph [Csardi06]_ are available.\n    Similar approaches have been used by [Zunder15]_ or [Weinreb17]_.\n\n    .. |fa2| replace:: `fa2`\n    .. _fa2: https://github.com/bhargavchippada/forceatlas2\n    .. _Force-directed graph drawing: https://en.wikipedia.org/wiki/Force-directed_graph_drawing\n\n    Parameters\n    ----------\n    adata\n        Annotated data matrix.\n    layout\n        'fa' (`ForceAtlas2`) or any valid `igraph layout\n        <http://igraph.org/c/doc/igraph-Layout.html>`__. Of particular interest\n        are 'fr' (Fruchterman Reingold), 'grid_fr' (Grid Fruchterman Reingold,\n        faster than 'fr'), 'kk' (Kamadi Kawai', slower than 'fr'), 'lgl' (Large\n        Graph, very fast), 'drl' (Distributed Recursive Layout, pretty fast) and\n        'rt' (Reingold Tilford tree layout).\n    root\n        Root for tree layouts.\n    random_state\n        For layouts with random initialization like 'fr', change this to use\n        different intial states for the optimization. If `None`, no seed is set.\n    adjacency\n        Sparse adjacency matrix of the graph, defaults to neighbors connectivities.\n    key_added_ext\n        By default, append `layout`.\n    proceed\n        Continue computation, starting off with 'X_draw_graph_`layout`'.\n    init_pos\n        `'paga'`/`True`, `None`/`False`, or any valid 2d-`.obsm` key.\n        Use precomputed coordinates for initialization.\n        If `False`/`None` (the default), initialize randomly.\n    neighbors_key\n        If not specified, draw_graph looks .obsp['connectivities'] for connectivities\n        (default storage place for pp.neighbors).\n        If specified, draw_graph looks\n        .obsp[.uns[neighbors_key]['connectivities_key']] for connectivities.\n    obsp\n        Use .obsp[obsp] as adjacency. You can't specify both\n        `obsp` and `neighbors_key` at the same time.\n    copy\n        Return a copy instead of writing to adata.\n    **kwds\n        Parameters of chosen igraph layout. See e.g. `fruchterman-reingold`_\n        [Fruchterman91]_. One of the most important ones is `maxiter`.\n\n        .. _fruchterman-reingold: http://igraph.org/python/doc/igraph.Graph-class.html#layout_fruchterman_reingold\n\n    Returns\n    -------\n    Depending on `copy`, returns or updates `adata` with the following field.\n\n    **X_draw_graph_layout** : `adata.obsm`\n        Coordinates of graph layout. E.g. for layout='fa' (the default),\n        the field is called 'X_draw_graph_fa'\n    \"\"\"\n    start = logg.info(f'drawing single-cell graph using layout {layout!r}')\n    if layout not in _LAYOUTS:\n        raise ValueError(f'Provide a valid layout, one of {_LAYOUTS}.')\n    adata = adata.copy() if copy else adata\n    if adjacency is None:\n        adjacency = _choose_graph(adata, obsp, neighbors_key)\n    # init coordinates\n    if init_pos in adata.obsm.keys():\n        init_coords = adata.obsm[init_pos]\n    elif init_pos == 'paga' or init_pos:\n        init_coords = get_init_pos_from_paga(\n            adata,\n            adjacency,\n            random_state=random_state,\n            neighbors_key=neighbors_key,\n            obsp=obsp,\n        )\n    else:\n        np.random.seed(random_state)\n        init_coords = np.random.random((adjacency.shape[0], 2))\n    # see whether fa2 is installed\n    if layout == 'fa':\n        try:\n            from fa2 import ForceAtlas2\n        except ImportError:\n            logg.warning(\n                \"Package 'fa2' is not installed, falling back to layout 'fr'.\"\n                \"To use the faster and better ForceAtlas2 layout, \"\n                \"install package 'fa2' (`pip install fa2`).\"\n            )\n            layout = 'fr'\n    # actual drawing\n    if layout == 'fa':\n        forceatlas2 = ForceAtlas2(\n            # Behavior alternatives\n            outboundAttractionDistribution=False,  # Dissuade hubs\n            linLogMode=False,  # NOT IMPLEMENTED\n            adjustSizes=False,  # Prevent overlap (NOT IMPLEMENTED)\n            edgeWeightInfluence=1.0,\n            # Performance\n            jitterTolerance=1.0,  # Tolerance\n            barnesHutOptimize=True,\n            barnesHutTheta=1.2,\n            multiThreaded=False,  # NOT IMPLEMENTED\n            # Tuning\n            scalingRatio=2.0,\n            strongGravityMode=False,\n            gravity=1.0,\n            # Log\n            verbose=False,\n        )\n        if 'maxiter' in kwds:\n            iterations = kwds['maxiter']\n        elif 'iterations' in kwds:\n            iterations = kwds['iterations']\n        else:\n            iterations = 500\n        positions = forceatlas2.forceatlas2(\n            adjacency, pos=init_coords, iterations=iterations\n        )\n        positions = np.array(positions)\n    else:\n        # igraph doesn't use numpy seed\n        random.seed(random_state)\n\n        g = _utils.get_igraph_from_adjacency(adjacency)\n        if layout in {'fr', 'drl', 'kk', 'grid_fr'}:\n            ig_layout = g.layout(layout, seed=init_coords.tolist(), **kwds)\n        elif 'rt' in layout:\n            if root is not None:\n                root = [root]\n            ig_layout = g.layout(layout, root=root, **kwds)\n        else:\n            ig_layout = g.layout(layout, **kwds)\n        positions = np.array(ig_layout.coords)\n    adata.uns['draw_graph'] = {}\n    adata.uns['draw_graph']['params'] = dict(layout=layout, random_state=random_state)\n    key_added = f'X_draw_graph_{key_added_ext or layout}'\n    adata.obsm[key_added] = positions\n    logg.info(\n        '    finished',\n        time=start,\n        deep=f'added\\n    {key_added!r}, graph_drawing coordinates (adata.obsm)',\n    )\n    return adata if copy else None", "idx": 774}
{"project": "Scanpy", "commit_id": "79_scanpy_1.9.0__metadata.py_refresh_entry_points.py", "target": 0, "func": "def refresh_entry_points():\n    \"\"\"\\\n    Under some circumstances, (e.g. when installing a PEP 517 package via pip),\n    pkg_resources.working_set.entries is stale. This tries to fix that.\n    See https://github.com/pypa/setuptools_scm/issues/513\n    \"\"\"\n    try:\n        import sys\n        import pkg_resources\n\n        ws: pkg_resources.WorkingSet = pkg_resources.working_set\n        for entry in sys.path:\n            ws.add_entry(entry)\n    except Exception:\n        pass", "idx": 775}
{"project": "Scanpy", "commit_id": "7_scanpy_1.9.0_conf.py_setup.py", "target": 0, "func": "def setup(app):\n    app.warningiserror = on_rtd", "idx": 776}
{"project": "Scanpy", "commit_id": "800_scanpy_1.9.0__embedding_density.py__calc_density.py", "target": 0, "func": "def _calc_density(x: np.ndarray, y: np.ndarray):\n    \"\"\"\\\n    Calculates the density of points in 2 dimensions.\n    \"\"\"\n    from scipy.stats import gaussian_kde\n\n    # Calculate the point density\n    xy = np.vstack([x, y])\n    z = gaussian_kde(xy)(xy)\n\n    min_z = np.min(z)\n    max_z = np.max(z)\n\n    # Scale between 0 and 1\n    scaled_z = (z - min_z) / (max_z - min_z)\n\n    return scaled_z", "idx": 777}
{"project": "Scanpy", "commit_id": "801_scanpy_1.9.0__embedding_density.py_embedding_density.py", "target": 0, "func": "def embedding_density(\n    adata: AnnData,\n    # there is no asterisk here for backward compat (previously, there was)\n    basis: str = 'umap',  # was positional before 1.4.5\n    groupby: Optional[str] = None,\n    key_added: Optional[str] = None,\n    components: Union[str, Sequence[str]] = None,\n) -> None:\n    \"\"\"\\\n    Calculate the density of cells in an embedding (per condition).\n\n    Gaussian kernel density estimation is used to calculate the density of\n    cells in an embedded space. This can be performed per category over a\n    categorical cell annotation. The cell density can be plotted using the\n    `pl.embedding_density` function.\n\n    Note that density values are scaled to be between 0 and 1. Thus, the\n    density value at each cell is only comparable to densities in\n    the same category.\n\n    Beware that the KDE estimate used (`scipy.stats.gaussian_kde`) becomes\n    unreliable if you don't have enough cells in a category.\n\n    This function was written by Sophie Tritschler and implemented into\n    Scanpy by Malte Luecken.\n\n    Parameters\n    ----------\n    adata\n        The annotated data matrix.\n    basis\n        The embedding over which the density will be calculated. This embedded\n        representation should be found in `adata.obsm['X_[basis]']``.\n    groupby\n        Key for categorical observation/cell annotation for which densities\n        are calculated per category.\n    key_added\n        Name of the `.obs` covariate that will be added with the density\n        estimates.\n    components\n        The embedding dimensions over which the density should be calculated.\n        This is limited to two components.\n\n    Returns\n    -------\n    Updates `adata.obs` with an additional field specified by the `key_added`\n    parameter. This parameter defaults to `[basis]_density_[groupby]`, where\n    `[basis]` is one of `umap`, `diffmap`, `pca`, `tsne`, or `draw_graph_fa`\n    and `[groupby]` denotes the parameter input.\n    Updates `adata.uns` with an additional field `[key_added]_params`.\n\n    Examples\n    --------\n\n    .. plot::\n        :context: close-figs\n\n        import scanpy as sc\n        adata = sc.datasets.pbmc68k_reduced()\n        sc.tl.umap(adata)\n        sc.tl.embedding_density(adata, basis='umap', groupby='phase')\n        sc.pl.embedding_density(\n            adata, basis='umap', key='umap_density_phase', group='G1'\n        )\n\n    .. plot::\n        :context: close-figs\n\n        sc.pl.embedding_density(\n            adata, basis='umap', key='umap_density_phase', group='S'\n        )\n\n    .. currentmodule:: scanpy\n\n    See also\n    --------\n    pl.embedding_density\n    \"\"\"\n    # to ensure that newly created covariates are categorical\n    # to test for category numbers\n    sanitize_anndata(adata)\n\n    logg.info(f'computing density on {basis!r}')\n\n    # Test user inputs\n    basis = basis.lower()\n\n    if basis == 'fa':\n        basis = 'draw_graph_fa'\n\n    if f'X_{basis}' not in adata.obsm_keys():\n        raise ValueError(\n            \"Cannot find the embedded representation \"\n            f\"`adata.obsm['X_{basis}']`. Compute the embedding first.\"\n        )\n\n    if components is None:\n        components = '1,2'\n    if isinstance(components, str):\n        components = components.split(',')\n    components = np.array(components).astype(int) - 1\n\n    if len(components) != 2:\n        raise ValueError('Please specify exactly 2 components, or `None`.')\n\n    if basis == 'diffmap':\n        components += 1\n\n    if groupby is not None:\n        if groupby not in adata.obs:\n            raise ValueError(f'Could not find {groupby!r} `.obs` column.')\n\n        if adata.obs[groupby].dtype.name != 'category':\n            raise ValueError(f'{groupby!r} column does not contain categorical data')\n\n    # Define new covariate name\n    if key_added is not None:\n        density_covariate = key_added\n    elif groupby is not None:\n        density_covariate = f'{basis}_density_{groupby}'\n    else:\n        density_covariate = f'{basis}_density'\n\n    # Calculate the densities over each category in the groupby column\n    if groupby is not None:\n        categories = adata.obs[groupby].cat.categories\n\n        density_values = np.zeros(adata.n_obs)\n\n        for cat in categories:\n            cat_mask = adata.obs[groupby] == cat\n            embed_x = adata.obsm[f'X_{basis}'][cat_mask, components[0]]\n            embed_y = adata.obsm[f'X_{basis}'][cat_mask, components[1]]\n\n            dens_embed = _calc_density(embed_x, embed_y)\n            density_values[cat_mask] = dens_embed\n\n        adata.obs[density_covariate] = density_values\n    else:  # if groupby is None\n        # Calculate the density over the whole embedding without subsetting\n        embed_x = adata.obsm[f'X_{basis}'][:, components[0]]\n        embed_y = adata.obsm[f'X_{basis}'][:, components[1]]\n\n        adata.obs[density_covariate] = _calc_density(embed_x, embed_y)\n\n    # Reduce diffmap components for labeling\n    # Note: plot_scatter takes care of correcting diffmap components\n    #       for plotting automatically\n    if basis != 'diffmap':\n        components += 1\n\n    adata.uns[f'{density_covariate}_params'] = dict(\n        covariate=groupby, components=components.tolist()\n    )\n\n    logg.hint(\n        f\"added\\n\"\n        f\"    '{density_covariate}', densities (adata.obs)\\n\"\n        f\"    '{density_covariate}_params', parameter (adata.uns)\"", "idx": 778}
{"project": "Scanpy", "commit_id": "802_scanpy_1.9.0__ingest.py_ingest.py", "target": 0, "func": "def ingest(\n    adata: AnnData,\n    adata_ref: AnnData,\n    obs: Optional[Union[str, Iterable[str]]] = None,\n    embedding_method: Union[str, Iterable[str]] = ('umap', 'pca'),\n    labeling_method: str = 'knn',\n    neighbors_key: Optional[str] = None,\n    inplace: bool = True,\n    **kwargs,\n):\n    \"\"\"\\\n    Map labels and embeddings from reference data to new data.\n\n    :tutorial:`integrating-data-using-ingest`\n\n    Integrates embeddings and annotations of an `adata` with a reference dataset\n    `adata_ref` through projecting on a PCA (or alternate\n    model) that has been fitted on the reference data. The function uses a knn\n    classifier for mapping labels and the UMAP package [McInnes18]_ for mapping\n    the embeddings.\n\n    .. note::\n\n        We refer to this *asymmetric* dataset integration as *ingesting*\n        annotations from reference data to new data. This is different from\n        learning a joint representation that integrates both datasets in an\n        unbiased way, as CCA (e.g. in Seurat) or a conditional VAE (e.g. in\n        scVI) would do.\n\n    You need to run :func:`~scanpy.pp.neighbors` on `adata_ref` before\n    passing it.\n\n    Parameters\n    ----------\n    adata\n        The annotated data matrix of shape `n_obs` \u00d7 `n_vars`. Rows correspond\n        to cells and columns to genes. This is the dataset without labels and\n        embeddings.\n    adata_ref\n        The annotated data matrix of shape `n_obs` \u00d7 `n_vars`. Rows correspond\n        to cells and columns to genes.\n        Variables (`n_vars` and `var_names`) of `adata_ref` should be the same\n        as in `adata`.\n        This is the dataset with labels and embeddings\n        which need to be mapped to `adata`.\n    obs\n        Labels' keys in `adata_ref.obs` which need to be mapped to `adata.obs`\n        (inferred for observation of `adata`).\n    embedding_method\n        Embeddings in `adata_ref` which need to be mapped to `adata`.\n        The only supported values are 'umap' and 'pca'.\n    labeling_method\n        The method to map labels in `adata_ref.obs` to `adata.obs`.\n        The only supported value is 'knn'.\n    neighbors_key\n        If not specified, ingest looks adata_ref.uns['neighbors']\n        for neighbors settings and adata_ref.obsp['distances'] for\n        distances (default storage places for pp.neighbors).\n        If specified, ingest looks adata_ref.uns[neighbors_key] for\n        neighbors settings and\n        adata_ref.obsp[adata_ref.uns[neighbors_key]['distances_key']] for distances.\n    inplace\n        Only works if `return_joint=False`.\n        Add labels and embeddings to the passed `adata` (if `True`)\n        or return a copy of `adata` with mapped embeddings and labels.\n\n    Returns\n    -------\n    * if `inplace=False` returns a copy of `adata`\n      with mapped embeddings and labels in `obsm` and `obs` correspondingly\n    * if `inplace=True` returns `None` and updates `adata.obsm` and `adata.obs`\n      with mapped embeddings and labels\n\n    Example\n    -------\n    Call sequence:\n\n    >>> import scanpy as sc\n    >>> sc.pp.neighbors(adata_ref)\n    >>> sc.tl.umap(adata_ref)\n    >>> sc.tl.ingest(adata, adata_ref, obs='cell_type')\n\n    .. _ingest PBMC tutorial: https://scanpy-tutorials.readthedocs.io/en/latest/integrating-pbmcs-using-ingest.html\n    .. _ingest Pancreas tutorial: https://scanpy-tutorials.readthedocs.io/en/latest/integrating-pancreas-using-ingest.html\n    \"\"\"\n    # anndata version check\n    anndata_version = pkg_version(\"anndata\")\n    if anndata_version < ANNDATA_MIN_VERSION:\n        raise ValueError(\n            f'ingest only works correctly with anndata>={ANNDATA_MIN_VERSION} '\n            f'(you have {anndata_version}) as prior to {ANNDATA_MIN_VERSION}, '\n            '`AnnData.concatenate` did not concatenate `.obsm`.'\n        )\n\n    start = logg.info('running ingest')\n    obs = [obs] if isinstance(obs, str) else obs\n    embedding_method = (\n        [embedding_method] if isinstance(embedding_method, str) else embedding_method\n    )\n    labeling_method = (\n        [labeling_method] if isinstance(labeling_method, str) else labeling_method\n    )\n\n    if len(labeling_method) == 1 and len(obs or []) > 1:\n        labeling_method = labeling_method * len(obs)\n\n    ing = Ingest(adata_ref, neighbors_key)\n    ing.fit(adata)\n\n    for method in embedding_method:\n        ing.map_embedding(method)\n\n    if obs is not None:\n        ing.neighbors(**kwargs)\n        for i, col in enumerate(obs):\n            ing.map_labels(col, labeling_method[i])\n\n    logg.info('    finished', time=start)\n    return ing.to_adata(inplace)", "idx": 779}
{"project": "Scanpy", "commit_id": "803_scanpy_1.9.0__ingest.py___init__.py", "target": 0, "func": "def __init__(self, adata, neighbors_key=None):\n        # assume rep is X if all initializations fail to identify it\n        self._rep = adata.X\n        self._use_rep = 'X'\n\n        self._n_pcs = None\n\n        self._adata_ref = adata\n        self._adata_new = None\n\n        # only use with umap > 0.5.0\n        self._use_pynndescent = False\n\n        if 'pca' in adata.uns:\n            self._init_pca(adata)\n\n        if neighbors_key is None:\n            neighbors_key = 'neighbors'\n\n        if neighbors_key in adata.uns:\n            self._init_neighbors(adata, neighbors_key)\n        else:\n            raise ValueError(\n                f'There is no neighbors data in `adata.uns[\"{neighbors_key}\"]`.\\n'\n                'Please run pp.neighbors.'\n            )\n\n        if 'X_umap' in adata.obsm:\n            self._init_umap(adata)\n\n        self._obsm = None\n        self._obs = None\n        self._labels = None\n\n        self._indices = None\n        self._distances = None", "idx": 780}
{"project": "Scanpy", "commit_id": "804_scanpy_1.9.0__ingest.py___setitem__.py", "target": 0, "func": "def __setitem__(self, key, value):\n        if value.shape[self._axis] != self._dim:\n            raise ValueError(\n                f\"Value passed for key '{key}' is of incorrect shape. \"\n                f\"Value has shape {value.shape[self._axis]} \"\n                f\"for dimension {self._axis} while \"\n                f\"it should have {self._dim}.\"\n            )\n        self._data[key] = value", "idx": 781}
{"project": "Scanpy", "commit_id": "805_scanpy_1.9.0__ingest.py___getitem__.py", "target": 0, "func": "def __getitem__(self, key):\n        return self._data[key]", "idx": 782}
{"project": "Scanpy", "commit_id": "806_scanpy_1.9.0__ingest.py___delitem__.py", "target": 0, "func": "def __delitem__(self, key):\n        del self._data[key]", "idx": 783}
{"project": "Scanpy", "commit_id": "807_scanpy_1.9.0__ingest.py___iter__.py", "target": 0, "func": "def __iter__(self):\n        return iter(self._data)", "idx": 784}
{"project": "Scanpy", "commit_id": "808_scanpy_1.9.0__ingest.py___len__.py", "target": 0, "func": "def __len__(self):\n        return len(self._data)", "idx": 785}
{"project": "Scanpy", "commit_id": "809_scanpy_1.9.0__ingest.py___repr__.py", "target": 0, "func": "def __repr__(self):\n        return f\"{type(self).__name__}({self._data})\"", "idx": 786}
{"project": "Scanpy", "commit_id": "80_scanpy_1.9.0__metadata.py_within_flit.py", "target": 0, "func": "def within_flit():\n    \"\"\"\\\n    Checks if we are being imported by flit.\n    This is necessary so flit can import __version__ without all depedencies installed.\n    There are a few options to make this hack unnecessary, see:\n    https://github.com/takluyver/flit/issues/253#issuecomment-737870438\n    \"\"\"\n    for frame in traceback.extract_stack():\n        if frame.name == 'get_docstring_and_version_via_import':\n            return True\n    return False", "idx": 787}
{"project": "Scanpy", "commit_id": "810_scanpy_1.9.0__ingest.py__init_umap.py", "target": 0, "func": "def _init_umap(self, adata):\n        import umap as u\n\n        if not self._use_pynndescent:\n            u.umap_._HAVE_PYNNDESCENT = False\n\n        self._umap = u.UMAP(\n            metric=self._metric,\n            random_state=adata.uns['umap']['params'].get('random_state', 0),\n        )\n\n        self._umap._initial_alpha = self._umap.learning_rate\n        self._umap._raw_data = self._rep\n        self._umap.knn_dists = None\n\n        self._umap._validate_parameters()\n\n        self._umap.embedding_ = adata.obsm['X_umap']\n        self._umap._sparse_data = issparse(self._rep)\n        self._umap._small_data = self._rep.shape[0] < 4096\n        self._umap._metric_kwds = self._metric_kwds\n\n        self._umap._n_neighbors = self._n_neighbors\n        self._umap.n_neighbors = self._n_neighbors\n\n        if not self._use_pynndescent:\n            if self._random_init is not None or self._tree_init is not None:\n                self._umap._random_init = self._random_init\n                self._umap._tree_init = self._tree_init\n                self._umap._search = self._search\n\n            if self._dist_func is not None:\n                self._umap._input_distance_func = self._dist_func\n\n            self._umap._rp_forest = self._rp_forest\n\n            self._umap._search_graph = self._search_graph\n        else:\n            self._umap._knn_search_index = self._nnd_idx\n\n        self._umap._a = adata.uns['umap']['params']['a']\n        self._umap._b = adata.uns['umap']['params']['b']\n\n        self._umap._input_hash = None", "idx": 788}
{"project": "Scanpy", "commit_id": "811_scanpy_1.9.0__ingest.py__init_dist_search.py", "target": 0, "func": "def _init_dist_search(self, dist_args):\n        from functools import partial\n        from umap.nndescent import initialise_search\n        from umap.distances import named_distances\n\n        self._random_init = None\n        self._tree_init = None\n\n        self._initialise_search = None\n        self._search = None\n\n        self._dist_func = None\n\n        dist_func = named_distances[self._metric]\n\n        if pkg_version('umap-learn') < version.parse(\"0.4.0\"):\n            from umap.nndescent import (\n                make_initialisations,\n                make_initialized_nnd_search,\n            )\n\n            self._random_init, self._tree_init = make_initialisations(\n                dist_func, dist_args\n            )\n            _initialise_search = partial(\n                initialise_search,\n                init_from_random=self._random_init,\n                init_from_tree=self._tree_init,\n            )\n            _search = make_initialized_nnd_search(dist_func, dist_args)\n\n        else:\n            from numba import njit\n            from umap.nndescent import initialized_nnd_search\n\n            @njit\n            def partial_dist_func(x, y):\n                return dist_func(x, y, *dist_args)\n\n            _initialise_search = partial(initialise_search, dist=partial_dist_func)\n            _search = partial(initialized_nnd_search, dist=partial_dist_func)\n\n            self._dist_func = partial_dist_func\n\n        self._initialise_search = _initialise_search\n        self._search = _search", "idx": 789}
{"project": "Scanpy", "commit_id": "812_scanpy_1.9.0__ingest.py__init_pynndescent.py", "target": 0, "func": "def _init_pynndescent(self, distances):\n        from pynndescent import NNDescent\n\n        self._use_pynndescent = True\n\n        first_col = np.arange(distances.shape[0])[:, None]\n        init_indices = np.hstack((first_col, np.stack(distances.tolil().rows)))\n\n        self._nnd_idx = NNDescent(\n            data=self._rep,\n            metric=self._metric,\n            metric_kwds=self._metric_kwds,\n            n_neighbors=self._n_neighbors,\n            init_graph=init_indices,\n            random_state=self._neigh_random_state,\n        )\n\n        # temporary hack for the broken forest storage\n        from pynndescent.rp_trees import make_forest\n\n        current_random_state = check_random_state(self._nnd_idx.random_state)\n        self._nnd_idx._rp_forest = make_forest(\n            self._nnd_idx._raw_data,\n            self._nnd_idx.n_neighbors,\n            self._nnd_idx.n_search_trees,\n            self._nnd_idx.leaf_size,\n            self._nnd_idx.rng_state,\n            current_random_state,\n            self._nnd_idx.n_jobs,\n            self._nnd_idx._angular_trees,", "idx": 790}
{"project": "Scanpy", "commit_id": "813_scanpy_1.9.0__ingest.py__init_neighbors.py", "target": 0, "func": "def _init_neighbors(self, adata, neighbors_key):\n        neighbors = NeighborsView(adata, neighbors_key)\n\n        self._n_neighbors = neighbors['params']['n_neighbors']\n\n        if 'use_rep' in neighbors['params']:\n            self._use_rep = neighbors['params']['use_rep']\n            self._rep = adata.X if self._use_rep == 'X' else adata.obsm[self._use_rep]\n        elif 'n_pcs' in neighbors['params']:\n            self._use_rep = 'X_pca'\n            self._n_pcs = neighbors['params']['n_pcs']\n            self._rep = adata.obsm['X_pca'][:, : self._n_pcs]\n        elif adata.n_vars > settings.N_PCS and 'X_pca' in adata.obsm.keys():\n            self._use_rep = 'X_pca'\n            self._rep = adata.obsm['X_pca'][:, : settings.N_PCS]\n            self._n_pcs = self._rep.shape[1]\n\n        if 'metric_kwds' in neighbors['params']:\n            self._metric_kwds = neighbors['params']['metric_kwds']\n            dist_args = tuple(self._metric_kwds.values())\n        else:\n            self._metric_kwds = {}\n            dist_args = ()\n\n        self._metric = neighbors['params']['metric']\n\n        if pkg_version('umap-learn') < version.parse(\"0.5.0\"):\n            self._init_dist_search(dist_args)\n\n            search_graph = neighbors['distances'].copy()\n            search_graph.data = (search_graph.data > 0).astype(np.int8)\n            self._search_graph = search_graph.maximum(search_graph.transpose())\n\n            if 'rp_forest' in neighbors:\n                self._rp_forest = _rp_forest_generate(neighbors['rp_forest'])\n            else:\n                self._rp_forest = None\n        else:\n            self._neigh_random_state = neighbors['params'].get('random_state', 0)\n            self._init_pynndescent(neighbors['distances'])", "idx": 791}
{"project": "Scanpy", "commit_id": "814_scanpy_1.9.0__ingest.py__init_pca.py", "target": 0, "func": "def _init_pca(self, adata):\n        self._pca_centered = adata.uns['pca']['params']['zero_center']\n        self._pca_use_hvg = adata.uns['pca']['params']['use_highly_variable']\n\n        if self._pca_use_hvg and 'highly_variable' not in adata.var.keys():\n            raise ValueError('Did not find adata.var[\\'highly_variable\\'].')\n\n        if self._pca_use_hvg:\n            self._pca_basis = adata.varm['PCs'][adata.var['highly_variable']]\n        else:\n            self._pca_basis = adata.varm['PCs']", "idx": 792}
{"project": "Scanpy", "commit_id": "815_scanpy_1.9.0__ingest.py__pca.py", "target": 0, "func": "def _pca(self, n_pcs=None):\n        X = self._adata_new.X\n        X = X.toarray() if issparse(X) else X.copy()\n        if self._pca_use_hvg:\n            X = X[:, self._adata_ref.var['highly_variable']]\n        if self._pca_centered:\n            X -= X.mean(axis=0)\n        X_pca = np.dot(X, self._pca_basis[:, :n_pcs])\n        return X_pca", "idx": 793}
{"project": "Scanpy", "commit_id": "816_scanpy_1.9.0__ingest.py__same_rep.py", "target": 0, "func": "def _same_rep(self):\n        adata = self._adata_new\n        if self._n_pcs is not None:\n            return self._pca(self._n_pcs)\n        if self._use_rep == 'X':\n            return adata.X\n        if self._use_rep in adata.obsm.keys():\n            return adata.obsm[self._use_rep]\n        return adata.X", "idx": 794}
{"project": "Scanpy", "commit_id": "817_scanpy_1.9.0__ingest.py_fit.py", "target": 0, "func": "def fit(self, adata_new):\n        \"\"\"\\\n        Map `adata_new` to the same representation as `adata`.\n\n        This function identifies the representation which was used to\n        calculate neighbors in 'adata' and maps `adata_new` to\n        this representation.\n        Variables (`n_vars` and `var_names`) of `adata_new` should be the same\n        as in `adata`.\n\n        `adata` refers to the :class:`~anndata.AnnData` object\n        that is passed during the initialization of an Ingest instance.\n        \"\"\"\n        ref_var_names = self._adata_ref.var_names.str.upper()\n        new_var_names = adata_new.var_names.str.upper()\n\n        if not ref_var_names.equals(new_var_names):\n            raise ValueError(\n                'Variables in the new adata are different '\n                'from variables in the reference adata'\n            )\n\n        self._obs = pd.DataFrame(index=adata_new.obs.index)\n        self._obsm = _DimDict(adata_new.n_obs, axis=0)\n\n        self._adata_new = adata_new\n        self._obsm['rep'] = self._same_rep()", "idx": 795}
{"project": "Scanpy", "commit_id": "818_scanpy_1.9.0__ingest.py_neighbors.py", "target": 0, "func": "def neighbors(self, k=None, queue_size=5, epsilon=0.1, random_state=0):\n        \"\"\"\\\n        Calculate neighbors of `adata_new` observations in `adata`.\n\n        This function calculates `k` neighbors in `adata` for\n        each observation of `adata_new`.\n        \"\"\"\n        from umap.umap_ import INT32_MAX, INT32_MIN\n\n        random_state = check_random_state(random_state)\n        rng_state = random_state.randint(INT32_MIN, INT32_MAX, 3).astype(np.int64)\n\n        train = self._rep\n        test = self._obsm['rep']\n\n        if k is None:\n            k = self._n_neighbors\n\n        if self._use_pynndescent:\n            self._nnd_idx.search_rng_state = rng_state\n\n            self._indices, self._distances = self._nnd_idx.query(test, k, epsilon)\n\n        else:\n            from umap.utils import deheap_sort\n\n            init = self._initialise_search(\n                self._rp_forest, train, test, int(k * queue_size), rng_state=rng_state\n            )\n\n            result = self._search(\n                train, self._search_graph.indptr, self._search_graph.indices, init, test\n            )\n            indices, dists = deheap_sort(result)\n            self._indices, self._distances = indices[:, :k], dists[:, :k]", "idx": 796}
{"project": "Scanpy", "commit_id": "819_scanpy_1.9.0__ingest.py__umap_transform.py", "target": 0, "func": "def _umap_transform(self):\n        return self._umap.transform(self._obsm['rep'])", "idx": 797}
{"project": "Scanpy", "commit_id": "81_scanpy_1.9.0__settings.py__type_check.py", "target": 0, "func": "def _type_check(var: Any, varname: str, types: Union[type, Tuple[type, ...]]):\n    if isinstance(var, types):\n        return\n    if isinstance(types, type):\n        possible_types_str = types.__name__\n    else:\n        type_names = [t.__name__ for t in types]\n        possible_types_str = \"{} or {}\".format(\n            \", \".join(type_names[:-1]), type_names[-1]\n        )\n    raise TypeError(f\"{varname} must be of type {possible_types_str}\")", "idx": 798}
{"project": "Scanpy", "commit_id": "820_scanpy_1.9.0__ingest.py_map_embedding.py", "target": 0, "func": "def map_embedding(self, method):\n        \"\"\"\\\n        Map embeddings of `adata` to `adata_new`.\n\n        This function infers embeddings, specified by `method`,\n        for `adata_new` from existing embeddings in `adata`.\n        `method` can be 'umap' or 'pca'.\n        \"\"\"\n        if method == 'umap':\n            self._obsm['X_umap'] = self._umap_transform()\n        elif method == 'pca':\n            self._obsm['X_pca'] = self._pca()\n        else:\n            raise NotImplementedError(\n                'Ingest supports only umap and pca embeddings for now.'", "idx": 799}
{"project": "Scanpy", "commit_id": "821_scanpy_1.9.0__ingest.py__knn_classify.py", "target": 0, "func": "def _knn_classify(self, labels):\n        cat_array = self._adata_ref.obs[labels].astype(\n            'category'\n        )  # ensure it's categorical\n        values = [cat_array[inds].mode()[0] for inds in self._indices]\n        return pd.Categorical(values=values, categories=cat_array.cat.categories)", "idx": 800}
{"project": "Scanpy", "commit_id": "822_scanpy_1.9.0__ingest.py_map_labels.py", "target": 0, "func": "def map_labels(self, labels, method):\n        \"\"\"\\\n        Map labels of `adata` to `adata_new`.\n\n        This function infers `labels` for `adata_new.obs`\n        from existing labels in `adata.obs`.\n        `method` can be only 'knn'.\n        \"\"\"\n        if method == 'knn':\n            self._obs[labels] = self._knn_classify(labels)\n        else:\n            raise NotImplementedError('Ingest supports knn labeling for now.')", "idx": 801}
{"project": "Scanpy", "commit_id": "823_scanpy_1.9.0__ingest.py_to_adata.py", "target": 0, "func": "def to_adata(self, inplace=False):\n        \"\"\"\\\n        Returns `adata_new` with mapped embeddings and labels.\n\n        If `inplace=False` returns a copy of `adata_new`\n        with mapped embeddings and labels in `obsm` and `obs` correspondingly.\n        If `inplace=True` returns nothing and updates `adata_new.obsm`\n        and `adata_new.obs` with mapped embeddings and labels.\n        \"\"\"\n        adata = self._adata_new if inplace else self._adata_new.copy()\n\n        adata.obsm.update(self._obsm)\n\n        for key in self._obs:\n            adata.obs[key] = self._obs[key]\n\n        if not inplace:\n            return adata", "idx": 802}
{"project": "Scanpy", "commit_id": "824_scanpy_1.9.0__ingest.py_to_adata_joint.py", "target": 0, "func": "def to_adata_joint(\n        self, batch_key='batch', batch_categories=None, index_unique='-'\n    ):\n        \"\"\"\\\n        Returns concatenated object.\n\n        This function returns the new :class:`~anndata.AnnData` object\n        with concatenated existing embeddings and labels of 'adata'\n        and inferred embeddings and labels for `adata_new`.\n        \"\"\"\n        adata = self._adata_ref.concatenate(\n            self._adata_new,\n            batch_key=batch_key,\n            batch_categories=batch_categories,\n            index_unique=index_unique,\n        )\n\n        obs_update = self._obs.copy()\n        obs_update.index = adata[adata.obs[batch_key] == '1'].obs_names\n        adata.obs.update(obs_update)\n\n        for key in self._obsm:\n            if key in self._adata_ref.obsm:\n                adata.obsm[key] = np.vstack(\n                    (self._adata_ref.obsm[key], self._obsm[key])\n                )\n\n        if self._use_rep not in ('X_pca', 'X'):\n            adata.obsm[self._use_rep] = np.vstack(\n                (self._adata_ref.obsm[self._use_rep], self._obsm['rep'])\n            )\n\n        if 'X_umap' in self._obsm:\n            adata.uns['umap'] = self._adata_ref.uns['umap']\n        if 'X_pca' in self._obsm:\n            adata.uns['pca'] = self._adata_ref.uns['pca']\n            adata.varm['PCs'] = self._adata_ref.varm['PCs']\n\n        return adata", "idx": 803}
{"project": "Scanpy", "commit_id": "825_scanpy_1.9.0__ingest.py_partial_dist_func.py", "target": 0, "func": "def partial_dist_func(x, y):\n                return dist_func(x, y, *dist_args)", "idx": 804}
{"project": "Scanpy", "commit_id": "826_scanpy_1.9.0__leiden.py_leiden.py", "target": 1, "func": "def leiden(\n    adata: AnnData,\n    resolution: float = 1,\n    *,\n    restrict_to: Optional[Tuple[str, Sequence[str]]] = None,\n    random_state: _utils.AnyRandom = 0,\n    key_added: str = 'leiden',\n    adjacency: Optional[sparse.spmatrix] = None,\n    directed: bool = True,\n    use_weights: bool = True,\n    n_iterations: int = -1,\n    partition_type: Optional[Type[MutableVertexPartition]] = None,\n    neighbors_key: Optional[str] = None,\n    obsp: Optional[str] = None,\n    copy: bool = False,\n    **partition_kwargs,\n) -> Optional[AnnData]:\n    \"\"\"\\\n    Cluster cells into subgroups [Traag18]_.\n\n    Cluster cells using the Leiden algorithm [Traag18]_,\n    an improved version of the Louvain algorithm [Blondel08]_.\n    It has been proposed for single-cell analysis by [Levine15]_.\n\n    This requires having ran :func:`~scanpy.pp.neighbors` or\n    :func:`~scanpy.external.pp.bbknn` first.\n\n    Parameters\n    ----------\n    adata\n        The annotated data matrix.\n    resolution\n        A parameter value controlling the coarseness of the clustering.\n        Higher values lead to more clusters.\n        Set to `None` if overriding `partition_type`\n        to one that doesn\u2019t accept a `resolution_parameter`.\n    random_state\n        Change the initialization of the optimization.\n    restrict_to\n        Restrict the clustering to the categories within the key for sample\n        annotation, tuple needs to contain `(obs_key, list_of_categories)`.\n    key_added\n        `adata.obs` key under which to add the cluster labels.\n    adjacency\n        Sparse adjacency matrix of the graph, defaults to neighbors connectivities.\n    directed\n        Whether to treat the graph as directed or undirected.\n    use_weights\n        If `True`, edge weights from the graph are used in the computation\n        (placing more emphasis on stronger edges).\n    n_iterations\n        How many iterations of the Leiden clustering algorithm to perform.\n        Positive values above 2 define the total number of iterations to perform,\n        -1 has the algorithm run until it reaches its optimal clustering.\n    partition_type\n        Type of partition to use.\n        Defaults to :class:`~leidenalg.RBConfigurationVertexPartition`.\n        For the available options, consult the documentation for\n        :func:`~leidenalg.find_partition`.\n    neighbors_key\n        Use neighbors connectivities as adjacency.\n        If not specified, leiden looks .obsp['connectivities'] for connectivities\n        (default storage place for pp.neighbors).\n        If specified, leiden looks\n        .obsp[.uns[neighbors_key]['connectivities_key']] for connectivities.\n    obsp\n        Use .obsp[obsp] as adjacency. You can't specify both\n        `obsp` and `neighbors_key` at the same time.\n    copy\n        Whether to copy `adata` or modify it inplace.\n    **partition_kwargs\n        Any further arguments to pass to `~leidenalg.find_partition`\n        (which in turn passes arguments to the `partition_type`).\n\n    Returns\n    -------\n    `adata.obs[key_added]`\n        Array of dim (number of samples) that stores the subgroup id\n        (`'0'`, `'1'`, ...) for each cell.\n    `adata.uns['leiden']['params']`\n        A dict with the values for the parameters `resolution`, `random_state`,\n        and `n_iterations`.\n    \"\"\"\n    try:\n        import leidenalg\n    except ImportError:\n        raise ImportError(\n            'Please install the leiden algorithm: `conda install -c conda-forge leidenalg` or `pip3 install leidenalg`.'\n        )\n    partition_kwargs = dict(partition_kwargs)\n\n    start = logg.info('running Leiden clustering')\n    adata = adata.copy() if copy else adata\n    # are we clustering a user-provided graph or the default AnnData one?\n    if adjacency is None:\n        adjacency = _utils._choose_graph(adata, obsp, neighbors_key)\n    if restrict_to is not None:\n        restrict_key, restrict_categories = restrict_to\n        adjacency, restrict_indices = restrict_adjacency(\n            adata,\n            restrict_key,\n            restrict_categories,\n            adjacency,\n        )\n    # convert it to igraph\n    g = _utils.get_igraph_from_adjacency(adjacency, directed=directed)\n    # flip to the default partition type if not overriden by the user\n    if partition_type is None:\n        partition_type = leidenalg.RBConfigurationVertexPartition\n    # Prepare find_partition arguments as a dictionary,\n    # appending to whatever the user provided. It needs to be this way\n    # as this allows for the accounting of a None resolution\n    # (in the case of a partition variant that doesn't take it on input)\n    if use_weights:\n        partition_kwargs['weights'] = np.array(g.es['weight']).astype(np.float64)\n    partition_kwargs['n_iterations'] = n_iterations\n    partition_kwargs['seed'] = random_state\n    if resolution is not None:\n        partition_kwargs['resolution_parameter'] = resolution\n    # clustering proper\n    part = leidenalg.find_partition(g, partition_type, **partition_kwargs)\n    # store output into adata.obs\n    groups = np.array(part.membership)\n    if restrict_to is not None:\n        if key_added == 'leiden':\n            key_added += '_R'\n        groups = rename_groups(\n            adata,\n            key_added,\n            restrict_key,\n            restrict_categories,\n            restrict_indices,\n            groups,\n        )\n    adata.obs[key_added] = pd.Categorical(\n        values=groups.astype('U'),\n        categories=natsorted(map(str, np.unique(groups))),\n    )\n    # store information on the clustering parameters\n    adata.uns['leiden'] = {}\n    adata.uns['leiden']['params'] = dict(\n        resolution=resolution,\n        random_state=random_state,\n        n_iterations=n_iterations,\n    )\n    logg.info(\n        '    finished',\n        time=start,\n        deep=(\n            f'found {len(np.unique(groups))} clusters and added\\n'\n            f'    {key_added!r}, the cluster labels (adata.obs, categorical)'\n        ),\n    )\n    return adata if copy else None", "idx": 805}
{"project": "Scanpy", "commit_id": "827_scanpy_1.9.0__louvain.py_louvain.py", "target": 0, "func": "def louvain(\n    adata: AnnData,\n    resolution: Optional[float] = None,\n    random_state: _utils.AnyRandom = 0,\n    restrict_to: Optional[Tuple[str, Sequence[str]]] = None,\n    key_added: str = 'louvain',\n    adjacency: Optional[spmatrix] = None,\n    flavor: Literal['vtraag', 'igraph', 'rapids'] = 'vtraag',\n    directed: bool = True,\n    use_weights: bool = False,\n    partition_type: Optional[Type[MutableVertexPartition]] = None,\n    partition_kwargs: Mapping[str, Any] = MappingProxyType({}),\n    neighbors_key: Optional[str] = None,\n    obsp: Optional[str] = None,\n    copy: bool = False,\n) -> Optional[AnnData]:\n    \"\"\"\\\n    Cluster cells into subgroups [Blondel08]_ [Levine15]_ [Traag17]_.\n\n    Cluster cells using the Louvain algorithm [Blondel08]_ in the implementation\n    of [Traag17]_. The Louvain algorithm has been proposed for single-cell\n    analysis by [Levine15]_.\n\n    This requires having ran :func:`~scanpy.pp.neighbors` or\n    :func:`~scanpy.external.pp.bbknn` first,\n    or explicitly passing a ``adjacency`` matrix.\n\n    Parameters\n    ----------\n    adata\n        The annotated data matrix.\n    resolution\n        For the default flavor (``'vtraag'``) or for ```RAPIDS```, you can provide a\n        resolution (higher resolution means finding more and smaller clusters),\n        which defaults to 1.0.\n        See \u201cTime as a resolution parameter\u201d in [Lambiotte09]_.\n    random_state\n        Change the initialization of the optimization.\n    restrict_to\n        Restrict the clustering to the categories within the key for sample\n        annotation, tuple needs to contain ``(obs_key, list_of_categories)``.\n    key_added\n        Key under which to add the cluster labels. (default: ``'louvain'``)\n    adjacency\n        Sparse adjacency matrix of the graph, defaults to neighbors connectivities.\n    flavor\n        Choose between to packages for computing the clustering.\n        ``'vtraag'`` is much more powerful, and the default.\n    directed\n        Interpret the ``adjacency`` matrix as directed graph?\n    use_weights\n        Use weights from knn graph.\n    partition_type\n        Type of partition to use.\n        Only a valid argument if ``flavor`` is ``'vtraag'``.\n    partition_kwargs\n        Key word arguments to pass to partitioning,\n        if ``vtraag`` method is being used.\n    neighbors_key\n        Use neighbors connectivities as adjacency.\n        If not specified, louvain looks .obsp['connectivities'] for connectivities\n        (default storage place for pp.neighbors).\n        If specified, louvain looks\n        .obsp[.uns[neighbors_key]['connectivities_key']] for connectivities.\n    obsp\n        Use .obsp[obsp] as adjacency. You can't specify both\n        `obsp` and `neighbors_key` at the same time.\n    copy\n        Copy adata or modify it inplace.\n\n    Returns\n    -------\n    :obj:`None`\n        By default (``copy=False``), updates ``adata`` with the following fields:\n\n        ``adata.obs['louvain']`` (:class:`pandas.Series`, dtype ``category``)\n            Array of dim (number of samples) that stores the subgroup id\n            (``'0'``, ``'1'``, ...) for each cell.\n\n    :class:`~anndata.AnnData`\n        When ``copy=True`` is set, a copy of ``adata`` with those fields is returned.\n    \"\"\"\n    partition_kwargs = dict(partition_kwargs)\n    start = logg.info('running Louvain clustering')\n    if (flavor != 'vtraag') and (partition_type is not None):\n        raise ValueError(\n            '`partition_type` is only a valid argument ' 'when `flavour` is \"vtraag\"'\n        )\n    adata = adata.copy() if copy else adata\n    if adjacency is None:\n        adjacency = _choose_graph(adata, obsp, neighbors_key)\n    if restrict_to is not None:\n        restrict_key, restrict_categories = restrict_to\n        adjacency, restrict_indices = restrict_adjacency(\n            adata,\n            restrict_key,\n            restrict_categories,\n            adjacency,\n        )\n    if flavor in {'vtraag', 'igraph'}:\n        if flavor == 'igraph' and resolution is not None:\n            logg.warning('`resolution` parameter has no effect for flavor \"igraph\"')\n        if directed and flavor == 'igraph':\n            directed = False\n        if not directed:\n            logg.debug('    using the undirected graph')\n        g = _utils.get_igraph_from_adjacency(adjacency, directed=directed)\n        if use_weights:\n            weights = np.array(g.es[\"weight\"]).astype(np.float64)\n        else:\n            weights = None\n        if flavor == 'vtraag':\n            import louvain\n\n            if partition_type is None:\n                partition_type = louvain.RBConfigurationVertexPartition\n            if resolution is not None:\n                partition_kwargs[\"resolution_parameter\"] = resolution\n            if use_weights:\n                partition_kwargs[\"weights\"] = weights\n            if version.parse(louvain.__version__) < version.parse(\"0.7.0\"):\n                louvain.set_rng_seed(random_state)\n            else:\n                partition_kwargs[\"seed\"] = random_state\n            logg.info('    using the \"louvain\" package of Traag (2017)')\n            part = louvain.find_partition(\n                g,\n                partition_type,\n                **partition_kwargs,\n            )\n            # adata.uns['louvain_quality'] = part.quality()\n        else:\n            part = g.community_multilevel(weights=weights)\n        groups = np.array(part.membership)\n    elif flavor == 'rapids':\n        # nvLouvain only works with undirected graphs,\n        # and `adjacency` must have a directed edge in both directions\n        import cudf\n        import cugraph\n\n        offsets = cudf.Series(adjacency.indptr)\n        indices = cudf.Series(adjacency.indices)\n        if use_weights:\n            sources, targets = adjacency.nonzero()\n            weights = adjacency[sources, targets]\n            if isinstance(weights, np.matrix):\n                weights = weights.A1\n            weights = cudf.Series(weights)\n        else:\n            weights = None\n        g = cugraph.Graph()\n\n        if hasattr(g, 'add_adj_list'):\n            g.add_adj_list(offsets, indices, weights)\n        else:\n            g.from_cudf_adjlist(offsets, indices, weights)\n\n        logg.info('    using the \"louvain\" package of rapids')\n        if resolution is not None:\n            louvain_parts, _ = cugraph.louvain(g, resolution=resolution)\n        else:\n            louvain_parts, _ = cugraph.louvain(g)\n        groups = (\n            louvain_parts.to_pandas()\n            .sort_values('vertex')[['partition']]\n            .to_numpy()\n            .ravel()\n        )\n    elif flavor == 'taynaud':\n        # this is deprecated\n        import networkx as nx\n        import community\n\n        g = nx.Graph(adjacency)\n        partition = community.best_partition(g)\n        groups = np.zeros(len(partition), dtype=int)\n        for k, v in partition.items():\n            groups[k] = v\n    else:\n        raise ValueError('`flavor` needs to be \"vtraag\" or \"igraph\" or \"taynaud\".')\n    if restrict_to is not None:\n        if key_added == 'louvain':\n            key_added += '_R'\n        groups = rename_groups(\n            adata,\n            key_added,\n            restrict_key,\n            restrict_categories,\n            restrict_indices,\n            groups,\n        )\n    adata.obs[key_added] = pd.Categorical(\n        values=groups.astype('U'),\n        categories=natsorted(map(str, np.unique(groups))),\n    )\n    adata.uns['louvain'] = {}\n    adata.uns['louvain']['params'] = dict(\n        resolution=resolution,\n        random_state=random_state,\n    )\n    logg.info(\n        '    finished',\n        time=start,\n        deep=(\n            f'found {len(np.unique(groups))} clusters and added\\n'\n            f'    {key_added!r}, the cluster labels (adata.obs, categorical)'\n        ),\n    )\n    return adata if copy else None", "idx": 806}
{"project": "Scanpy", "commit_id": "828_scanpy_1.9.0__marker_gene_overlap.py__calc_overlap_count.py", "target": 0, "func": "def _calc_overlap_count(markers1: dict, markers2: dict):\n    \"\"\"\\\n    Calculate overlap count between the values of two dictionaries\n\n    Note: dict values must be sets\n    \"\"\"\n    overlaps = np.zeros((len(markers1), len(markers2)))\n\n    for j, marker_group in enumerate(markers1):\n        tmp = [\n            len(markers2[i].intersection(markers1[marker_group]))\n            for i in markers2.keys()\n        ]\n        overlaps[j, :] = tmp\n\n    return overlaps", "idx": 807}
{"project": "Scanpy", "commit_id": "829_scanpy_1.9.0__marker_gene_overlap.py__calc_overlap_coef.py", "target": 0, "func": "def _calc_overlap_coef(markers1: dict, markers2: dict):\n    \"\"\"\\\n    Calculate overlap coefficient between the values of two dictionaries\n\n    Note: dict values must be sets\n    \"\"\"\n    overlap_coef = np.zeros((len(markers1), len(markers2)))\n\n    for j, marker_group in enumerate(markers1):\n        tmp = [\n            len(markers2[i].intersection(markers1[marker_group]))\n            / max(min(len(markers2[i]), len(markers1[marker_group])), 1)\n            for i in markers2.keys()\n        ]\n        overlap_coef[j, :] = tmp\n\n    return overlap_coef", "idx": 808}
{"project": "Scanpy", "commit_id": "82_scanpy_1.9.0__settings.py_level.py", "target": 0, "func": "def level(self) -> int:\n        # getLevelName(str) returns the int level\u2026\n        return getLevelName(_VERBOSITY_TO_LOGLEVEL[self])", "idx": 809}
{"project": "Scanpy", "commit_id": "830_scanpy_1.9.0__marker_gene_overlap.py__calc_jaccard.py", "target": 0, "func": "def _calc_jaccard(markers1: dict, markers2: dict):\n    \"\"\"\\\n    Calculate jaccard index between the values of two dictionaries\n\n    Note: dict values must be sets\n    \"\"\"\n    jacc_results = np.zeros((len(markers1), len(markers2)))\n\n    for j, marker_group in enumerate(markers1):\n        tmp = [\n            len(markers2[i].intersection(markers1[marker_group]))\n            / len(markers2[i].union(markers1[marker_group]))\n            for i in markers2.keys()\n        ]\n        jacc_results[j, :] = tmp\n\n    return jacc_results", "idx": 810}
{"project": "Scanpy", "commit_id": "831_scanpy_1.9.0__marker_gene_overlap.py_marker_gene_overlap.py", "target": 0, "func": "def marker_gene_overlap(\n    adata: AnnData,\n    reference_markers: Union[Dict[str, set], Dict[str, list]],\n    *,\n    key: str = 'rank_genes_groups',\n    method: _Method = 'overlap_count',\n    normalize: Optional[Literal['reference', 'data']] = None,\n    top_n_markers: Optional[int] = None,\n    adj_pval_threshold: Optional[float] = None,\n    key_added: str = 'marker_gene_overlap',\n    inplace: bool = False,\n):\n    \"\"\"\\\n    Calculate an overlap score between data-deriven marker genes and\n    provided markers\n\n    Marker gene overlap scores can be quoted as overlap counts, overlap\n    coefficients, or jaccard indices. The method returns a pandas dataframe\n    which can be used to annotate clusters based on marker gene overlaps.\n\n    This function was written by Malte Luecken.\n\n    Parameters\n    ----------\n    adata\n        The annotated data matrix.\n    reference_markers\n        A marker gene dictionary object. Keys should be strings with the\n        cell identity name and values are sets or lists of strings which match\n        format of `adata.var_name`.\n    key\n        The key in `adata.uns` where the rank_genes_groups output is stored.\n        By default this is `'rank_genes_groups'`.\n    method\n        (default: `overlap_count`)\n        Method to calculate marker gene overlap. `'overlap_count'` uses the\n        intersection of the gene set, `'overlap_coef'` uses the overlap\n        coefficient, and `'jaccard'` uses the Jaccard index.\n    normalize\n        Normalization option for the marker gene overlap output. This parameter\n        can only be set when `method` is set to `'overlap_count'`. `'reference'`\n        normalizes the data by the total number of marker genes given in the\n        reference annotation per group. `'data'` normalizes the data by the\n        total number of marker genes used for each cluster.\n    top_n_markers\n        The number of top data-derived marker genes to use. By default the top\n        100 marker genes are used. If `adj_pval_threshold` is set along with\n        `top_n_markers`, then `adj_pval_threshold` is ignored.\n    adj_pval_threshold\n        A significance threshold on the adjusted p-values to select marker\n        genes. This can only be used when adjusted p-values are calculated by\n        `sc.tl.rank_genes_groups()`. If `adj_pval_threshold` is set along with\n        `top_n_markers`, then `adj_pval_threshold` is ignored.\n    key_added\n        Name of the `.uns` field that will contain the marker overlap scores.\n    inplace\n        Return a marker gene dataframe or store it inplace in `adata.uns`.\n\n    Returns\n    -------\n    A pandas dataframe with the marker gene overlap scores if `inplace=False`.\n    For `inplace=True` `adata.uns` is updated with an additional field\n    specified by the `key_added` parameter (default = 'marker_gene_overlap').\n\n    Examples\n    --------\n    >>> import scanpy as sc\n    >>> adata = sc.datasets.pbmc68k_reduced()\n    >>> sc.pp.pca(adata, svd_solver='arpack')\n    >>> sc.pp.neighbors(adata)\n    >>> sc.tl.louvain(adata)\n    >>> sc.tl.rank_genes_groups(adata, groupby='louvain')\n    >>> marker_genes = {\n    ...     'CD4 T cells': {'IL7R'},\n    ...     'CD14+ Monocytes': {'CD14', 'LYZ'},\n    ...     'B cells': {'MS4A1'},\n    ...     'CD8 T cells': {'CD8A'},\n    ...     'NK cells': {'GNLY', 'NKG7'},\n    ...     'FCGR3A+ Monocytes': {'FCGR3A', 'MS4A7'},\n    ...     'Dendritic Cells': {'FCER1A', 'CST3'},\n    ...     'Megakaryocytes': {'PPBP'}\n    ... }\n    >>> marker_matches = sc.tl.marker_gene_overlap(adata, marker_genes)\n    \"\"\"\n    # Test user inputs\n    if inplace:\n        raise NotImplementedError(\n            'Writing Pandas dataframes to h5ad is currently under development.'\n            '\\nPlease use `inplace=False`.'\n        )\n\n    if key not in adata.uns:\n        raise ValueError(\n            'Could not find marker gene data. '\n            'Please run `sc.tl.rank_genes_groups()` first.'\n        )\n\n    avail_methods = {'overlap_count', 'overlap_coef', 'jaccard', 'enrich'}\n    if method not in avail_methods:\n        raise ValueError(f'Method must be one of {avail_methods}.')\n\n    if normalize == 'None':\n        normalize = None\n\n    avail_norm = {'reference', 'data', None}\n    if normalize not in avail_norm:\n        raise ValueError(f'Normalize must be one of {avail_norm}.')\n\n    if normalize is not None and method != 'overlap_count':\n        raise ValueError('Can only normalize with method=`overlap_count`.')\n\n    if not all(isinstance(val, cabc.Set) for val in reference_markers.values()):\n        try:\n            reference_markers = {\n                key: set(val) for key, val in reference_markers.items()\n            }\n        except Exception:\n            raise ValueError(\n                'Please ensure that `reference_markers` contains '\n                'sets or lists of markers as values.'\n            )\n\n    if adj_pval_threshold is not None:\n        if 'pvals_adj' not in adata.uns[key]:\n            raise ValueError(\n                'Could not find adjusted p-value data. '\n                'Please run `sc.tl.rank_genes_groups()` with a '\n                'method that outputs adjusted p-values.'\n            )\n\n        if adj_pval_threshold < 0:\n            logg.warning(\n                '`adj_pval_threshold` was set below 0. Threshold will be set to 0.'\n            )\n            adj_pval_threshold = 0\n        elif adj_pval_threshold > 1:\n            logg.warning(\n                '`adj_pval_threshold` was set above 1. Threshold will be set to 1.'\n            )\n            adj_pval_threshold = 1\n\n        if top_n_markers is not None:\n            logg.warning(\n                'Both `adj_pval_threshold` and `top_n_markers` is set. '\n                '`adj_pval_threshold` will be ignored.'\n            )\n\n    if top_n_markers is not None and top_n_markers < 1:\n        logg.warning(\n            '`top_n_markers` was set below 1. `top_n_markers` will be set to 1.'\n        )\n        top_n_markers = 1\n\n    # Get data-derived marker genes in a dictionary of sets\n    data_markers = dict()\n    cluster_ids = adata.uns[key]['names'].dtype.names\n\n    for group in cluster_ids:\n        if top_n_markers is not None:\n            n_genes = min(top_n_markers, adata.uns[key]['names'].shape[0])\n            data_markers[group] = set(adata.uns[key]['names'][group][:n_genes])\n        elif adj_pval_threshold is not None:\n            n_genes = (adata.uns[key]['pvals_adj'][group] < adj_pval_threshold).sum()\n            data_markers[group] = set(adata.uns[key]['names'][group][:n_genes])\n            if n_genes == 0:\n                logg.warning(\n                    'No marker genes passed the significance threshold of '\n                    f'{adj_pval_threshold} for cluster {group!r}.'\n                )\n        # Use top 100 markers as default if top_n_markers = None\n        else:\n            data_markers[group] = set(adata.uns[key]['names'][group][:100])\n\n    # Find overlaps\n    if method == 'overlap_count':\n        marker_match = _calc_overlap_count(reference_markers, data_markers)\n        if normalize == 'reference':\n            # Ensure rows sum to 1\n            ref_lengths = np.array(\n                [len(reference_markers[m_group]) for m_group in reference_markers]\n            )\n            marker_match = marker_match / ref_lengths[:, np.newaxis]\n            marker_match = np.nan_to_num(marker_match)\n        elif normalize == 'data':\n            # Ensure columns sum to 1\n            data_lengths = np.array(\n                [len(data_markers[dat_group]) for dat_group in data_markers]\n            )\n            marker_match = marker_match / data_lengths\n            marker_match = np.nan_to_num(marker_match)\n    elif method == 'overlap_coef':\n        marker_match = _calc_overlap_coef(reference_markers, data_markers)\n    elif method == 'jaccard':\n        marker_match = _calc_jaccard(reference_markers, data_markers)\n\n    # Note:\n    # Could add an 'enrich' option here\n    # (fisher's exact test or hypergeometric test),\n    # but that would require knowledge of the size of the space from which\n    # the reference marker gene set was taken.\n    # This is at best approximately known.\n\n    # Create a pandas dataframe with the results\n    marker_groups = list(reference_markers.keys())\n    clusters = list(cluster_ids)\n    marker_matching_df = pd.DataFrame(\n        marker_match, index=marker_groups, columns=clusters\n    )\n\n    # Store the results\n    if inplace:\n        adata.uns[key_added] = marker_matching_df\n        logg.hint(f'added\\n    {key_added!r}, marker overlap scores (adata.uns)')\n    else:\n        return marker_matching_df", "idx": 811}
{"project": "Scanpy", "commit_id": "832_scanpy_1.9.0__paga.py_paga.py", "target": 0, "func": "def paga(\n    adata: AnnData,\n    groups: Optional[str] = None,\n    use_rna_velocity: bool = False,\n    model: Literal['v1.2', 'v1.0'] = 'v1.2',\n    neighbors_key: Optional[str] = None,\n    copy: bool = False,\n):\n    \"\"\"\\\n    Mapping out the coarse-grained connectivity structures of complex manifolds [Wolf19]_.\n\n    By quantifying the connectivity of partitions (groups, clusters) of the\n    single-cell graph, partition-based graph abstraction (PAGA) generates a much\n    simpler abstracted graph (*PAGA graph*) of partitions, in which edge weights\n    represent confidence in the presence of connections. By tresholding this\n    confidence in :func:`~scanpy.pl.paga`, a much simpler representation of the\n    manifold data is obtained, which is nonetheless faithful to the topology of\n    the manifold.\n\n    The confidence should be interpreted as the ratio of the actual versus the\n    expected value of connetions under the null model of randomly connecting\n    partitions. We do not provide a p-value as this null model does not\n    precisely capture what one would consider \"connected\" in real data, hence it\n    strongly overestimates the expected value. See an extensive discussion of\n    this in [Wolf19]_.\n\n    .. note::\n        Note that you can use the result of :func:`~scanpy.pl.paga` in\n        :func:`~scanpy.tl.umap` and :func:`~scanpy.tl.draw_graph` via\n        `init_pos='paga'` to get single-cell embeddings that are typically more\n        faithful to the global topology.\n\n    Parameters\n    ----------\n    adata\n        An annotated data matrix.\n    groups\n        Key for categorical in `adata.obs`. You can pass your predefined groups\n        by choosing any categorical annotation of observations. Default:\n        The first present key of `'leiden'` or `'louvain'`.\n    use_rna_velocity\n        Use RNA velocity to orient edges in the abstracted graph and estimate\n        transitions. Requires that `adata.uns` contains a directed single-cell\n        graph with key `['velocity_graph']`. This feature might be subject\n        to change in the future.\n    model\n        The PAGA connectivity model.\n    neighbors_key\n        If not specified, paga looks `.uns['neighbors']` for neighbors settings\n        and `.obsp['connectivities']`, `.obsp['distances']` for connectivities and\n        distances respectively (default storage places for `pp.neighbors`).\n        If specified, paga looks `.uns[neighbors_key]` for neighbors settings and\n        `.obsp[.uns[neighbors_key]['connectivities_key']]`,\n        `.obsp[.uns[neighbors_key]['distances_key']]` for connectivities and distances\n        respectively.\n    copy\n        Copy `adata` before computation and return a copy. Otherwise, perform\n        computation inplace and return `None`.\n\n    Returns\n    -------\n    **connectivities** : :class:`numpy.ndarray` (adata.uns['connectivities'])\n        The full adjacency matrix of the abstracted graph, weights correspond to\n        confidence in the connectivities of partitions.\n    **connectivities_tree** : :class:`scipy.sparse.csr_matrix` (adata.uns['connectivities_tree'])\n        The adjacency matrix of the tree-like subgraph that best explains\n        the topology.\n\n    Notes\n    -----\n    Together with a random walk-based distance measure\n    (e.g. :func:`scanpy.tl.dpt`) this generates a partial coordinatization of\n    data useful for exploring and explaining its variation.\n\n    .. currentmodule:: scanpy\n\n    See Also\n    --------\n    pl.paga\n    pl.paga_path\n    pl.paga_compare\n    \"\"\"\n    check_neighbors = 'neighbors' if neighbors_key is None else neighbors_key\n    if check_neighbors not in adata.uns:\n        raise ValueError(\n            'You need to run `pp.neighbors` first to compute a neighborhood graph.'\n        )\n    if groups is None:\n        for k in (\"leiden\", \"louvain\"):\n            if k in adata.obs.columns:\n                groups = k\n                break\n    if groups is None:\n        raise ValueError(\n            'You need to run `tl.leiden` or `tl.louvain` to compute '\n            \"community labels, or specify `groups='an_existing_key'`\"\n        )\n    elif groups not in adata.obs.columns:\n        raise KeyError(f'`groups` key {groups!r} not found in `adata.obs`.')\n\n    adata = adata.copy() if copy else adata\n    _utils.sanitize_anndata(adata)\n    start = logg.info('running PAGA')\n    paga = PAGA(adata, groups, model=model, neighbors_key=neighbors_key)\n    # only add if not present\n    if 'paga' not in adata.uns:\n        adata.uns['paga'] = {}\n    if not use_rna_velocity:\n        paga.compute_connectivities()\n        adata.uns['paga']['connectivities'] = paga.connectivities\n        adata.uns['paga']['connectivities_tree'] = paga.connectivities_tree\n        # adata.uns['paga']['expected_n_edges_random'] = paga.expected_n_edges_random\n        adata.uns[groups + '_sizes'] = np.array(paga.ns)\n    else:\n        paga.compute_transitions()\n        adata.uns['paga']['transitions_confidence'] = paga.transitions_confidence\n        # adata.uns['paga']['transitions_ttest'] = paga.transitions_ttest\n    adata.uns['paga']['groups'] = groups\n    logg.info(\n        '    finished',\n        time=start,\n        deep='added\\n'\n        + (\n            \"    'paga/transitions_confidence', connectivities adjacency (adata.uns)\"\n            # \"    'paga/transitions_ttest', t-test on transitions (adata.uns)\"\n            if use_rna_velocity\n            else \"    'paga/connectivities', connectivities adjacency (adata.uns)\\n\"\n            \"    'paga/connectivities_tree', connectivities subtree (adata.uns)\"\n        ),\n    )\n    return adata if copy else None", "idx": 812}
{"project": "Scanpy", "commit_id": "833_scanpy_1.9.0__paga.py_paga_degrees.py", "target": 0, "func": "def paga_degrees(adata: AnnData) -> List[int]:\n    \"\"\"Compute the degree of each node in the abstracted graph.\n\n    Parameters\n    ----------\n    adata\n        Annotated data matrix.\n\n    Returns\n    -------\n    List of degrees for each node.\n    \"\"\"\n    import networkx as nx\n\n    g = nx.Graph(adata.uns['paga']['connectivities'])\n    degrees = [d for _, d in g.degree(weight='weight')]\n    return degrees", "idx": 813}
{"project": "Scanpy", "commit_id": "834_scanpy_1.9.0__paga.py_paga_expression_entropies.py", "target": 0, "func": "def paga_expression_entropies(adata) -> List[float]:\n    \"\"\"Compute the median expression entropy for each node-group.\n\n    Parameters\n    ----------\n    adata : AnnData\n        Annotated data matrix.\n\n    Returns\n    -------\n    Entropies of median expressions for each node.\n    \"\"\"\n    from scipy.stats import entropy\n\n    groups_order, groups_masks = _utils.select_groups(\n        adata, key=adata.uns['paga']['groups']\n    )\n    entropies = []\n    for mask in groups_masks:\n        X_mask = adata.X[mask].todense()\n        x_median = np.nanmedian(X_mask, axis=1, overwrite_input=True)\n        x_probs = (x_median - np.nanmin(x_median)) / (\n            np.nanmax(x_median) - np.nanmin(x_median)\n        )\n        entropies.append(entropy(x_probs))\n    return entropies", "idx": 814}
{"project": "Scanpy", "commit_id": "835_scanpy_1.9.0__paga.py_paga_compare_paths.py", "target": 0, "func": "def paga_compare_paths(\n    adata1: AnnData,\n    adata2: AnnData,\n    adjacency_key: str = 'connectivities',\n    adjacency_key2: Optional[str] = None,\n) -> PAGAComparePathsResult:\n    \"\"\"Compare paths in abstracted graphs in two datasets.\n\n    Compute the fraction of consistent paths between leafs, a measure for the\n    topological similarity between graphs.\n\n    By increasing the verbosity to level 4 and 5, the paths that do not agree\n    and the paths that agree are written to the output, respectively.\n\n    The PAGA \"groups key\" needs to be the same in both objects.\n\n    Parameters\n    ----------\n    adata1, adata2\n        Annotated data matrices to compare.\n    adjacency_key\n        Key for indexing the adjacency matrices in `.uns['paga']` to be used in\n        adata1 and adata2.\n    adjacency_key2\n        If provided, used for adata2.\n\n    Returns\n    -------\n    NamedTuple with attributes\n\n    frac_steps\n        fraction of consistent steps\n    n_steps\n        total number of steps in paths\n    frac_paths\n        Fraction of consistent paths\n    n_paths\n        Number of paths\n    \"\"\"\n    import networkx as nx\n\n    g1 = nx.Graph(adata1.uns['paga'][adjacency_key])\n    g2 = nx.Graph(\n        adata2.uns['paga'][\n            adjacency_key2 if adjacency_key2 is not None else adjacency_key\n        ]\n    )\n    leaf_nodes1 = [str(x) for x in g1.nodes() if g1.degree(x) == 1]\n    logg.debug(f'leaf nodes in graph 1: {leaf_nodes1}')\n    paga_groups = adata1.uns['paga']['groups']\n    asso_groups1 = _utils.identify_groups(\n        adata1.obs[paga_groups].values,\n        adata2.obs[paga_groups].values,\n    )\n    asso_groups2 = _utils.identify_groups(\n        adata2.obs[paga_groups].values,\n        adata1.obs[paga_groups].values,\n    )\n    orig_names1 = adata1.obs[paga_groups].cat.categories\n    orig_names2 = adata2.obs[paga_groups].cat.categories\n\n    import itertools\n\n    n_steps = 0\n    n_agreeing_steps = 0\n    n_paths = 0\n    n_agreeing_paths = 0\n    # loop over all pairs of leaf nodes in the reference adata1\n    for (r, s) in itertools.combinations(leaf_nodes1, r=2):\n        r2, s2 = asso_groups1[r][0], asso_groups1[s][0]\n        on1_g1, on2_g1 = [orig_names1[int(i)] for i in [r, s]]\n        on1_g2, on2_g2 = [orig_names2[int(i)] for i in [r2, s2]]\n        logg.debug(\n            f'compare shortest paths between leafs ({on1_g1}, {on2_g1}) '\n            f'in graph1 and ({on1_g2}, {on2_g2}) in graph2:'\n        )\n        try:\n            path1 = [str(x) for x in nx.shortest_path(g1, int(r), int(s))]\n        except nx.NetworkXNoPath:\n            path1 = None\n        try:\n            path2 = [str(x) for x in nx.shortest_path(g2, int(r2), int(s2))]\n        except nx.NetworkXNoPath:\n            path2 = None\n        if path1 is None and path2 is None:\n            # consistent behavior\n            n_paths += 1\n            n_agreeing_paths += 1\n            n_steps += 1\n            n_agreeing_steps += 1\n            logg.debug('there are no connecting paths in both graphs')\n            continue\n        elif path1 is None or path2 is None:\n            # non-consistent result\n            n_paths += 1\n            n_steps += 1\n            continue\n        if len(path1) >= len(path2):\n            path_mapped = [asso_groups1[l] for l in path1]\n            path_compare = path2\n            path_compare_id = 2\n            path_compare_orig_names = [\n                [orig_names2[int(s)] for s in l] for l in path_compare\n            ]\n            path_mapped_orig_names = [\n                [orig_names2[int(s)] for s in l] for l in path_mapped\n            ]\n        else:\n            path_mapped = [asso_groups2[l] for l in path2]\n            path_compare = path1\n            path_compare_id = 1\n            path_compare_orig_names = [\n                [orig_names1[int(s)] for s in l] for l in path_compare\n            ]\n            path_mapped_orig_names = [\n                [orig_names1[int(s)] for s in l] for l in path_mapped\n            ]\n        n_agreeing_steps_path = 0\n        ip_progress = 0\n        for il, l in enumerate(path_compare[:-1]):\n            for ip, p in enumerate(path_mapped):\n                if (\n                    ip < ip_progress\n                    or l not in p\n                    or not (\n                        ip + 1 < len(path_mapped)\n                        and path_compare[il + 1] in path_mapped[ip + 1]\n                    )\n                ):\n                    continue\n                # make sure that a step backward leads us to the same value of l\n                # in case we \"jumped\"\n                logg.debug(\n                    f'found matching step ({l} -> {path_compare_orig_names[il + 1]}) '\n                    f'at position {il} in path{path_compare_id} and position {ip} in path_mapped'\n                )\n                consistent_history = True\n                for iip in range(ip, ip_progress, -1):\n                    if l not in path_mapped[iip - 1]:\n                        consistent_history = False\n                if consistent_history:\n                    # here, we take one step further back (ip_progress - 1); it's implied that this\n                    # was ok in the previous step\n                    poss = list(range(ip - 1, ip_progress - 2, -1))\n                    logg.debug(\n                        f'    step(s) backward to position(s) {poss} '\n                        'in path_mapped are fine, too: valid step'\n                    )\n                    n_agreeing_steps_path += 1\n                    ip_progress = ip + 1\n                    break\n        n_steps_path = len(path_compare) - 1\n        n_agreeing_steps += n_agreeing_steps_path\n        n_steps += n_steps_path\n        n_paths += 1\n        if n_agreeing_steps_path == n_steps_path:\n            n_agreeing_paths += 1\n\n        # only for the output, use original names\n        path1_orig_names = [orig_names1[int(s)] for s in path1]\n        path2_orig_names = [orig_names2[int(s)] for s in path2]\n        logg.debug(\n            f'      path1 = {path1_orig_names},\\n'\n            f'path_mapped = {[list(p) for p in path_mapped_orig_names]},\\n'\n            f'      path2 = {path2_orig_names},\\n'\n            f'-> n_agreeing_steps = {n_agreeing_steps_path} / n_steps = {n_steps_path}.',\n        )\n    return PAGAComparePathsResult(\n        frac_steps=n_agreeing_steps / n_steps if n_steps > 0 else np.nan,\n        n_steps=n_steps if n_steps > 0 else np.nan,\n        frac_paths=n_agreeing_paths / n_paths if n_steps > 0 else np.nan,\n        n_paths=n_paths if n_steps > 0 else np.nan,", "idx": 815}
{"project": "Scanpy", "commit_id": "836_scanpy_1.9.0__paga.py___init__.py", "target": 0, "func": "def __init__(self, adata, groups, model='v1.2', neighbors_key=None):\n        assert groups in adata.obs.columns\n        self._adata = adata\n        self._neighbors = Neighbors(adata, neighbors_key=neighbors_key)\n        self._model = model\n        self._groups_key = groups", "idx": 816}
{"project": "Scanpy", "commit_id": "837_scanpy_1.9.0__paga.py_compute_connectivities.py", "target": 0, "func": "def compute_connectivities(self):\n        if self._model == 'v1.2':\n            return self._compute_connectivities_v1_2()\n        elif self._model == 'v1.0':\n            return self._compute_connectivities_v1_0()\n        else:\n            raise ValueError(\n                f'`model` {self._model} needs to be one of {_AVAIL_MODELS}.'", "idx": 817}
{"project": "Scanpy", "commit_id": "838_scanpy_1.9.0__paga.py__compute_connectivities_v1_2.py", "target": 1, "func": "def _compute_connectivities_v1_2(self):\n        import igraph\n\n        ones = self._neighbors.distances.copy()\n        ones.data = np.ones(len(ones.data))\n        # should be directed if we deal with distances\n        g = _utils.get_igraph_from_adjacency(ones, directed=True)\n        vc = igraph.VertexClustering(\n            g, membership=self._adata.obs[self._groups_key].cat.codes.values\n        )\n        ns = vc.sizes()\n        n = sum(ns)\n        es_inner_cluster = [vc.subgraph(i).ecount() for i in range(len(ns))]\n        cg = vc.cluster_graph(combine_edges='sum')\n        inter_es = _utils.get_sparse_from_igraph(cg, weight_attr='weight')\n        es = np.array(es_inner_cluster) + inter_es.sum(axis=1).A1\n        inter_es = inter_es + inter_es.T  # \\epsilon_i + \\epsilon_j\n        connectivities = inter_es.copy()\n        expected_n_edges = inter_es.copy()\n        inter_es = inter_es.tocoo()\n        for i, j, v in zip(inter_es.row, inter_es.col, inter_es.data):\n            expected_random_null = (es[i] * ns[j] + es[j] * ns[i]) / (n - 1)\n            if expected_random_null != 0:\n                scaled_value = v / expected_random_null\n            else:\n                scaled_value = 1\n            if scaled_value > 1:\n                scaled_value = 1\n            connectivities[i, j] = scaled_value\n            expected_n_edges[i, j] = expected_random_null\n        # set attributes\n        self.ns = ns\n        self.expected_n_edges_random = expected_n_edges\n        self.connectivities = connectivities\n        self.connectivities_tree = self._get_connectivities_tree_v1_2()\n        return inter_es.tocsr(), connectivities", "idx": 818}
{"project": "Scanpy", "commit_id": "839_scanpy_1.9.0__paga.py__compute_connectivities_v1_0.py", "target": 1, "func": "def _compute_connectivities_v1_0(self):\n        import igraph\n\n        ones = self._neighbors.connectivities.copy()\n        ones.data = np.ones(len(ones.data))\n        g = _utils.get_igraph_from_adjacency(ones)\n        vc = igraph.VertexClustering(\n            g, membership=self._adata.obs[self._groups_key].cat.codes.values\n        )\n        ns = vc.sizes()\n        cg = vc.cluster_graph(combine_edges='sum')\n        inter_es = _utils.get_sparse_from_igraph(cg, weight_attr='weight') / 2\n        connectivities = inter_es.copy()\n        inter_es = inter_es.tocoo()\n        n_neighbors_sq = self._neighbors.n_neighbors**2\n        for i, j, v in zip(inter_es.row, inter_es.col, inter_es.data):\n            # have n_neighbors**2 inside sqrt for backwards compat\n            geom_mean_approx_knn = np.sqrt(n_neighbors_sq * ns[i] * ns[j])\n            if geom_mean_approx_knn != 0:\n                scaled_value = v / geom_mean_approx_knn\n            else:\n                scaled_value = 1\n            connectivities[i, j] = scaled_value\n        # set attributes\n        self.ns = ns\n        self.connectivities = connectivities\n        self.connectivities_tree = self._get_connectivities_tree_v1_0(inter_es)\n        return inter_es.tocsr(), connectivities", "idx": 819}
{"project": "Scanpy", "commit_id": "83_scanpy_1.9.0__settings.py_override.py", "target": 0, "func": "def override(self, verbosity: \"Verbosity\") -> ContextManager[\"Verbosity\"]:\n        \"\"\"\\\n        Temporarily override verbosity\n        \"\"\"\n        settings.verbosity = verbosity\n        yield self\n        settings.verbosity = self", "idx": 820}
{"project": "Scanpy", "commit_id": "840_scanpy_1.9.0__paga.py__get_connectivities_tree_v1_2.py", "target": 0, "func": "def _get_connectivities_tree_v1_2(self):\n        inverse_connectivities = self.connectivities.copy()\n        inverse_connectivities.data = 1.0 / inverse_connectivities.data\n        connectivities_tree = minimum_spanning_tree(inverse_connectivities)\n        connectivities_tree_indices = [\n            connectivities_tree[i].nonzero()[1]\n            for i in range(connectivities_tree.shape[0])\n        ]\n        connectivities_tree = sp.sparse.lil_matrix(\n            self.connectivities.shape, dtype=float\n        )\n        for i, neighbors in enumerate(connectivities_tree_indices):\n            if len(neighbors) > 0:\n                connectivities_tree[i, neighbors] = self.connectivities[i, neighbors]\n        return connectivities_tree.tocsr()", "idx": 821}
{"project": "Scanpy", "commit_id": "841_scanpy_1.9.0__paga.py__get_connectivities_tree_v1_0.py", "target": 0, "func": "def _get_connectivities_tree_v1_0(self, inter_es):\n        inverse_inter_es = inter_es.copy()\n        inverse_inter_es.data = 1.0 / inverse_inter_es.data\n        connectivities_tree = minimum_spanning_tree(inverse_inter_es)\n        connectivities_tree_indices = [\n            connectivities_tree[i].nonzero()[1]\n            for i in range(connectivities_tree.shape[0])\n        ]\n        connectivities_tree = sp.sparse.lil_matrix(inter_es.shape, dtype=float)\n        for i, neighbors in enumerate(connectivities_tree_indices):\n            if len(neighbors) > 0:\n                connectivities_tree[i, neighbors] = self.connectivities[i, neighbors]\n        return connectivities_tree.tocsr()", "idx": 822}
{"project": "Scanpy", "commit_id": "842_scanpy_1.9.0__paga.py_compute_transitions.py", "target": 1, "func": "def compute_transitions(self):\n        vkey = 'velocity_graph'\n        if vkey not in self._adata.uns:\n            if 'velocyto_transitions' in self._adata.uns:\n                self._adata.uns[vkey] = self._adata.uns['velocyto_transitions']\n                logg.debug(\n                    \"The key 'velocyto_transitions' has been changed to 'velocity_graph'.\"\n                )\n            else:\n                raise ValueError(\n                    'The passed AnnData needs to have an `uns` annotation '\n                    \"with key 'velocity_graph' - a sparse matrix from RNA velocity.\"\n                )\n        if self._adata.uns[vkey].shape != (self._adata.n_obs, self._adata.n_obs):\n            raise ValueError(\n                f\"The passed 'velocity_graph' have shape {self._adata.uns[vkey].shape} \"\n                f\"but shoud have shape {(self._adata.n_obs, self._adata.n_obs)}\"\n            )\n        # restore this at some point\n        # if 'expected_n_edges_random' not in self._adata.uns['paga']:\n        #     raise ValueError(\n        #         'Before running PAGA with `use_rna_velocity=True`, run it with `False`.')\n        import igraph\n\n        g = _utils.get_igraph_from_adjacency(\n            self._adata.uns[vkey].astype('bool'),\n            directed=True,\n        )\n        vc = igraph.VertexClustering(\n            g, membership=self._adata.obs[self._groups_key].cat.codes.values\n        )\n        # set combine_edges to False if you want self loops\n        cg_full = vc.cluster_graph(combine_edges='sum')\n        transitions = _utils.get_sparse_from_igraph(cg_full, weight_attr='weight')\n        transitions = transitions - transitions.T\n        transitions_conf = transitions.copy()\n        transitions = transitions.tocoo()\n        total_n = self._neighbors.n_neighbors * np.array(vc.sizes())\n        # total_n_sum = sum(total_n)\n        # expected_n_edges_random = self._adata.uns['paga']['expected_n_edges_random']\n        for i, j, v in zip(transitions.row, transitions.col, transitions.data):\n            # if expected_n_edges_random[i, j] != 0:\n            #     # factor 0.5 because of asymmetry\n            #     reference = 0.5 * expected_n_edges_random[i, j]\n            # else:\n            #     # approximate\n            #     reference = self._neighbors.n_neighbors * total_n[i] * total_n[j] / total_n_sum\n            reference = np.sqrt(total_n[i] * total_n[j])\n            transitions_conf[i, j] = 0 if v < 0 else v / reference\n        transitions_conf.eliminate_zeros()\n        # transpose in order to match convention of stochastic matrices\n        # entry ij means transition from j to i\n        self.transitions_confidence = transitions_conf.T", "idx": 823}
{"project": "Scanpy", "commit_id": "843_scanpy_1.9.0__paga.py_compute_transitions_old.py", "target": 1, "func": "def compute_transitions_old(self):\n        import igraph\n\n        g = _utils.get_igraph_from_adjacency(\n            self._adata.uns['velocyto_transitions'],\n            directed=True,\n        )\n        vc = igraph.VertexClustering(\n            g, membership=self._adata.obs[self._groups_key].cat.codes.values\n        )\n        # this stores all single-cell edges in the cluster graph\n        cg_full = vc.cluster_graph(combine_edges=False)\n        # this is the boolean version that simply counts edges in the clustered graph\n        g_bool = _utils.get_igraph_from_adjacency(\n            self._adata.uns['velocyto_transitions'].astype('bool'),\n            directed=True,\n        )\n        vc_bool = igraph.VertexClustering(\n            g_bool, membership=self._adata.obs[self._groups_key].cat.codes.values\n        )\n        cg_bool = vc_bool.cluster_graph(combine_edges='sum')  # collapsed version\n        transitions = _utils.get_sparse_from_igraph(cg_bool, weight_attr='weight')\n        total_n = self._neighbors.n_neighbors * np.array(vc_bool.sizes())\n        transitions_ttest = transitions.copy()\n        transitions_confidence = transitions.copy()\n        from scipy.stats import ttest_1samp\n\n        for i in range(transitions.shape[0]):\n            neighbors = transitions[i].nonzero()[1]\n            for j in neighbors:\n                forward = cg_full.es.select(_source=i, _target=j)['weight']\n                backward = cg_full.es.select(_source=j, _target=i)['weight']\n                # backward direction: add minus sign\n                values = np.array(list(forward) + list(-np.array(backward)))\n                # require some minimal number of observations\n                if len(values) < 5:\n                    transitions_ttest[i, j] = 0\n                    transitions_ttest[j, i] = 0\n                    transitions_confidence[i, j] = 0\n                    transitions_confidence[j, i] = 0\n                    continue\n                t, prob = ttest_1samp(values, 0.0)\n                if t > 0:\n                    # number of outgoing edges greater than number of ingoing edges\n                    # i.e., transition from i to j\n                    transitions_ttest[i, j] = -np.log10(max(prob, 1e-10))\n                    transitions_ttest[j, i] = 0\n                else:\n                    transitions_ttest[j, i] = -np.log10(max(prob, 1e-10))\n                    transitions_ttest[i, j] = 0\n                # geom_mean\n                geom_mean = np.sqrt(total_n[i] * total_n[j])\n                diff = (len(forward) - len(backward)) / geom_mean\n                if diff > 0:\n                    transitions_confidence[i, j] = diff\n                    transitions_confidence[j, i] = 0\n                else:\n                    transitions_confidence[j, i] = -diff\n                    transitions_confidence[i, j] = 0\n        transitions_ttest.eliminate_zeros()\n        transitions_confidence.eliminate_zeros()\n        # transpose in order to match convention of stochastic matrices\n        # entry ij means transition from j to i\n        self.transitions_ttest = transitions_ttest.T\n        self.transitions_confidence = transitions_confidence.T", "idx": 824}
{"project": "Scanpy", "commit_id": "844_scanpy_1.9.0__rank_genes_groups.py__select_top_n.py", "target": 0, "func": "def _select_top_n(scores, n_top):\n    n_from = scores.shape[0]\n    reference_indices = np.arange(n_from, dtype=int)\n    partition = np.argpartition(scores, -n_top)[-n_top:]\n    partial_indices = np.argsort(scores[partition])[::-1]\n    global_indices = reference_indices[partition][partial_indices]\n\n    return global_indices", "idx": 825}
{"project": "Scanpy", "commit_id": "845_scanpy_1.9.0__rank_genes_groups.py__ranks.py", "target": 0, "func": "def _ranks(X, mask=None, mask_rest=None):\n    CONST_MAX_SIZE = 10000000\n\n    n_genes = X.shape[1]\n\n    if issparse(X):\n        merge = lambda tpl: vstack(tpl).toarray()\n        adapt = lambda X: X.toarray()\n    else:\n        merge = np.vstack\n        adapt = lambda X: X\n\n    masked = mask is not None and mask_rest is not None\n\n    if masked:\n        n_cells = np.count_nonzero(mask) + np.count_nonzero(mask_rest)\n        get_chunk = lambda X, left, right: merge(\n            (X[mask, left:right], X[mask_rest, left:right])\n        )\n    else:\n        n_cells = X.shape[0]\n        get_chunk = lambda X, left, right: adapt(X[:, left:right])\n\n    # Calculate chunk frames\n    max_chunk = floor(CONST_MAX_SIZE / n_cells)\n\n    for left in range(0, n_genes, max_chunk):\n        right = min(left + max_chunk, n_genes)\n\n        df = pd.DataFrame(data=get_chunk(X, left, right))\n        ranks = df.rank()\n        yield ranks, left, right", "idx": 826}
{"project": "Scanpy", "commit_id": "846_scanpy_1.9.0__rank_genes_groups.py__tiecorrect.py", "target": 0, "func": "def _tiecorrect(ranks):\n    size = np.float64(ranks.shape[0])\n    if size < 2:\n        return np.repeat(ranks.shape[1], 1.0)\n\n    arr = np.sort(ranks, axis=0)\n    tf = np.insert(arr[1:] != arr[:-1], (0, arr.shape[0] - 1), True, axis=0)\n    idx = np.where(tf, np.arange(tf.shape[0])[:, None], 0)\n    idx = np.sort(idx, axis=0)\n    cnt = np.diff(idx, axis=0).astype(np.float64)\n\n    return 1.0 - (cnt**3 - cnt).sum(axis=0) / (size**3 - size)", "idx": 827}
{"project": "Scanpy", "commit_id": "847_scanpy_1.9.0__rank_genes_groups.py_rank_genes_groups.py", "target": 0, "func": "def rank_genes_groups(\n    adata: AnnData,\n    groupby: str,\n    use_raw: Optional[bool] = None,\n    groups: Union[Literal['all'], Iterable[str]] = 'all',\n    reference: str = 'rest',\n    n_genes: Optional[int] = None,\n    rankby_abs: bool = False,\n    pts: bool = False,\n    key_added: Optional[str] = None,\n    copy: bool = False,\n    method: _Method = None,\n    corr_method: _CorrMethod = 'benjamini-hochberg',\n    tie_correct: bool = False,\n    layer: Optional[str] = None,\n    **kwds,\n) -> Optional[AnnData]:\n    \"\"\"\\\n    Rank genes for characterizing groups.\n\n    Expects logarithmized data.\n\n    Parameters\n    ----------\n    adata\n        Annotated data matrix.\n    groupby\n        The key of the observations grouping to consider.\n    use_raw\n        Use `raw` attribute of `adata` if present.\n    layer\n        Key from `adata.layers` whose value will be used to perform tests on.\n    groups\n        Subset of groups, e.g. [`'g1'`, `'g2'`, `'g3'`], to which comparison\n        shall be restricted, or `'all'` (default), for all groups.\n    reference\n        If `'rest'`, compare each group to the union of the rest of the group.\n        If a group identifier, compare with respect to this group.\n    n_genes\n        The number of genes that appear in the returned tables.\n        Defaults to all genes.\n    method\n        The default method is `'t-test'`,\n        `'t-test_overestim_var'` overestimates variance of each group,\n        `'wilcoxon'` uses Wilcoxon rank-sum,\n        `'logreg'` uses logistic regression. See [Ntranos18]_,\n        `here <https://github.com/theislab/scanpy/issues/95>`__ and `here\n        <http://www.nxn.se/valent/2018/3/5/actionable-scrna-seq-clusters>`__,\n        for why this is meaningful.\n    corr_method\n        p-value correction method.\n        Used only for `'t-test'`, `'t-test_overestim_var'`, and `'wilcoxon'`.\n    tie_correct\n        Use tie correction for `'wilcoxon'` scores.\n        Used only for `'wilcoxon'`.\n    rankby_abs\n        Rank genes by the absolute value of the score, not by the\n        score. The returned scores are never the absolute values.\n    pts\n        Compute the fraction of cells expressing the genes.\n    key_added\n        The key in `adata.uns` information is saved to.\n    **kwds\n        Are passed to test methods. Currently this affects only parameters that\n        are passed to :class:`sklearn.linear_model.LogisticRegression`.\n        For instance, you can pass `penalty='l1'` to try to come up with a\n        minimal set of genes that are good predictors (sparse solution meaning\n        few non-zero fitted coefficients).\n\n    Returns\n    -------\n    **names** : structured `np.ndarray` (`.uns['rank_genes_groups']`)\n        Structured array to be indexed by group id storing the gene\n        names. Ordered according to scores.\n    **scores** : structured `np.ndarray` (`.uns['rank_genes_groups']`)\n        Structured array to be indexed by group id storing the z-score\n        underlying the computation of a p-value for each gene for each\n        group. Ordered according to scores.\n    **logfoldchanges** : structured `np.ndarray` (`.uns['rank_genes_groups']`)\n        Structured array to be indexed by group id storing the log2\n        fold change for each gene for each group. Ordered according to\n        scores. Only provided if method is 't-test' like.\n        Note: this is an approximation calculated from mean-log values.\n    **pvals** : structured `np.ndarray` (`.uns['rank_genes_groups']`)\n        p-values.\n    **pvals_adj** : structured `np.ndarray` (`.uns['rank_genes_groups']`)\n        Corrected p-values.\n    **pts** : `pandas.DataFrame` (`.uns['rank_genes_groups']`)\n        Fraction of cells expressing the genes for each group.\n    **pts_rest** : `pandas.DataFrame` (`.uns['rank_genes_groups']`)\n        Only if `reference` is set to `'rest'`.\n        Fraction of cells from the union of the rest of each group\n        expressing the genes.\n\n    Notes\n    -----\n    There are slight inconsistencies depending on whether sparse\n    or dense data are passed. See `here <https://github.com/theislab/scanpy/blob/master/scanpy/tests/test_rank_genes_groups.py>`__.\n\n    Examples\n    --------\n    >>> import scanpy as sc\n    >>> adata = sc.datasets.pbmc68k_reduced()\n    >>> sc.tl.rank_genes_groups(adata, 'bulk_labels', method='wilcoxon')\n    >>> # to visualize the results\n    >>> sc.pl.rank_genes_groups(adata)\n    \"\"\"\n    if use_raw is None:\n        use_raw = adata.raw is not None\n    elif use_raw is True and adata.raw is None:\n        raise ValueError(\"Received `use_raw=True`, but `adata.raw` is empty.\")\n\n    if method is None:\n        logg.warning(\n            \"Default of the method has been changed to 't-test' from 't-test_overestim_var'\"\n        )\n        method = 't-test'\n\n    if 'only_positive' in kwds:\n        rankby_abs = not kwds.pop('only_positive')  # backwards compat\n\n    start = logg.info('ranking genes')\n    avail_methods = {'t-test', 't-test_overestim_var', 'wilcoxon', 'logreg'}\n    if method not in avail_methods:\n        raise ValueError(f'Method must be one of {avail_methods}.')\n\n    avail_corr = {'benjamini-hochberg', 'bonferroni'}\n    if corr_method not in avail_corr:\n        raise ValueError(f'Correction method must be one of {avail_corr}.')\n\n    adata = adata.copy() if copy else adata\n    _utils.sanitize_anndata(adata)\n    # for clarity, rename variable\n    if groups == 'all':\n        groups_order = 'all'\n    elif isinstance(groups, (str, int)):\n        raise ValueError('Specify a sequence of groups')\n    else:\n        groups_order = list(groups)\n        if isinstance(groups_order[0], int):\n            groups_order = [str(n) for n in groups_order]\n        if reference != 'rest' and reference not in set(groups_order):\n            groups_order += [reference]\n    if reference != 'rest' and reference not in adata.obs[groupby].cat.categories:\n        cats = adata.obs[groupby].cat.categories.tolist()\n        raise ValueError(\n            f'reference = {reference} needs to be one of groupby = {cats}.'\n        )\n\n    if key_added is None:\n        key_added = 'rank_genes_groups'\n    adata.uns[key_added] = {}\n    adata.uns[key_added]['params'] = dict(\n        groupby=groupby,\n        reference=reference,\n        method=method,\n        use_raw=use_raw,\n        layer=layer,\n        corr_method=corr_method,\n    )\n\n    test_obj = _RankGenes(adata, groups_order, groupby, reference, use_raw, layer, pts)\n\n    if check_nonnegative_integers(test_obj.X) and method != 'logreg':\n        logg.warning(\n            \"It seems you use rank_genes_groups on the raw count data. \"\n            \"Please logarithmize your data before calling rank_genes_groups.\"\n        )\n\n    # for clarity, rename variable\n    n_genes_user = n_genes\n    # make sure indices are not OoB in case there are less genes than n_genes\n    # defaults to all genes\n    if n_genes_user is None or n_genes_user > test_obj.X.shape[1]:\n        n_genes_user = test_obj.X.shape[1]\n\n    logg.debug(f'consider {groupby!r} groups:')\n    logg.debug(f'with sizes: {np.count_nonzero(test_obj.groups_masks, axis=1)}')\n\n    test_obj.compute_statistics(\n        method, corr_method, n_genes_user, rankby_abs, tie_correct, **kwds\n    )\n\n    if test_obj.pts is not None:\n        groups_names = [str(name) for name in test_obj.groups_order]\n        adata.uns[key_added]['pts'] = pd.DataFrame(\n            test_obj.pts.T, index=test_obj.var_names, columns=groups_names\n        )\n    if test_obj.pts_rest is not None:\n        adata.uns[key_added]['pts_rest'] = pd.DataFrame(\n            test_obj.pts_rest.T, index=test_obj.var_names, columns=groups_names\n        )\n\n    test_obj.stats.columns = test_obj.stats.columns.swaplevel()\n\n    dtypes = {\n        'names': 'O',\n        'scores': 'float32',\n        'logfoldchanges': 'float32',\n        'pvals': 'float64',\n        'pvals_adj': 'float64',\n    }\n\n    for col in test_obj.stats.columns.levels[0]:\n        adata.uns[key_added][col] = test_obj.stats[col].to_records(\n            index=False, column_dtypes=dtypes[col]\n        )\n\n    logg.info(\n        '    finished',\n        time=start,\n        deep=(\n            f'added to `.uns[{key_added!r}]`\\n'\n            \"    'names', sorted np.recarray to be indexed by group ids\\n\"\n            \"    'scores', sorted np.recarray to be indexed by group ids\\n\"\n            + (\n                \"    'logfoldchanges', sorted np.recarray to be indexed by group ids\\n\"\n                \"    'pvals', sorted np.recarray to be indexed by group ids\\n\"\n                \"    'pvals_adj', sorted np.recarray to be indexed by group ids\"\n                if method in {'t-test', 't-test_overestim_var', 'wilcoxon'}\n                else ''\n            )\n        ),\n    )\n    return adata if copy else None", "idx": 828}
{"project": "Scanpy", "commit_id": "848_scanpy_1.9.0__rank_genes_groups.py__calc_frac.py", "target": 0, "func": "def _calc_frac(X):\n    if issparse(X):\n        n_nonzero = X.getnnz(axis=0)\n    else:\n        n_nonzero = np.count_nonzero(X, axis=0)\n    return n_nonzero / X.shape[0]", "idx": 829}
{"project": "Scanpy", "commit_id": "849_scanpy_1.9.0__rank_genes_groups.py_filter_rank_genes_groups.py", "target": 1, "func": "def filter_rank_genes_groups(\n    adata: AnnData,\n    key=None,\n    groupby=None,\n    use_raw=None,\n    key_added='rank_genes_groups_filtered',\n    min_in_group_fraction=0.25,\n    min_fold_change=1,\n    max_out_group_fraction=0.5,\n    compare_abs=False,\n) -> None:\n    \"\"\"\\\n    Filters out genes based on log fold change and fraction of genes expressing the\n    gene within and outside the `groupby` categories.\n\n    See :func:`~scanpy.tl.rank_genes_groups`.\n\n    Results are stored in `adata.uns[key_added]`\n    (default: 'rank_genes_groups_filtered').\n\n    To preserve the original structure of adata.uns['rank_genes_groups'],\n    filtered genes are set to `NaN`.\n\n    Parameters\n    ----------\n    adata\n    key\n    groupby\n    use_raw\n    key_added\n    min_in_group_fraction\n    min_fold_change\n    max_out_group_fraction\n    compare_abs\n        If `True`, compare absolute values of log fold change with `min_fold_change`.\n\n    Returns\n    -------\n    Same output as :func:`scanpy.tl.rank_genes_groups` but with filtered genes names set to\n    `nan`\n\n    Examples\n    --------\n    >>> import scanpy as sc\n    >>> adata = sc.datasets.pbmc68k_reduced()\n    >>> sc.tl.rank_genes_groups(adata, 'bulk_labels', method='wilcoxon')\n    >>> sc.tl.filter_rank_genes_groups(adata, min_fold_change=3)\n    >>> # visualize results\n    >>> sc.pl.rank_genes_groups(adata, key='rank_genes_groups_filtered')\n    >>> # visualize results using dotplot\n    >>> sc.pl.rank_genes_groups_dotplot(adata, key='rank_genes_groups_filtered')\n    \"\"\"\n    if key is None:\n        key = 'rank_genes_groups'\n\n    if groupby is None:\n        groupby = adata.uns[key]['params']['groupby']\n\n    if use_raw is None:\n        use_raw = adata.uns[key]['params']['use_raw']\n\n    same_params = (\n        adata.uns[key]['params']['groupby'] == groupby\n        and adata.uns[key]['params']['reference'] == 'rest'\n        and adata.uns[key]['params']['use_raw'] == use_raw\n    )\n\n    use_logfolds = same_params and 'logfoldchanges' in adata.uns[key]\n    use_fraction = same_params and 'pts_rest' in adata.uns[key]\n\n    # convert structured numpy array into DataFrame\n    gene_names = pd.DataFrame(adata.uns[key]['names'])\n\n    fraction_in_cluster_matrix = pd.DataFrame(\n        np.zeros(gene_names.shape),\n        columns=gene_names.columns,\n        index=gene_names.index,\n    )\n    fraction_out_cluster_matrix = pd.DataFrame(\n        np.zeros(gene_names.shape),\n        columns=gene_names.columns,\n        index=gene_names.index,\n    )\n\n    if use_logfolds:\n        fold_change_matrix = pd.DataFrame(adata.uns[key]['logfoldchanges'])\n    else:\n        fold_change_matrix = pd.DataFrame(\n            np.zeros(gene_names.shape),\n            columns=gene_names.columns,\n            index=gene_names.index,\n        )\n\n        if 'log1p' in adata.uns_keys() and adata.uns['log1p']['base'] is not None:\n            expm1_func = lambda x: np.expm1(x * np.log(adata.uns['log1p']['base']))\n        else:\n            expm1_func = np.expm1\n\n    logg.info(\n        f\"Filtering genes using: \"\n        f\"min_in_group_fraction: {min_in_group_fraction} \"\n        f\"min_fold_change: {min_fold_change}, \"\n        f\"max_out_group_fraction: {max_out_group_fraction}\"\n    )\n\n    for cluster in gene_names.columns:\n        # iterate per column\n        var_names = gene_names[cluster].values\n\n        if not use_logfolds or not use_fraction:\n            sub_X = adata.raw[:, var_names].X if use_raw else adata[:, var_names].X\n            in_group = adata.obs[groupby] == cluster\n            X_in = sub_X[in_group]\n            X_out = sub_X[~in_group]\n\n        if use_fraction:\n            fraction_in_cluster_matrix.loc[:, cluster] = (\n                adata.uns[key]['pts'][cluster].loc[var_names].values\n            )\n            fraction_out_cluster_matrix.loc[:, cluster] = (\n                adata.uns[key]['pts_rest'][cluster].loc[var_names].values\n            )\n        else:\n            fraction_in_cluster_matrix.loc[:, cluster] = _calc_frac(X_in)\n            fraction_out_cluster_matrix.loc[:, cluster] = _calc_frac(X_out)\n\n        if not use_logfolds:\n            # compute mean value\n            mean_in_cluster = np.ravel(X_in.mean(0))\n            mean_out_cluster = np.ravel(X_out.mean(0))\n            # compute fold change\n            fold_change_matrix.loc[:, cluster] = np.log2(\n                (expm1_func(mean_in_cluster) + 1e-9)\n                / (expm1_func(mean_out_cluster) + 1e-9)\n            )\n\n    if compare_abs:\n        fold_change_matrix = fold_change_matrix.abs()\n    # filter original_matrix\n    gene_names = gene_names[\n        (fraction_in_cluster_matrix > min_in_group_fraction)\n        & (fraction_out_cluster_matrix < max_out_group_fraction)\n        & (fold_change_matrix > min_fold_change)\n    ]\n    # create new structured array using 'key_added'.\n    adata.uns[key_added] = adata.uns[key].copy()\n    adata.uns[key_added]['names'] = gene_names.to_records(index=False)", "idx": 830}
{"project": "Scanpy", "commit_id": "84_scanpy_1.9.0__settings.py___init__.py", "target": 0, "func": "def __init__(\n        self,\n        *,\n        verbosity: str = \"warning\",\n        plot_suffix: str = \"\",\n        file_format_data: str = \"h5ad\",\n        file_format_figs: str = \"pdf\",\n        autosave: bool = False,\n        autoshow: bool = True,\n        writedir: Union[str, Path] = \"./write/\",\n        cachedir: Union[str, Path] = \"./cache/\",\n        datasetdir: Union[str, Path] = \"./data/\",\n        figdir: Union[str, Path] = \"./figures/\",\n        cache_compression: Union[str, None] = 'lzf',\n        max_memory=15,\n        n_jobs=1,\n        logfile: Union[str, Path, None] = None,\n        categories_to_ignore: Iterable[str] = (\"N/A\", \"dontknow\", \"no_gate\", \"?\"),\n        _frameon: bool = True,\n        _vector_friendly: bool = False,\n        _low_resolution_warning: bool = True,\n        n_pcs=50,\n    ):\n        # logging\n        self._root_logger = _RootLogger(logging.INFO)  # level will be replaced\n        self.logfile = logfile\n        self.verbosity = verbosity\n        # rest\n        self.plot_suffix = plot_suffix\n        self.file_format_data = file_format_data\n        self.file_format_figs = file_format_figs\n        self.autosave = autosave\n        self.autoshow = autoshow\n        self.writedir = writedir\n        self.cachedir = cachedir\n        self.datasetdir = datasetdir\n        self.figdir = figdir\n        self.cache_compression = cache_compression\n        self.max_memory = max_memory\n        self.n_jobs = n_jobs\n        self.categories_to_ignore = categories_to_ignore\n        self._frameon = _frameon\n        \"\"\"bool: See set_figure_params.\"\"\"\n\n        self._vector_friendly = _vector_friendly\n        \"\"\"Set to true if you want to include pngs in svgs and pdfs.\"\"\"\n\n        self._low_resolution_warning = _low_resolution_warning\n        \"\"\"Print warning when saving a figure with low resolution.\"\"\"\n\n        self._start = time()\n        \"\"\"Time when the settings module is first imported.\"\"\"\n\n        self._previous_time = self._start\n        \"\"\"Variable for timing program parts.\"\"\"\n\n        self._previous_memory_usage = -1\n        \"\"\"Stores the previous memory usage.\"\"\"\n\n        self.N_PCS = n_pcs\n        \"\"\"Default number of principal components to use.\"\"\"", "idx": 831}
{"project": "Scanpy", "commit_id": "850_scanpy_1.9.0__rank_genes_groups.py___init__.py", "target": 1, "func": "def __init__(\n        self,\n        adata,\n        groups,\n        groupby,\n        reference='rest',\n        use_raw=True,\n        layer=None,\n        comp_pts=False,\n    ):\n\n        if 'log1p' in adata.uns_keys() and adata.uns['log1p']['base'] is not None:\n            self.expm1_func = lambda x: np.expm1(x * np.log(adata.uns['log1p']['base']))\n        else:\n            self.expm1_func = np.expm1\n\n        self.groups_order, self.groups_masks = _utils.select_groups(\n            adata, groups, groupby\n        )\n\n        # Singlet groups cause division by zero errors\n        invalid_groups_selected = set(self.groups_order) & set(\n            adata.obs[groupby].value_counts().loc[lambda x: x < 2].index\n        )\n\n        if len(invalid_groups_selected) > 0:\n            raise ValueError(\n                \"Could not calculate statistics for groups {} since they only \"\n                \"contain one sample.\".format(', '.join(invalid_groups_selected))\n            )\n\n        adata_comp = adata\n        if layer is not None:\n            if use_raw:\n                raise ValueError(\"Cannot specify `layer` and have `use_raw=True`.\")\n            X = adata_comp.layers[layer]\n        else:\n            if use_raw and adata.raw is not None:\n                adata_comp = adata.raw\n            X = adata_comp.X\n\n        # for correct getnnz calculation\n        if issparse(X):\n            X.eliminate_zeros()\n\n        self.X = X\n        self.var_names = adata_comp.var_names\n\n        self.ireference = None\n        if reference != 'rest':\n            self.ireference = np.where(self.groups_order == reference)[0][0]\n\n        self.means = None\n        self.vars = None\n\n        self.means_rest = None\n        self.vars_rest = None\n\n        self.comp_pts = comp_pts\n        self.pts = None\n        self.pts_rest = None\n\n        self.stats = None\n\n        # for logreg only\n        self.grouping_mask = adata.obs[groupby].isin(self.groups_order)\n        self.grouping = adata.obs.loc[self.grouping_mask, groupby]", "idx": 832}
{"project": "Scanpy", "commit_id": "851_scanpy_1.9.0__rank_genes_groups.py__basic_stats.py", "target": 0, "func": "def _basic_stats(self):\n        n_genes = self.X.shape[1]\n        n_groups = self.groups_masks.shape[0]\n\n        self.means = np.zeros((n_groups, n_genes))\n        self.vars = np.zeros((n_groups, n_genes))\n        self.pts = np.zeros((n_groups, n_genes)) if self.comp_pts else None\n\n        if self.ireference is None:\n            self.means_rest = np.zeros((n_groups, n_genes))\n            self.vars_rest = np.zeros((n_groups, n_genes))\n            self.pts_rest = np.zeros((n_groups, n_genes)) if self.comp_pts else None\n        else:\n            mask_rest = self.groups_masks[self.ireference]\n            X_rest = self.X[mask_rest]\n            self.means[self.ireference], self.vars[self.ireference] = _get_mean_var(\n                X_rest\n            )\n            # deleting the next line causes a memory leak for some reason\n            del X_rest\n\n        if issparse(self.X):\n            get_nonzeros = lambda X: X.getnnz(axis=0)\n        else:\n            get_nonzeros = lambda X: np.count_nonzero(X, axis=0)\n\n        for imask, mask in enumerate(self.groups_masks):\n            X_mask = self.X[mask]\n\n            if self.comp_pts:\n                self.pts[imask] = get_nonzeros(X_mask) / X_mask.shape[0]\n\n            if self.ireference is not None and imask == self.ireference:\n                continue\n\n            self.means[imask], self.vars[imask] = _get_mean_var(X_mask)\n\n            if self.ireference is None:\n                mask_rest = ~mask\n                X_rest = self.X[mask_rest]\n                self.means_rest[imask], self.vars_rest[imask] = _get_mean_var(X_rest)\n                # this can be costly for sparse data\n                if self.comp_pts:\n                    self.pts_rest[imask] = get_nonzeros(X_rest) / X_rest.shape[0]\n                # deleting the next line causes a memory leak for some reason\n                del X_rest", "idx": 833}
{"project": "Scanpy", "commit_id": "852_scanpy_1.9.0__rank_genes_groups.py_t_test.py", "target": 0, "func": "def t_test(self, method):\n        from scipy import stats\n\n        self._basic_stats()\n\n        for group_index, mask in enumerate(self.groups_masks):\n            if self.ireference is not None and group_index == self.ireference:\n                continue\n\n            mean_group = self.means[group_index]\n            var_group = self.vars[group_index]\n            ns_group = np.count_nonzero(mask)\n\n            if self.ireference is not None:\n                mean_rest = self.means[self.ireference]\n                var_rest = self.vars[self.ireference]\n                ns_other = np.count_nonzero(self.groups_masks[self.ireference])\n            else:\n                mean_rest = self.means_rest[group_index]\n                var_rest = self.vars_rest[group_index]\n                ns_other = self.X.shape[0] - ns_group\n\n            if method == 't-test':\n                ns_rest = ns_other\n            elif method == 't-test_overestim_var':\n                # hack for overestimating the variance for small groups\n                ns_rest = ns_group\n            else:\n                raise ValueError('Method does not exist.')\n\n            # TODO: Come up with better solution. Mask unexpressed genes?\n            # See https://github.com/scipy/scipy/issues/10269\n            with np.errstate(invalid=\"ignore\"):\n                scores, pvals = stats.ttest_ind_from_stats(\n                    mean1=mean_group,\n                    std1=np.sqrt(var_group),\n                    nobs1=ns_group,\n                    mean2=mean_rest,\n                    std2=np.sqrt(var_rest),\n                    nobs2=ns_rest,\n                    equal_var=False,  # Welch's\n                )\n\n            # I think it's only nan when means are the same and vars are 0\n            scores[np.isnan(scores)] = 0\n            # This also has to happen for Benjamini Hochberg\n            pvals[np.isnan(pvals)] = 1\n\n            yield group_index, scores, pvals", "idx": 834}
{"project": "Scanpy", "commit_id": "853_scanpy_1.9.0__rank_genes_groups.py_wilcoxon.py", "target": 0, "func": "def wilcoxon(self, tie_correct):\n        from scipy import stats\n\n        self._basic_stats()\n\n        n_genes = self.X.shape[1]\n        # First loop: Loop over all genes\n        if self.ireference is not None:\n            # initialize space for z-scores\n            scores = np.zeros(n_genes)\n            # initialize space for tie correction coefficients\n            if tie_correct:\n                T = np.zeros(n_genes)\n            else:\n                T = 1\n\n            for group_index, mask in enumerate(self.groups_masks):\n                if group_index == self.ireference:\n                    continue\n\n                mask_rest = self.groups_masks[self.ireference]\n\n                n_active = np.count_nonzero(mask)\n                m_active = np.count_nonzero(mask_rest)\n\n                if n_active <= 25 or m_active <= 25:\n                    logg.hint(\n                        'Few observations in a group for '\n                        'normal approximation (<=25). Lower test accuracy.'\n                    )\n\n                # Calculate rank sums for each chunk for the current mask\n                for ranks, left, right in _ranks(self.X, mask, mask_rest):\n                    scores[left:right] = np.sum(ranks.iloc[0:n_active, :])\n                    if tie_correct:\n                        T[left:right] = _tiecorrect(ranks)\n\n                std_dev = np.sqrt(\n                    T * n_active * m_active * (n_active + m_active + 1) / 12.0\n                )\n\n                scores = (\n                    scores - (n_active * ((n_active + m_active + 1) / 2.0))\n                ) / std_dev\n                scores[np.isnan(scores)] = 0\n                pvals = 2 * stats.distributions.norm.sf(np.abs(scores))\n\n                yield group_index, scores, pvals\n        # If no reference group exists,\n        # ranking needs only to be done once (full mask)\n        else:\n            n_groups = self.groups_masks.shape[0]\n            scores = np.zeros((n_groups, n_genes))\n            n_cells = self.X.shape[0]\n\n            if tie_correct:\n                T = np.zeros((n_groups, n_genes))\n\n            for ranks, left, right in _ranks(self.X):\n                # sum up adjusted_ranks to calculate W_m,n\n                for imask, mask in enumerate(self.groups_masks):\n                    scores[imask, left:right] = np.sum(ranks.iloc[mask, :])\n                    if tie_correct:\n                        T[imask, left:right] = _tiecorrect(ranks)\n\n            for group_index, mask in enumerate(self.groups_masks):\n                n_active = np.count_nonzero(mask)\n\n                if tie_correct:\n                    T_i = T[group_index]\n                else:\n                    T_i = 1\n\n                std_dev = np.sqrt(\n                    T_i * n_active * (n_cells - n_active) * (n_cells + 1) / 12.0\n                )\n\n                scores[group_index, :] = (\n                    scores[group_index, :] - (n_active * (n_cells + 1) / 2.0)\n                ) / std_dev\n                scores[np.isnan(scores)] = 0\n                pvals = 2 * stats.distributions.norm.sf(np.abs(scores[group_index, :]))\n\n                yield group_index, scores[group_index], pvals", "idx": 835}
{"project": "Scanpy", "commit_id": "854_scanpy_1.9.0__rank_genes_groups.py_logreg.py", "target": 1, "func": "def logreg(self, **kwds):\n        # if reference is not set, then the groups listed will be compared to the rest\n        # if reference is set, then the groups listed will be compared only to the other groups listed\n        from sklearn.linear_model import LogisticRegression\n\n        # Indexing with a series causes issues, possibly segfault\n        X = self.X[self.grouping_mask.values, :]\n\n        if len(self.groups_order) == 1:\n            raise ValueError('Cannot perform logistic regression on a single cluster.')\n\n        clf = LogisticRegression(**kwds)\n        clf.fit(X, self.grouping.cat.codes)\n        scores_all = clf.coef_\n        for igroup, _ in enumerate(self.groups_order):\n            if len(self.groups_order) <= 2:  # binary logistic regression\n                scores = scores_all[0]\n            else:\n                scores = scores_all[igroup]\n\n            yield igroup, scores, None\n\n            if len(self.groups_order) <= 2:\n                break", "idx": 836}
{"project": "Scanpy", "commit_id": "855_scanpy_1.9.0__rank_genes_groups.py_compute_statistics.py", "target": 0, "func": "def compute_statistics(\n        self,\n        method,\n        corr_method='benjamini-hochberg',\n        n_genes_user=None,\n        rankby_abs=False,\n        tie_correct=False,\n        **kwds,\n    ):\n\n        if method in {'t-test', 't-test_overestim_var'}:\n            generate_test_results = self.t_test(method)\n        elif method == 'wilcoxon':\n            generate_test_results = self.wilcoxon(tie_correct)\n        elif method == 'logreg':\n            generate_test_results = self.logreg(**kwds)\n\n        self.stats = None\n\n        n_genes = self.X.shape[1]\n\n        for group_index, scores, pvals in generate_test_results:\n            group_name = str(self.groups_order[group_index])\n\n            if n_genes_user is not None:\n                scores_sort = np.abs(scores) if rankby_abs else scores\n                global_indices = _select_top_n(scores_sort, n_genes_user)\n                first_col = 'names'\n            else:\n                global_indices = slice(None)\n                first_col = 'scores'\n\n            if self.stats is None:\n                idx = pd.MultiIndex.from_tuples([(group_name, first_col)])\n                self.stats = pd.DataFrame(columns=idx)\n\n            if n_genes_user is not None:\n                self.stats[group_name, 'names'] = self.var_names[global_indices]\n\n            self.stats[group_name, 'scores'] = scores[global_indices]\n\n            if pvals is not None:\n                self.stats[group_name, 'pvals'] = pvals[global_indices]\n                if corr_method == 'benjamini-hochberg':\n                    from statsmodels.stats.multitest import multipletests\n\n                    pvals[np.isnan(pvals)] = 1\n                    _, pvals_adj, _, _ = multipletests(\n                        pvals, alpha=0.05, method='fdr_bh'\n                    )\n                elif corr_method == 'bonferroni':\n                    pvals_adj = np.minimum(pvals * n_genes, 1.0)\n                self.stats[group_name, 'pvals_adj'] = pvals_adj[global_indices]\n\n            if self.means is not None:\n                mean_group = self.means[group_index]\n                if self.ireference is None:\n                    mean_rest = self.means_rest[group_index]\n                else:\n                    mean_rest = self.means[self.ireference]\n                foldchanges = (self.expm1_func(mean_group) + 1e-9) / (\n                    self.expm1_func(mean_rest) + 1e-9\n                )  # add small value to remove 0's\n                self.stats[group_name, 'logfoldchanges'] = np.log2(\n                    foldchanges[global_indices]\n                )\n\n        if n_genes_user is None:\n            self.stats.index = self.var_names", "idx": 837}
{"project": "Scanpy", "commit_id": "856_scanpy_1.9.0__score_genes.py__sparse_nanmean.py", "target": 0, "func": "def _sparse_nanmean(X, axis):\n    \"\"\"\n    np.nanmean equivalent for sparse matrices\n    \"\"\"\n    if not issparse(X):\n        raise TypeError(\"X must be a sparse matrix\")\n\n    # count the number of nan elements per row/column (dep. on axis)\n    Z = X.copy()\n    Z.data = np.isnan(Z.data)\n    Z.eliminate_zeros()\n    n_elements = Z.shape[axis] - Z.sum(axis)\n\n    # set the nans to 0, so that a normal .sum() works\n    Y = X.copy()\n    Y.data[np.isnan(Y.data)] = 0\n    Y.eliminate_zeros()\n\n    # the average\n    s = Y.sum(axis, dtype='float64')  # float64 for score_genes function compatibility)\n    m = s / n_elements\n\n    return m", "idx": 838}
{"project": "Scanpy", "commit_id": "857_scanpy_1.9.0__score_genes.py_score_genes.py", "target": 1, "func": "def score_genes(\n    adata: AnnData,\n    gene_list: Sequence[str],\n    ctrl_size: int = 50,\n    gene_pool: Optional[Sequence[str]] = None,\n    n_bins: int = 25,\n    score_name: str = 'score',\n    random_state: AnyRandom = 0,\n    copy: bool = False,\n    use_raw: Optional[bool] = None,\n) -> Optional[AnnData]:\n    \"\"\"\\\n    Score a set of genes [Satija15]_.\n\n    The score is the average expression of a set of genes subtracted with the\n    average expression of a reference set of genes. The reference set is\n    randomly sampled from the `gene_pool` for each binned expression value.\n\n    This reproduces the approach in Seurat [Satija15]_ and has been implemented\n    for Scanpy by Davide Cittaro.\n\n    Parameters\n    ----------\n    adata\n        The annotated data matrix.\n    gene_list\n        The list of gene names used for score calculation.\n    ctrl_size\n        Number of reference genes to be sampled from each bin. If `len(gene_list)` is not too\n        low, you can set `ctrl_size=len(gene_list)`.\n    gene_pool\n        Genes for sampling the reference set. Default is all genes.\n    n_bins\n        Number of expression level bins for sampling.\n    score_name\n        Name of the field to be added in `.obs`.\n    random_state\n        The random seed for sampling.\n    copy\n        Copy `adata` or modify it inplace.\n    use_raw\n        Whether to use `raw` attribute of `adata`. Defaults to `True` if `.raw` is present.\n\n        .. versionchanged:: 1.4.5\n           Default value changed from `False` to `None`.\n\n    Returns\n    -------\n    Depending on `copy`, returns or updates `adata` with an additional field\n    `score_name`.\n\n    Examples\n    --------\n    See this `notebook <https://github.com/theislab/scanpy_usage/tree/master/180209_cell_cycle>`__.\n    \"\"\"\n    start = logg.info(f'computing score {score_name!r}')\n    adata = adata.copy() if copy else adata\n    use_raw = _check_use_raw(adata, use_raw)\n\n    if random_state is not None:\n        np.random.seed(random_state)\n\n    gene_list_in_var = []\n    var_names = adata.raw.var_names if use_raw else adata.var_names\n    genes_to_ignore = []\n    for gene in gene_list:\n        if gene in var_names:\n            gene_list_in_var.append(gene)\n        else:\n            genes_to_ignore.append(gene)\n    if len(genes_to_ignore) > 0:\n        logg.warning(f'genes are not in var_names and ignored: {genes_to_ignore}')\n    gene_list = set(gene_list_in_var[:])\n\n    if len(gene_list) == 0:\n        raise ValueError(\"No valid genes were passed for scoring.\")\n\n    if gene_pool is None:\n        gene_pool = list(var_names)\n    else:\n        gene_pool = [x for x in gene_pool if x in var_names]\n    if not gene_pool:\n        raise ValueError(\"No valid genes were passed for reference set.\")\n\n    # Trying here to match the Seurat approach in scoring cells.\n    # Basically we need to compare genes against random genes in a matched\n    # interval of expression.\n\n    _adata = adata.raw if use_raw else adata\n    _adata_subset = (\n        _adata[:, gene_pool] if len(gene_pool) < len(_adata.var_names) else _adata\n    )\n    if issparse(_adata_subset.X):\n        obs_avg = pd.Series(\n            np.array(_sparse_nanmean(_adata_subset.X, axis=0)).flatten(),\n            index=gene_pool,\n        )  # average expression of genes\n    else:\n        obs_avg = pd.Series(\n            np.nanmean(_adata_subset.X, axis=0), index=gene_pool\n        )  # average expression of genes\n\n    obs_avg = obs_avg[\n        np.isfinite(obs_avg)\n    ]  # Sometimes (and I don't know how) missing data may be there, with nansfor\n\n    n_items = int(np.round(len(obs_avg) / (n_bins - 1)))\n    obs_cut = obs_avg.rank(method='min') // n_items\n    control_genes = set()\n\n    # now pick `ctrl_size` genes from every cut\n    for cut in np.unique(obs_cut.loc[gene_list]):\n        r_genes = np.array(obs_cut[obs_cut == cut].index)\n        np.random.shuffle(r_genes)\n        # uses full r_genes if ctrl_size > len(r_genes)\n        control_genes.update(set(r_genes[:ctrl_size]))\n\n    # To index, we need a list \u2013 indexing implies an order.\n    control_genes = list(control_genes - gene_list)\n    gene_list = list(gene_list)\n\n    X_list = _adata[:, gene_list].X\n    if issparse(X_list):\n        X_list = np.array(_sparse_nanmean(X_list, axis=1)).flatten()\n    else:\n        X_list = np.nanmean(X_list, axis=1, dtype='float64')\n\n    X_control = _adata[:, control_genes].X\n    if issparse(X_control):\n        X_control = np.array(_sparse_nanmean(X_control, axis=1)).flatten()\n    else:\n        X_control = np.nanmean(X_control, axis=1, dtype='float64')\n\n    score = X_list - X_control\n\n    adata.obs[score_name] = pd.Series(\n        np.array(score).ravel(), index=adata.obs_names, dtype='float64'\n    )\n\n    logg.info(\n        '    finished',\n        time=start,\n        deep=(\n            'added\\n'\n            f'    {score_name!r}, score of gene set (adata.obs).\\n'\n            f'    {len(control_genes)} total control genes are used.'\n        ),\n    )\n    return adata if copy else None", "idx": 839}
{"project": "Scanpy", "commit_id": "858_scanpy_1.9.0__score_genes.py_score_genes_cell_cycle.py", "target": 0, "func": "def score_genes_cell_cycle(\n    adata: AnnData,\n    s_genes: Sequence[str],\n    g2m_genes: Sequence[str],\n    copy: bool = False,\n    **kwargs,\n) -> Optional[AnnData]:\n    \"\"\"\\\n    Score cell cycle genes [Satija15]_.\n\n    Given two lists of genes associated to S phase and G2M phase, calculates\n    scores and assigns a cell cycle phase (G1, S or G2M). See\n    :func:`~scanpy.tl.score_genes` for more explanation.\n\n    Parameters\n    ----------\n    adata\n        The annotated data matrix.\n    s_genes\n        List of genes associated with S phase.\n    g2m_genes\n        List of genes associated with G2M phase.\n    copy\n        Copy `adata` or modify it inplace.\n    **kwargs\n        Are passed to :func:`~scanpy.tl.score_genes`. `ctrl_size` is not\n        possible, as it's set as `min(len(s_genes), len(g2m_genes))`.\n\n    Returns\n    -------\n    Depending on `copy`, returns or updates `adata` with the following fields.\n\n    **S_score** : `adata.obs`, dtype `object`\n        The score for S phase for each cell.\n    **G2M_score** : `adata.obs`, dtype `object`\n        The score for G2M phase for each cell.\n    **phase** : `adata.obs`, dtype `object`\n        The cell cycle phase (`S`, `G2M` or `G1`) for each cell.\n\n    See also\n    --------\n    score_genes\n\n    Examples\n    --------\n    See this `notebook <https://github.com/theislab/scanpy_usage/tree/master/180209_cell_cycle>`__.\n    \"\"\"\n    logg.info('calculating cell cycle phase')\n\n    adata = adata.copy() if copy else adata\n    ctrl_size = min(len(s_genes), len(g2m_genes))\n    # add s-score\n    score_genes(\n        adata, gene_list=s_genes, score_name='S_score', ctrl_size=ctrl_size, **kwargs\n    )\n    # add g2m-score\n    score_genes(\n        adata,\n        gene_list=g2m_genes,\n        score_name='G2M_score',\n        ctrl_size=ctrl_size,\n        **kwargs,\n    )\n    scores = adata.obs[['S_score', 'G2M_score']]\n\n    # default phase is S\n    phase = pd.Series('S', index=scores.index)\n\n    # if G2M is higher than S, it's G2M\n    phase[scores.G2M_score > scores.S_score] = 'G2M'\n\n    # if all scores are negative, it's G1...\n    phase[np.all(scores < 0, axis=1)] = 'G1'\n\n    adata.obs['phase'] = phase\n    logg.hint('    \\'phase\\', cell cycle phase (adata.obs)')\n    return adata if copy else None", "idx": 840}
{"project": "Scanpy", "commit_id": "859_scanpy_1.9.0__sim.py_sim.py", "target": 0, "func": "def sim(\n    model: Literal['krumsiek11', 'toggleswitch'],\n    params_file: bool = True,\n    tmax: Optional[int] = None,\n    branching: Optional[bool] = None,\n    nrRealizations: Optional[int] = None,\n    noiseObs: Optional[float] = None,\n    noiseDyn: Optional[float] = None,\n    step: Optional[int] = None,\n    seed: Optional[int] = None,\n    writedir: Optional[Union[str, Path]] = None,\n) -> AnnData:\n    \"\"\"\\\n    Simulate dynamic gene expression data [Wittmann09]_ [Wolf18]_.\n\n    Sample from a stochastic differential equation model built from\n    literature-curated boolean gene regulatory networks, as suggested by\n    [Wittmann09]_. The Scanpy implementation is due to [Wolf18]_.\n\n    Parameters\n    ----------\n    model\n        Model file in 'sim_models' directory.\n    params_file\n        Read default params from file.\n    tmax\n        Number of time steps per realization of time series.\n    branching\n        Only write realizations that contain new branches.\n    nrRealizations\n        Number of realizations.\n    noiseObs\n        Observatory/Measurement noise.\n    noiseDyn\n        Dynamic noise.\n    step\n        Interval for saving state of system.\n    seed\n        Seed for generation of random numbers.\n    writedir\n        Path to directory for writing output files.\n\n    Returns\n    -------\n    Annotated data matrix.\n\n    Examples\n    --------\n    See this `use case <https://github.com/theislab/scanpy_usage/tree/master/170430_krumsiek11>`__\n    \"\"\"\n    params = locals()\n    if params_file:\n        model_key = Path(model).with_suffix('').name\n        from .. import sim_models\n\n        pfile_sim = Path(sim_models.__file__).parent / f'{model_key}_params.txt'\n        default_params = readwrite.read_params(pfile_sim)\n        params = _utils.update_params(default_params, params)\n    adata = sample_dynamic_data(**params)\n    adata.uns['iroot'] = 0\n    return adata", "idx": 841}
{"project": "Scanpy", "commit_id": "85_scanpy_1.9.0__settings.py_verbosity.py", "target": 0, "func": "def verbosity(self, verbosity: Union[Verbosity, int, str]):\n        verbosity_str_options = [\n            v for v in _VERBOSITY_TO_LOGLEVEL if isinstance(v, str)\n        ]\n        if isinstance(verbosity, Verbosity):\n            self._verbosity = verbosity\n        elif isinstance(verbosity, int):\n            self._verbosity = Verbosity(verbosity)\n        elif isinstance(verbosity, str):\n            verbosity = verbosity.lower()\n            if verbosity not in verbosity_str_options:\n                raise ValueError(\n                    f\"Cannot set verbosity to {verbosity}. \"\n                    f\"Accepted string values are: {verbosity_str_options}\"\n                )\n            else:\n                self._verbosity = Verbosity(verbosity_str_options.index(verbosity))\n        else:\n            _type_check(verbosity, \"verbosity\", (str, int))\n        _set_log_level(self, _VERBOSITY_TO_LOGLEVEL[self._verbosity])", "idx": 842}
{"project": "Scanpy", "commit_id": "860_scanpy_1.9.0__sim.py_add_args.py", "target": 0, "func": "def add_args(p):\n    \"\"\"\n    Update parser with tool specific arguments.\n\n    This overwrites was is done in utils.uns_args.\n    \"\"\"\n    # dictionary for adding arguments\n    dadd_args = {\n        '--opfile': {\n            'default': '',\n            'metavar': 'f',\n            'type': str,\n            'help': 'Specify a parameter file ' '(default: \"sim/${exkey}_params.txt\")',\n        }\n    }\n    p = _utils.add_args(p, dadd_args)\n    return p", "idx": 843}
{"project": "Scanpy", "commit_id": "861_scanpy_1.9.0__sim.py_sample_dynamic_data.py", "target": 0, "func": "def sample_dynamic_data(**params):\n    \"\"\"\n    Helper function.\n    \"\"\"\n    model_key = Path(params['model']).with_suffix('').name\n    writedir = params.get('writedir')\n    if writedir is None:\n        writedir = settings.writedir / (model_key + '_sim')\n    else:\n        writedir = Path(writedir)\n    writedir.mkdir(parents=True, exist_ok=True)\n    readwrite.write_params(writedir / 'params.txt', params)\n    # init variables\n    tmax = params['tmax']\n    branching = params['branching']\n    noiseObs = params['noiseObs']\n    noiseDyn = params['noiseDyn']\n    nrRealizations = params['nrRealizations']\n    step = params['step']  # step size for saving the figure\n\n    nrSamples = 1  # how many files?\n    maxRestarts = 1000\n    maxNrSamples = 1\n\n    # simple vector auto regressive process or\n    # hill kinetics process simulation\n    if 'krumsiek11' not in model_key:\n        # create instance, set seed\n        grnsim = GRNsim(model=model_key, params=params)\n        nrOffEdges_list = np.zeros(nrSamples)\n        for sample in range(nrSamples):\n            # random topology / for a given edge density\n            if 'hill' not in model_key:\n                Coupl = np.array(grnsim.Coupl)\n                for sampleCoupl in range(10):\n                    nrOffEdges = 0\n                    for gp in range(grnsim.dim):\n                        for g in range(grnsim.dim):\n                            # only consider off-diagonal edges\n                            if g != gp:\n                                Coupl[gp, g] = 0.7 if np.random.rand() < 0.4 else 0\n                                nrOffEdges += 1 if Coupl[gp, g] > 0 else 0\n                            else:\n                                Coupl[gp, g] = 0.7\n                    # check that the coupling matrix does not have eigenvalues\n                    # greater than 1, which would lead to an exploding var process\n                    if max(sp.linalg.eig(Coupl)[0]) < 1:\n                        break\n                nrOffEdges_list[sample] = nrOffEdges\n                grnsim.set_coupl(Coupl)\n            # init type\n            real = 0\n            X0 = np.random.rand(grnsim.dim)\n            Xsamples = []\n            for restart in range(nrRealizations + maxRestarts):\n                # slightly break symmetry in initial conditions\n                if 'toggleswitch' in model_key:\n                    X0 = np.array(\n                        [0.8 for i in range(grnsim.dim)]\n                    ) + 0.01 * np.random.randn(grnsim.dim)\n                X = grnsim.sim_model(tmax=tmax, X0=X0, noiseDyn=noiseDyn)\n                # check branching\n                check = True\n                if branching:\n                    check, Xsamples = _check_branching(X, Xsamples, restart)\n                if check:\n                    real += 1\n                    grnsim.write_data(\n                        X[::step],\n                        dir=writedir,\n                        noiseObs=noiseObs,\n                        append=(False if restart == 0 else True),\n                        branching=branching,\n                        nrRealizations=nrRealizations,\n                    )\n                # append some zeros\n                if 'zeros' in writedir.name and real == 2:\n                    grnsim.write_data(\n                        noiseDyn * np.random.randn(500, 3),\n                        dir=writedir,\n                        noiseObs=noiseObs,\n                        append=(False if restart == 0 else True),\n                        branching=branching,\n                        nrRealizations=nrRealizations,\n                    )\n                if real >= nrRealizations:\n                    break\n        logg.debug(\n            f'mean nr of offdiagonal edges {nrOffEdges_list.mean()} '\n            f'compared to total nr {grnsim.dim*(grnsim.dim-1)/2.}'\n        )\n\n    # more complex models\n    else:\n        initType = 'random'\n\n        dim = 11\n        step = 5\n\n        grnsim = GRNsim(dim=dim, initType=initType, model=model_key, params=params)\n        Xsamples = []\n        for sample in range(maxNrSamples):\n            # choose initial conditions such that branchings result\n            if initType == 'branch':\n                X0mean = grnsim.branch_init_model1(tmax)\n                if X0mean is None:\n                    grnsim.set_coupl()\n                    continue\n            real = 0\n            for restart in range(nrRealizations + maxRestarts):\n                if initType == 'branch':\n                    # vary initial conditions around mean\n                    X0 = X0mean + (0.05 * np.random.rand(dim) - 0.025 * np.ones(dim))\n                else:\n                    # generate random initial conditions within [0.3,0.7]\n                    X0 = 0.4 * np.random.rand(dim) + 0.3\n                if model_key in [5, 6]:\n                    X0 = np.array([0.3, 0.3, 0, 0, 0, 0])\n                if model_key in [7, 8, 9, 10]:\n                    X0 = 0.6 * np.random.rand(dim) + 0.2\n                    X0[2:] = np.zeros(4)\n                if 'krumsiek11' in model_key:\n                    X0 = np.zeros(dim)\n                    X0[grnsim.varNames['Gata2']] = 0.8\n                    X0[grnsim.varNames['Pu.1']] = 0.8\n                    X0[grnsim.varNames['Cebpa']] = 0.8\n                    X0 += 0.001 * np.random.randn(dim)\n                    if False:\n                        switch_gene = restart - (nrRealizations - dim)\n                        if switch_gene >= dim:\n                            break\n                        X0[switch_gene] = 0 if X0[switch_gene] > 0.1 else 0.8\n                X = grnsim.sim_model(tmax, X0=X0, noiseDyn=noiseDyn, restart=restart)\n                # check branching\n                check = True\n                if branching:\n                    check, Xsamples = _check_branching(X, Xsamples, restart)\n                if check:\n                    real += 1\n                    grnsim.write_data(\n                        X[::step],\n                        dir=writedir,\n                        noiseObs=noiseObs,\n                        append=(False if restart == 0 else True),\n                        branching=branching,\n                        nrRealizations=nrRealizations,\n                    )\n                if real >= nrRealizations:\n                    break\n    # load the last simulation file\n    filename = None\n    for filename in writedir.glob('sim*.txt'):\n        pass\n    logg.info(f'reading simulation results {filename}')\n    adata = readwrite._read(\n        filename, first_column_names=True, suppress_cache_warning=True\n    )\n    adata.uns['tmax_write'] = tmax / step\n    return adata", "idx": 844}
{"project": "Scanpy", "commit_id": "862_scanpy_1.9.0__sim.py_write_data.py", "target": 0, "func": "def write_data(\n        self,\n        X,\n        dir=Path('sim/test'),\n        noiseObs=0.0,\n        append=False,\n        branching=False,\n        nrRealizations=1,\n        seed=0,\n    ):\n        header = self.header\n        tmax = int(X.shape[0])\n        header += 'tmax = ' + str(tmax) + '\\n'\n        header += 'branching = ' + str(branching) + '\\n'\n        header += 'nrRealizations = ' + str(nrRealizations) + '\\n'\n        header += 'noiseObs = ' + str(noiseObs) + '\\n'\n        header += 'noiseDyn = ' + str(self.noiseDyn) + '\\n'\n        header += 'seed = ' + str(seed) + '\\n'\n        # add observational noise\n        X += noiseObs * np.random.randn(tmax, self.dim)\n        # call helper function\n        write_data(\n            X,\n            dir,\n            append,\n            header,\n            varNames=self.varNames,\n            Adj=self.Adj,\n            Coupl=self.Coupl,\n            model=self.model,\n            modelType=self.modelType,\n            boolRules=self.boolRules,\n            invTimeStep=self.invTimeStep,", "idx": 845}
{"project": "Scanpy", "commit_id": "863_scanpy_1.9.0__sim.py__check_branching.py", "target": 0, "func": "def _check_branching(\n    X: np.ndarray, Xsamples: np.ndarray, restart: int, threshold: float = 0.25\n) -> Tuple[bool, List[np.ndarray]]:\n    \"\"\"\\\n    Check whether time series branches.\n\n    Parameters\n    ----------\n    X\n        current time series data.\n    Xsamples\n        list of previous branching samples.\n    restart\n        counts number of restart trials.\n    threshold\n        sets threshold for attractor identification.\n\n    Returns\n    -------\n    check\n        true if branching realization\n    Xsamples\n        updated list\n    \"\"\"\n    check = True\n    Xsamples = list(Xsamples)\n    if restart == 0:\n        Xsamples.append(X)\n    else:\n        for Xcompare in Xsamples:\n            Xtmax_diff = np.absolute(X[-1, :] - Xcompare[-1, :])\n            # If the second largest element is smaller than threshold\n            # set check to False, i.e. at least two elements\n            # need to change in order to have a branching.\n            # If we observe all parameters of the system,\n            # a new attractor state must involve changes in two\n            # variables.\n            if np.partition(Xtmax_diff, -2)[-2] < threshold:\n                check = False\n        if check:\n            Xsamples.append(X)\n    logg.debug(f'realization {restart}: {\"\" if check else \"no\"} new branch')\n    return check, Xsamples", "idx": 846}
{"project": "Scanpy", "commit_id": "864_scanpy_1.9.0__sim.py_check_nocycles.py", "target": 0, "func": "def check_nocycles(Adj: np.ndarray, verbosity: int = 2) -> bool:\n    \"\"\"\\\n    Checks that there are no cycles in graph described by adjacancy matrix.\n\n    Parameters\n    ----------\n    Adj\n        adjancancy matrix of dimension (dim, dim)\n\n    Returns\n    -------\n    True if there is no cycle, False otherwise.\n    \"\"\"\n    dim = Adj.shape[0]\n    for g in range(dim):\n        v = np.zeros(dim)\n        v[g] = 1\n        for i in range(dim):\n            v = Adj.dot(v)\n            if v[g] > 1e-10:\n                if verbosity > 2:\n                    settings.m(0, Adj)\n                    settings.m(\n                        0,\n                        'contains a cycle of length',\n                        i + 1,\n                        'starting from node',\n                        g,\n                        '-> reject',\n                    )\n                return False\n    return True", "idx": 847}
{"project": "Scanpy", "commit_id": "865_scanpy_1.9.0__sim.py_sample_coupling_matrix.py", "target": 0, "func": "def sample_coupling_matrix(\n    dim: int = 3, connectivity: float = 0.5\n) -> Tuple[np.ndarray, np.ndarray, np.ndarray, int]:\n    \"\"\"\\\n    Sample coupling matrix.\n\n    Checks that returned graphs contain no self-cycles.\n\n    Parameters\n    ----------\n    dim\n        dimension of coupling matrix.\n    connectivity\n        fraction of connectivity, fully connected means 1.,\n        not-connected means 0, in the case of fully connected, one has\n        dim*(dim-1)/2 edges in the graph.\n\n    Returns\n    -------\n    coupl\n        coupling matrix\n    adj\n        adjancancy matrix\n    adj_signed\n        signed adjacancy matrix\n    n_edges\n        Number of edges\n    \"\"\"\n    max_trial = 10\n    check = False\n    for trial in range(max_trial):\n        # random topology for a given connectivity / edge density\n        Coupl = np.zeros((dim, dim))\n        n_edges = 0\n        for gp in range(dim):\n            for g in range(dim):\n                if gp == g:\n                    continue\n                # need to have the factor 0.5, otherwise\n                # connectivity=1 would lead to dim*(dim-1) edges\n                if np.random.rand() < 0.5 * connectivity:\n                    Coupl[gp, g] = 0.7\n                    n_edges += 1\n        # obtain adjacancy matrix\n        Adj_signed = np.zeros((dim, dim), dtype='int_')\n        Adj_signed = np.sign(Coupl)\n        Adj = np.abs(Adj_signed)\n        # check for cycles and whether there is at least one edge\n        if check_nocycles(Adj) and n_edges > 0:\n            check = True\n            break\n    if not check:\n        raise ValueError(\n            'did not find graph without cycles after' f'{max_trial} trials'\n        )\n    return Coupl, Adj, Adj_signed, n_edges", "idx": 848}
{"project": "Scanpy", "commit_id": "866_scanpy_1.9.0__sim.py_sample_static_data.py", "target": 0, "func": "def sample_static_data(model, dir, verbosity=0):\n    # fraction of connectivity as compared to fully connected\n    # in one direction, which amounts to dim*(dim-1)/2 edges\n    connectivity = 0.8\n    dim = 3\n    n_Coupls = 50\n    model = model.replace('static-', '')\n    np.random.seed(0)\n\n    if model != 'combi':\n        n_edges = np.zeros(n_Coupls)\n        for icoupl in range(n_Coupls):\n            Coupl, Adj, Adj_signed, n_e = sample_coupling_matrix(dim, connectivity)\n            if verbosity > 1:\n                settings.m(0, icoupl)\n                settings.m(0, Adj)\n            n_edges[icoupl] = n_e\n            # sample data\n            X = StaticCauseEffect().sim_givenAdj(Adj, model)\n            write_data(X, dir, Adj=Adj)\n        settings.m(0, 'mean edge number:', n_edges.mean())\n\n    else:\n        X = StaticCauseEffect().sim_combi()\n        Adj = np.zeros((3, 3))\n        Adj[2, 0] = Adj[2, 1] = 0\n        write_data(X, dir, Adj=Adj)", "idx": 849}
{"project": "Scanpy", "commit_id": "867_scanpy_1.9.0__sim.py___init__.py", "target": 0, "func": "def __init__(self):\n        # define a set of available functions\n        self.funcs = dict(\n            line=lambda x: x,\n            noise=lambda x: 0,\n            absline=np.abs,\n            parabola=lambda x: x**2,\n            sawtooth=lambda x: 0.5 * x - np.floor(0.5 * x),\n            tanh=lambda x: np.tanh(2 * x),", "idx": 850}
{"project": "Scanpy", "commit_id": "868_scanpy_1.9.0__sim.py_sim_model.py", "target": 0, "func": "def sim_model(self, tmax, X0, noiseDyn=0, restart=0):\n        \"\"\"Simulate the model.\"\"\"\n        self.noiseDyn = noiseDyn\n        #\n        X = np.zeros((tmax, self.dim))\n        X[0] = X0 + noiseDyn * np.random.randn(self.dim)\n        # run simulation\n        for t in range(1, tmax):\n            if self.modelType == 'hill':\n                Xdiff = self.Xdiff_hill(X[t - 1])\n            elif self.modelType == 'var':\n                Xdiff = self.Xdiff_var(X[t - 1])\n            else:\n                raise ValueError(f\"Unknown modelType {self.modelType!r}\")\n            X[t] = X[t - 1] + Xdiff\n            # add dynamic noise\n            X[t] += noiseDyn * np.random.randn(self.dim)\n        return X", "idx": 851}
{"project": "Scanpy", "commit_id": "869_scanpy_1.9.0__sim.py_Xdiff_hill.py", "target": 0, "func": "def Xdiff_hill(self, Xt):\n        \"\"\"Build Xdiff from coefficients of boolean network,\n        that is, using self.boolCoeff. The employed functions\n        are Hill type activation and deactivation functions.\n\n        See Wittmann et al., BMC Syst. Biol. 3, 98 (2009),\n        doi:10.1186/1752-0509-3-98 for more details.\n        \"\"\"\n        verbosity = self.verbosity > 0 and self.writeOutputOnce\n        self.writeOutputOnce = False\n        Xdiff = np.zeros(self.dim)\n        for ichild, child in enumerate(self.pas.keys()):\n            # check whether list of parents is non-empty,\n            # otherwise continue\n            if self.pas[child]:\n                Xdiff_syn = 0  # synthesize term\n                if verbosity > 0:\n                    Xdiff_syn_str = ''\n            else:\n                continue\n            # loop over all tuples for which the boolean update\n            # rule returns true, these are stored in self.boolCoeff\n            for ituple, tuple in enumerate(self.boolCoeff[child]):\n                Xdiff_syn_tuple = 1\n                Xdiff_syn_tuple_str = ''\n                for iv, v in enumerate(tuple):\n                    iparent = self.varNames[self.pas[child][iv]]\n                    x = Xt[iparent]\n                    threshold = 0.1 / np.abs(self.Coupl[ichild, iparent])\n                    Xdiff_syn_tuple *= (\n                        self.hill_a(x, threshold) if v else self.hill_i(x, threshold)\n                    )\n                    if verbosity > 0:\n                        Xdiff_syn_tuple_str += (\n                            f'{\"a\" if v else \"i\"}'\n                            f'({self.pas[child][iv]}, {threshold:.2})'\n                        )\n                Xdiff_syn += Xdiff_syn_tuple\n                if verbosity > 0:\n                    Xdiff_syn_str += ('+' if ituple != 0 else '') + Xdiff_syn_tuple_str\n            # multiply with degradation term\n            Xdiff[ichild] = self.invTimeStep * (Xdiff_syn - Xt[ichild])\n            if verbosity > 0:\n                Xdiff_str = (\n                    f'{child}_{child}-{child} = '\n                    f'{self.invTimeStep}*({Xdiff_syn_str}-{child})'\n                )\n                settings.m(0, Xdiff_str)\n        return Xdiff", "idx": 852}
{"project": "Scanpy", "commit_id": "86_scanpy_1.9.0__settings.py_plot_suffix.py", "target": 0, "func": "def plot_suffix(self, plot_suffix: str):\n        _type_check(plot_suffix, \"plot_suffix\", str)\n        self._plot_suffix = plot_suffix", "idx": 853}
{"project": "Scanpy", "commit_id": "870_scanpy_1.9.0__sim.py_Xdiff_var.py", "target": 0, "func": "def Xdiff_var(self, Xt, verbosity=0):\n        \"\"\"\"\"\"\n        # subtract the current state\n        Xdiff = -Xt\n        # add the information from the past\n        Xdiff += np.dot(self.Coupl, Xt)\n        return Xdiff", "idx": 854}
{"project": "Scanpy", "commit_id": "871_scanpy_1.9.0__sim.py_hill_a.py", "target": 0, "func": "def hill_a(self, x, threshold=0.1, power=2):\n        \"\"\"Activating hill function.\"\"\"\n        x_pow = np.power(x, power)\n        threshold_pow = np.power(threshold, power)\n        return x_pow / (x_pow + threshold_pow)", "idx": 855}
{"project": "Scanpy", "commit_id": "872_scanpy_1.9.0__sim.py_hill_i.py", "target": 0, "func": "def hill_i(self, x, threshold=0.1, power=2):\n        \"\"\"Inhibiting hill function.\n\n        Is equivalent to 1-hill_a(self,x,power,threshold).\n        \"\"\"\n        x_pow = np.power(x, power)\n        threshold_pow = np.power(threshold, power)\n        return threshold_pow / (x_pow + threshold_pow)", "idx": 856}
{"project": "Scanpy", "commit_id": "873_scanpy_1.9.0__sim.py_nhill_a.py", "target": 0, "func": "def nhill_a(self, x, threshold=0.1, power=2, ichild=2):\n        \"\"\"Normalized activating hill function.\"\"\"\n        x_pow = np.power(x, power)\n        threshold_pow = np.power(threshold, power)\n        return x_pow / (x_pow + threshold_pow) * (1 + threshold_pow)", "idx": 857}
{"project": "Scanpy", "commit_id": "874_scanpy_1.9.0__sim.py_nhill_i.py", "target": 0, "func": "def nhill_i(self, x, threshold=0.1, power=2):\n        \"\"\"Normalized inhibiting hill function.\n\n        Is equivalent to 1-nhill_a(self,x,power,threshold).\n        \"\"\"\n        x_pow = np.power(x, power)\n        threshold_pow = np.power(threshold, power)\n        return threshold_pow / (x_pow + threshold_pow) * (1 - x_pow)", "idx": 858}
{"project": "Scanpy", "commit_id": "875_scanpy_1.9.0__sim.py_read_model.py", "target": 0, "func": "def read_model(self):\n        \"\"\"Read the model and the couplings from the model file.\"\"\"\n        if self.verbosity > 0:\n            settings.m(0, 'reading model', self.model)\n        # read model\n        boolRules = []\n        for line in self.model.open():\n            if line.startswith('#') and 'modelType =' in line:\n                keyval = line\n                if '|' in line:\n                    keyval, type = line.split('|')[:2]\n                self.modelType = keyval.split('=')[1].strip()\n            if line.startswith('#') and 'invTimeStep =' in line:\n                keyval = line\n                if '|' in line:\n                    keyval, type = line.split('|')[:2]\n                self.invTimeStep = float(keyval.split('=')[1].strip())\n            if not line.startswith('#'):\n                boolRules.append([s.strip() for s in line.split('=')])\n            if line.startswith('# coupling list:'):\n                break\n        self.dim = len(boolRules)\n        self.boolRules = dict(boolRules)\n        self.varNames = {s: i for i, s in enumerate(self.boolRules.keys())}\n        names = self.varNames\n        # read couplings via names\n        self.Coupl = np.zeros((self.dim, self.dim))\n        boolContinue = True\n        for (\n            line\n        ) in self.model.open():  # open(self.model.replace('/model','/couplList')):\n            if line.startswith('# coupling list:'):\n                boolContinue = False\n            if boolContinue:\n                continue\n            if not line.startswith('#'):\n                gps, gs, val = line.strip().split()\n                self.Coupl[int(names[gps]), int(names[gs])] = float(val)\n        # adjancecy matrices\n        self.Adj_signed = np.sign(self.Coupl)\n        self.Adj = np.abs(np.array(self.Adj_signed))\n        # build bool coefficients (necessary for odefy type\n        # version of the discrete model)\n        self.build_boolCoeff()", "idx": 859}
{"project": "Scanpy", "commit_id": "876_scanpy_1.9.0__sim.py_set_coupl.py", "target": 0, "func": "def set_coupl(self, Coupl=None):\n        \"\"\"Construct the coupling matrix (and adjacancy matrix) from predefined models\n        or via sampling.\n        \"\"\"\n        self.varNames = {str(i): i for i in range(self.dim)}\n        if self.model not in self.availModels.keys() and Coupl is None:\n            self.read_model()\n        elif 'var' in self.model.name:\n            # vector auto regressive process\n            self.Coupl = Coupl\n            self.boolRules = {s: '' for s in self.varNames.keys()}\n            names = list(self.varNames.keys())\n            for gp in range(self.dim):\n                pas = []\n                for g in range(self.dim):\n                    if np.abs(self.Coupl[gp, g] > 1e-10):\n                        pas.append(names[g])\n                self.boolRules[names[gp]] = ''.join(\n                    pas[:1] + [' or ' + pa for pa in pas[1:]]\n                )\n                self.Adj_signed = np.sign(Coupl)\n        elif self.model in ['6', '7', '8', '9', '10']:\n            self.Adj_signed = np.zeros((self.dim, self.dim))\n            n_sinknodes = 2\n            #             sinknodes = np.random.choice(np.arange(0,self.dim),\n            #                                              size=n_sinknodes,replace=False)\n            sinknodes = np.array([0, 1])\n            # assume sinknodes have feeback\n            self.Adj_signed[sinknodes, sinknodes] = np.ones(n_sinknodes)\n            #             # allow negative feedback\n            #             if self.model == 10:\n            #                 plus_minus = (np.random.randint(0,2,n_sinknodes) - 0.5)*2\n            #                 self.Adj_signed[sinknodes,sinknodes] = plus_minus\n            leafnodes = np.array(sinknodes)\n            availnodes = np.array([i for i in range(self.dim) if i not in sinknodes])\n            #             settings.m(0,leafnodes,availnodes)\n            while len(availnodes) != 0:\n                # parent\n                parent_idx = np.random.choice(\n                    np.arange(0, len(leafnodes)), size=1, replace=False\n                )\n                parent = leafnodes[parent_idx]\n                # children\n                children_ids = np.random.choice(\n                    np.arange(0, len(availnodes)), size=2, replace=False\n                )\n                children = availnodes[children_ids]\n                settings.m(0, parent, children)\n                self.Adj_signed[children, parent] = np.ones(2)\n                if self.model == 8:\n                    self.Adj_signed[children[0], children[1]] = -1\n                if self.model in [9, 10]:\n                    self.Adj_signed[children[0], children[1]] = -1\n                    self.Adj_signed[children[1], children[0]] = -1\n                # update leafnodes\n                leafnodes = np.delete(leafnodes, parent_idx)\n                leafnodes = np.append(leafnodes, children)\n                # update availnodes\n                availnodes = np.delete(availnodes, children_ids)\n        #                 settings.m(0,availnodes)\n        #                 settings.m(0,leafnodes)\n        #                 settings.m(0,self.Adj)\n        #                 settings.m(0,'-')\n        else:\n            self.Adj = np.zeros((self.dim, self.dim))\n            for i in range(self.dim):\n                indep = np.random.binomial(1, self.p_indep)\n                if indep == 0:\n                    # this number includes parents (other variables)\n                    # and the variable itself, therefore its\n                    # self.maxnpar+2 in the following line\n                    nr = np.random.randint(1, self.maxnpar + 2)\n                    j_par = np.random.choice(\n                        np.arange(0, self.dim), size=nr, replace=False\n                    )\n                    self.Adj[i, j_par] = 1\n                else:\n                    self.Adj[i, i] = 1\n        #\n        self.Adj = np.abs(np.array(self.Adj_signed))", "idx": 860}
{"project": "Scanpy", "commit_id": "877_scanpy_1.9.0__sim.py_set_coupl_old.py", "target": 0, "func": "def set_coupl_old(self):\n        \"\"\"Using the adjacency matrix, sample a coupling matrix.\"\"\"\n        if self.model == 'krumsiek11' or self.model == 'var':\n            # we already built the coupling matrix in set_coupl20()\n            return\n        self.Coupl = np.zeros((self.dim, self.dim))\n        for i in range(self.Adj.shape[0]):\n            for j, a in enumerate(self.Adj[i]):\n                # if there is a 1 in Adj, specify co and antiregulation\n                # and strength of regulation\n                if a != 0:\n                    co_anti = np.random.randint(2)\n                    # set a lower bound for the coupling parameters\n                    # they ought not to be smaller than 0.1\n                    # and not be larger than 0.4\n                    self.Coupl[i, j] = 0.0 * np.random.rand() + 0.1\n                    # set sign for coupling\n                    if co_anti == 1:\n                        self.Coupl[i, j] *= -1\n        # enforce certain requirements on models\n        if self.model == 1:\n            self.coupl_model1()\n        elif self.model == 5:\n            self.coupl_model5()\n        elif self.model in [6, 7]:\n            self.coupl_model6()\n        elif self.model in [8, 9, 10]:\n            self.coupl_model8()\n        # output\n        if self.verbosity > 1:\n            settings.m(0, self.Coupl)", "idx": 861}
{"project": "Scanpy", "commit_id": "878_scanpy_1.9.0__sim.py_coupl_model1.py", "target": 0, "func": "def coupl_model1(self):\n        \"\"\"In model 1, we want enforce the following signs\n        on the couplings. Model 2 has the same couplings\n        but arbitrary signs.\n        \"\"\"\n        self.Coupl[0, 0] = np.abs(self.Coupl[0, 0])\n        self.Coupl[0, 1] = -np.abs(self.Coupl[0, 1])\n        self.Coupl[1, 1] = np.abs(self.Coupl[1, 1])", "idx": 862}
{"project": "Scanpy", "commit_id": "879_scanpy_1.9.0__sim.py_coupl_model5.py", "target": 0, "func": "def coupl_model5(self):\n        \"\"\"Toggle switch.\"\"\"\n        self.Coupl = -0.2 * self.Adj\n        self.Coupl[2, 0] *= -1\n        self.Coupl[3, 0] *= -1\n        self.Coupl[4, 1] *= -1\n        self.Coupl[5, 1] *= -1", "idx": 863}
{"project": "Scanpy", "commit_id": "87_scanpy_1.9.0__settings.py_file_format_data.py", "target": 0, "func": "def file_format_data(self, file_format: str):\n        _type_check(file_format, \"file_format_data\", str)\n        file_format_options = {\"txt\", \"csv\", \"h5ad\"}\n        if file_format not in file_format_options:\n            raise ValueError(\n                f\"Cannot set file_format_data to {file_format}. \"\n                f\"Must be one of {file_format_options}\"\n            )\n        self._file_format_data = file_format", "idx": 864}
{"project": "Scanpy", "commit_id": "880_scanpy_1.9.0__sim.py_coupl_model6.py", "target": 0, "func": "def coupl_model6(self):\n        \"\"\"Variant of toggle switch.\"\"\"\n        self.Coupl = 0.5 * self.Adj_signed", "idx": 865}
{"project": "Scanpy", "commit_id": "881_scanpy_1.9.0__sim.py_coupl_model8.py", "target": 0, "func": "def coupl_model8(self):\n        \"\"\"Variant of toggle switch.\"\"\"\n        self.Coupl = 0.5 * self.Adj_signed\n        # reduce the value of the coupling of the repressing genes\n        # otherwise completely unstable solutions are obtained\n        for x in np.nditer(self.Coupl, op_flags=['readwrite']):\n            if x < -1e-6:\n                x[...] = -0.2", "idx": 866}
{"project": "Scanpy", "commit_id": "882_scanpy_1.9.0__sim.py_coupl_model_krumsiek11.py", "target": 0, "func": "def coupl_model_krumsiek11(self):\n        \"\"\"Variant of toggle switch.\"\"\"\n        self.Coupl = self.Adj_signed", "idx": 867}
{"project": "Scanpy", "commit_id": "883_scanpy_1.9.0__sim.py_sim_model_back_help.py", "target": 0, "func": "def sim_model_back_help(self, Xt, Xt1):\n        \"\"\"Yields zero when solved for X_t\n        given X_{t+1}.\n        \"\"\"\n        return -Xt1 + Xt + self.Xdiff(Xt)", "idx": 868}
{"project": "Scanpy", "commit_id": "884_scanpy_1.9.0__sim.py_sim_model_backwards.py", "target": 0, "func": "def sim_model_backwards(self, tmax, X0):\n        \"\"\"Simulate the model backwards in time.\"\"\"\n        X = np.zeros((tmax, self.dim))\n        X[tmax - 1] = X0\n        for t in range(tmax - 2, -1, -1):\n            sol = sp.optimize.root(\n                self.sim_model_back_help, X[t + 1], args=(X[t + 1]), method='hybr'\n            )\n            X[t] = sol.x\n        return X", "idx": 869}
{"project": "Scanpy", "commit_id": "885_scanpy_1.9.0__sim.py_branch_init_model1.py", "target": 0, "func": "def branch_init_model1(self, tmax=100):\n        # check whether we can define trajectories\n        Xfix = np.array([self.Coupl[0, 1] / self.Coupl[0, 0], 1])\n        if Xfix[0] > 0.97 or Xfix[0] < 0.03:\n            settings.m(\n                0,\n                '... either no fixed point in [0,1]^2! \\n'\n                + '    or fixed point is too close to bounds',\n            )\n            return None\n        #\n        XbackUp = self.sim_model_backwards(\n            tmax=tmax / 3, X0=Xfix + np.array([0.02, -0.02])\n        )\n        XbackDo = self.sim_model_backwards(\n            tmax=tmax / 3, X0=Xfix + np.array([-0.02, -0.02])\n        )\n        #\n        Xup = self.sim_model(tmax=tmax, X0=XbackUp[0])\n        Xdo = self.sim_model(tmax=tmax, X0=XbackDo[0])\n        # compute mean\n        X0mean = 0.5 * (Xup[0] + Xdo[0])\n        #\n        if np.min(X0mean) < 0.025 or np.max(X0mean) > 0.975:\n            settings.m(0, '... initial point is too close to bounds')\n            return None\n        if self.show and self.verbosity > 1:\n            pl.figure()  # noqa: F821  TODO Fix me\n            pl.plot(XbackUp[:, 0], '.b', XbackUp[:, 1], '.g')  # noqa: F821  TODO Fix me\n            pl.plot(XbackDo[:, 0], '.b', XbackDo[:, 1], '.g')  # noqa: F821  TODO Fix me\n            pl.plot(Xup[:, 0], 'b', Xup[:, 1], 'g')  # noqa: F821  TODO Fix me\n            pl.plot(Xdo[:, 0], 'b', Xdo[:, 1], 'g')  # noqa: F821  TODO Fix me\n        return X0mean", "idx": 870}
{"project": "Scanpy", "commit_id": "886_scanpy_1.9.0__sim.py_parents_from_boolRule.py", "target": 0, "func": "def parents_from_boolRule(self, rule):\n        \"\"\"Determine parents based on boolean updaterule.\n\n        Returns list of parents.\n        \"\"\"\n        rule_pa = (\n            rule.replace('(', '')\n            .replace(')', '')\n            .replace('or', '')\n            .replace('and', '')\n            .replace('not', '')\n        )\n        rule_pa = rule_pa.split()\n        # if there are no parents, continue\n        if not rule_pa:\n            return []\n        # check whether these are meaningful parents\n        pa_old = []\n        pa_delete = []\n        for pa in rule_pa:\n            if pa not in self.varNames.keys():\n                settings.m(0, 'list of available variables:')\n                settings.m(0, list(self.varNames.keys()))\n                message = (\n                    'processing of rule \"'\n                    + rule\n                    + ' yields an invalid parent: '\n                    + pa\n                    + ' | check whether the syntax is correct: \\n'\n                    + 'only python expressions \"(\",\")\",\"or\",\"and\",\"not\" '\n                    + 'are allowed, variable names and expressions have to be separated '\n                    + 'by white spaces'\n                )\n                raise ValueError(message)\n            if pa in pa_old:\n                pa_delete.append(pa)\n        for pa in pa_delete:\n            rule_pa.remove(pa)\n        return rule_pa", "idx": 871}
{"project": "Scanpy", "commit_id": "887_scanpy_1.9.0__sim.py_build_boolCoeff.py", "target": 0, "func": "def build_boolCoeff(self):\n        \"\"\"Compute coefficients for tuple space.\"\"\"\n        # coefficients for hill functions from boolean update rules\n        self.boolCoeff = {s: [] for s in self.varNames.keys()}\n        # parents\n        self.pas = {s: [] for s in self.varNames.keys()}\n        #\n        for key in self.boolRules.keys():\n            rule = self.boolRules[key]\n            self.pas[key] = self.parents_from_boolRule(rule)\n            pasIndices = [self.varNames[pa] for pa in self.pas[key]]\n            # check whether there are coupling matrix entries for each parent\n            for g in range(self.dim):\n                if g in pasIndices:\n                    if np.abs(self.Coupl[self.varNames[key], g]) < 1e-10:\n                        raise ValueError(f'specify coupling value for {key} <- {g}')\n                else:\n                    if np.abs(self.Coupl[self.varNames[key], g]) > 1e-10:\n                        raise ValueError(\n                            'there should be no coupling value for ' f'{key} <- {g}'\n                        )\n            if self.verbosity > 1:\n                settings.m(0, '...' + key)\n                settings.m(0, rule)\n                settings.m(0, rule_pa)  # noqa: F821\n            # now evaluate coefficients\n            for tuple in list(\n                itertools.product([False, True], repeat=len(self.pas[key]))\n            ):\n                if self.process_rule(rule, self.pas[key], tuple):\n                    self.boolCoeff[key].append(tuple)\n            #\n            if self.verbosity > 1:\n                settings.m(0, self.boolCoeff[key])", "idx": 872}
{"project": "Scanpy", "commit_id": "888_scanpy_1.9.0__sim.py_process_rule.py", "target": 0, "func": "def process_rule(self, rule, pa, tuple):\n        \"\"\"Process a string that denotes a boolean rule.\"\"\"\n        for i, v in enumerate(tuple):\n            rule = rule.replace(pa[i], str(v))\n        return eval(rule)", "idx": 873}
{"project": "Scanpy", "commit_id": "889_scanpy_1.9.0__sim.py_sim_givenAdj.py", "target": 0, "func": "def sim_givenAdj(self, Adj: np.ndarray, model='line'):\n        \"\"\"\\\n        Simulate data given only an adjacancy matrix and a model.\n\n        The model is a bivariate funtional dependence. The adjacancy matrix\n        needs to be acyclic.\n\n        Parameters\n        ----------\n        Adj\n            adjacancy matrix of shape (dim,dim).\n\n        Returns\n        -------\n        Data array of shape (n_samples,dim).\n        \"\"\"\n        # nice examples\n        examples = [  # noqa: F841 TODO We are really unsure whether this is needed.\n            dict(\n                func='sawtooth',\n                gdist='uniform',\n                sigma_glob=1.8,\n                sigma_noise=0.1,\n            )\n        ]\n\n        # nr of samples\n        n_samples = 100\n\n        # noise\n        sigma_glob = 1.8\n        sigma_noise = 0.4\n\n        # coupling function / model\n        func = self.funcs[model]\n\n        # glob distribution\n        sourcedist = 'uniform'\n\n        # loop over source nodes\n        dim = Adj.shape[0]\n        X = np.zeros((n_samples, dim))\n        # source nodes have no parents themselves\n        nrpar = 0\n        children = list(range(dim))\n        parents = []\n        for gp in range(dim):\n            if Adj[gp, :].sum() == nrpar:\n                if sourcedist == 'gaussian':\n                    X[:, gp] = np.random.normal(0, sigma_glob, n_samples)\n                if sourcedist == 'uniform':\n                    X[:, gp] = np.random.uniform(-sigma_glob, sigma_glob, n_samples)\n                parents.append(gp)\n                children.remove(gp)\n\n        # all of the following guarantees for 3 dim, that we generate the data\n        # in the correct sequence\n        # then compute all nodes that have 1 parent, then those with 2 parents\n        children_sorted = []\n        nrchildren_par = np.zeros(dim)\n        nrchildren_par[0] = len(parents)\n        for nrpar in range(1, dim):\n            # loop over child nodes\n            for gp in children:\n                if Adj[gp, :].sum() == nrpar:\n                    children_sorted.append(gp)\n                    nrchildren_par[nrpar] += 1\n        # if there is more than a child with a single parent\n        # order these children (there are two in three dim)\n        # by distance to the source/parent\n        if nrchildren_par[1] > 1:\n            if Adj[children_sorted[0], parents[0]] == 0:\n                help = children_sorted[0]\n                children_sorted[0] = children_sorted[1]\n                children_sorted[1] = help\n\n        for gp in children_sorted:\n            for g in range(dim):\n                if Adj[gp, g] > 0:\n                    X[:, gp] += 1.0 / Adj[gp, :].sum() * func(X[:, g])\n            X[:, gp] += np.random.normal(0, sigma_noise, n_samples)\n\n        #         fig = pl.figure()\n        #         fig.add_subplot(311)\n        #         pl.plot(X[:,0],X[:,1],'.',mec='white')\n        #         fig.add_subplot(312)\n        #         pl.plot(X[:,1],X[:,2],'.',mec='white')\n        #         fig.add_subplot(313)\n        #         pl.plot(X[:,2],X[:,0],'.',mec='white')\n        #         pl.show()\n\n        return X", "idx": 874}
{"project": "Scanpy", "commit_id": "88_scanpy_1.9.0__settings.py_file_format_figs.py", "target": 0, "func": "def file_format_figs(self, figure_format: str):\n        _type_check(figure_format, \"figure_format_data\", str)\n        self._file_format_figs = figure_format", "idx": 875}
{"project": "Scanpy", "commit_id": "890_scanpy_1.9.0__sim.py_sim_combi.py", "target": 0, "func": "def sim_combi(self):\n        \"\"\"Simulate data to model combi regulation.\"\"\"\n        n_samples = 500\n        sigma_glob = 1.8\n\n        X = np.zeros((n_samples, 3))\n\n        X[:, 0] = np.random.uniform(-sigma_glob, sigma_glob, n_samples)\n        X[:, 1] = np.random.uniform(-sigma_glob, sigma_glob, n_samples)\n\n        func = self.funcs['tanh']\n\n        # XOR type\n        #         X[:,2] = (func(X[:,0])*sp.stats.norm.pdf(X[:,1],0,0.2)\n        #                   + func(X[:,1])*sp.stats.norm.pdf(X[:,0],0,0.2))\n        # AND type / diagonal\n        #         X[:,2] = (func(X[:,0]+X[:,1])*sp.stats.norm.pdf(X[:,1]-X[:,0],0,0.2))\n        # AND type / horizontal\n        X[:, 2] = func(X[:, 0]) * sp.stats.norm.cdf(X[:, 1], 1, 0.2)\n\n        pl.scatter(  # noqa: F821  TODO Fix me\n            X[:, 0], X[:, 1], c=X[:, 2], edgecolor='face'\n        )\n        pl.show()  # noqa: F821  TODO Fix me\n\n        pl.plot(X[:, 1], X[:, 2], '.')  # noqa: F821  TODO Fix me\n        pl.show()  # noqa: F821  TODO Fix me\n\n        return X", "idx": 876}
{"project": "Scanpy", "commit_id": "891_scanpy_1.9.0__top_genes.py_correlation_matrix.py", "target": 0, "func": "def correlation_matrix(\n    adata: AnnData,\n    name_list: Optional[Collection[str]] = None,\n    groupby: Optional[str] = None,\n    group: Optional[int] = None,\n    n_genes: int = 20,\n    data: Literal['Complete', 'Group', 'Rest'] = 'Complete',\n    method: Literal['pearson', 'kendall', 'spearman'] = 'pearson',\n    annotation_key: Optional[str] = None,\n) -> None:\n    \"\"\"\\\n    Calculate correlation matrix.\n\n    Calculate a correlation matrix for genes strored in sample annotation\n    using :func:`~scanpy.tl.rank_genes_groups`.\n\n    Parameters\n    ----------\n    adata\n        Annotated data matrix.\n    name_list\n        Takes a list of genes for which to calculate the correlation matrix\n    groupby\n        If no name list is passed, genes are selected from the\n        results of rank_gene_groups. Then this is the key of the sample grouping to consider.\n        Note that in this case also a group index has to be specified.\n    group\n        Group index for which the correlation matrix for top_ranked genes should be calculated.\n        Currently only int is supported, will change very soon\n    n_genes\n        For how many genes to calculate correlation matrix? If specified, cuts the name list\n        (in whatever order it is passed).\n    data\n        At the moment, this is only relevant for the case that name_list is drawn from rank_gene_groups results.\n        If specified, collects mask for the called group and then takes only those cells specified.\n        If 'Complete', calculate correlation using full data\n        If 'Group', calculate correlation within the selected group.\n        If 'Rest', calculate corrlation for everything except the group\n    method\n        Which kind of correlation coefficient to use\n\n        pearson\n            standard correlation coefficient\n        kendall\n            Kendall Tau correlation coefficient\n        spearman\n            Spearman rank correlation\n    annotation_key\n        Allows to define the name of the anndata entry where results are stored.\n    \"\"\"\n\n    # TODO: At the moment, only works for int identifiers\n\n    # If no genes are passed, selects ranked genes from sample annotation.\n    # At the moment, only calculate one table (Think about what comes next)\n    if name_list is None:\n        name_list = list()\n        for j, k in enumerate(adata.uns['rank_genes_groups_gene_names']):\n            if j >= n_genes:\n                break\n            name_list.append(adata.uns['rank_genes_groups_gene_names'][j][group])\n    else:\n        if len(name_list) > n_genes:\n            name_list = name_list[0:n_genes]\n\n    # If special method (later) , truncate\n    adata_relevant = adata[:, name_list]\n    # This line just makes group_mask access easier. Nothing else but 'all' will stand here.\n    groups = 'all'\n    if data == 'Complete' or groupby is None:\n        if issparse(adata_relevant.X):\n            Data_array = adata_relevant.X.todense()\n        else:\n            Data_array = adata_relevant.X\n    else:\n        # get group_mask\n        groups_order, groups_masks = select_groups(adata, groups, groupby)\n        if data == 'Group':\n            if issparse(adata_relevant.X):\n                Data_array = adata_relevant.X[groups_masks[group], :].todense()\n            else:\n                Data_array = adata_relevant.X[groups_masks[group], :]\n        elif data == 'Rest':\n            if issparse(adata_relevant.X):\n                Data_array = adata_relevant.X[~groups_masks[group], :].todense()\n            else:\n                Data_array = adata_relevant.X[~groups_masks[group], :]\n        else:\n            logg.error('data argument should be either <Complete> or <Group> or <Rest>')\n\n    # Distinguish between sparse and non-sparse data\n\n    DF_array = pd.DataFrame(Data_array, columns=name_list)\n    cor_table = DF_array.corr(method=method)\n    if annotation_key is None:\n        if groupby is None:\n            adata.uns['Correlation_matrix'] = cor_table\n        else:\n            adata.uns['Correlation_matrix' + groupby + str(group)] = cor_table\n    else:\n        adata.uns[annotation_key] = cor_table", "idx": 877}
{"project": "Scanpy", "commit_id": "892_scanpy_1.9.0__top_genes.py_ROC_AUC_analysis.py", "target": 0, "func": "def ROC_AUC_analysis(\n    adata: AnnData,\n    groupby: str,\n    group: Optional[str] = None,\n    n_genes: int = 100,\n):\n    \"\"\"\\\n    Calculate correlation matrix.\n\n    Calculate a correlation matrix for genes strored in sample annotation\n\n    Parameters\n    ----------\n    adata\n        Annotated data matrix.\n    groupby\n        The key of the sample grouping to consider.\n    group\n        Group name or index for which the correlation matrix for top ranked\n        genes should be calculated.\n        If no parameter is passed, ROC/AUC is calculated for all groups\n    n_genes\n        For how many genes to calculate ROC and AUC. If no parameter is passed,\n        calculation is done for all stored top ranked genes.\n    \"\"\"\n    if group is None:\n        pass\n        # TODO: Loop over all groups instead of just taking one.\n\n    # Assume group takes an int value for one group for the moment.\n    name_list = list()\n    for j, k in enumerate(adata.uns['rank_genes_groups_gene_names']):\n        if j >= n_genes:\n            break\n        name_list.append(adata.uns['rank_genes_groups_gene_names'][j][group])\n\n    # TODO: For the moment, see that everything works for comparison against the rest. Resolve issues later.\n    groups = 'all'\n    groups_order, groups_masks = select_groups(adata, groups, groupby)\n\n    # Use usual convention, better for looping later.\n    mask = groups_masks[group]\n\n    # TODO: Allow for sample weighting requires better mask access... later\n\n    # We store calculated data in dict, access it via dict to dict. Check if this is the best way.\n    fpr = {}\n    tpr = {}\n    thresholds = {}\n    roc_auc = {}\n    y_true = mask\n    for i, j in enumerate(name_list):\n        vec = adata[:, [j]].X\n        if issparse(vec):\n            y_score = vec.todense()\n        else:\n            y_score = vec\n\n        (\n            fpr[name_list[i]],\n            tpr[name_list[i]],\n            thresholds[name_list[i]],\n        ) = metrics.roc_curve(\n            y_true, y_score, pos_label=None, sample_weight=None, drop_intermediate=False\n        )\n        roc_auc[name_list[i]] = metrics.auc(fpr[name_list[i]], tpr[name_list[i]])\n    adata.uns['ROCfpr' + groupby + str(group)] = fpr\n    adata.uns['ROCtpr' + groupby + str(group)] = tpr\n    adata.uns['ROCthresholds' + groupby + str(group)] = thresholds\n    adata.uns['ROC_AUC' + groupby + str(group)] = roc_auc", "idx": 878}
{"project": "Scanpy", "commit_id": "893_scanpy_1.9.0__top_genes.py_subsampled_estimates.py", "target": 0, "func": "def subsampled_estimates(mask, mask_rest=None, precision=0.01, probability=0.99):\n    # Simple method that can be called by rank_gene_group. It uses masks that have been passed to the function and\n    # calculates how much has to be subsampled in order to reach a certain precision with a certain probability\n    # Then it subsamples for mask, mask rest\n    # Since convergence speed varies, we take the slower one, i.e. the variance. This might have future speed-up\n    # potential\n    if mask_rest is None:\n        mask_rest = ~mask", "idx": 879}
{"project": "Scanpy", "commit_id": "894_scanpy_1.9.0__top_genes.py_dominated_ROC_elimination.py", "target": 0, "func": "def dominated_ROC_elimination(adata, grouby):\n    # This tool has the purpose to take a set of genes (possibly already pre-selected) and analyze AUC.\n    # Those and only those are eliminated who are dominated completely\n    # TODO: Potentially (But not till tomorrow), this can be adapted to only consider the AUC in the given\n    # TODO: optimization frame\n    pass", "idx": 880}
{"project": "Scanpy", "commit_id": "895_scanpy_1.9.0__top_genes.py__gene_preselection.py", "target": 0, "func": "def _gene_preselection(adata, mask, thresholds):\n    # This tool serves to\n    # It is not thought to be addressed directly but rather using rank_genes_group or ROC analysis or comparable\n    # TODO: Pass back a truncated adata object with only those genes that fullfill thresholding criterias\n    # This function should be accessible by both rank_genes_groups and ROC_curve analysis\n    pass", "idx": 881}
{"project": "Scanpy", "commit_id": "896_scanpy_1.9.0__tsne.py_tsne.py", "target": 0, "func": "def tsne(\n    adata: AnnData,\n    n_pcs: Optional[int] = None,\n    use_rep: Optional[str] = None,\n    perplexity: Union[float, int] = 30,\n    early_exaggeration: Union[float, int] = 12,\n    learning_rate: Union[float, int] = 1000,\n    random_state: AnyRandom = 0,\n    use_fast_tsne: bool = False,\n    n_jobs: Optional[int] = None,\n    copy: bool = False,\n    *,\n    metric: str = \"euclidean\",\n) -> Optional[AnnData]:\n    \"\"\"\\\n    t-SNE [Maaten08]_ [Amir13]_ [Pedregosa11]_.\n\n    t-distributed stochastic neighborhood embedding (tSNE) [Maaten08]_ has been\n    proposed for visualizating single-cell data by [Amir13]_. Here, by default,\n    we use the implementation of *scikit-learn* [Pedregosa11]_. You can achieve\n    a huge speedup and better convergence if you install `Multicore-tSNE\n    <https://github.com/DmitryUlyanov/Multicore-TSNE>`__ by [Ulyanov16]_, which\n    will be automatically detected by Scanpy.\n\n    Parameters\n    ----------\n    adata\n        Annotated data matrix.\n    {doc_n_pcs}\n    {use_rep}\n    perplexity\n        The perplexity is related to the number of nearest neighbors that\n        is used in other manifold learning algorithms. Larger datasets\n        usually require a larger perplexity. Consider selecting a value\n        between 5 and 50. The choice is not extremely critical since t-SNE\n        is quite insensitive to this parameter.\n    metric\n        Distance metric calculate neighbors on.\n    early_exaggeration\n        Controls how tight natural clusters in the original space are in the\n        embedded space and how much space will be between them. For larger\n        values, the space between natural clusters will be larger in the\n        embedded space. Again, the choice of this parameter is not very\n        critical. If the cost function increases during initial optimization,\n        the early exaggeration factor or the learning rate might be too high.\n    learning_rate\n        Note that the R-package \"Rtsne\" uses a default of 200.\n        The learning rate can be a critical parameter. It should be\n        between 100 and 1000. If the cost function increases during initial\n        optimization, the early exaggeration factor or the learning rate\n        might be too high. If the cost function gets stuck in a bad local\n        minimum increasing the learning rate helps sometimes.\n    random_state\n        Change this to use different intial states for the optimization.\n        If `None`, the initial state is not reproducible.\n    n_jobs\n        Number of jobs for parallel computation.\n        `None` means using :attr:`scanpy._settings.ScanpyConfig.n_jobs`.\n    copy\n        Return a copy instead of writing to `adata`.\n\n    Returns\n    -------\n    Depending on `copy`, returns or updates `adata` with the following fields.\n\n    **X_tsne** : `np.ndarray` (`adata.obs`, dtype `float`)\n        tSNE coordinates of data.\n    \"\"\"\n    import sklearn\n\n    start = logg.info('computing tSNE')\n    adata = adata.copy() if copy else adata\n    X = _choose_representation(adata, use_rep=use_rep, n_pcs=n_pcs)\n    # params for sklearn\n    n_jobs = settings.n_jobs if n_jobs is None else n_jobs\n    params_sklearn = dict(\n        perplexity=perplexity,\n        random_state=random_state,\n        verbose=settings.verbosity > 3,\n        early_exaggeration=early_exaggeration,\n        learning_rate=learning_rate,\n        n_jobs=n_jobs,\n        metric=metric,\n    )\n    # square_distances will default to true in the future, we'll get ahead of the\n    # warning for now\n    if metric != \"euclidean\":\n        sklearn_version = version.parse(sklearn.__version__)\n        if sklearn_version >= version.parse(\"0.24.0\"):\n            params_sklearn[\"square_distances\"] = True\n        else:\n            warnings.warn(\n                \"Results for non-euclidean metrics changed in sklearn 0.24.0, while \"\n                f\"you are using {sklearn.__version__}.\",\n                UserWarning,\n            )\n\n    # Backwards compat handling: Remove in scanpy 1.9.0\n    if n_jobs != 1 and not use_fast_tsne:\n        warnings.warn(\n            UserWarning(\n                \"In previous versions of scanpy, calling tsne with n_jobs > 1 would use \"\n                \"MulticoreTSNE. Now this uses the scikit-learn version of TSNE by default. \"\n                \"If you'd like the old behaviour (which is deprecated), pass \"\n                \"'use_fast_tsne=True'. Note, MulticoreTSNE is not actually faster anymore.\"\n            )\n        )\n    if use_fast_tsne:\n        warnings.warn(\n            FutureWarning(\n                \"Argument `use_fast_tsne` is deprecated, and support for MulticoreTSNE \"\n                \"will be dropped in a future version of scanpy.\"\n            )\n        )\n\n    # deal with different tSNE implementations\n    if use_fast_tsne:\n        try:\n            from MulticoreTSNE import MulticoreTSNE as TSNE\n\n            tsne = TSNE(**params_sklearn)\n            logg.info(\"    using the 'MulticoreTSNE' package by Ulyanov (2017)\")\n            # need to transform to float64 for MulticoreTSNE...\n            X_tsne = tsne.fit_transform(X.astype('float64'))\n        except ImportError:\n            use_fast_tsne = False\n            warnings.warn(\n                UserWarning(\n                    \"Could not import 'MulticoreTSNE'. Falling back to scikit-learn.\"\n                )\n            )\n    if use_fast_tsne is False:  # In case MultiCore failed to import\n        from sklearn.manifold import TSNE\n\n        # unfortunately, sklearn does not allow to set a minimum number\n        # of iterations for barnes-hut tSNE\n        tsne = TSNE(**params_sklearn)\n        logg.info('    using sklearn.manifold.TSNE')\n        X_tsne = tsne.fit_transform(X)\n\n    # update AnnData instance\n    adata.obsm['X_tsne'] = X_tsne  # annotate samples with tSNE coordinates\n    adata.uns[\"tsne\"] = {\n        \"params\": {\n            k: v\n            for k, v in {\n                \"perplexity\": perplexity,\n                \"early_exaggeration\": early_exaggeration,\n                \"learning_rate\": learning_rate,\n                \"n_jobs\": n_jobs,\n                \"metric\": metric,\n                \"use_rep\": use_rep,\n            }.items()\n            if v is not None\n        }\n    }\n\n    logg.info(\n        '    finished',\n        time=start,\n        deep=\"added\\n    'X_tsne', tSNE coordinates (adata.obsm)\",\n    )\n\n    return adata if copy else None", "idx": 882}
{"project": "Scanpy", "commit_id": "897_scanpy_1.9.0__umap.py_umap.py", "target": 0, "func": "def umap(\n    adata: AnnData,\n    min_dist: float = 0.5,\n    spread: float = 1.0,\n    n_components: int = 2,\n    maxiter: Optional[int] = None,\n    alpha: float = 1.0,\n    gamma: float = 1.0,\n    negative_sample_rate: int = 5,\n    init_pos: Union[_InitPos, np.ndarray, None] = 'spectral',\n    random_state: AnyRandom = 0,\n    a: Optional[float] = None,\n    b: Optional[float] = None,\n    copy: bool = False,\n    method: Literal['umap', 'rapids'] = 'umap',\n    neighbors_key: Optional[str] = None,\n) -> Optional[AnnData]:\n    \"\"\"\\\n    Embed the neighborhood graph using UMAP [McInnes18]_.\n\n    UMAP (Uniform Manifold Approximation and Projection) is a manifold learning\n    technique suitable for visualizing high-dimensional data. Besides tending to\n    be faster than tSNE, it optimizes the embedding such that it best reflects\n    the topology of the data, which we represent throughout Scanpy using a\n    neighborhood graph. tSNE, by contrast, optimizes the distribution of\n    nearest-neighbor distances in the embedding such that these best match the\n    distribution of distances in the high-dimensional space.  We use the\n    implementation of `umap-learn <https://github.com/lmcinnes/umap>`__\n    [McInnes18]_. For a few comparisons of UMAP with tSNE, see this `preprint\n    <https://doi.org/10.1101/298430>`__.\n\n    Parameters\n    ----------\n    adata\n        Annotated data matrix.\n    min_dist\n        The effective minimum distance between embedded points. Smaller values\n        will result in a more clustered/clumped embedding where nearby points on\n        the manifold are drawn closer together, while larger values will result\n        on a more even dispersal of points. The value should be set relative to\n        the ``spread`` value, which determines the scale at which embedded\n        points will be spread out. The default of in the `umap-learn` package is\n        0.1.\n    spread\n        The effective scale of embedded points. In combination with `min_dist`\n        this determines how clustered/clumped the embedded points are.\n    n_components\n        The number of dimensions of the embedding.\n    maxiter\n        The number of iterations (epochs) of the optimization. Called `n_epochs`\n        in the original UMAP.\n    alpha\n        The initial learning rate for the embedding optimization.\n    gamma\n        Weighting applied to negative samples in low dimensional embedding\n        optimization. Values higher than one will result in greater weight\n        being given to negative samples.\n    negative_sample_rate\n        The number of negative edge/1-simplex samples to use per positive\n        edge/1-simplex sample in optimizing the low dimensional embedding.\n    init_pos\n        How to initialize the low dimensional embedding. Called `init` in the\n        original UMAP. Options are:\n\n        * Any key for `adata.obsm`.\n        * 'paga': positions from :func:`~scanpy.pl.paga`.\n        * 'spectral': use a spectral embedding of the graph.\n        * 'random': assign initial embedding positions at random.\n        * A numpy array of initial embedding positions.\n    random_state\n        If `int`, `random_state` is the seed used by the random number generator;\n        If `RandomState` or `Generator`, `random_state` is the random number generator;\n        If `None`, the random number generator is the `RandomState` instance used\n        by `np.random`.\n    a\n        More specific parameters controlling the embedding. If `None` these\n        values are set automatically as determined by `min_dist` and\n        `spread`.\n    b\n        More specific parameters controlling the embedding. If `None` these\n        values are set automatically as determined by `min_dist` and\n        `spread`.\n    copy\n        Return a copy instead of writing to adata.\n    method\n        Use the original 'umap' implementation, or 'rapids' (experimental, GPU only)\n    neighbors_key\n        If not specified, umap looks .uns['neighbors'] for neighbors settings\n        and .obsp['connectivities'] for connectivities\n        (default storage places for pp.neighbors).\n        If specified, umap looks .uns[neighbors_key] for neighbors settings and\n        .obsp[.uns[neighbors_key]['connectivities_key']] for connectivities.\n\n    Returns\n    -------\n    Depending on `copy`, returns or updates `adata` with the following fields.\n\n    **X_umap** : `adata.obsm` field\n        UMAP coordinates of data.\n    \"\"\"\n    adata = adata.copy() if copy else adata\n\n    if neighbors_key is None:\n        neighbors_key = 'neighbors'\n\n    if neighbors_key not in adata.uns:\n        raise ValueError(\n            f'Did not find .uns[\"{neighbors_key}\"]. Run `sc.pp.neighbors` first.'\n        )\n    start = logg.info('computing UMAP')\n\n    neighbors = NeighborsView(adata, neighbors_key)\n\n    if 'params' not in neighbors or neighbors['params']['method'] != 'umap':\n        logg.warning(\n            f'.obsp[\"{neighbors[\"connectivities_key\"]}\"] have not been computed using umap'\n        )\n\n    # Compat for umap 0.4 -> 0.5\n    with warnings.catch_warnings():\n        # umap 0.5.0\n        warnings.filterwarnings(\"ignore\", message=r\"Tensorflow not installed\")\n        import umap\n\n    if version.parse(umap.__version__) >= version.parse(\"0.5.0\"):\n\n        def simplicial_set_embedding(*args, **kwargs):\n            from umap.umap_ import simplicial_set_embedding\n\n            X_umap, _ = simplicial_set_embedding(\n                *args,\n                densmap=False,\n                densmap_kwds={},\n                output_dens=False,\n                **kwargs,\n            )\n            return X_umap\n\n    else:\n        from umap.umap_ import simplicial_set_embedding\n    from umap.umap_ import find_ab_params\n\n    if a is None or b is None:\n        a, b = find_ab_params(spread, min_dist)\n    else:\n        a = a\n        b = b\n    adata.uns['umap'] = {'params': {'a': a, 'b': b}}\n    if isinstance(init_pos, str) and init_pos in adata.obsm.keys():\n        init_coords = adata.obsm[init_pos]\n    elif isinstance(init_pos, str) and init_pos == 'paga':\n        init_coords = get_init_pos_from_paga(\n            adata, random_state=random_state, neighbors_key=neighbors_key\n        )\n    else:\n        init_coords = init_pos  # Let umap handle it\n    if hasattr(init_coords, \"dtype\"):\n        init_coords = check_array(init_coords, dtype=np.float32, accept_sparse=False)\n\n    if random_state != 0:\n        adata.uns['umap']['params']['random_state'] = random_state\n    random_state = check_random_state(random_state)\n\n    neigh_params = neighbors['params']\n    X = _choose_representation(\n        adata,\n        neigh_params.get('use_rep', None),\n        neigh_params.get('n_pcs', None),\n        silent=True,\n    )\n    if method == 'umap':\n        # the data matrix X is really only used for determining the number of connected components\n        # for the init condition in the UMAP embedding\n        default_epochs = 500 if neighbors['connectivities'].shape[0] <= 10000 else 200\n        n_epochs = default_epochs if maxiter is None else maxiter\n        X_umap = simplicial_set_embedding(\n            X,\n            neighbors['connectivities'].tocoo(),\n            n_components,\n            alpha,\n            a,\n            b,\n            gamma,\n            negative_sample_rate,\n            n_epochs,\n            init_coords,\n            random_state,\n            neigh_params.get('metric', 'euclidean'),\n            neigh_params.get('metric_kwds', {}),\n            verbose=settings.verbosity > 3,\n        )\n    elif method == 'rapids':\n        metric = neigh_params.get('metric', 'euclidean')\n        if metric != 'euclidean':\n            raise ValueError(\n                f'`sc.pp.neighbors` was called with `metric` {metric!r}, '\n                \"but umap `method` 'rapids' only supports the 'euclidean' metric.\"\n            )\n        from cuml import UMAP\n\n        n_neighbors = neighbors['params']['n_neighbors']\n        n_epochs = (\n            500 if maxiter is None else maxiter\n        )  # 0 is not a valid value for rapids, unlike original umap\n        X_contiguous = np.ascontiguousarray(X, dtype=np.float32)\n        umap = UMAP(\n            n_neighbors=n_neighbors,\n            n_components=n_components,\n            n_epochs=n_epochs,\n            learning_rate=alpha,\n            init=init_pos,\n            min_dist=min_dist,\n            spread=spread,\n            negative_sample_rate=negative_sample_rate,\n            a=a,\n            b=b,\n            verbose=settings.verbosity > 3,\n            random_state=random_state,\n        )\n        X_umap = umap.fit_transform(X_contiguous)\n    adata.obsm['X_umap'] = X_umap  # annotate samples with UMAP coordinates\n    logg.info(\n        '    finished',\n        time=start,\n        deep=('added\\n' \"    'X_umap', UMAP coordinates (adata.obsm)\"),\n    )\n    return adata if copy else None", "idx": 883}
{"project": "Scanpy", "commit_id": "898_scanpy_1.9.0__umap.py_simplicial_set_embedding.py", "target": 0, "func": "def simplicial_set_embedding(*args, **kwargs):\n            from umap.umap_ import simplicial_set_embedding\n\n            X_umap, _ = simplicial_set_embedding(\n                *args,\n                densmap=False,\n                densmap_kwds={},\n                output_dens=False,\n                **kwargs,\n            )\n            return X_umap", "idx": 884}
{"project": "Scanpy", "commit_id": "899_scanpy_1.9.0__utils.py__choose_representation.py", "target": 1, "func": "def _choose_representation(adata, use_rep=None, n_pcs=None, silent=False):\n    verbosity = settings.verbosity\n    if silent and settings.verbosity > 1:\n        settings.verbosity = 1\n    if use_rep is None and n_pcs == 0:  # backwards compat for specifying `.X`\n        use_rep = 'X'\n    if use_rep is None:\n        if adata.n_vars > settings.N_PCS:\n            if 'X_pca' in adata.obsm.keys():\n                if n_pcs is not None and n_pcs > adata.obsm['X_pca'].shape[1]:\n                    raise ValueError(\n                        '`X_pca` does not have enough PCs. Rerun `sc.pp.pca` with adjusted `n_comps`.'\n                    )\n                X = adata.obsm['X_pca'][:, :n_pcs]\n                logg.info(f'    using \\'X_pca\\' with n_pcs = {X.shape[1]}')\n            else:\n                logg.warning(\n                    f'You\u2019re trying to run this on {adata.n_vars} dimensions of `.X`, '\n                    'if you really want this, set `use_rep=\\'X\\'`.\\n         '\n                    'Falling back to preprocessing with `sc.pp.pca` and default params.'\n                )\n                X = pca(adata.X)\n                adata.obsm['X_pca'] = X[:, :n_pcs]\n        else:\n            logg.info('    using data matrix X directly')\n            X = adata.X\n    else:\n        if use_rep in adata.obsm.keys() and n_pcs is not None:\n            if n_pcs > adata.obsm[use_rep].shape[1]:\n                raise ValueError(\n                    f'{use_rep} does not have enough Dimensions. Provide a '\n                    'Representation with equal or more dimensions than'\n                    '`n_pcs` or lower `n_pcs` '\n                )\n            X = adata.obsm[use_rep][:, :n_pcs]\n        elif use_rep in adata.obsm.keys() and n_pcs is None:\n            X = adata.obsm[use_rep]\n        elif use_rep == 'X':\n            X = adata.X\n        else:\n            raise ValueError(\n                'Did not find {} in `.obsm.keys()`. '\n                'You need to compute it first.'.format(use_rep)\n            )\n    settings.verbosity = verbosity  # resetting verbosity\n    return X", "idx": 885}
{"project": "Scanpy", "commit_id": "89_scanpy_1.9.0__settings.py_autosave.py", "target": 0, "func": "def autosave(self, autosave: bool):\n        _type_check(autosave, \"autosave\", bool)\n        self._autosave = autosave", "idx": 886}
{"project": "Scanpy", "commit_id": "8_scanpy_1.9.0_debug_docstrings.py_pd_new.py", "target": 0, "func": "def pd_new(app, what, name, obj, options, lines):\n    _pd_orig(app, what, name, obj, options, lines)\n    print(*lines, sep='\\n')", "idx": 887}
{"project": "Scanpy", "commit_id": "900_scanpy_1.9.0__utils.py_preprocess_with_pca.py", "target": 0, "func": "def preprocess_with_pca(adata, n_pcs: Optional[int] = None, random_state=0):\n    \"\"\"\n    Parameters\n    ----------\n    n_pcs\n        If `n_pcs=0`, do not preprocess with PCA.\n        If `None` and there is a PCA version of the data, use this.\n        If an integer, compute the PCA.\n    \"\"\"\n    if n_pcs == 0:\n        logg.info('    using data matrix X directly (no PCA)')\n        return adata.X\n    elif n_pcs is None and 'X_pca' in adata.obsm_keys():\n        logg.info(f'    using \\'X_pca\\' with n_pcs = {adata.obsm[\"X_pca\"].shape[1]}')\n        return adata.obsm['X_pca']\n    elif 'X_pca' in adata.obsm_keys() and adata.obsm['X_pca'].shape[1] >= n_pcs:\n        logg.info(f'    using \\'X_pca\\' with n_pcs = {n_pcs}')\n        return adata.obsm['X_pca'][:, :n_pcs]\n    else:\n        n_pcs = settings.N_PCS if n_pcs is None else n_pcs\n        if adata.X.shape[1] > n_pcs:\n            logg.info(f'    computing \\'X_pca\\' with n_pcs = {n_pcs}')\n            logg.hint('avoid this by setting n_pcs = 0')\n            X = pca(adata.X, n_comps=n_pcs, random_state=random_state)\n            adata.obsm['X_pca'] = X\n            return X\n        else:\n            logg.info('    using data matrix X directly (no PCA)')\n            return adata.X", "idx": 888}
{"project": "Scanpy", "commit_id": "901_scanpy_1.9.0__utils.py_get_init_pos_from_paga.py", "target": 0, "func": "def get_init_pos_from_paga(\n    adata, adjacency=None, random_state=0, neighbors_key=None, obsp=None\n):\n    np.random.seed(random_state)\n    if adjacency is None:\n        adjacency = _choose_graph(adata, obsp, neighbors_key)\n    if 'paga' in adata.uns and 'pos' in adata.uns['paga']:\n        groups = adata.obs[adata.uns['paga']['groups']]\n        pos = adata.uns['paga']['pos']\n        connectivities_coarse = adata.uns['paga']['connectivities']\n        init_pos = np.ones((adjacency.shape[0], 2))\n        for i, group_pos in enumerate(pos):\n            subset = (groups == groups.cat.categories[i]).values\n            neighbors = connectivities_coarse[i].nonzero()\n            if len(neighbors[1]) > 0:\n                connectivities = connectivities_coarse[i][neighbors]\n                nearest_neighbor = neighbors[1][np.argmax(connectivities)]\n                noise = np.random.random((len(subset[subset]), 2))\n                dist = pos[i] - pos[nearest_neighbor]\n                noise = noise * dist\n                init_pos[subset] = group_pos - 0.5 * dist + noise\n            else:\n                init_pos[subset] = group_pos\n    else:\n        raise ValueError(\n            'Plot PAGA first, so that adata.uns[\\'paga\\']' 'with key \\'pos\\'.'\n        )\n    return init_pos", "idx": 889}
{"project": "Scanpy", "commit_id": "902_scanpy_1.9.0__utils_clustering.py_rename_groups.py", "target": 0, "func": "def rename_groups(\n    adata, key_added, restrict_key, restrict_categories, restrict_indices, groups\n):\n    key_added = restrict_key + '_R' if key_added is None else key_added\n    all_groups = adata.obs[restrict_key].astype('U')\n    prefix = '-'.join(restrict_categories) + ','\n    new_groups = [prefix + g for g in groups.astype('U')]\n    all_groups.iloc[restrict_indices] = new_groups\n    return all_groups", "idx": 890}
{"project": "Scanpy", "commit_id": "903_scanpy_1.9.0__utils_clustering.py_restrict_adjacency.py", "target": 0, "func": "def restrict_adjacency(adata, restrict_key, restrict_categories, adjacency):\n    if not isinstance(restrict_categories[0], str):\n        raise ValueError(\n            'You need to use strings to label categories, ' 'e.g. \\'1\\' instead of 1.'\n        )\n    for c in restrict_categories:\n        if c not in adata.obs[restrict_key].cat.categories:\n            raise ValueError(\n                '\\'{}\\' is not a valid category for \\'{}\\''.format(c, restrict_key)\n            )\n    restrict_indices = adata.obs[restrict_key].isin(restrict_categories).values\n    adjacency = adjacency[restrict_indices, :]\n    adjacency = adjacency[:, restrict_indices]\n    return adjacency, restrict_indices", "idx": 891}
{"project": "Scanpy", "commit_id": "904_scanpy_1.9.0___init__.py_check_versions.py", "target": 0, "func": "def check_versions():\n    from .._compat import pkg_version\n\n    umap_version = pkg_version(\"umap-learn\")\n\n    if version.parse(anndata_version) < version.parse('0.6.10'):\n        from .. import __version__\n\n        raise ImportError(\n            f'Scanpy {__version__} needs anndata version >=0.6.10, '\n            f'not {anndata_version}.\\nRun `pip install anndata -U --no-deps`.'\n        )\n\n    if umap_version < version.parse('0.3.0'):\n        from . import __version__\n\n        # make this a warning, not an error\n        # it might be useful for people to still be able to run it\n        logg.warning(\n            f'Scanpy {__version__} needs umap ' f'version >=0.3.0, not {umap_version}.'", "idx": 892}
{"project": "Scanpy", "commit_id": "905_scanpy_1.9.0___init__.py_getdoc.py", "target": 0, "func": "def getdoc(c_or_f: Union[Callable, type]) -> Optional[str]:\n    if getattr(c_or_f, '__doc__', None) is None:\n        return None\n    doc = inspect.getdoc(c_or_f)\n    if isinstance(c_or_f, type) and hasattr(c_or_f, '__init__'):\n        sig = inspect.signature(c_or_f.__init__)\n    else:\n        sig = inspect.signature(c_or_f)\n\n    def type_doc(name: str):\n        param: inspect.Parameter = sig.parameters[name]\n        cls = getattr(param.annotation, '__qualname__', repr(param.annotation))\n        if param.default is not param.empty:\n            return f'{cls}, optional (default: {param.default!r})'\n        else:\n            return cls\n\n    return '\\n'.join(\n        f'{line} : {type_doc(line)}' if line.strip() in sig.parameters else line\n        for line in doc.split('\\n')", "idx": 893}
{"project": "Scanpy", "commit_id": "906_scanpy_1.9.0___init__.py_deprecated_arg_names.py", "target": 0, "func": "def deprecated_arg_names(arg_mapping: Mapping[str, str]):\n    \"\"\"\n    Decorator which marks a functions keyword arguments as deprecated. It will\n    result in a warning being emitted when the deprecated keyword argument is\n    used, and the function being called with the new argument.\n\n    Parameters\n    ----------\n    arg_mapping\n        Mapping from deprecated argument name to current argument name.\n    \"\"\"\n\n    def decorator(func):\n        @wraps(func)\n        def func_wrapper(*args, **kwargs):\n            warnings.simplefilter('always', DeprecationWarning)  # turn off filter\n            for old, new in arg_mapping.items():\n                if old in kwargs:\n                    warnings.warn(\n                        f\"Keyword argument '{old}' has been \"\n                        f\"deprecated in favour of '{new}'. \"\n                        f\"'{old}' will be removed in a future version.\",\n                        category=DeprecationWarning,\n                        stacklevel=2,\n                    )\n                    val = kwargs.pop(old)\n                    kwargs[new] = val\n            # reset filter\n            warnings.simplefilter('default', DeprecationWarning)\n            return func(*args, **kwargs)\n\n        return func_wrapper\n\n    return decorator", "idx": 894}
{"project": "Scanpy", "commit_id": "907_scanpy_1.9.0___init__.py__one_of_ours.py", "target": 0, "func": "def _one_of_ours(obj, root: str):\n    return (\n        hasattr(obj, \"__name__\")\n        and not obj.__name__.split(\".\")[-1].startswith(\"_\")\n        and getattr(\n            obj, '__module__', getattr(obj, '__qualname__', obj.__name__)\n        ).startswith(root)", "idx": 895}
{"project": "Scanpy", "commit_id": "908_scanpy_1.9.0___init__.py_descend_classes_and_funcs.py", "target": 0, "func": "def descend_classes_and_funcs(mod: ModuleType, root: str, encountered=None):\n    if encountered is None:\n        encountered = WeakSet()\n    for obj in vars(mod).values():\n        if not _one_of_ours(obj, root):\n            continue\n        if callable(obj) and not isinstance(obj, MethodType):\n            yield obj\n            if isinstance(obj, type):\n                for m in vars(obj).values():\n                    if callable(m) and _one_of_ours(m, root):\n                        yield m\n        elif isinstance(obj, ModuleType) and obj not in encountered:\n            if obj.__name__.startswith('scanpy.tests'):\n                # Python\u2019s import mechanism seems to add this to `scanpy`\u2019s attributes\n                continue\n            encountered.add(obj)\n            yield from descend_classes_and_funcs(obj, root, encountered)", "idx": 896}
{"project": "Scanpy", "commit_id": "909_scanpy_1.9.0___init__.py_annotate_doc_types.py", "target": 0, "func": "def annotate_doc_types(mod: ModuleType, root: str):\n    for c_or_f in descend_classes_and_funcs(mod, root):\n        c_or_f.getdoc = partial(getdoc, c_or_f)", "idx": 897}
{"project": "Scanpy", "commit_id": "90_scanpy_1.9.0__settings.py_autoshow.py", "target": 0, "func": "def autoshow(self, autoshow: bool):\n        _type_check(autoshow, \"autoshow\", bool)\n        self._autoshow = autoshow", "idx": 898}
{"project": "Scanpy", "commit_id": "910_scanpy_1.9.0___init__.py__doc_params.py", "target": 0, "func": "def _doc_params(**kwds):\n    \"\"\"\\\n    Docstrings should start with \"\\\" in the first line for proper formatting.\n    \"\"\"\n\n    def dec(obj):\n        obj.__orig_doc__ = obj.__doc__\n        obj.__doc__ = dedent(obj.__doc__).format_map(kwds)\n        return obj\n\n    return dec", "idx": 899}
{"project": "Scanpy", "commit_id": "911_scanpy_1.9.0___init__.py__check_array_function_arguments.py", "target": 0, "func": "def _check_array_function_arguments(**kwargs):\n    \"\"\"Checks for invalid arguments when an array is passed.\n\n    Helper for functions that work on either AnnData objects or array-likes.\n    \"\"\"\n    # TODO: Figure out a better solution for documenting dispatched functions\n    invalid_args = [k for k, v in kwargs.items() if v is not None]\n    if len(invalid_args) > 0:\n        raise TypeError(\n            f\"Arguments {invalid_args} are only valid if an AnnData object is passed.\"", "idx": 900}
{"project": "Scanpy", "commit_id": "912_scanpy_1.9.0___init__.py__check_use_raw.py", "target": 0, "func": "def _check_use_raw(adata: AnnData, use_raw: Union[None, bool]) -> bool:\n    \"\"\"\n    Normalize checking `use_raw`.\n\n    My intentention here is to also provide a single place to throw a deprecation warning from in future.\n    \"\"\"\n    if use_raw is not None:\n        return use_raw\n    else:\n        if adata.raw is not None:\n            return True\n        else:\n            return False", "idx": 901}
{"project": "Scanpy", "commit_id": "913_scanpy_1.9.0___init__.py_get_igraph_from_adjacency.py", "target": 0, "func": "def get_igraph_from_adjacency(adjacency, directed=None):\n    \"\"\"Get igraph graph from adjacency matrix.\"\"\"\n    import igraph as ig\n\n    sources, targets = adjacency.nonzero()\n    weights = adjacency[sources, targets]\n    if isinstance(weights, np.matrix):\n        weights = weights.A1\n    g = ig.Graph(directed=directed)\n    g.add_vertices(adjacency.shape[0])  # this adds adjacency.shape[0] vertices\n    g.add_edges(list(zip(sources, targets)))\n    try:\n        g.es['weight'] = weights\n    except KeyError:\n        pass\n    if g.vcount() != adjacency.shape[0]:\n        logg.warning(\n            f'The constructed graph has only {g.vcount()} nodes. '\n            'Your adjacency matrix contained redundant nodes.'\n        )\n    return g", "idx": 902}
{"project": "Scanpy", "commit_id": "914_scanpy_1.9.0___init__.py_get_sparse_from_igraph.py", "target": 0, "func": "def get_sparse_from_igraph(graph, weight_attr=None):\n    from scipy.sparse import csr_matrix\n\n    edges = graph.get_edgelist()\n    if weight_attr is None:\n        weights = [1] * len(edges)\n    else:\n        weights = graph.es[weight_attr]\n    if not graph.is_directed():\n        edges.extend([(v, u) for u, v in edges])\n        weights.extend(weights)\n    shape = graph.vcount()\n    shape = (shape, shape)\n    if len(edges) > 0:\n        return csr_matrix((weights, zip(*edges)), shape=shape)\n    else:\n        return csr_matrix(shape)", "idx": 903}
{"project": "Scanpy", "commit_id": "915_scanpy_1.9.0___init__.py_compute_association_matrix_of_groups.py", "target": 0, "func": "def compute_association_matrix_of_groups(\n    adata: AnnData,\n    prediction: str,\n    reference: str,\n    normalization: Literal['prediction', 'reference'] = 'prediction',\n    threshold: float = 0.01,\n    max_n_names: Optional[int] = 2,\n):\n    \"\"\"Compute overlaps between groups.\n\n    See ``identify_groups`` for identifying the groups.\n\n    Parameters\n    ----------\n    adata\n    prediction\n        Field name of adata.obs.\n    reference\n        Field name of adata.obs.\n    normalization\n        Whether to normalize with respect to the predicted groups or the\n        reference groups.\n    threshold\n        Do not consider associations whose overlap is below this fraction.\n    max_n_names\n        Control how many reference names you want to be associated with per\n        predicted name. Set to `None`, if you want all.\n\n    Returns\n    -------\n    asso_names\n        List of associated reference names\n        (`max_n_names` for each predicted name).\n    asso_matrix\n        Matrix where rows correspond to the predicted labels and columns to the\n        reference labels, entries are proportional to degree of association.\n    \"\"\"\n    if normalization not in {'prediction', 'reference'}:\n        raise ValueError(\n            '`normalization` needs to be either \"prediction\" or \"reference\".'\n        )\n    sanitize_anndata(adata)\n    cats = adata.obs[reference].cat.categories\n    for cat in cats:\n        if cat in settings.categories_to_ignore:\n            logg.info(\n                f'Ignoring category {cat!r} '\n                'as it\u2019s in `settings.categories_to_ignore`.'\n            )\n    asso_names = []\n    asso_matrix = []\n    for ipred_group, pred_group in enumerate(adata.obs[prediction].cat.categories):\n        if '?' in pred_group:\n            pred_group = str(ipred_group)\n        # starting from numpy version 1.13, subtractions of boolean arrays are deprecated\n        mask_pred = adata.obs[prediction].values == pred_group\n        mask_pred_int = mask_pred.astype(np.int8)\n        asso_matrix += [[]]\n        for ref_group in adata.obs[reference].cat.categories:\n            mask_ref = (adata.obs[reference].values == ref_group).astype(np.int8)\n            mask_ref_or_pred = mask_ref.copy()\n            mask_ref_or_pred[mask_pred] = 1\n            # e.g. if the pred group is contained in mask_ref, mask_ref and\n            # mask_ref_or_pred are the same\n            if normalization == 'prediction':\n                # compute which fraction of the predicted group is contained in\n                # the ref group\n                ratio_contained = (\n                    np.sum(mask_pred_int) - np.sum(mask_ref_or_pred - mask_ref)\n                ) / np.sum(mask_pred_int)\n            else:\n                # compute which fraction of the reference group is contained in\n                # the predicted group\n                ratio_contained = (\n                    np.sum(mask_ref) - np.sum(mask_ref_or_pred - mask_pred_int)\n                ) / np.sum(mask_ref)\n            asso_matrix[-1] += [ratio_contained]\n        name_list_pred = [\n            cats[i] if cats[i] not in settings.categories_to_ignore else ''\n            for i in np.argsort(asso_matrix[-1])[::-1]\n            if asso_matrix[-1][i] > threshold\n        ]\n        asso_names += ['\\n'.join(name_list_pred[:max_n_names])]\n    Result = namedtuple(\n        'compute_association_matrix_of_groups', ['asso_names', 'asso_matrix']\n    )\n    return Result(asso_names=asso_names, asso_matrix=np.array(asso_matrix))", "idx": 904}
{"project": "Scanpy", "commit_id": "916_scanpy_1.9.0___init__.py_get_associated_colors_of_groups.py", "target": 0, "func": "def get_associated_colors_of_groups(reference_colors, asso_matrix):\n    return [\n        {\n            reference_colors[i_ref]: asso_matrix[i_pred, i_ref]\n            for i_ref in range(asso_matrix.shape[1])\n        }\n        for i_pred in range(asso_matrix.shape[0])", "idx": 905}
{"project": "Scanpy", "commit_id": "917_scanpy_1.9.0___init__.py_identify_groups.py", "target": 0, "func": "def identify_groups(ref_labels, pred_labels, return_overlaps=False):\n    \"\"\"Which predicted label explains which reference label?\n\n    A predicted label explains the reference label which maximizes the minimum\n    of ``relative_overlaps_pred`` and ``relative_overlaps_ref``.\n\n    Compare this with ``compute_association_matrix_of_groups``.\n\n    Returns\n    -------\n    A dictionary of length ``len(np.unique(ref_labels))`` that stores for each\n    reference label the predicted label that best explains it.\n\n    If ``return_overlaps`` is ``True``, this will in addition return the overlap\n    of the reference group with the predicted group; normalized with respect to\n    the reference group size and the predicted group size, respectively.\n    \"\"\"\n    ref_unique, ref_counts = np.unique(ref_labels, return_counts=True)\n    ref_dict = dict(zip(ref_unique, ref_counts))\n    pred_unique, pred_counts = np.unique(pred_labels, return_counts=True)\n    pred_dict = dict(zip(pred_unique, pred_counts))\n    associated_predictions = {}\n    associated_overlaps = {}\n    for ref_label in ref_unique:\n        sub_pred_unique, sub_pred_counts = np.unique(\n            pred_labels[ref_label == ref_labels], return_counts=True\n        )\n        relative_overlaps_pred = [\n            sub_pred_counts[i] / pred_dict[n] for i, n in enumerate(sub_pred_unique)\n        ]\n        relative_overlaps_ref = [\n            sub_pred_counts[i] / ref_dict[ref_label]\n            for i, n in enumerate(sub_pred_unique)\n        ]\n        relative_overlaps = np.c_[relative_overlaps_pred, relative_overlaps_ref]\n        relative_overlaps_min = np.min(relative_overlaps, axis=1)\n        pred_best_index = np.argsort(relative_overlaps_min)[::-1]\n        associated_predictions[ref_label] = sub_pred_unique[pred_best_index]\n        associated_overlaps[ref_label] = relative_overlaps[pred_best_index]\n    if return_overlaps:\n        return associated_predictions, associated_overlaps\n    else:\n        return associated_predictions", "idx": 906}
{"project": "Scanpy", "commit_id": "918_scanpy_1.9.0___init__.py_sanitize_anndata.py", "target": 0, "func": "def sanitize_anndata(adata):\n    \"\"\"Transform string annotations to categoricals.\"\"\"\n    adata._sanitize()", "idx": 907}
{"project": "Scanpy", "commit_id": "919_scanpy_1.9.0___init__.py_view_to_actual.py", "target": 0, "func": "def view_to_actual(adata):\n    if adata.is_view:\n        warnings.warn(\n            \"Received a view of an AnnData. Making a copy.\",\n            stacklevel=2,\n        )\n        adata._init_as_actual(adata.copy())", "idx": 908}
{"project": "Scanpy", "commit_id": "91_scanpy_1.9.0__settings.py_writedir.py", "target": 0, "func": "def writedir(self, writedir: Union[str, Path]):\n        _type_check(writedir, \"writedir\", (str, Path))\n        self._writedir = Path(writedir)", "idx": 909}
{"project": "Scanpy", "commit_id": "920_scanpy_1.9.0___init__.py_moving_average.py", "target": 0, "func": "def moving_average(a: np.ndarray, n: int):\n    \"\"\"Moving average over one-dimensional array.\n\n    Parameters\n    ----------\n    a\n        One-dimensional array.\n    n\n        Number of entries to average over. n=2 means averaging over the currrent\n        the previous entry.\n\n    Returns\n    -------\n    An array view storing the moving average.\n    \"\"\"\n    ret = np.cumsum(a, dtype=float)\n    ret[n:] = ret[n:] - ret[:-n]\n    return ret[n - 1 :] / n", "idx": 910}
{"project": "Scanpy", "commit_id": "921_scanpy_1.9.0___init__.py_update_params.py", "target": 0, "func": "def update_params(\n    old_params: Mapping[str, Any],\n    new_params: Mapping[str, Any],\n    check=False,\n) -> Dict[str, Any]:\n    \"\"\"\\\n    Update old_params with new_params.\n\n    If check==False, this merely adds and overwrites the content of old_params.\n\n    If check==True, this only allows updating of parameters that are already\n    present in old_params.\n\n    Parameters\n    ----------\n    old_params\n    new_params\n    check\n\n    Returns\n    -------\n    updated_params\n    \"\"\"\n    updated_params = dict(old_params)\n    if new_params:  # allow for new_params to be None\n        for key, val in new_params.items():\n            if key not in old_params and check:\n                raise ValueError(\n                    '\\''\n                    + key\n                    + '\\' is not a valid parameter key, '\n                    + 'consider one of \\n'\n                    + str(list(old_params.keys()))\n                )\n            if val is not None:\n                updated_params[key] = val\n    return updated_params", "idx": 911}
{"project": "Scanpy", "commit_id": "922_scanpy_1.9.0___init__.py_check_nonnegative_integers.py", "target": 1, "func": "def check_nonnegative_integers(X: Union[np.ndarray, sparse.spmatrix]):\n    \"\"\"Checks values of X to ensure it is count data\"\"\"\n    from numbers import Integral\n\n    data = X if isinstance(X, np.ndarray) else X.data\n    # Check no negatives\n    if np.signbit(data).any():\n        return False\n    # Check all are integers\n    elif issubclass(data.dtype.type, Integral):\n        return True\n    elif np.any(~np.equal(np.mod(data, 1), 0)):\n        return False\n    else:\n        return True", "idx": 912}
{"project": "Scanpy", "commit_id": "923_scanpy_1.9.0___init__.py_select_groups.py", "target": 0, "func": "def select_groups(adata, groups_order_subset='all', key='groups'):\n    \"\"\"Get subset of groups in adata.obs[key].\"\"\"\n    groups_order = adata.obs[key].cat.categories\n    if key + '_masks' in adata.uns:\n        groups_masks = adata.uns[key + '_masks']\n    else:\n        groups_masks = np.zeros(\n            (len(adata.obs[key].cat.categories), adata.obs[key].values.size), dtype=bool\n        )\n        for iname, name in enumerate(adata.obs[key].cat.categories):\n            # if the name is not found, fallback to index retrieval\n            if adata.obs[key].cat.categories[iname] in adata.obs[key].values:\n                mask = adata.obs[key].cat.categories[iname] == adata.obs[key].values\n            else:\n                mask = str(iname) == adata.obs[key].values\n            groups_masks[iname] = mask\n    groups_ids = list(range(len(groups_order)))\n    if groups_order_subset != 'all':\n        groups_ids = []\n        for name in groups_order_subset:\n            groups_ids.append(\n                np.where(adata.obs[key].cat.categories.values == name)[0][0]\n            )\n        if len(groups_ids) == 0:\n            # fallback to index retrieval\n            groups_ids = np.where(\n                np.in1d(\n                    np.arange(len(adata.obs[key].cat.categories)).astype(str),\n                    np.array(groups_order_subset),\n                )\n            )[0]\n        if len(groups_ids) == 0:\n            logg.debug(\n                f'{np.array(groups_order_subset)} invalid! specify valid '\n                f'groups_order (or indices) from {adata.obs[key].cat.categories}',\n            )\n            from sys import exit\n\n            exit(0)\n        groups_masks = groups_masks[groups_ids]\n        groups_order_subset = adata.obs[key].cat.categories[groups_ids].values\n    else:\n        groups_order_subset = groups_order.values\n    return groups_order_subset, groups_masks", "idx": 913}
{"project": "Scanpy", "commit_id": "924_scanpy_1.9.0___init__.py_warn_with_traceback.py", "target": 0, "func": "def warn_with_traceback(message, category, filename, lineno, file=None, line=None):\n    \"\"\"Get full tracebacks when warning is raised by setting\n\n    warnings.showwarning = warn_with_traceback\n\n    See also\n    --------\n    http://stackoverflow.com/questions/22373927/get-traceback-of-warnings\n    \"\"\"\n    import traceback\n\n    traceback.print_stack()\n    log = (  # noqa: F841  # TODO Does this need fixing?\n        file if hasattr(file, 'write') else sys.stderr\n    )\n    settings.write(warnings.formatwarning(message, category, filename, lineno, line))", "idx": 914}
{"project": "Scanpy", "commit_id": "925_scanpy_1.9.0___init__.py_subsample.py", "target": 0, "func": "def subsample(\n    X: np.ndarray,\n    subsample: int = 1,\n    seed: int = 0,\n) -> Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\\\n    Subsample a fraction of 1/subsample samples from the rows of X.\n\n    Parameters\n    ----------\n    X\n        Data array.\n    subsample\n        1/subsample is the fraction of data sampled, n = X.shape[0]/subsample.\n    seed\n        Seed for sampling.\n\n    Returns\n    -------\n    Xsampled\n        Subsampled X.\n    rows\n        Indices of rows that are stored in Xsampled.\n    \"\"\"\n    if subsample == 1 and seed == 0:\n        return X, np.arange(X.shape[0], dtype=int)\n    if seed == 0:\n        # this sequence is defined simply by skipping rows\n        # is faster than sampling\n        rows = np.arange(0, X.shape[0], subsample, dtype=int)\n        n = rows.size\n        Xsampled = np.array(X[rows])\n    else:\n        if seed < 0:\n            raise ValueError(f'Invalid seed value < 0: {seed}')\n        n = int(X.shape[0] / subsample)\n        np.random.seed(seed)\n        Xsampled, rows = subsample_n(X, n=n)\n    logg.debug(f'... subsampled to {n} of {X.shape[0]} data points')\n    return Xsampled, rows", "idx": 915}
{"project": "Scanpy", "commit_id": "926_scanpy_1.9.0___init__.py_subsample_n.py", "target": 0, "func": "def subsample_n(\n    X: np.ndarray, n: int = 0, seed: int = 0\n) -> Tuple[np.ndarray, np.ndarray]:\n    \"\"\"Subsample n samples from rows of array.\n\n    Parameters\n    ----------\n    X\n        Data array.\n    n\n        Sample size.\n    seed\n        Seed for sampling.\n\n    Returns\n    -------\n    Xsampled\n        Subsampled X.\n    rows\n        Indices of rows that are stored in Xsampled.\n    \"\"\"\n    if n < 0:\n        raise ValueError('n must be greater 0')\n    np.random.seed(seed)\n    n = X.shape[0] if (n == 0 or n > X.shape[0]) else n\n    rows = np.random.choice(X.shape[0], size=n, replace=False)\n    Xsampled = X[rows]\n    return Xsampled, rows", "idx": 916}
{"project": "Scanpy", "commit_id": "927_scanpy_1.9.0___init__.py_check_presence_download.py", "target": 0, "func": "def check_presence_download(filename: Path, backup_url):\n    \"\"\"Check if file is present otherwise download.\"\"\"\n    if not filename.is_file():\n        from ..readwrite import _download\n\n        _download(backup_url, filename)", "idx": 917}
{"project": "Scanpy", "commit_id": "928_scanpy_1.9.0___init__.py_lazy_import.py", "target": 0, "func": "def lazy_import(full_name):\n    \"\"\"Imports a module in a way that it\u2019s only executed on member access\"\"\"\n    try:\n        return sys.modules[full_name]\n    except KeyError:\n        spec = importlib.util.find_spec(full_name)\n        module = importlib.util.module_from_spec(spec)\n        loader = importlib.util.LazyLoader(spec.loader)\n        # Make module with proper locking and get it inserted into sys.modules.\n        loader.exec_module(module)\n        return module", "idx": 918}
{"project": "Scanpy", "commit_id": "929_scanpy_1.9.0___init__.py__fallback_to_uns.py", "target": 0, "func": "def _fallback_to_uns(dct, conns, dists, conns_key, dists_key):\n    if conns is None and conns_key in dct:\n        conns = dct[conns_key]\n    if dists is None and dists_key in dct:\n        dists = dct[dists_key]\n\n    return conns, dists", "idx": 919}
{"project": "Scanpy", "commit_id": "92_scanpy_1.9.0__settings.py_cachedir.py", "target": 0, "func": "def cachedir(self, cachedir: Union[str, Path]):\n        _type_check(cachedir, \"cachedir\", (str, Path))\n        self._cachedir = Path(cachedir)", "idx": 920}
{"project": "Scanpy", "commit_id": "930_scanpy_1.9.0___init__.py__choose_graph.py", "target": 0, "func": "def _choose_graph(adata, obsp, neighbors_key):\n    \"\"\"Choose connectivities from neighbbors or another obsp column\"\"\"\n    if obsp is not None and neighbors_key is not None:\n        raise ValueError(\n            'You can\\'t specify both obsp, neighbors_key. ' 'Please select only one.'\n        )\n\n    if obsp is not None:\n        return adata.obsp[obsp]\n    else:\n        neighbors = NeighborsView(adata, neighbors_key)\n        if 'connectivities' not in neighbors:\n            raise ValueError(\n                'You need to run `pp.neighbors` first '\n                'to compute a neighborhood graph.'\n            )\n        return neighbors['connectivities']", "idx": 921}
{"project": "Scanpy", "commit_id": "931_scanpy_1.9.0___init__.py_type_doc.py", "target": 0, "func": "def type_doc(name: str):\n        param: inspect.Parameter = sig.parameters[name]\n        cls = getattr(param.annotation, '__qualname__', repr(param.annotation))\n        if param.default is not param.empty:\n            return f'{cls}, optional (default: {param.default!r})'\n        else:\n            return cls", "idx": 922}
{"project": "Scanpy", "commit_id": "932_scanpy_1.9.0___init__.py_decorator.py", "target": 0, "func": "def decorator(func):\n        @wraps(func)\n        def func_wrapper(*args, **kwargs):\n            warnings.simplefilter('always', DeprecationWarning)  # turn off filter\n            for old, new in arg_mapping.items():\n                if old in kwargs:\n                    warnings.warn(\n                        f\"Keyword argument '{old}' has been \"\n                        f\"deprecated in favour of '{new}'. \"\n                        f\"'{old}' will be removed in a future version.\",\n                        category=DeprecationWarning,\n                        stacklevel=2,\n                    )\n                    val = kwargs.pop(old)\n                    kwargs[new] = val\n            # reset filter\n            warnings.simplefilter('default', DeprecationWarning)\n            return func(*args, **kwargs)\n\n        return func_wrapper", "idx": 923}
{"project": "Scanpy", "commit_id": "933_scanpy_1.9.0___init__.py_dec.py", "target": 0, "func": "def dec(obj):\n        obj.__orig_doc__ = obj.__doc__\n        obj.__doc__ = dedent(obj.__doc__).format_map(kwds)\n        return obj", "idx": 924}
{"project": "Scanpy", "commit_id": "934_scanpy_1.9.0___init__.py___init__.py", "target": 0, "func": "def __init__(self, adata, key=None):\n        self._connectivities = None\n        self._distances = None\n\n        if key is None or key == 'neighbors':\n            if 'neighbors' not in adata.uns:\n                raise KeyError('No \"neighbors\" in .uns')\n            self._neighbors_dict = adata.uns['neighbors']\n            self._conns_key = 'connectivities'\n            self._dists_key = 'distances'\n        else:\n            if key not in adata.uns:\n                raise KeyError(f'No \"{key}\" in .uns')\n            self._neighbors_dict = adata.uns[key]\n            self._conns_key = self._neighbors_dict['connectivities_key']\n            self._dists_key = self._neighbors_dict['distances_key']\n\n        if self._conns_key in adata.obsp:\n            self._connectivities = adata.obsp[self._conns_key]\n        if self._dists_key in adata.obsp:\n            self._distances = adata.obsp[self._dists_key]\n\n        # fallback to uns\n        self._connectivities, self._distances = _fallback_to_uns(\n            self._neighbors_dict,\n            self._connectivities,\n            self._distances,\n            self._conns_key,\n            self._dists_key,", "idx": 925}
{"project": "Scanpy", "commit_id": "935_scanpy_1.9.0___init__.py___getitem__.py", "target": 0, "func": "def __getitem__(self, key):\n        if key == 'distances':\n            if 'distances' not in self:\n                raise KeyError(f'No \"{self._dists_key}\" in .obsp')\n            return self._distances\n        elif key == 'connectivities':\n            if 'connectivities' not in self:\n                raise KeyError(f'No \"{self._conns_key}\" in .obsp')\n            return self._connectivities\n        else:\n            return self._neighbors_dict[key]", "idx": 926}
{"project": "Scanpy", "commit_id": "936_scanpy_1.9.0___init__.py___contains__.py", "target": 0, "func": "def __contains__(self, key):\n        if key == 'distances':\n            return self._distances is not None\n        elif key == 'connectivities':\n            return self._connectivities is not None\n        else:\n            return key in self._neighbors_dict", "idx": 927}
{"project": "Scanpy", "commit_id": "937_scanpy_1.9.0___init__.py_func_wrapper.py", "target": 0, "func": "def func_wrapper(*args, **kwargs):\n            warnings.simplefilter('always', DeprecationWarning)  # turn off filter\n            for old, new in arg_mapping.items():\n                if old in kwargs:\n                    warnings.warn(\n                        f\"Keyword argument '{old}' has been \"\n                        f\"deprecated in favour of '{new}'. \"\n                        f\"'{old}' will be removed in a future version.\",\n                        category=DeprecationWarning,\n                        stacklevel=2,\n                    )\n                    val = kwargs.pop(old)\n                    kwargs[new] = val\n            # reset filter\n            warnings.simplefilter('default', DeprecationWarning)\n            return func(*args, **kwargs)", "idx": 928}
{"project": "Scanpy", "commit_id": "938_scanpy_1.9.0_is_constant.py_is_constant.py", "target": 0, "func": "def is_constant(a, axis=None) -> np.ndarray:\n    \"\"\"\n    Check whether values in array are constant.\n\n    Params\n    ------\n    a\n        Array to check\n    axis\n        Axis to reduce over.\n\n\n    Returns\n    -------\n    Boolean array, True values were constant.\n\n    Example\n    -------\n\n    >>> a = np.array([[0, 1], [0, 0]])\n    >>> a\n    array([[0, 1],\n            [0, 0]])\n    >>> is_constant(a)\n    False\n    >>> is_constant(a, axis=0)\n    array([ False, True])\n    >>> is_constant(a, axis=1)\n    array([ True, False])\n    \"\"\"\n    raise NotImplementedError()", "idx": 929}
{"project": "Scanpy", "commit_id": "939_scanpy_1.9.0_is_constant.py__.py", "target": 1, "func": "def _(a, axis=None):\n    if axis is None:\n        if len(a.data) == np.multiply(*a.shape):\n            return is_constant(a.data)\n        else:\n            return (a.data == 0).all()\n    if not isinstance(axis, Integral):\n        raise TypeError(\"axis must be integer or None.\")\n    assert axis in (0, 1)\n    if axis == 0:\n        return _is_constant_csr_rows(a.data, a.indices, a.indptr, a.shape[::-1])\n    elif axis == 1:\n        a = a.T.tocsc()\n        return _is_constant_csr_rows(a.data, a.indices, a.indptr, a.shape[::-1])", "idx": 930}
{"project": "Scanpy", "commit_id": "93_scanpy_1.9.0__settings.py_datasetdir.py", "target": 0, "func": "def datasetdir(self, datasetdir: Union[str, Path]):\n        _type_check(datasetdir, \"datasetdir\", (str, Path))\n        self._datasetdir = Path(datasetdir).resolve()", "idx": 931}
{"project": "Scanpy", "commit_id": "940_scanpy_1.9.0_is_constant.py__is_constant_rows.py", "target": 0, "func": "def _is_constant_rows(a):\n    b = np.broadcast_to(a[:, 0][:, np.newaxis], a.shape)\n    return (a == b).all(axis=1)", "idx": 932}
{"project": "Scanpy", "commit_id": "941_scanpy_1.9.0_is_constant.py__is_constant_csr_rows.py", "target": 0, "func": "def _is_constant_csr_rows(data, indices, indptr, shape):\n    N = len(indptr) - 1\n    result = np.ones(N, dtype=np.bool_)\n    for i in range(N):\n        start = indptr[i]\n        stop = indptr[i + 1]\n        if stop - start == shape[1]:\n            val = data[start]\n        else:\n            val = 0\n        for j in range(start, stop):\n            if data[j] != val:\n                result[i] = False\n                break\n    return result", "idx": 933}
{"project": "Scanpy", "commit_id": "94_scanpy_1.9.0__settings.py_figdir.py", "target": 0, "func": "def figdir(self, figdir: Union[str, Path]):\n        _type_check(figdir, \"figdir\", (str, Path))\n        self._figdir = Path(figdir)", "idx": 934}
{"project": "Scanpy", "commit_id": "95_scanpy_1.9.0__settings.py_cache_compression.py", "target": 0, "func": "def cache_compression(self, cache_compression: Optional[str]):\n        if cache_compression not in {'lzf', 'gzip', None}:\n            raise ValueError(\n                f\"`cache_compression` ({cache_compression}) \"\n                \"must be in {'lzf', 'gzip', None}\"\n            )\n        self._cache_compression = cache_compression", "idx": 935}
{"project": "Scanpy", "commit_id": "96_scanpy_1.9.0__settings.py_max_memory.py", "target": 0, "func": "def max_memory(self, max_memory: Union[int, float]):\n        _type_check(max_memory, \"max_memory\", (int, float))\n        self._max_memory = max_memory", "idx": 936}
{"project": "Scanpy", "commit_id": "97_scanpy_1.9.0__settings.py_n_jobs.py", "target": 0, "func": "def n_jobs(self, n_jobs: int):\n        _type_check(n_jobs, \"n_jobs\", int)\n        self._n_jobs = n_jobs", "idx": 937}
{"project": "Scanpy", "commit_id": "98_scanpy_1.9.0__settings.py_logpath.py", "target": 0, "func": "def logpath(self, logpath: Union[str, Path, None]):\n        _type_check(logpath, \"logfile\", (str, Path))\n        # set via \u201cfile object\u201d branch of logfile.setter\n        self.logfile = Path(logpath).open('a')\n        self._logpath = Path(logpath)", "idx": 938}
{"project": "Scanpy", "commit_id": "99_scanpy_1.9.0__settings.py_logfile.py", "target": 0, "func": "def logfile(self, logfile: Union[str, Path, TextIO, None]):\n        if not hasattr(logfile, 'write') and logfile:\n            self.logpath = logfile\n        else:  # file object\n            if not logfile:  # None or ''\n                logfile = sys.stdout if self._is_run_from_ipython() else sys.stderr\n            self._logfile = logfile\n            self._logpath = None\n            _set_log_file(self)", "idx": 939}
{"project": "Scanpy", "commit_id": "9_scanpy_1.9.0_debug_docstrings.py_setup.py", "target": 0, "func": "def setup(app: Sphinx):\n    if os.environ.get('DEBUG') is not None:\n        sphinx.ext.napoleon._process_docstring = pd_new", "idx": 940}
