{"project": "Scanpy", "commit_id": "476_scanpy_1.9.0_test_binary.py_test_error_wrong_command.py", "target": 0, "func": "def test_error_wrong_command(capsys: CaptureFixture):\n    with pytest.raises(SystemExit, match='^2$'):\n        main(['idonotexist--'])\n    captured = capsys.readouterr()\n    assert 'No command \u201cidonotexist--\u201d. Choose from' in captured.err", "idx": 13}
{"project": "Scanpy", "commit_id": "416_scanpy_0.2.8_scanpy_plotting_tools.py__aga_graph.py", "target": 1, "func": "def _aga_graph(\n        adata,\n        ax,\n        solid_edges=None,\n        dashed_edges=None,\n        root=0,\n        rootlevel=None,\n        color=None,\n        groups=None,\n        fontsize=None,\n        node_size_scale=1,\n        node_size_power=0.5,\n        edge_width_scale=1,\n        title=None,\n        layout=None,\n        pos=None,\n        cmap=None,\n        frameon=True,\n        min_edge_width=None,\n        max_edge_width=None,\n        random_state=0):\n    if groups is not None and isinstance(groups, str) and groups not in adata.smp_keys():\n        raise ValueError('Groups {} are not in adata.smp.'.format(groups))\n    groups_name = groups if isinstance(groups, str) else None\n    if groups is None and 'aga_groups_order_original' in adata.add:\n        groups = adata.add['aga_groups_order_original']\n        groups_name = adata.add['aga_groups_original']\n    elif groups in adata.smp_keys():\n        groups = adata.add[groups + '_order']\n    elif groups is None:\n        groups = adata.add['aga_groups_order']\n        groups_name = 'aga_groups'\n    if color is None and groups_name is not None:\n        if 'aga_groups_original' in adata.add and groups_name == adata.add['aga_groups_original']:\n            color = adata.add['aga_groups_colors_original']\n        else:\n            if (groups_name + '_colors' not in adata.add\n                or len(adata.add[groups_name + '_order'])\n                   != len(adata.add[groups_name + '_colors'])):\n                utils.add_colors_for_categorical_sample_annotation(adata, groups_name)\n            color = adata.add[groups_name + '_colors']\n        for iname, name in enumerate(adata.add[groups_name + '_order']):\n            if name in settings._ignore_categories: color[iname] = 'grey'\n    if isinstance(root, str) and root in groups:\n        root = list(groups).index(root)\n    if isinstance(root, list) and root[0] in groups:\n        root = [list(groups).index(r) for r in root]\n    # define the objects\n    adjacency_solid = adata.add[solid_edges]\n    nx_g_solid = nx.Graph(adjacency_solid)\n    if dashed_edges is not None:\n        adjacency_dashed = adata.add[dashed_edges]\n        nx_g_dashed = nx.Graph(adjacency_dashed)\n    # degree of the graph for coloring\n    if isinstance(color, str) and color.startswith('degree'):\n        # see also tools.aga.aga_degrees\n        if color == 'degree_dashed':\n            color = [d for _, d in nx_g_dashed.degree_iter(weight='weight')]\n        elif color == 'degree_solid':\n            color = [d for _, d in nx_g_solid.degree_iter(weight='weight')]\n        else:\n            raise ValueError('`degree` either \"degree_dashed\" or \"degree_solid\".')\n        color = (np.array(color) - np.min(color)) / (np.max(color) - np.min(color))\n    # plot numeric colors\n    colorbar = False\n    if isinstance(color, (list, np.ndarray)) and not isinstance(color[0], (str, dict)):\n        import matplotlib\n        norm = matplotlib.colors.Normalize()\n        color = norm(color)\n        if cmap is None: cmap = rcParams['image.cmap']\n        cmap = matplotlib.cm.get_cmap(cmap)\n        color = [cmap(c) for c in color]\n        colorbar = True\n    if len(color) < len(groups):\n        print(groups, color)\n        raise ValueError('`color` list need to be at least as long as `groups` list.')\n    # node positions from adjacency_solid\n    if pos is None:\n        if layout is None:\n            layout = 'fr'\n        # igraph layouts\n        if layout != 'eq_tree':\n            from .. import utils as sc_utils\n            g = sc_utils.get_igraph_from_adjacency(adjacency_solid)\n            if 'rt' in layout:\n                pos_list = g.layout(layout, root=root if isinstance(root, list) else [root],\n                                    rootlevel=rootlevel).coords\n            elif layout == 'circle':\n                pos_list = g.layout(layout).coords\n            else:\n                np.random.seed(random_state)\n                init_coords = np.random.random((adjacency_solid.shape[0], 2)).tolist()\n                pos_list = g.layout(layout, seed=init_coords).coords\n            pos = {n: [p[0], -p[1]] for n, p in enumerate(pos_list)}\n        # equally-spaced tree\n        else:\n            pos = utils.hierarchy_pos(nx_g_solid, root)\n            if len(pos) < adjacency_solid.shape[0]:\n                raise ValueError('This is a forest and not a single tree. '\n                                 'Try another `layout`, e.g., {\\'fr\\'}.')\n        pos_array = np.array([pos[n] for count, n in enumerate(nx_g_solid)])\n    else:\n        pos_array = pos\n        # convert to dictionary\n        pos = {n: [p[0], p[1]] for n, p in enumerate(pos)}\n    if len(pos) == 1: pos[0] = (0.5, 0.5)\n    # edge widths\n    base_edge_width = edge_width_scale * rcParams['lines.linewidth']\n    # draw dashed edges\n    if dashed_edges is not None:\n        widths = [x[-1]['weight'] for x in nx_g_dashed.edges(data=True)]\n        widths = base_edge_width * np.array(widths)\n        if max_edge_width is not None:\n            widths = np.clip(widths, None, max_edge_width)\n        nx.draw_networkx_edges(nx_g_dashed, pos, ax=ax, width=widths, edge_color='grey',\n                               style='dashed', alpha=0.5)\n    # draw solid edges\n    widths = [x[-1]['weight'] for x in nx_g_solid.edges(data=True)]\n    widths = base_edge_width * np.array(widths)\n    if min_edge_width is not None or max_edge_width is not None:\n        widths = np.clip(widths, min_edge_width, max_edge_width)\n    nx.draw_networkx_edges(nx_g_solid, pos, ax=ax, width=widths, edge_color='black')\n\n    # deal with empty graph\n    ax.plot(pos_array[:, 0], pos_array[:, 1], '.', c='white')\n\n    # draw the nodes (pie charts)\n    trans = ax.transData.transform\n    bbox = ax.get_position().get_points()\n    ax_x_min = bbox[0, 0]\n    ax_x_max = bbox[1, 0]\n    ax_y_min = bbox[0, 1]\n    ax_y_max = bbox[1, 1]\n    ax_len_x = ax_x_max - ax_x_min\n    ax_len_y = ax_y_max - ax_y_min\n    # print([ax_x_min, ax_x_max, ax_y_min, ax_y_max])\n    # print([ax_len_x, ax_len_y])\n    trans2 = ax.transAxes.inverted().transform\n    ax.set_frame_on(frameon)\n    ax.set_xticks([])\n    ax.set_yticks([])\n    base_pie_size = 1/(np.sqrt(adjacency_solid.shape[0]) + 10) * node_size_scale\n    if (groups_name is not None and groups_name in adata.add):\n        groups_sizes = adata.add[groups_name + '_sizes']\n    elif 'aga_groups_sizes' in adata.add:\n        groups_sizes = adata.add['aga_groups_sizes']\n    else:\n        groups_sizes = np.ones(len(groups))\n    median_group_size = np.median(groups_sizes)\n    force_labels_to_front = True  # TODO: solve this differently!\n    for count, n in enumerate(nx_g_solid.nodes_iter()):\n        pie_size = base_pie_size\n        pie_size *= np.power(groups_sizes[count] / median_group_size,\n                             node_size_power)\n        xx, yy = trans(pos[n])     # data coordinates\n        xa, ya = trans2((xx, yy))  # axis coordinates\n        xa = ax_x_min + (xa - pie_size/2) * ax_len_x\n        ya = ax_y_min + (ya - pie_size/2) * ax_len_y\n        if ya < 0: ya = 0  # clip, the fruchterman layout sometimes places below figure\n        if xa < 0: xa = 0\n        a = pl.axes([xa, ya, pie_size * ax_len_x, pie_size * ax_len_y])\n        if is_color_like(color[count]):\n            fracs = [100]\n            color_single = [color[count]]\n        elif isinstance(color[count], dict):\n            color_single = color[count].keys()\n            fracs = [color[count][c] for c in color_single]\n            if sum(fracs) < 1:\n                color_single = list(color_single)\n                color_single.append('grey')\n                fracs.append(1-sum(fracs))\n        else:\n            raise ValueError('{} is neither a dict of valid matplotlib colors '\n                             'nor a valid matplotlib color.'.format(color[count]))\n        a.pie(fracs, colors=color_single)\n        if not force_labels_to_front and groups is not None:\n            a.text(0.5, 0.5, groups[count],\n                   verticalalignment='center',\n                   horizontalalignment='center',\n                   transform=a.transAxes,\n                   size=fontsize)\n    # TODO: this is a terrible hack, but if we use the solution above (``not\n    # force_labels_to_front``), labels get hidden behind pies\n    if force_labels_to_front and groups is not None:\n        for count, n in enumerate(nx_g_solid.nodes_iter()):\n            # all copy and paste from above\n            pie_size = base_pie_size\n            pie_size *= np.power(groups_sizes[count] / median_group_size,\n                                 node_size_power)\n            xx, yy = trans(pos[n])     # data coordinates\n            xa, ya = trans2((xx, yy))  # axis coordinates\n            xa = ax_x_min + (xa - pie_size/2.0000001) * ax_len_x  # make sure a new axis is created\n            ya = ax_y_min + (ya - pie_size/2.0000001) * ax_len_y\n            if ya < 0: ya = 0  # clip, the fruchterman layout sometimes places below figure\n            if xa < 0: xa = 0\n            a = pl.axes([xa, ya, pie_size * ax_len_x, pie_size * ax_len_y])\n            a.set_frame_on(False)\n            a.set_xticks([])\n            a.set_yticks([])\n            a.text(0.5, 0.5, groups[count],\n                   verticalalignment='center',\n                   horizontalalignment='center',\n                   transform=a.transAxes, size=fontsize)\n    if title is not None: ax.set_title(title)\n    if colorbar:\n        ax1 = pl.axes([0.95, 0.1, 0.03, 0.7])\n        cb = matplotlib.colorbar.ColorbarBase(ax1, cmap=cmap,\n                                              norm=norm)\n    return pos_array", "idx": 14}
{"project": "Scanpy", "commit_id": "620_scanpy_1.0.4_scanpy_preprocessing_simple.py_pca.py", "target": 1, "func": "def pca(data, n_comps=None, zero_center=True, svd_solver='auto', random_state=0,\n        dtype='float32', copy=False, chunked=False, chunk_size=None):\n    \"\"\"Principal component analysis [Pedregosa11]_.\n    Computes PCA coordinates, loadings and variance decomposition. Uses the\n    implementation of *scikit-learn* [Pedregosa11]_.\n    Parameters\n    ----------\n    data : :class:`~scanpy.api.AnnData`\n        The (annotated) data matrix of shape `n_obs` \u00d7 `n_vars`. Rows correspond\n        to cells and columns to genes.\n    n_comps : `int`, optional (default: 50)\n        Number of principal components to compute.\n    zero_center : `bool` or `None`, optional (default: `True`)\n        If `True`, compute standard PCA from covariance matrix. If `False`, omit\n        zero-centering variables (uses *TruncatedSVD* from scikit-learn), which\n        allows to handle sparse input efficiently.\n    svd_solver : `str`, optional (default: 'auto')\n        SVD solver to use. Either 'arpack' for the ARPACK wrapper in SciPy\n        (scipy.sparse.linalg.svds), or 'randomized' for the randomized algorithm\n        due to Halko (2009). \"auto\" chooses automatically depending on the size\n        of the problem.\n    random_state : `int`, optional (default: 0)\n        Change to use different intial states for the optimization.\n    dtype : `str` (default: 'float32')\n        Numpy data type string to which to convert the result.\n    copy : `bool`, optional (default: `False`)\n        If an :class:`~scanpy.api.AnnData` is passed, determines whether a copy\n        is returned.\n    Returns\n    -------\n    Returns AnnData object if copy=True or adds to `data`:\n    X_pca : `.obsm`\n         PCA representation of data.\n    PCs : `.varm`\n         The principal components containing the loadings.\n    variance_ratio : `.uns['pca']`\n         Ratio of explained variance.\n    variance : `.uns['pca']`\n         Explained variance, equivalent to the eigenvalues of the covariance matrix.\n    \"\"\"\n\n    if n_comps is None: n_comps = N_PCS\n\n    adata = data.copy() if copy else data\n    #adata = AnnData(data) if not isinstance(data, AnnData) else ...\n    logg.msg('computing PCA with n_comps =', n_comps, r=True, v=4)\n\n    if adata.n_vars < n_comps:\n        n_comps = adata.n_vars - 1\n        logg.msg('reducing number of computed PCs to',\n               n_comps, 'as dim of data is only', adata.n_vars, v=4)\n    if chunked:\n        if not zero_center or random_state or svd_solver != 'auto':\n            logg.msg('Ignoring zero_center, random_state, svd_solver', v=4)\n        from sklearn.decomposition import IncrementalPCA\n        X_pca = np.zeros((adata.X.shape[0], n_comps), adata.X.dtype)\n        pca_ = IncrementalPCA(n_components=n_comps)\n        for chunk, _, _ in adata.chunked_X(chunk_size):\n            chunk = chunk.toarray() if issparse(chunk) else chunk\n            pca_.partial_fit(chunk)\n        for chunk, start, end in adata.chunked_X(chunk_size):\n            chunk = chunk.toarray() if issparse(chunk) else chunk\n            X_pca[start:end] = pca_.transform(chunk)\n    else:\n        zero_center = zero_center if zero_center is not None else False if issparse(adata.X) else True\n        if zero_center:\n            from sklearn.decomposition import PCA\n            if issparse(adata.X):\n                logg.msg('    as `zero_center=True`, '\n                       'sparse input is densified and may '\n                       'lead to huge memory consumption', v=4)\n                X = adata.X.toarray() #Copying the whole adata.X here, could cause memory problems\n            else:\n                X = adata.X\n            pca_ = PCA(n_components=n_comps, svd_solver=svd_solver, random_state=random_state)\n        else:\n            from sklearn.decomposition import TruncatedSVD\n            logg.msg('    without zero-centering: \\n'\n                   '    the explained variance does not correspond to the exact statistical defintion\\n'\n                   '    the first component, e.g., might be heavily influenced by different means\\n'\n                   '    the following components often resemble the exact PCA very closely', v=4)\n            pca_ = TruncatedSVD(n_components=n_comps, random_state=random_state)\n            X = adata.X\n        X_pca = pca_.fit_transform(X)\n\n    if X_pca.dtype.descr != np.dtype(dtype).descr: X_pca = X_pca.astype(dtype)\n\n    adata.obsm['X_pca'] = X_pca\n    adata.varm['PCs'] = pca_.components_.T\n    adata.uns['pca'] = {}\n    adata.uns['pca']['variance'] = pca_.explained_variance_\n    adata.uns['pca']['variance_ratio'] = pca_.explained_variance_ratio_\n    logg.msg('    finished', t=True, end=' ', v=4)\n    logg.msg('and added\\n'\n             '    \\'X_pca\\', the PCA coordinates (adata.obs)\\n'\n             '    \\'PC1\\', \\'PC2\\', ..., the loadings (adata.var)\\n'\n             '    \\'pca_variance\\', the variance / eigenvalues (adata.uns)\\n'\n             '    \\'pca_variance_ratio\\', the variance ratio (adata.uns)', v=4)\n    return adata if copy else None", "idx": 16}
{"project": "Scanpy", "commit_id": "451_scanpy_0.2.9.1_scanpy_plotting_tools.py__aga_graph.py", "target": 1, "func": "def _aga_graph(\n        adata,\n        ax,\n        solid_edges=None,\n        dashed_edges=None,\n        root=0,\n        rootlevel=None,\n        color=None,\n        groups=None,\n        fontsize=None,\n        node_size_scale=1,\n        node_size_power=0.5,\n        edge_width_scale=1,\n        title=None,\n        layout=None,\n        pos=None,\n        cmap=None,\n        frameon=True,\n        min_edge_width=None,\n        max_edge_width=None,\n        export_to_gexf=False,\n        random_state=0):\n    if groups is not None and isinstance(groups, str) and groups not in adata.smp_keys():\n        raise ValueError('Groups {} are not in adata.smp.'.format(groups))\n\n    groups_name = groups if isinstance(groups, str) else None\n    if groups is None and 'aga_groups_order_original' in adata.uns:\n        groups = adata.uns['aga_groups_order_original']\n        groups_name = adata.uns['aga_groups_original']\n    elif groups in adata.smp_keys():\n        groups = adata.uns[groups + '_order']\n    elif groups is None:\n        groups = adata.uns['aga_groups_order']\n        groups_name = 'aga_groups'\n\n    if color is None and groups_name is not None:\n        if 'aga_groups_original' in adata.uns and groups_name == adata.uns['aga_groups_original']:\n            color = adata.uns['aga_groups_colors_original']\n        else:\n            if (groups_name + '_colors' not in adata.uns\n                or len(adata.uns[groups_name + '_order'])\n                   != len(adata.uns[groups_name + '_colors'])):\n                utils.add_colors_for_categorical_sample_annotation(adata, groups_name)\n            color = adata.uns[groups_name + '_colors']\n        for iname, name in enumerate(adata.uns[groups_name + '_order']):\n            if name in settings._ignore_categories: color[iname] = 'grey'\n\n    if isinstance(root, str) and root in groups:\n        root = list(groups).index(root)\n    if isinstance(root, list) and root[0] in groups:\n        root = [list(groups).index(r) for r in root]\n\n    # define the objects\n    adjacency_solid = adata.uns[solid_edges]\n    nx_g_solid = nx.Graph(adjacency_solid)\n    if dashed_edges is not None:\n        adjacency_dashed = adata.uns[dashed_edges]\n        nx_g_dashed = nx.Graph(adjacency_dashed)\n    # degree of the graph for coloring\n    if isinstance(color, str) and color.startswith('degree'):\n        # see also tools.aga.aga_degrees\n        if color == 'degree_dashed':\n            color = [d for _, d in nx_g_dashed.degree_iter(weight='weight')]\n        elif color == 'degree_solid':\n            color = [d for _, d in nx_g_solid.degree_iter(weight='weight')]\n        else:\n            raise ValueError('`degree` either \"degree_dashed\" or \"degree_solid\".')\n        color = (np.array(color) - np.min(color)) / (np.max(color) - np.min(color))\n    # plot numeric colors\n    colorbar = False\n    if isinstance(color, (list, np.ndarray)) and not isinstance(color[0], (str, dict)):\n        import matplotlib\n        norm = matplotlib.colors.Normalize()\n        color = norm(color)\n        if cmap is None: cmap = rcParams['image.cmap']\n        cmap = matplotlib.cm.get_cmap(cmap)\n        color = [cmap(c) for c in color]\n        colorbar = True\n\n    if len(color) < len(groups):\n        print(groups, color)\n        raise ValueError('`color` list need to be at least as long as `groups` list.')\n\n    # node positions from adjacency_solid\n    if pos is None:\n        if layout is None:\n            layout = 'fr'\n        # igraph layouts\n        if layout != 'eq_tree':\n            from .. import utils as sc_utils\n            g = sc_utils.get_igraph_from_adjacency(adjacency_solid)\n            if 'rt' in layout:\n                pos_list = g.layout(layout, root=root if isinstance(root, list) else [root],\n                                    rootlevel=rootlevel).coords\n            elif layout == 'circle':\n                pos_list = g.layout(layout).coords\n            else:\n                np.random.seed(random_state)\n                init_coords = np.random.random((adjacency_solid.shape[0], 2)).tolist()\n                pos_list = g.layout(layout, seed=init_coords).coords\n            pos = {n: [p[0], -p[1]] for n, p in enumerate(pos_list)}\n        # equally-spaced tree\n        else:\n            pos = utils.hierarchy_pos(nx_g_solid, root)\n            if len(pos) < adjacency_solid.shape[0]:\n                raise ValueError('This is a forest and not a single tree. '\n                                 'Try another `layout`, e.g., {\\'fr\\'}.')\n        pos_array = np.array([pos[n] for count, n in enumerate(nx_g_solid)])\n    else:\n        if isinstance(pos, str):\n            if not pos.endswith('.gdf'):\n                raise ValueError('Currently only supporting reading positions from .gdf files.'\n                                 'Consider generating them using, for instance, Gephi.')\n            s = ''  # read the node definition from the file\n            with open(pos) as f:\n                f.readline()\n                for line in f:\n                    if line.startswith('edgedef>'):\n                        break\n                    s += line\n            from io import StringIO\n            df = pd.read_csv(StringIO(s), header=-1)\n            pos = df[[4, 5]].values\n        pos_array = pos\n        # convert to dictionary\n        pos = {n: [p[0], p[1]] for n, p in enumerate(pos)}\n    if len(pos) == 1: pos[0] = (0.5, 0.5)\n    # edge widths\n    base_edge_width = edge_width_scale * rcParams['lines.linewidth']\n    # draw dashed edges\n    if dashed_edges is not None:\n        widths = [x[-1]['weight'] for x in nx_g_dashed.edges(data=True)]\n        widths = base_edge_width * np.array(widths)\n        if max_edge_width is not None:\n            widths = np.clip(widths, None, max_edge_width)\n        nx.draw_networkx_edges(nx_g_dashed, pos, ax=ax, width=widths, edge_color='grey',\n                               style='dashed', alpha=0.5)\n    # draw solid edges\n    widths = [x[-1]['weight'] for x in nx_g_solid.edges(data=True)]\n    widths = base_edge_width * np.array(widths)\n    if min_edge_width is not None or max_edge_width is not None:\n        widths = np.clip(widths, min_edge_width, max_edge_width)\n    nx.draw_networkx_edges(nx_g_solid, pos, ax=ax, width=widths, edge_color='black')\n\n    if export_to_gexf:\n        for count, n in enumerate(nx_g_dashed.nodes()):\n            nx_g_dashed.node[count]['label'] = groups[count]\n            nx_g_dashed.node[count]['color'] = color[count]\n            nx_g_dashed.node[count]['viz'] = {'position': {'x': 100*pos[count][0], 'y': 100*pos[count][1], 'z': 0}}\n        logg.msg('exporting to {}'.format(settings.writedir + 'aga_graph.gexf'), v=1)\n        nx.write_gexf(nx_g_dashed, settings.writedir + 'aga_graph.gexf')\n\n    # deal with empty graph\n    ax.plot(pos_array[:, 0], pos_array[:, 1], '.', c='white')\n\n    # draw the nodes (pie charts)\n    trans = ax.transData.transform\n    bbox = ax.get_position().get_points()\n    ax_x_min = bbox[0, 0]\n    ax_x_max = bbox[1, 0]\n    ax_y_min = bbox[0, 1]\n    ax_y_max = bbox[1, 1]\n    ax_len_x = ax_x_max - ax_x_min\n    ax_len_y = ax_y_max - ax_y_min\n    # print([ax_x_min, ax_x_max, ax_y_min, ax_y_max])\n    # print([ax_len_x, ax_len_y])\n    trans2 = ax.transAxes.inverted().transform\n    ax.set_frame_on(frameon)\n    ax.set_xticks([])\n    ax.set_yticks([])\n    base_pie_size = 1/(np.sqrt(adjacency_solid.shape[0]) + 10) * node_size_scale\n    if (groups_name is not None and groups_name in adata.uns):\n        groups_sizes = adata.uns[groups_name + '_sizes']\n    elif 'aga_groups_sizes' in adata.uns:\n        groups_sizes = adata.uns['aga_groups_sizes']\n    else:\n        groups_sizes = np.ones(len(groups))\n    median_group_size = np.median(groups_sizes)\n    force_labels_to_front = True  # TODO: solve this differently!\n    for count, n in enumerate(nx_g_solid.nodes()):\n        pie_size = base_pie_size\n        pie_size *= np.power(groups_sizes[count] / median_group_size,\n                             node_size_power)\n        xx, yy = trans(pos[n])     # data coordinates\n        xa, ya = trans2((xx, yy))  # axis coordinates\n        xa = ax_x_min + (xa - pie_size/2) * ax_len_x\n        ya = ax_y_min + (ya - pie_size/2) * ax_len_y\n        if ya < 0: ya = 0  # clip, the fruchterman layout sometimes places below figure\n        if xa < 0: xa = 0\n        a = pl.axes([xa, ya, pie_size * ax_len_x, pie_size * ax_len_y])\n        if is_color_like(color[count]):\n            fracs = [100]\n            color_single = [color[count]]\n        elif isinstance(color[count], dict):\n            color_single = color[count].keys()\n            fracs = [color[count][c] for c in color_single]\n            if sum(fracs) < 1:\n                color_single = list(color_single)\n                color_single.append('grey')\n                fracs.append(1-sum(fracs))\n        else:\n            raise ValueError('{} is neither a dict of valid matplotlib colors '\n                             'nor a valid matplotlib color.'.format(color[count]))\n        a.pie(fracs, colors=color_single)\n        if not force_labels_to_front and groups is not None:\n            a.text(0.5, 0.5, groups[count],\n                   verticalalignment='center',\n                   horizontalalignment='center',\n                   transform=a.transAxes,\n                   size=fontsize)\n    # TODO: this is a terrible hack, but if we use the solution above (``not\n    # force_labels_to_front``), labels get hidden behind pies\n    if force_labels_to_front and groups is not None:\n        for count, n in enumerate(nx_g_solid.nodes()):\n            # all copy and paste from above\n            pie_size = base_pie_size\n            pie_size *= np.power(groups_sizes[count] / median_group_size,\n                                 node_size_power)\n            xx, yy = trans(pos[n])     # data coordinates\n            xa, ya = trans2((xx, yy))  # axis coordinates\n            xa = ax_x_min + (xa - pie_size/2.0000001) * ax_len_x  # make sure a new axis is created\n            ya = ax_y_min + (ya - pie_size/2.0000001) * ax_len_y\n            if ya < 0: ya = 0  # clip, the fruchterman layout sometimes places below figure\n            if xa < 0: xa = 0\n            a = pl.axes([xa, ya, pie_size * ax_len_x, pie_size * ax_len_y])\n            a.set_frame_on(False)\n            a.set_xticks([])\n            a.set_yticks([])\n            a.text(0.5, 0.5, groups[count],\n                   verticalalignment='center',\n                   horizontalalignment='center',\n                   transform=a.transAxes, size=fontsize)\n    if title is not None: ax.set_title(title)\n    if colorbar:\n        ax1 = pl.axes([0.95, 0.1, 0.03, 0.7])\n        cb = matplotlib.colorbar.ColorbarBase(ax1, cmap=cmap,\n                                              norm=norm)\n    return pos_array", "idx": 19}
{"project": "Scanpy", "commit_id": "448_scanpy_1.9.0__queries.py_simple_query.py", "target": 0, "func": "def simple_query(\n    org: str,\n    attrs: Union[Iterable[str], str],\n    *,\n    filters: Optional[Dict[str, Any]] = None,\n    host: str = \"www.ensembl.org\",\n    use_cache: bool = False,\n) -> pd.DataFrame:\n    \"\"\"\\\n    A simple interface to biomart.\n\n    Params\n    ------\n    {doc_org}\n    attrs\n        What you want returned.\n    filters\n        What you want to pick out.\n    {doc_host}\n    {doc_use_cache}\n    \"\"\"\n    if isinstance(attrs, str):\n        attrs = [attrs]\n    elif isinstance(attrs, cabc.Iterable):\n        attrs = list(attrs)\n    else:\n        raise TypeError(f\"attrs must be of type list or str, was {type(attrs)}.\")\n    try:\n        from pybiomart import Server\n    except ImportError:\n        raise ImportError(\n            \"This method requires the `pybiomart` module to be installed.\"\n        )\n    server = Server(host, use_cache=use_cache)\n    dataset = server.marts[\"ENSEMBL_MART_ENSEMBL\"].datasets[\n        \"{}_gene_ensembl\".format(org)\n    ]\n    res = dataset.query(attributes=attrs, filters=filters, use_attr_names=True)\n    return res", "idx": 25}
{"project": "Scanpy", "commit_id": "208_scanpy_0.0_scanpy_preprocess_simple.py_filter_genes_dispersion.py", "target": 1, "func": "def filter_genes_dispersion(data, log=True,\n                            min_disp=0.5, max_disp=None,\n                            min_mean=0.0125, max_mean=3,\n                            n_top_genes=None,\n                            norm_method='seurat',\n                            plot=False):\n    \"\"\"\n    Extract highly variable genes.\n    Similar functions are used, for example, by Cell Ranger (Zheng et al., 2017)\n    and Seurat (Macosko et al., 2015).\n    Parameters\n    ----------\n    X : AnnData or array-like\n        Data matrix storing unlogarithmized data.\n    log : bool\n        Use the logarithm of mean and variance.\n    min_mean=0.0125, max_mean=3, min_disp=0.5, max_disp=None : float\n        Cutoffs for the gene expression.\n    n_top_genes : int or None (default: None)\n        Number of highly-variable genes to keep.\n    plot : bool (default: False)\n        Plot the result.\n    Returns\n    -------\n    adata : AnnData\n        Filtered AnnData object.\n    Writes the following fields to adata.var:\n        means : np.ndarray of shape n_genes\n            Means per gene.\n        dispersions : np.ndarray of shape n_genes\n            Dispersions per gene.\n    \"\"\"\n    if isinstance(data, AnnData):\n        adata = data\n        result = filter_genes_dispersion(adata.X, log=log,\n                                         min_disp=min_disp, max_disp=max_disp,\n                                         min_mean=min_mean, max_mean=max_mean,\n                                         n_top_genes=n_top_genes,\n                                         norm_method=norm_method, plot=plot)\n        gene_filter, means, dispersions, dispersions_norm = result\n        adata.var['means'] = means\n        adata.var['dispersions'] = dispersions\n        adata.var['dispersions_norm'] = dispersions_norm\n        if plot:\n            plot_filter_genes_dispersion(adata, gene_filter=gene_filter, log=not log)\n        return adata[:, gene_filter]\n    sett.m(0, '... filter highly varying genes by dispersion and mean')\n    X = data  # proceed with data matrix\n    if False:  # the following is less efficient and has no support for sparse matrices\n        mean = np.mean(X, axis=0)\n        std = np.std(X, axis=0, ddof=1)  # use R convention\n        var = np.var(X, axis=0, ddof=1)\n    else:\n        from sklearn.preprocessing import StandardScaler\n        scaler = StandardScaler(with_mean=False).partial_fit(X)\n        mean = scaler.mean_\n        var = scaler.var_ * (X.shape[0]/(X.shape[0]-1))  # user R convention (unbiased estimator)\n        dispersion = var / (mean + 1e-12)\n        if log:  # consider logarithmized mean as in Seurat\n            dispersion[dispersion == 0] = np.nan\n            dispersion = np.log(dispersion)\n            mean = np.log1p(mean)\n    # all of the following quantities are \"per-gene\" here\n    import pandas as pd\n    df = pd.DataFrame()\n    df['mean'] = mean\n    df['dispersion'] = dispersion\n    if norm_method == 'seurat':\n        df['mean_bin'] = pd.cut(df['mean'], bins=20)\n        disp_grouped = df.groupby('mean_bin')['dispersion']\n        disp_mean_bin = disp_grouped.mean()\n        disp_std_bin = disp_grouped.std(ddof=1)\n        df['dispersion_norm'] = (df['dispersion'].values  # use values here as index differs\n                                 - disp_mean_bin[df['mean_bin']].values) \\\n                                 / disp_std_bin[df['mean_bin']].values\n    elif norm_method == 'cell_ranger':\n        df['mean_bin'] = pd.cut(df['mean'],\n                                np.r_[-np.inf, np.percentile(df['mean'],\n                                                             np.arange(10, 105, 5)),\n                                      np.inf])\n        var_by_bin = pd.DataFrame()\n        from statsmodels import robust\n        import warnings  # this raises a warning we do not want to display\n        with warnings.catch_warnings():\n            warnings.simplefilter('ignore')\n            var_by_bin['bin_disp_median'] = df.groupby('mean_bin').apply(\n                lambda group: np.median(group['dispersion']))\n            var_by_bin['bin_disp_mad'] = df.groupby('mean_bin').apply(\n                lambda group: robust.mad(group['dispersion']))\n        df = df.merge(var_by_bin, left_on='mean_bin', right_index=True)\n        df['dispersion_norm'] = np.abs(df['dispersion']\n                                       - df['bin_disp_median']) \\\n                                       / df['bin_disp_mad']\n    else:\n        raise ValueError('norm_method needs to be `seurat` or `cell_ranger`')\n    dispersion_norm = np.array(df['dispersion_norm'].values)\n    if n_top_genes is not None:\n        dispersion_norm[::-1].sort()  # interestingly, np.argpartition is slightly slower\n        disp_cut_off = dispersion_norm[n_top_genes-1]\n        gene_filter = df['dispersion_norm'].values >= disp_cut_off\n        sett.m(0, 'dispersion cutoff', disp_cut_off)\n    else:\n        sett.m(0, '    using `min_disp`, `max_disp`, `min_mean` and `max_mean`')\n        sett.m(0, '--> set `n_top_genes` to simply select top-scoring genes instead')\n        max_disp = np.inf if max_disp is None else max_disp\n        dispersion_norm[np.isnan(dispersion_norm)] = 0  # similar to Seurat\n        gene_filter = np.logical_and.reduce((mean > min_mean, mean < max_mean,\n                                             dispersion_norm > min_disp,\n                                             dispersion_norm < max_disp))\n    return gene_filter, df['mean'].values, df['dispersion'].values, df['dispersion_norm'].values", "idx": 34}
{"project": "Scanpy", "commit_id": "889_scanpy_1.4.3_scanpy_get.py_obs_df.py", "target": 1, "func": "def obs_df(\n    adata: AnnData,\n    keys: Iterable[str] = (),\n    obsm_keys: Iterable[Tuple[str, int]] = (),\n    *,\n    layer: str = None,\n    gene_symbols: str = None,\n) -> pd.DataFrame:\n    \"\"\"\\\n    Return values for observations in adata.\n    Params\n    ------\n    adata\n        AnnData object to get values from.\n    keys\n        Keys from either `.var_names`, `.var[gene_symbols]`, or `.obs.columns`.\n    obsm_keys\n        Tuple of `(key from obsm, column index of obsm[key])`.\n    layer\n        Layer of `adata` to use as expression values.\n    gene_symbols\n        Column of `adata.var` to search for `keys` in.\n    Returns\n    -------\n    A dataframe with `adata.obs_names` as index, and values specified by `keys`\n    and `obsm_keys`.\n    Examples\n    --------\n    Getting value for plotting:\n    >>> pbmc = sc.datasets.pbmc68k_reduced()\n    >>> plotdf = sc.get.obs_df(\n            pbmc,\n            keys=[\"CD8B\", \"n_genes\"],\n            obsm_keys=[(\"X_umap\", 0), (\"X_umap\", 1)]\n        )\n    >>> plotdf.plot.scatter(\"X_umap0\", \"X_umap1\", c=\"CD8B\")\n    Calculating mean expression for marker genes by cluster:\n    >>> pbmc = sc.datasets.pbmc68k_reduced()\n    >>> marker_genes = ['CD79A', 'MS4A1', 'CD8A', 'CD8B', 'LYZ']\n    >>> genedf = sc.get.obs_df(\n            pbmc,\n            keys=[\"louvain\", *marker_genes]\n        )\n    >>> grouped = genedf.groupby(\"louvain\")\n    >>> mean, var = grouped.mean(), grouped.var()\n    \"\"\"\n    # Argument handling\n    if gene_symbols is not None:\n        gene_names = pd.Series(adata.var_names, index=adata.var[gene_symbols])\n    else:\n        gene_names = pd.Series(adata.var_names, index=adata.var_names)\n    lookup_keys = []\n    not_found = []\n    for key in keys:\n        if key in adata.obs.columns:\n            lookup_keys.append(key)\n        elif key in gene_names.index:\n            lookup_keys.append(gene_names[key])\n        else:\n            not_found.append(key)\n    if len(not_found) > 0:\n        if gene_symbols is None:\n            gene_error = \"`adata.var_names`\"\n        else:\n            gene_error = \"gene_symbols column `adata.var[{}].values`\".format(gene_symbols)\n        raise KeyError(\n            f\"Could not find keys '{not_found}' in columns of `adata.obs` or in\"\n            f\" {gene_error}.\"\n        )\n    # Make df\n    df = pd.DataFrame(index=adata.obs_names)\n    for k, l in zip(keys, lookup_keys):\n        df[k] = adata.obs_vector(l, layer=layer)\n    for k, idx in obsm_keys:\n        added_k = f\"{k}-{idx}\"\n        if isinstance(adata.obsm[k], (np.ndarray, sparse.csr_matrix)):\n            df[added_k] = np.ravel(adata.obsm[k][:, idx])\n        elif isinstance(adata.obsm[k], pd.DataFrame):\n            df[added_k] = adata.obsm[k].loc[:, idx]\n    return df", "idx": 42}
{"project": "Scanpy", "commit_id": "287_scanpy_1.9.0__matrixplot.py___init__.py", "target": 0, "func": "def __init__(\n        self,\n        adata: AnnData,\n        var_names: Union[_VarNames, Mapping[str, _VarNames]],\n        groupby: Union[str, Sequence[str]],\n        use_raw: Optional[bool] = None,\n        log: bool = False,\n        num_categories: int = 7,\n        categories_order: Optional[Sequence[str]] = None,\n        title: Optional[str] = None,\n        figsize: Optional[Tuple[float, float]] = None,\n        gene_symbols: Optional[str] = None,\n        var_group_positions: Optional[Sequence[Tuple[int, int]]] = None,\n        var_group_labels: Optional[Sequence[str]] = None,\n        var_group_rotation: Optional[float] = None,\n        layer: Optional[str] = None,\n        standard_scale: Literal['var', 'group'] = None,\n        ax: Optional[_AxesSubplot] = None,\n        values_df: Optional[pd.DataFrame] = None,\n        vmin: Optional[float] = None,\n        vmax: Optional[float] = None,\n        vcenter: Optional[float] = None,\n        norm: Optional[Normalize] = None,\n        **kwds,\n    ):\n        BasePlot.__init__(\n            self,\n            adata,\n            var_names,\n            groupby,\n            use_raw=use_raw,\n            log=log,\n            num_categories=num_categories,\n            categories_order=categories_order,\n            title=title,\n            figsize=figsize,\n            gene_symbols=gene_symbols,\n            var_group_positions=var_group_positions,\n            var_group_labels=var_group_labels,\n            var_group_rotation=var_group_rotation,\n            layer=layer,\n            ax=ax,\n            vmin=vmin,\n            vmax=vmax,\n            vcenter=vcenter,\n            norm=norm,\n            **kwds,\n        )\n\n        if values_df is None:\n            # compute mean value\n            values_df = self.obs_tidy.groupby(level=0).mean()\n\n            if standard_scale == 'group':\n                values_df = values_df.sub(values_df.min(1), axis=0)\n                values_df = values_df.div(values_df.max(1), axis=0).fillna(0)\n            elif standard_scale == 'var':\n                values_df -= values_df.min(0)\n                values_df = (values_df / values_df.max(0)).fillna(0)\n            elif standard_scale is None:\n                pass\n            else:\n                logg.warning('Unknown type for standard_scale, ignored')\n\n        self.values_df = values_df\n\n        self.cmap = self.DEFAULT_COLORMAP\n        self.edge_color = self.DEFAULT_EDGE_COLOR\n        self.edge_lw = self.DEFAULT_EDGE_LW", "idx": 43}
{"project": "Scanpy", "commit_id": "223_scanpy_0.1_scanpy_plotting___init__.py_paths.py", "target": 1, "func": "def paths(adata,\n          basis='diffmap',\n          dist_threshold=None,\n          single_panel=True,\n          color=None,\n          names=None,\n          comps=None,\n          cont=None,\n          layout='2d',\n          legendloc='right margin',\n          cmap=None,\n          right_margin=None,\n          size=3,\n          titles=None,\n          show=None):\n    \"\"\"Plot paths\n    Parameters\n    ----------\n    adata : AnnData\n        Annotated data matrix.\n    dist_threshold : float\n        Distance threshold to decide what still belongs in the path.\n    single_panel : bool (default: True)\n        If False, show separate panel for each group.\n    color : str, optional (default: first annotation)\n        Sample/Cell annotation for coloring in the form \"ann1,ann2,...\". String\n        annotation is plotted assuming categorical annotation, float and integer\n        annotation is plotted assuming continuous annoation. Option 'cont'\n        allows to switch between these default choices.\n    names : str, optional (default: all names in color)\n        Allows to restrict groups in sample annotation (color) to a few.\n    comps : str, optional (default: '1,2')\n         String in the form '1,2,3'.\n    cont : bool, None (default: None)\n        Switch on continuous layout, switch off categorical layout.\n    layout : {'2d', '3d', 'unfolded 3d'}, optional (default: '2d')\n         Layout of plot.\n    legendloc : see matplotlib.legend, optional (default: 'lower right')\n         Options for keyword argument 'loc'.\n    cmap : str (default: continuous: inferno/ categorical: finite palette)\n         String denoting matplotlib color map.\n    right_margin : float (default: None)\n         Adjust how far the plotting panel extends to the right.\n    size : float (default: 3)\n         Point size.\n    \"\"\"\n    from ..examples import check_adata\n    adata = check_adata(adata)\n    names = None if names is None else names.split(',') if isinstance(names, str) else names\n    from ..tools.paths import process_dists_from_paths\n    process_dists_from_paths(adata, dist_threshold)\n    color_base = ['paths_groups']\n    if color is not None:\n        color_base += [color]\n    adata.add['highlights'] = [adata.add['iroot']]\n    # add continuous distance coloring\n    if single_panel:\n        for iname, name in enumerate(adata.add['paths_groups_names']):\n            if names is None or (names is not None and name in names):\n                title = 'dist_from_path_' + name\n                adata.smp[title] = adata.add['paths_dists_from_paths'][iname]\n                color_base.append(title)\n                adata.add['highlights'] += [adata.add['paths_groups_fateidx'][iname]]\n\n        colors = scatter(adata,\n                       basis=basis,\n                       color=color_base,\n                       names=names,\n                       comps=comps,\n                       cont=cont,\n                       layout=layout,\n                       legendloc=legendloc,\n                       cmap=cmap,\n                       right_margin=right_margin,\n                       size=size,\n                       titles=titles,\n                       show=False)\n        writekey = sett.basekey + '_paths_' + basis + '_' + adata.add['paths_type']\n        writekey += '_' + ('-'.join(colors) if colors[0] is not None else '') + sett.plotsuffix\n        if sett.savefigs: savefig(writekey)\n    else:\n        for iname, name in enumerate(adata.add['paths_groups_names']):\n            if names is None or (names is not None and name in names):\n                title = 'dist_from_path_' + name\n                adata.smp[title] = adata.add['paths_dists_from_paths'][iname]\n                color_base.append(title)\n                adata.add['highlights'] = ([adata.add['iroot']]\n                                       + [adata.add['paths_groups_fateidx'][iname]])\n            colors = scatter(adata,\n                           basis=basis,\n                           color=color_base,\n                           names=[name],\n                           comps=comps,\n                           cont=cont,\n                           layout=layout,\n                           legendloc=legendloc,\n                           cmap=cmap,\n                           right_margin=right_margin,\n                           size=size,\n                           titles=titles,\n                           show=False)\n            del color_base[-1]\n            writekey = sett.basekey + '_paths_' + basis + '_' + adata.add['paths_type']\n            writekey += '_' + ('-'.join(colors) if colors[0] is not None else '') + '_' + name + sett.plotsuffix\n            if sett.savefigs: savefig(writekey)\n    show = sett.autoshow if show is None else show\n    if not sett.savefigs and show: pl.show()", "idx": 49}
{"project": "Scanpy", "commit_id": "34_scanpy_0.0_scanpy_exs_builtin.py_paul15pca.py", "target": 1, "func": "def paul15pca():\n    ddata = paul15_raw()\n    ddata['X'] = sc.pp(ddata['X'], 'log')\n    # reduce to 50 components\n    ddata['Xpca'] = sc.pca(ddata['X'])\n    # adjust expression vector of root cell\n    ddata['xroot'] = ddata['Xpca'][ddata['iroot']]\n    return ddata", "idx": 53}
{"project": "Scanpy", "commit_id": "384_scanpy_1.9.0___init__.py_sim.py", "target": 0, "func": "def sim(\n    adata,\n    tmax_realization: Optional[int] = None,\n    as_heatmap: bool = False,\n    shuffle: bool = False,\n    show: Optional[bool] = None,\n    save: Union[bool, str, None] = None,\n):\n    \"\"\"\\\n    Plot results of simulation.\n\n    Parameters\n    ----------\n    tmax_realization\n        Number of observations in one realization of the time series. The data matrix\n        adata.X consists in concatenated realizations.\n    as_heatmap\n        Plot the timeseries as heatmap.\n    shuffle\n        Shuffle the data.\n    show\n        Show the plot, do not return axis.\n    save\n        If `True` or a `str`, save the figure.\n        A string is appended to the default filename.\n        Infer the filetype if ending on {{`'.pdf'`, `'.png'`, `'.svg'`}}.\n    \"\"\"\n    if tmax_realization is not None:\n        tmax = tmax_realization\n    elif 'tmax_write' in adata.uns:\n        tmax = adata.uns['tmax_write']\n    else:\n        tmax = adata.n_obs\n    n_realizations = adata.n_obs / tmax\n    if not shuffle:\n        if not as_heatmap:\n            timeseries(\n                adata.X,\n                var_names=adata.var_names,\n                xlim=[0, 1.25 * adata.n_obs],\n                highlights_x=np.arange(tmax, n_realizations * tmax, tmax),\n                xlabel='realizations',\n            )\n        else:\n            # plot time series as heatmap, as in Haghverdi et al. (2016), Fig. 1d\n            timeseries_as_heatmap(\n                adata.X,\n                var_names=adata.var_names,\n                highlights_x=np.arange(tmax, n_realizations * tmax, tmax),\n            )\n        pl.xticks(\n            np.arange(0, n_realizations * tmax, tmax),\n            np.arange(n_realizations).astype(int) + 1,\n        )\n        savefig_or_show('sim', save=save, show=show)\n    else:\n        # shuffled data\n        X = adata.X\n        X, rows = subsample(X, seed=1)\n        timeseries(\n            X,\n            var_names=adata.var_names,\n            xlim=[0, 1.25 * adata.n_obs],\n            highlights_x=np.arange(tmax, n_realizations * tmax, tmax),\n            xlabel='index (arbitrary order)',\n        )\n        savefig_or_show('sim_shuffled', save=save, show=show)", "idx": 55}
{"project": "Scanpy", "commit_id": "972_scanpy_1.4.5_scanpy_readwrite.py_read_10x_mtx.py", "target": 1, "func": "def read_10x_mtx(\n    path: Union[Path, str],\n    var_names: Literal['gene_symbols', 'gene_ids'] = 'gene_symbols',\n    make_unique: bool = True,\n    cache: bool = False,\n    cache_compression: Union[Literal['gzip', 'lzf'], None, Empty] = _empty,\n    gex_only: bool = True,\n) -> AnnData:\n    \"\"\"\\\n    Read 10x-Genomics-formatted mtx directory.\n    Parameters\n    ----------\n    path\n        Path to directory for `.mtx` and `.tsv` files,\n        e.g. './filtered_gene_bc_matrices/hg19/'.\n    var_names\n        The variables index.\n    make_unique\n        Whether to make the variables index unique by appending '-1',\n        '-2' etc. or not.\n    cache\n        If `False`, read from source, if `True`, read from fast 'h5ad' cache.\n    cache_compression\n        See the h5py :ref:`dataset_compression`.\n        (Default: `settings.cache_compression`)\n    gex_only\n        Only keep 'Gene Expression' data and ignore other feature types,\n        e.g. 'Antibody Capture', 'CRISPR Guide Capture', or 'Custom'\n    Returns\n    -------\n    An :class:`~anndata.AnnData` object\n    \"\"\"\n    path = Path(path)\n    genefile_exists = (path / 'genes.tsv').is_file()\n    read = _read_legacy_10x_mtx if genefile_exists else _read_v3_10x_mtx\n    adata = read(\n        str(path),\n        var_names=var_names,\n        make_unique=make_unique,\n        cache=cache,\n        cache_compression=cache_compression,\n    )\n    if genefile_exists or not gex_only:\n        return adata\n    else:\n        gex_rows = list(map(lambda x: x == 'Gene Expression', adata.var['feature_types']))\n        return adata[:, gex_rows]", "idx": 60}
{"project": "Scanpy", "commit_id": "524_scanpy_0.4.4_scanpy_plotting_tools.py_aga_graph.py", "target": 1, "func": "def aga_graph(\n        adata,\n        solid_edges='aga_adjacency_full_confidence',\n        dashed_edges=None,\n        layout=None,\n        root=0,\n        groups=None,\n        color=None,\n        threshold_solid=None,\n        threshold_dashed=1e-6,\n        fontsize=None,\n        node_size_scale=1,\n        node_size_power=0.5,\n        edge_width_scale=1,\n        min_edge_width=None,\n        max_edge_width=None,\n        title='abstracted graph',\n        left_margin=0.01,\n        random_state=0,\n        pos=None,\n        cmap=None,\n        frameon=True,\n        rootlevel=None,\n        return_pos=False,\n        export_to_gexf=False,\n        show=None,\n        save=None,\n        ax=None):\n    \"\"\"Plot the abstracted graph.\n    This uses igraph's layout algorithms for most layouts [Csardi06]_.\n    Parameters\n    ----------\n    adata : :class:`~scanpy.api.AnnData`\n        Annotated data matrix.\n    solid_edges : `str`, optional (default: 'aga_adjacency_tree_confidence')\n        Key for `adata.uns` that specifies the matrix that stores the edges\n        to be drawn solid black.\n    dashed_edges : `str` or `None`, optional (default: 'aga_adjacency_full_confidence')\n        Key for `adata.uns` that specifies the matrix that stores the edges\n        to be drawn dashed grey. If `None`, no dashed edges are drawn.\n    layout : {'fr', 'rt', 'rt_circular', 'eq_tree', ...}, optional (default: 'fr')\n        Plotting layout. 'fr' stands for Fruchterman-Reingold, 'rt' stands for\n        Reingold Tilford. 'eq_tree' stands for 'eqally spaced tree'. All but\n        'eq_tree' use the igraph layout function. All other igraph layouts are\n        also permitted. See also parameter `pos`.\n    random_state : `int` or `None`, optional (default: 0)\n        For layouts with random initialization like 'fr', change this to use\n        different intial states for the optimization. If `None`, the initial\n        state is not reproducible.\n    root : int, str or list of int, optional (default: 0)\n        If choosing a tree layout, this is the index of the root node or root\n        nodes. If this is a non-empty vector then the supplied node IDs are used\n        as the roots of the trees (or a single tree if the graph is\n        connected. If this is `None` or an empty list, the root vertices are\n        automatically calculated based on topological sorting.\n    groups : `str`, `list`, `dict`\n        The node (groups) labels.\n    color : color string or iterable, {'degree_dashed', 'degree_solid'}, optional (default: None)\n        The node colors.  Besides cluster colors, lists and uniform colors this\n        also acceppts {'degree_dashed', 'degree_solid'} which are plotted using\n        continuous color map.\n    threshold_solid : `float` or `None`, optional (default: `None`)\n        Do not draw edges for weights below this threshold. Set to `None` if you\n        want all edges.\n    threshold_dashed : `float` or `None`, optional (default: 1e-6)\n        Do not draw edges for weights below this threshold. Set to `None` if you\n        want all edges.\n    fontsize : int (default: None)\n        Font size for node labels.\n    node_size_scale : float (default: 1.0)\n        Increase or decrease the size of the nodes.\n    node_size_power : float (default: 0.5)\n        The power with which groups sizes influence the radius of the nodes.\n    edge_width_scale : `float`, optional (default: 1.5)\n        Edge with scale in units of `rcParams['lines.linewidth']`.\n    min_edge_width : `float`, optional (default: `None`)\n        Min width of solid edges.\n    max_edge_width : `float`, optional (default: `None`)\n        Max width of solid and dashed edges.\n    pos : filename of `.gdf` file, array-like, optional (default: `None`)\n        Two-column array/list storing the x and y coordinates for drawing.\n        Otherwise, path to a `.gdf` file that has been exported from Gephi or\n        a similar graph visualization software.\n    export_to_gexf : `bool`, optional (default: `None`)\n        Export to gexf format to be read by graph visualization programs such as\n        Gephi.\n    return_pos : `bool`, optional (default: `False`)\n        Return the positions.\n    title : `str`, optional (default: `None`)\n         Provide title for panels either as `['title1', 'title2', ...]` or\n         `'title1,title2,...'`.\n    frameon : `bool`, optional (default: `True`)\n         Draw a frame around the abstracted graph.\n    show : `bool`, optional (default: `None`)\n         Show the plot, do not return axis.\n    save : `bool` or `str`, optional (default: `None`)\n        If `True` or a `str`, save the figure. A string is appended to the\n        default filename. Infer the filetype if ending on \\{'.pdf', '.png', '.svg'\\}.\n    ax : `matplotlib.Axes`\n         A matplotlib axes object.\n    Returns\n    -------\n    If `show==False`, a list of `matplotlib.Axis` objects. Every second element\n    corresponds to the 'right margin' drawing area for color bars and legends.\n    If `return_pos` is `True`, in addition, the positions of the nodes.\n    \"\"\"\n    import matplotlib as mpl\n    from distutils.version import LooseVersion\n    if mpl.__version__ > LooseVersion('2.0.0'):\n        logg.warn('Currently, `aga_graph` sometimes crashes with matplotlib version > 2.0, you have {}.\\n'\n                  'Run `pip install matplotlib==2.0` if this hits you.'\n                  .format(mpl.__version__))\n    # colors is a list that contains no lists\n    if isinstance(color, list) and True not in [isinstance(c, list) for c in color]: color = [color]\n    if color is None or isinstance(color, str): color = [color]\n    # groups is a list that contains no lists\n    if isinstance(groups, list) and True not in [isinstance(g, list) for g in groups]: groups = [groups]\n    if groups is None or isinstance(groups, dict) or isinstance(groups, str): groups = [groups]\n    if title is None or isinstance(title, str): title = [title for name in groups]\n    if ax is None:\n        axs, _, _, _ = utils.setup_axes(colors=color)\n    else:\n        axs = ax\n    if len(color) == 1 and not isinstance(axs, list): axs = [axs]\n    for icolor, c in enumerate(color):\n        pos = _aga_graph(\n            adata,\n            axs[icolor],\n            solid_edges=solid_edges,\n            dashed_edges=dashed_edges,\n            threshold_solid=threshold_solid,\n            threshold_dashed=threshold_dashed,\n            layout=layout,\n            root=root,\n            rootlevel=rootlevel,\n            color=c,\n            groups=groups[icolor],\n            fontsize=fontsize,\n            node_size_scale=node_size_scale,\n            node_size_power=node_size_power,\n            edge_width_scale=edge_width_scale,\n            min_edge_width=min_edge_width,\n            max_edge_width=max_edge_width,\n            frameon=frameon,\n            cmap=cmap,\n            title=title[icolor],\n            random_state=0,\n            export_to_gexf=export_to_gexf,\n            pos=pos)\n    utils.savefig_or_show('aga_graph', show=show, save=save)\n    if len(color) == 1 and isinstance(axs, list): axs = axs[0]\n    if return_pos:\n        return axs, pos if ax is None and show == False else pos\n    else:\n        return axs if ax is None and show == False else None", "idx": 70}
{"project": "Scanpy", "commit_id": "13_scanpy_0.0_scanpy_utils.py_add_args.py", "target": 1, "func": "def add_args(p, dadd_args=None):\n    \"\"\"\n    Add arguments to parser.\n    Parameters\n    -------\n    dadd_args : dict\n        Dictionary of additional arguments formatted as\n            {'arg': {'type': int, 'default': 0, ... }}\n    \"\"\"\n    aa = p.add_argument_group('Tool parameters').add_argument\n    aa('exkey',\n       type=str, default='', metavar='exkey',\n       help='Specify the \"example key\" (just a shorthand), which is used'\n            ' to look up a data dictionary and parameters. ' \n            'Use Scanpy subcommand \"examples\" to inspect possible values.')\n    # example key default argument\n    aa('-p', '--params',\n       nargs='*', default=None, metavar='k v',\n       help='Provide optional parameters as list, '\n            'e.g., \"sigma 5 knn True\" for setting \"sigma\" and \"knn\". See possible '\n            'keys in the function definition above (default: \"\").')\n    # make sure there are is conflict with dadd_args\n    if dadd_args is None or '--paramsfile' not in dadd_args:\n        aa('--paramsfile',\n           type=str, default='', metavar='pf',\n           help='Alternatively, specify the path to a parameter file (default: \"\").')\n    # arguments from dadd_args\n    if dadd_args is not None:\n        for key, val in dadd_args.items():\n            if key != 'arg':\n                aa(key, **val)\n    aa = p.add_argument_group('Plotting').add_argument\n    aa('-q', '--plotparams',\n       nargs='*', default=None, metavar='k v',\n       help='Provide specific plotting parameters as list, '\n            'e.g., \"layout 3d cmap viridis\". ' \n            'See possible keys by calling \"--plotparams help\" (default: \"\").')\n    aa = p.add_argument_group('Toolchain').add_argument\n    aa('--pre',\n       type=str, default='', metavar='tool',\n       help='Tool whose output should be used as input, ' \n            'often a tool that detects subgroups (default: tool dependent).')\n    # standard arguments\n    p = sett.add_args(p)\n    return p", "idx": 71}
{"project": "Scanpy", "commit_id": "861_scanpy_1.4.1_scanpy_tools__rank_genes_groups.py_rank_genes_groups.py", "target": 1, "func": "def rank_genes_groups(\n    adata,\n    groupby,\n    use_raw=True,\n    groups: Union[str, Iterable[str]] = 'all',\n    reference='rest',\n    n_genes=100,\n    rankby_abs=False,\n    key_added=None,\n    copy=False,\n    method='t-test_overestim_var',\n    corr_method='benjamini-hochberg',\n    log_transformed=True,\n    **kwds\n):\n    \"\"\"Rank genes for characterizing groups.\n    Parameters\n    ----------\n    adata : :class:`~anndata.AnnData`\n        Annotated data matrix.\n    groupby : `str`\n        The key of the observations grouping to consider.\n    use_raw : `bool`, optional (default: `True`)\n        Use `raw` attribute of `adata` if present.\n    groups\n        Subset of groups, e.g. `['g1', 'g2', 'g3']`, to which comparison shall\n        be restricted, or `'all'` (default), for all groups.\n    reference : `str`, optional (default: `'rest'`)\n        If `'rest'`, compare each group to the union of the rest of the group.  If\n        a group identifier, compare with respect to this group.\n    n_genes : `int`, optional (default: 100)\n        The number of genes that appear in the returned tables.\n    method : `{'logreg', 't-test', 'wilcoxon', 't-test_overestim_var'}`, optional (default: 't-test_overestim_var')\n        If 't-test', uses t-test, if 'wilcoxon', uses Wilcoxon-Rank-Sum. If\n        't-test_overestim_var', overestimates variance of each group. If\n        'logreg' uses logistic regression, see [Ntranos18]_, `here\n        <https://github.com/theislab/scanpy/issues/95>`__ and `here\n        <http://www.nxn.se/valent/2018/3/5/actionable-scrna-seq-clusters>`__, for\n        why this is meaningful.\n    corr_method : `{'benjamini-hochberg', 'bonferroni'}`, optional (default: 'benjamini-hochberg')\n        p-value correction method. Used only for 't-test', 't-test_overestim_var',\n        and 'wilcoxon' methods.\n    rankby_abs : `bool`, optional (default: `False`)\n        Rank genes by the absolute value of the score, not by the\n        score. The returned scores are never the absolute values.\n    **kwds : keyword parameters\n        Are passed to test methods. Currently this affects only parameters that\n        are passed to `sklearn.linear_model.LogisticRegression\n        <http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html>`__.\n        For instance, you can pass `penalty='l1'` to try to come up with a\n        minimal set of genes that are good predictors (sparse solution meaning\n        few non-zero fitted coefficients).\n    Returns\n    -------\n    **names** : structured `np.ndarray` (`.uns['rank_genes_groups']`)\n        Structured array to be indexed by group id storing the gene\n        names. Ordered according to scores.\n    **scores** : structured `np.ndarray` (`.uns['rank_genes_groups']`)\n        Structured array to be indexed by group id storing the z-score\n        underlying the computation of a p-value for each gene for each\n        group. Ordered according to scores.\n    **logfoldchanges** : structured `np.ndarray` (`.uns['rank_genes_groups']`)\n        Structured array to be indexed by group id storing the log2\n        fold change for each gene for each group. Ordered according to\n        scores. Only provided if method is 't-test' like.\n        Note: this is an approximation calculated from mean-log values.\n    **pvals** : structured `np.ndarray` (`.uns['rank_genes_groups']`)\n        p-values.\n    **pvals_adj** : structured `np.ndarray` (`.uns['rank_genes_groups']`)\n        Corrected p-values.\n    Notes\n    -----\n    There are slight inconsistencies depending on whether sparse\n    or dense data are passed. See `here <https://github.com/theislab/scanpy/blob/master/scanpy/tests/test_rank_genes_groups.py>`__.\n    Examples\n    --------\n    >>> adata = sc.datasets.pbmc68k_reduced()\n    >>> sc.tl.rank_genes_groups(adata, 'bulk_labels', method='wilcoxon')\n    # to visualize the results\n    >>> sc.pl.rank_genes_groups(adata)\n    \"\"\"\n    if 'only_positive' in kwds:\n        rankby_abs = not kwds.pop('only_positive')  # backwards compat\n    logg.info('ranking genes', r=True)\n    avail_methods = {'t-test', 't-test_overestim_var', 'wilcoxon', 'logreg'}\n    if method not in avail_methods:\n        raise ValueError('Method must be one of {}.'.format(avail_methods))\n    avail_corr = {'benjamini-hochberg', 'bonferroni'}\n    if corr_method not in avail_corr:\n        raise ValueError('Correction method must be one of {}.'.format(avail_corr))\n    adata = adata.copy() if copy else adata\n    utils.sanitize_anndata(adata)\n    # for clarity, rename variable\n    groups_order = groups if isinstance(groups, str) else list(groups)\n    if isinstance(groups_order, list) and isinstance(groups_order[0], int):\n        groups_order = [str(n) for n in groups_order]\n    if reference != 'rest' and reference not in set(groups_order):\n        groups_order += [reference]\n    if (reference != 'rest'\n        and reference not in set(adata.obs[groupby].cat.categories)):\n        raise ValueError('reference = {} needs to be one of groupby = {}.'\n                         .format(reference,\n                                 adata.obs[groupby].cat.categories.tolist()))\n    groups_order, groups_masks = utils.select_groups(\n        adata, groups_order, groupby)\n    if key_added is None:\n        key_added = 'rank_genes_groups'\n    adata.uns[key_added] = {}\n    adata.uns[key_added]['params'] = {\n        'groupby': groupby,\n        'reference': reference,\n        'method': method,\n        'use_raw': use_raw,\n        'corr_method': corr_method,\n    }\n    # adata_comp mocks an AnnData object if use_raw is True\n    # otherwise it's just the AnnData object\n    adata_comp = adata\n    if adata.raw is not None and use_raw:\n        adata_comp = adata.raw\n    X = adata_comp.X\n    # for clarity, rename variable\n    n_genes_user = n_genes\n    # make sure indices are not OoB in case there are less genes than n_genes\n    if n_genes_user > X.shape[1]:\n        n_genes_user = X.shape[1]\n    # in the following, n_genes is simply another name for the total number of genes\n    n_genes = X.shape[1]\n    n_groups = groups_masks.shape[0]\n    ns = np.zeros(n_groups, dtype=int)\n    for imask, mask in enumerate(groups_masks):\n        ns[imask] = np.where(mask)[0].size\n    logg.msg('consider \\'{}\\' groups:'.format(groupby), groups_order, v=4)\n    logg.msg('with sizes:', ns, v=4)\n    if reference != 'rest':\n        ireference = np.where(groups_order == reference)[0][0]\n    reference_indices = np.arange(adata_comp.n_vars, dtype=int)\n    rankings_gene_scores = []\n    rankings_gene_names = []\n    rankings_gene_logfoldchanges = []\n    rankings_gene_pvals = []\n    rankings_gene_pvals_adj = []\n    if method in {'t-test', 't-test_overestim_var'}:\n        from scipy import stats\n        from statsmodels.stats.multitest import multipletests\n        # loop over all masks and compute means, variances and sample numbers\n        means = np.zeros((n_groups, n_genes))\n        vars = np.zeros((n_groups, n_genes))\n        for imask, mask in enumerate(groups_masks):\n            means[imask], vars[imask] = _get_mean_var(X[mask])\n        # test each either against the union of all other groups or against a\n        # specific group\n        for igroup in range(n_groups):\n            if reference == 'rest':\n                mask_rest = ~groups_masks[igroup]\n            else:\n                if igroup == ireference: continue\n                else: mask_rest = groups_masks[ireference]\n            mean_rest, var_rest = _get_mean_var(X[mask_rest])\n            ns_group = ns[igroup]  # number of observations in group\n            if method == 't-test': ns_rest = np.where(mask_rest)[0].size\n            elif method == 't-test_overestim_var': ns_rest = ns[igroup]  # hack for overestimating the variance for small groups\n            else: raise ValueError('Method does not exist.')\n\n            denominator = np.sqrt(vars[igroup]/ns_group + var_rest/ns_rest)\n            with np.errstate(divide='ignore', invalid='ignore'):\n                scores = np.divide((means[igroup] - mean_rest), denominator) #Welch t-test\n            scores = (means[igroup] - mean_rest) / denominator #Welch t-test\n            scores[np.isnan(scores)] = 0\n            # Fold change\n            foldchanges = (np.expm1(means[igroup]) + 1e-9) / (np.expm1(mean_rest) + 1e-9) #add small value to remove 0's\n\n            #Get p-values\n            denominator_dof = (np.square(vars[igroup]) / (np.square(ns_group)*(ns_group-1))) + (\n                (np.square(var_rest) / (np.square(ns_rest) * (ns_rest - 1))))\n            with np.errstate(divide='ignore', invalid='ignore'):\n                dof = np.divide(np.square(vars[igroup]/ns_group + var_rest/ns_rest), denominator_dof) # dof calculation for Welch t-test\n            dof[np.isnan(dof)] = 0\n            pvals = stats.t.sf(abs(scores), dof)*2 # *2 because of two-tailed t-test\n\n            if corr_method == 'benjamini-hochberg':\n                pvals[np.isnan(pvals)] = 1  # set Nan values to 1 to properly convert using Benhjamini Hochberg\n                _, pvals_adj, _, _ = multipletests(pvals, alpha=0.05, method='fdr_bh')\n            elif corr_method == 'bonferroni':\n                pvals_adj = np.minimum(pvals * n_genes, 1.0)\n            scores_sort = np.abs(scores) if rankby_abs else scores\n            partition = np.argpartition(scores_sort, -n_genes_user)[-n_genes_user:]\n            partial_indices = np.argsort(scores_sort[partition])[::-1]\n            global_indices = reference_indices[partition][partial_indices]\n            rankings_gene_scores.append(scores[global_indices])\n            rankings_gene_logfoldchanges.append(np.log2(foldchanges[global_indices]))\n            rankings_gene_names.append(adata_comp.var_names[global_indices])\n            rankings_gene_pvals.append(pvals[global_indices])\n            rankings_gene_pvals_adj.append(pvals_adj[global_indices])\n    elif method == 'logreg':\n        # if reference is not set, then the groups listed will be compared to the rest\n        # if reference is set, then the groups listed will be compared only to the other groups listed\n        from sklearn.linear_model import LogisticRegression\n        reference = groups_order[0]\n        if len(groups) == 1:\n            raise Exception('Cannot perform logistic regression on a single cluster.')\n        adata_copy = adata[adata.obs[groupby].isin(groups_order)]\n        adata_comp = adata_copy\n        if adata.raw is not None and use_raw:\n            adata_comp = adata_copy.raw\n        X = adata_comp.X\n        clf = LogisticRegression(**kwds)\n        clf.fit(X, adata_copy.obs[groupby].cat.codes)\n        scores_all = clf.coef_\n        for igroup, group in enumerate(groups_order):\n            if len(groups_order) <= 2:  # binary logistic regression\n                scores = scores_all[0]\n            else:\n                scores = scores_all[igroup]\n            partition = np.argpartition(scores, -n_genes_user)[-n_genes_user:]\n            partial_indices = np.argsort(scores[partition])[::-1]\n            global_indices = reference_indices[partition][partial_indices]\n            rankings_gene_scores.append(scores[global_indices])\n            rankings_gene_names.append(adata_comp.var_names[global_indices])\n            if len(groups_order) <= 2:\n                break\n    elif method == 'wilcoxon':\n        from scipy import stats\n        from statsmodels.stats.multitest import multipletests\n        CONST_MAX_SIZE = 10000000\n        means = np.zeros((n_groups, n_genes))\n        vars = np.zeros((n_groups, n_genes))\n        # initialize space for z-scores\n        scores = np.zeros(n_genes)\n        # First loop: Loop over all genes\n        if reference != 'rest':\n            for imask, mask in enumerate(groups_masks):\n                means[imask], vars[imask] = _get_mean_var(X[mask])  # for fold-change only\n                if imask == ireference: continue\n                else: mask_rest = groups_masks[ireference]\n                ns_rest = np.where(mask_rest)[0].size\n                mean_rest, var_rest = _get_mean_var(X[mask_rest]) # for fold-change only\n                if ns_rest <= 25 or ns[imask] <= 25:\n                    logg.hint('Few observations in a group for '\n                              'normal approximation (<=25). Lower test accuracy.')\n                n_active = ns[imask]\n                m_active = ns_rest\n                # Now calculate gene expression ranking in chunkes:\n                chunk = []\n                # Calculate chunk frames\n                n_genes_max_chunk = floor(CONST_MAX_SIZE / (n_active + m_active))\n                if n_genes_max_chunk < n_genes:\n                    chunk_index = n_genes_max_chunk\n                    while chunk_index < n_genes:\n                        chunk.append(chunk_index)\n                        chunk_index = chunk_index + n_genes_max_chunk\n                    chunk.append(n_genes)\n                else:\n                    chunk.append(n_genes)\n                left = 0\n                # Calculate rank sums for each chunk for the current mask\n                for chunk_index, right in enumerate(chunk):\n                    # Check if issparse is true: AnnData objects are currently sparse.csr or ndarray.\n                    if issparse(X):\n                        df1 = pd.DataFrame(data=X[mask, left:right].todense())\n                        df2 = pd.DataFrame(data=X[mask_rest, left:right].todense(),\n                                           index=np.arange(start=n_active, stop=n_active + m_active))\n                    else:\n                        df1 = pd.DataFrame(data=X[mask, left:right])\n                        df2 = pd.DataFrame(data=X[mask_rest, left:right],\n                                           index=np.arange(start=n_active, stop=n_active + m_active))\n                    df1 = df1.append(df2)\n                    ranks = df1.rank()\n                    # sum up adjusted_ranks to calculate W_m,n\n                    scores[left:right] = np.sum(ranks.loc[0:n_active, :])\n                    left = right\n                scores = (scores - (n_active * (n_active + m_active + 1) / 2)) / sqrt(\n                    (n_active * m_active * (n_active + m_active + 1) / 12))\n                scores[np.isnan(scores)] = 0\n                pvals = 2 * stats.distributions.norm.sf(np.abs(scores))\n                if corr_method == 'benjamini-hochberg':\n                    pvals[np.isnan(pvals)] = 1  # set Nan values to 1 to properly convert using Benhjamini Hochberg\n                    _, pvals_adj, _, _ = multipletests(pvals, alpha=0.05, method='fdr_bh')\n                elif corr_method == 'bonferroni':\n                    pvals_adj = np.minimum(pvals * n_genes, 1.0)\n                # Fold change\n                foldchanges = (np.expm1(means[imask]) + 1e-9) / (np.expm1(mean_rest) + 1e-9)  # add small value to remove 0's\n                scores_sort = np.abs(scores) if rankby_abs else scores\n                partition = np.argpartition(scores_sort, -n_genes_user)[-n_genes_user:]\n                partial_indices = np.argsort(scores_sort[partition])[::-1]\n                global_indices = reference_indices[partition][partial_indices]\n                rankings_gene_scores.append(scores[global_indices])\n                rankings_gene_names.append(adata_comp.var_names[global_indices])\n                rankings_gene_logfoldchanges.append(np.log2(foldchanges[global_indices]))\n                rankings_gene_pvals.append(pvals[global_indices])\n                rankings_gene_pvals_adj.append(pvals_adj[global_indices])\n        # If no reference group exists, ranking needs only to be done once (full mask)\n        else:\n            scores = np.zeros((n_groups, n_genes))\n            chunk = []\n            n_cells = X.shape[0]\n            n_genes_max_chunk = floor(CONST_MAX_SIZE / n_cells)\n            if n_genes_max_chunk < n_genes:\n                chunk_index = n_genes_max_chunk\n                while chunk_index < n_genes:\n                    chunk.append(chunk_index)\n                    chunk_index = chunk_index + n_genes_max_chunk\n                chunk.append(n_genes)\n            else:\n                chunk.append(n_genes)\n            left = 0\n            for chunk_index, right in enumerate(chunk):\n                # Check if issparse is true\n                if issparse(X):\n                    df1 = pd.DataFrame(data=X[:, left:right].todense())\n                else:\n                    df1 = pd.DataFrame(data=X[:, left:right])\n                ranks = df1.rank()\n                # sum up adjusted_ranks to calculate W_m,n\n                for imask, mask in enumerate(groups_masks):\n                    scores[imask, left:right] = np.sum(ranks.loc[mask, :])\n                left = right\n            for imask, mask in enumerate(groups_masks):\n                mask_rest = ~groups_masks[imask]\n                means[imask], vars[imask] = _get_mean_var(X[mask]) #for fold-change\n                mean_rest, var_rest = _get_mean_var(X[mask_rest])  # for fold-change\n                scores[imask, :] = (scores[imask, :] - (ns[imask] * (n_cells + 1) / 2)) / sqrt(\n                    (ns[imask] * (n_cells - ns[imask]) * (n_cells + 1) / 12))\n                scores[np.isnan(scores)] = 0\n                pvals = 2 * stats.distributions.norm.sf(np.abs(scores[imask,:]))\n                if corr_method == 'benjamini-hochberg':\n                    pvals[np.isnan(pvals)] = 1  # set Nan values to 1 to properly convert using Benhjamini Hochberg\n                    _, pvals_adj, _, _ = multipletests(pvals, alpha=0.05, method='fdr_bh')\n                elif corr_method == 'bonferroni':\n                    pvals_adj = np.minimum(pvals * n_genes, 1.0)\n                # Fold change\n                foldchanges = (np.expm1(means[imask]) + 1e-9) / (np.expm1(mean_rest) + 1e-9)  # add small value to remove 0's\n                scores_sort = np.abs(scores) if rankby_abs else scores\n                partition = np.argpartition(scores_sort[imask, :], -n_genes_user)[-n_genes_user:]\n                partial_indices = np.argsort(scores_sort[imask, partition])[::-1]\n                global_indices = reference_indices[partition][partial_indices]\n                rankings_gene_scores.append(scores[imask, global_indices])\n                rankings_gene_names.append(adata_comp.var_names[global_indices])\n                rankings_gene_logfoldchanges.append(np.log2(foldchanges[global_indices]))\n                rankings_gene_pvals.append(pvals[global_indices])\n                rankings_gene_pvals_adj.append(pvals_adj[global_indices])\n    groups_order_save = [str(g) for g in groups_order]\n    if (reference != 'rest' and method != 'logreg') or (method == 'logreg' and len(groups) == 2):\n        groups_order_save = [g for g in groups_order if g != reference]\n    adata.uns[key_added]['scores'] = np.rec.fromarrays(\n        [n for n in rankings_gene_scores],\n        dtype=[(rn, 'float32') for rn in groups_order_save])\n    adata.uns[key_added]['names'] = np.rec.fromarrays(\n        [n for n in rankings_gene_names],\n        dtype=[(rn, 'U50') for rn in groups_order_save])\n    if method in {'t-test', 't-test_overestim_var', 'wilcoxon'}:\n        adata.uns[key_added]['logfoldchanges'] = np.rec.fromarrays(\n            [n for n in rankings_gene_logfoldchanges],\n            dtype=[(rn, 'float32') for rn in groups_order_save])\n        adata.uns[key_added]['pvals'] = np.rec.fromarrays(\n            [n for n in rankings_gene_pvals],\n            dtype=[(rn, 'float64') for rn in groups_order_save])\n        adata.uns[key_added]['pvals_adj'] = np.rec.fromarrays(\n            [n for n in rankings_gene_pvals_adj],\n            dtype=[(rn, 'float64') for rn in groups_order_save])\n    logg.info('    finished', time=True, end=' ' if _settings_verbosity_greater_or_equal_than(3) else '\\n')\n    logg.hint(\n        'added to `.uns[\\'{}\\']`\\n'\n        '    \\'names\\', sorted np.recarray to be indexed by group ids\\n'\n        '    \\'scores\\', sorted np.recarray to be indexed by group ids\\n'\n        .format(key_added)\n        + ('    \\'logfoldchanges\\', sorted np.recarray to be indexed by group ids\\n'\n           '    \\'pvals\\', sorted np.recarray to be indexed by group ids\\n'\n           '    \\'pvals_adj\\', sorted np.recarray to be indexed by group ids'\n           if method in {'t-test', 't-test_overestim_var', 'wilcoxon'} else ''))\n    return adata if copy else None", "idx": 72}
{"project": "Scanpy", "commit_id": "206_scanpy_1.9.0___init__.py_neighbors.py", "target": 0, "func": "def neighbors(\n    adata: AnnData,\n    n_neighbors: int = 15,\n    n_pcs: Optional[int] = None,\n    use_rep: Optional[str] = None,\n    knn: bool = True,\n    random_state: AnyRandom = 0,\n    method: Optional[_Method] = 'umap',\n    metric: Union[_Metric, _MetricFn] = 'euclidean',\n    metric_kwds: Mapping[str, Any] = MappingProxyType({}),\n    key_added: Optional[str] = None,\n    copy: bool = False,\n) -> Optional[AnnData]:\n    \"\"\"\\\n    Compute a neighborhood graph of observations [McInnes18]_.\n\n    The neighbor search efficiency of this heavily relies on UMAP [McInnes18]_,\n    which also provides a method for estimating connectivities of data points -\n    the connectivity of the manifold (`method=='umap'`). If `method=='gauss'`,\n    connectivities are computed according to [Coifman05]_, in the adaption of\n    [Haghverdi16]_.\n\n    Parameters\n    ----------\n    adata\n        Annotated data matrix.\n    n_neighbors\n        The size of local neighborhood (in terms of number of neighboring data\n        points) used for manifold approximation. Larger values result in more\n        global views of the manifold, while smaller values result in more local\n        data being preserved. In general values should be in the range 2 to 100.\n        If `knn` is `True`, number of nearest neighbors to be searched. If `knn`\n        is `False`, a Gaussian kernel width is set to the distance of the\n        `n_neighbors` neighbor.\n    {n_pcs}\n    {use_rep}\n    knn\n        If `True`, use a hard threshold to restrict the number of neighbors to\n        `n_neighbors`, that is, consider a knn graph. Otherwise, use a Gaussian\n        Kernel to assign low weights to neighbors more distant than the\n        `n_neighbors` nearest neighbor.\n    random_state\n        A numpy random seed.\n    method\n        Use 'umap' [McInnes18]_ or 'gauss' (Gauss kernel following [Coifman05]_\n        with adaptive width [Haghverdi16]_) for computing connectivities.\n        Use 'rapids' for the RAPIDS implementation of UMAP (experimental, GPU\n        only).\n    metric\n        A known metric\u2019s name or a callable that returns a distance.\n    metric_kwds\n        Options for the metric.\n    key_added\n        If not specified, the neighbors data is stored in .uns['neighbors'],\n        distances and connectivities are stored in .obsp['distances'] and\n        .obsp['connectivities'] respectively.\n        If specified, the neighbors data is added to .uns[key_added],\n        distances are stored in .obsp[key_added+'_distances'] and\n        connectivities in .obsp[key_added+'_connectivities'].\n    copy\n        Return a copy instead of writing to adata.\n\n    Returns\n    -------\n    Depending on `copy`, updates or returns `adata` with the following:\n\n    See `key_added` parameter description for the storage path of\n    connectivities and distances.\n\n    **connectivities** : sparse matrix of dtype `float32`.\n        Weighted adjacency matrix of the neighborhood graph of data\n        points. Weights should be interpreted as connectivities.\n    **distances** : sparse matrix of dtype `float32`.\n        Instead of decaying weights, this stores distances for each pair of\n        neighbors.\n\n    Notes\n    -----\n    If `method='umap'`, it's highly recommended to install pynndescent ``pip install pynndescent``.\n    Installing `pynndescent` can significantly increase performance,\n    and in later versions it will become a hard dependency.\n    \"\"\"\n    start = logg.info('computing neighbors')\n    adata = adata.copy() if copy else adata\n    if adata.is_view:  # we shouldn't need this here...\n        adata._init_as_actual(adata.copy())\n    neighbors = Neighbors(adata)\n    neighbors.compute_neighbors(\n        n_neighbors=n_neighbors,\n        knn=knn,\n        n_pcs=n_pcs,\n        use_rep=use_rep,\n        method=method,\n        metric=metric,\n        metric_kwds=metric_kwds,\n        random_state=random_state,\n    )\n\n    if key_added is None:\n        key_added = 'neighbors'\n        conns_key = 'connectivities'\n        dists_key = 'distances'\n    else:\n        conns_key = key_added + '_connectivities'\n        dists_key = key_added + '_distances'\n\n    adata.uns[key_added] = {}\n\n    neighbors_dict = adata.uns[key_added]\n\n    neighbors_dict['connectivities_key'] = conns_key\n    neighbors_dict['distances_key'] = dists_key\n\n    neighbors_dict['params'] = {'n_neighbors': neighbors.n_neighbors, 'method': method}\n    neighbors_dict['params']['random_state'] = random_state\n    neighbors_dict['params']['metric'] = metric\n    if metric_kwds:\n        neighbors_dict['params']['metric_kwds'] = metric_kwds\n    if use_rep is not None:\n        neighbors_dict['params']['use_rep'] = use_rep\n    if n_pcs is not None:\n        neighbors_dict['params']['n_pcs'] = n_pcs\n\n    adata.obsp[dists_key] = neighbors.distances\n    adata.obsp[conns_key] = neighbors.connectivities\n\n    if neighbors.rp_forest is not None:\n        neighbors_dict['rp_forest'] = neighbors.rp_forest\n    logg.info(\n        '    finished',\n        time=start,\n        deep=(\n            f'added to `.uns[{key_added!r}]`\\n'\n            f'    `.obsp[{dists_key!r}]`, distances for each pair of neighbors\\n'\n            f'    `.obsp[{conns_key!r}]`, weighted adjacency matrix'\n        ),\n    )\n    return adata if copy else None", "idx": 79}
{"project": "Scanpy", "commit_id": "770_scanpy_1.3.7_scanpy_preprocessing__qc.py_calculate_qc_metrics.py", "target": 1, "func": "def calculate_qc_metrics(adata, expr_type=\"counts\", var_type=\"genes\", qc_vars=(),\n                         percent_top=(50, 100, 200, 500), inplace=False):\n    \"\"\"\n    Calculate quality control metrics.\n    Calculates a number of qc metrics for an AnnData object, largely based on\n    `calculateQCMetrics` from scater [McCarthy17]_. Currently is most efficient\n    on a sparse CSR or dense matrix.\n    Parameters\n    ----------\n    adata : :class:`~anndata.AnnData`\n        Annotated data matrix.\n    expr_type : `str`, optional (default: `\"counts\"`)\n        Name of kind of values in X.\n    var_type : `str`, optional (default: `\"genes\"`)\n        The kind of thing the variables are.\n    qc_vars : `Container`, optional (default: `()`)\n        Keys for boolean columns of `.var` which identify variables you could\n        want to control for (e.g. \"ERCC\" or \"mito\").\n    percent_top : `Container[int]`, optional (default: `(50, 100, 200, 500)`)\n        Which proportions of top genes to cover. If empty or `None` don't\n        calculate.\n    inplace : bool, optional (default: `False`)\n        Whether to place calculated metrics in `.obs` and `.var`\n    Returns\n    -------\n    Union[NoneType, Tuple[pd.DataFrame, pd.DataFrame]]\n        Depending on `inplace` returns calculated metrics (`pd.DataFrame`) or\n        updates `adata`'s `obs` and `var`.\n        Observation level metrics include:\n        * `total_{var_type}_by_{expr_type}`\n        * `total_{expr_type}`\n        * `pct_{expr_type}_in_top_{n}_{var_type}` - for `n` in `percent_top`\n        * `total_{expr_type}_{qc_var}` - for `qc_var` in `qc_vars`\n        * `pct_{expr_type}_{qc_var}` - for `qc_var` in `qc_vars`\n        Variable level metrics include:\n        * `total_{expr_type}`\n        * `mean_{expr_type}`\n        * `n_cells_by_{expr_type}`\n        * `pct_dropout_by_{expr_type}`\n    \"\"\"\n    X = adata.X\n    obs_metrics = pd.DataFrame(index=adata.obs_names)\n    var_metrics = pd.DataFrame(index=adata.var_names)\n    if isspmatrix_coo(X):\n        X = csr_matrix(X)  # COO not subscriptable\n    if issparse(X):\n        X.eliminate_zeros()\n    # Calculate obs metrics\n    if issparse(X):\n        obs_metrics[\"n_{var_type}_by_{expr_type}\"] = X.getnnz(axis=1)\n    else:\n        obs_metrics[\"n_{var_type}_by_{expr_type}\"] = np.count_nonzero(X, axis=1)\n    obs_metrics[\"log1p_n_{var_type}_by_{expr_type}\"] = np.log1p(\n        obs_metrics[\"n_{var_type}_by_{expr_type}\"])\n    obs_metrics[\"total_{expr_type}\"] = X.sum(axis=1)\n    obs_metrics[\"log1p_total_{expr_type}\"] = np.log1p(\n        obs_metrics[\"total_{expr_type}\"])\n    proportions = top_segment_proportions(X, percent_top)\n    # Since there are local loop variables, formatting must occur in their scope\n    # Probably worth looking into a python3.5 compatable way to make this better\n    for i, n in enumerate(percent_top):\n        obs_metrics[\"pct_{expr_type}_in_top_{n}_{var_type}\".format(**locals())] = \\\n            proportions[:, i] * 100\n    for qc_var in qc_vars:\n        obs_metrics[\"total_{expr_type}_{qc_var}\".format(**locals())] = \\\n            X[:, adata.var[qc_var].values].sum(axis=1)\n        obs_metrics[\"log1p_total_{expr_type}_{qc_var}\".format(**locals())] = \\\n            np.log1p(\n                obs_metrics[\"total_{expr_type}_{qc_var}\".format(**locals())])\n        # \"total_{expr_type}\" not formatted yet\n        obs_metrics[\"pct_{expr_type}_{qc_var}\".format(**locals())] = \\\n            obs_metrics[\"total_{expr_type}_{qc_var}\".format(**locals())] / \\\n            obs_metrics[\"total_{expr_type}\"] * 100\n    # Calculate var metrics\n    if issparse(X):\n        # Current memory bottleneck for csr matrices:\n        var_metrics[\"n_cells_by_{expr_type}\"] = X.getnnz(axis=0)\n        var_metrics[\"mean_{expr_type}\"] = mean_variance_axis(X, axis=0)[0]\n    else:\n        var_metrics[\"n_cells_by_{expr_type}\"] = np.count_nonzero(X, axis=0)\n        var_metrics[\"mean_{expr_type}\"] = X.mean(axis=0)\n    var_metrics[\"log1p_mean_{expr_type}\"] = np.log1p(\n        var_metrics[\"mean_{expr_type}\"])\n    var_metrics[\"pct_dropout_by_{expr_type}\"] = \\\n        (1 - var_metrics[\"n_cells_by_{expr_type}\"] / X.shape[0]) * 100\n    var_metrics[\"total_{expr_type}\"] = np.ravel(X.sum(axis=0))\n    var_metrics[\"log1p_total_{expr_type}\"] = np.log1p(\n        var_metrics[\"total_{expr_type}\"])\n    # Format strings\n    for df in obs_metrics, var_metrics:\n        new_colnames = []\n        for col in df.columns:\n            new_colnames.append(col.format(**locals()))\n        df.columns = new_colnames\n    # Return\n    if inplace:\n        adata.obs[obs_metrics.columns] = obs_metrics\n        adata.var[var_metrics.columns] = var_metrics\n    else:\n        return obs_metrics, var_metrics", "idx": 80}
{"project": "Scanpy", "commit_id": "219_scanpy_1.9.0___init__.py___getitem__.py", "target": 0, "func": "def __getitem__(self, index):\n        if isinstance(index, (int, np.integer)):\n            if self.restrict_array is None:\n                glob_index = index\n            else:\n                # map the index back to the global index\n                glob_index = self.restrict_array[index]\n            if glob_index not in self.rows:\n                self.rows[glob_index] = self.get_row(glob_index)\n            row = self.rows[glob_index]\n            if self.restrict_array is None:\n                return row\n            else:\n                return row[self.restrict_array]\n        else:\n            if self.restrict_array is None:\n                glob_index_0, glob_index_1 = index\n            else:\n                glob_index_0 = self.restrict_array[index[0]]\n                glob_index_1 = self.restrict_array[index[1]]\n            if glob_index_0 not in self.rows:\n                self.rows[glob_index_0] = self.get_row(glob_index_0)\n            return self.rows[glob_index_0][glob_index_1]", "idx": 81}
{"project": "Scanpy", "commit_id": "726_scanpy_1.9.0_test_read_10x.py_assert_anndata_equal.py", "target": 0, "func": "def assert_anndata_equal(a1, a2):\n    assert a1.shape == a2.shape\n    assert (a1.obs == a2.obs).all(axis=None)\n    assert (a1.var == a2.var).all(axis=None)\n    assert np.allclose(a1.X.todense(), a2.X.todense())", "idx": 84}
{"project": "Scanpy", "commit_id": "834_scanpy_1.4_scanpy_plotting__anndata.py_correlation_matrix.py", "target": 1, "func": "def correlation_matrix(adata, groupby, show_correlation_numbers=False, dendrogram=True, figsize=None,\n                       show=None, save=None, **kwds):\n    \"\"\"\n    Parameters\n    ----------\n    adata : :class:`~anndata.AnnData`\n    groupby : `str`\n        Categorical data column used to create the dendrogram\n    show_correlation_numbers : `bool`, optional(default: `False`)\n        If `show_correlation` is True, plot the correlation number on top of each cell.\n    dendrogram: `bool` or `str`, optional (default: `False`)\n        If True or a valid dendrogram key, a dendrogram based on the hierarchical clustering\n        between the `groupby` categories is added. The dendrogram information is computed\n        using :ref:`scanpy.tl.dendrogram`. If `tl.dendrogram` has not been called previously\n        the function is called with default parameters.\n    figsize : (`float`, `float`), optional (default: `None`)\n        By default a figure size that aims to produce a squared correlation matrix plot is used.\n        Format is (width, height)\n    {show_save_ax}\n    **kwds : keyword arguments\n        Only if `show_correlation` is True: Are passed to `matplotlib.pyplot.pcolormesh` when plotting\n        the correlation heatmap. Useful values to pas are `vmax`, `vmin` and `cmap`.\n    Returns\n    -------\n    Examples\n    --------\n    >>> adata = sc.datasets.pbmc68k_reduced()\n    >>> sc.tl.dendrogram(adata, 'bulk_labels')\n    >>> sc.pl.correlation(adata, 'bulk_labels')\n    \"\"\"\n    dendrogram_key = _get_dendrogram_key(adata, dendrogram, groupby)\n    index = adata.uns[dendrogram_key]['categories_idx_ordered']\n    corr_matrix = adata.uns[dendrogram_key]['correlation_matrix']\n    # reorder matrix columns according to the dendrogram\n    assert(len(index)) == corr_matrix.shape[0]\n    corr_matrix = corr_matrix[index, :]\n    corr_matrix = corr_matrix[:, index]\n    num_rows = len(index)\n    labels = list(adata.obs[groupby].cat.categories)\n    colorbar_height = 0.2\n    if dendrogram:\n        dendrogram_width = 1.8\n    else:\n        dendrogram_width = 0\n    if figsize is None:\n        corr_matrix_height = num_rows * 0.6\n        height = corr_matrix_height + colorbar_height\n        width = corr_matrix_height + dendrogram_width\n    else:\n        width, height = figsize\n\n    fig = pl.figure(figsize=(width, height))\n    # layout with 2 rows and 2  columns:\n    # row 1: dendrogram + correlation matrix\n    # row 2: nothing + colormap bar (horizontal)\n    gs = gridspec.GridSpec(nrows=2, ncols=2, width_ratios=[dendrogram_width, corr_matrix_height],\n                           height_ratios=[corr_matrix_height, colorbar_height], wspace=0.01, hspace=0.05)\n    axs = []\n    corr_matrix_ax = fig.add_subplot(gs[1])\n    if dendrogram:\n        dendro_ax = fig.add_subplot(gs[0], sharey=corr_matrix_ax)\n        _plot_dendrogram(dendro_ax, adata, groupby, dendrogram_key=dendrogram_key,\n                         remove_labels=True, orientation='left',\n                         ticks=np.arange(corr_matrix .shape[0]) + 0.5)\n        axs.append(dendro_ax)\n    # define some default pcolormesh parameters\n    if 'edge_color' not in kwds:\n        if corr_matrix.shape[0] > 30:\n            # when there are too many rows it is better to remove\n            # the black lines surrounding the boxes in the heatmap\n            kwds['edgecolors'] = 'none'\n        else:\n            kwds['edgecolors'] = 'black'\n            kwds['linewidth'] = 0.01\n    if 'vmax' not in kwds and 'vmin' not in kwds:\n        kwds['vmax'] = 1\n        kwds['vmin'] = -1\n    if 'cmap' not in kwds:\n        # by default use a diverget color map\n        kwds['cmap'] = 'bwr'\n    img_mat = corr_matrix_ax.pcolormesh(corr_matrix, **kwds)\n    corr_matrix_ax.set_xlim(0, num_rows)\n    corr_matrix_ax.set_ylim(0, num_rows)\n    corr_matrix_ax.yaxis.tick_right()\n    corr_matrix_ax.set_yticks(np.arange(corr_matrix .shape[0]) + 0.5)\n    corr_matrix_ax.set_yticklabels(np.array(labels).astype('str')[index])\n    corr_matrix_ax.xaxis.set_tick_params(labeltop=True)\n    corr_matrix_ax.xaxis.set_tick_params(labelbottom=False)\n    corr_matrix_ax.set_xticks(np.arange(corr_matrix .shape[0]) + 0.5)\n    corr_matrix_ax.set_xticklabels(np.array(labels).astype('str')[index], rotation=45, ha='left')\n    corr_matrix_ax.tick_params(\n        axis='x',\n        which='both',\n        bottom=False,\n        top=False)\n    corr_matrix_ax.tick_params(\n        axis='y',\n        which='both',\n        left=False,\n        right=False)\n    print(corr_matrix_ax.get_ylim())\n    if show_correlation_numbers:\n        for row in range(num_rows):\n            for col in range(num_rows):\n                corr_matrix_ax.text(row + 0.5, col + 0.5,\n                              \"{:.2f}\".format(corr_matrix[row, col]),\n                              ha='center', va='center')\n    axs.append(corr_matrix_ax)\n    # Plot colorbar\n    colormap_ax = fig.add_subplot(gs[3])\n    cobar = pl.colorbar(img_mat, cax=colormap_ax, orientation='horizontal')\n    cobar.solids.set_edgecolor(\"face\")\n    axs.append(colormap_ax)\n    return axs", "idx": 87}
{"project": "Scanpy", "commit_id": "178_scanpy_1.9.0__wishbone.py_wishbone.py", "target": 0, "func": "def wishbone(\n    adata: AnnData,\n    start_cell: str,\n    branch: bool = True,\n    k: int = 15,\n    components: Iterable[int] = (1, 2, 3),\n    num_waypoints: Union[int, Collection] = 250,\n):\n    \"\"\"\\\n    Wishbone identifies bifurcating developmental trajectories from single-cell data\n    [Setty16]_.\n\n    Wishbone is an algorithm for positioning single cells along bifurcating\n    developmental trajectories with high resolution. Wishbone uses multi-dimensional\n    single-cell data, such as mass cytometry or RNA-Seq data, as input and orders cells\n    according to their developmental progression, and it pinpoints bifurcation points\n    by labeling each cell as pre-bifurcation or as one of two post-bifurcation cell\n    fates.\n\n    .. note::\n       More information and bug reports `here\n       <https://github.com/dpeerlab/wishbone>`__.\n\n    Parameters\n    ----------\n    adata\n        Annotated data matrix.\n    start_cell\n        Desired start cell from `obs_names`.\n    branch\n        Use True for Wishbone and False for Wanderlust.\n    k\n        Number of nearest neighbors for graph construction.\n    components\n        Components to use for running Wishbone.\n    num_waypoints\n        Number of waypoints to sample.\n\n    Returns\n    -------\n    Updates `adata` with the following fields:\n\n    `trajectory_wishbone` : (`adata.obs`, dtype `float64`)\n        Computed trajectory positions.\n    `branch_wishbone` : (`adata.obs`, dtype `int64`)\n        Assigned branches.\n\n    Example\n    -------\n\n    >>> import scanpy.external as sce\n    >>> import scanpy as sc\n\n    **Loading Data and Pre-processing**\n\n    >>> adata = sc.datasets.pbmc3k()\n    >>> sc.pp.normalize_per_cell(adata)\n    >>> sc.pp.pca(adata)\n    >>> sc.tl.tsne(adata=adata, n_pcs=5, perplexity=30)\n    >>> sc.pp.neighbors(adata, n_pcs=15, n_neighbors=10)\n    >>> sc.tl.diffmap(adata, n_comps=10)\n\n    **Running Wishbone Core Function**\n\n    Usually, the start cell for a dataset should be chosen based on high expression of\n    the gene of interest:\n\n    >>> sce.tl.wishbone(\n    ...     adata=adata, start_cell='ACAAGAGACTTATC-1',\n    ...     components=[2, 3], num_waypoints=150,\n    ... )\n\n    **Visualizing Wishbone results**\n\n    >>> sc.pl.tsne(adata, color=['trajectory_wishbone', 'branch_wishbone'])\n    >>> markers = ['C1QA', 'PSAP', 'CD79A', 'CD79B', 'CST3', 'LYZ', 'MALAT1']\n    >>> sce.pl.wishbone_marker_trajectory(adata, markers, show=True)\n\n    For further demonstration of Wishbone methods and visualization please follow the\n    notebooks in the package `Wishbone_for_single_cell_RNAseq.ipynb\n    <https://github.com/dpeerlab/wishbone/tree/master/notebooks>`_.\\\n    \"\"\"\n    try:\n        from wishbone.core import wishbone as c_wishbone\n    except ImportError:\n        raise ImportError(\n            \"\\nplease install wishbone:\\n\\n\\thttps://github.com/dpeerlab/wishbone\"\n        )\n\n    # Start cell index\n    s = np.where(adata.obs_names == start_cell)[0]\n    if len(s) == 0:\n        raise RuntimeError(\n            f\"Start cell {start_cell} not found in data. \"\n            \"Please rerun with correct start cell.\"\n        )\n    if isinstance(num_waypoints, cabc.Collection):\n        diff = np.setdiff1d(num_waypoints, adata.obs.index)\n        if diff.size > 0:\n            logging.warning(\n                \"Some of the specified waypoints are not in the data. \"\n                \"These will be removed\"\n            )\n            num_waypoints = diff.tolist()\n    elif num_waypoints > adata.shape[0]:\n        raise RuntimeError(\n            \"num_waypoints parameter is higher than the number of cells in the \"\n            \"dataset. Please select a smaller number\"\n        )\n    s = s[0]\n\n    # Run the algorithm\n    components = list(components)\n    res = c_wishbone(\n        adata.obsm['X_diffmap'][:, components],\n        s=s,\n        k=k,\n        l=k,\n        num_waypoints=num_waypoints,\n        branch=branch,\n    )\n\n    # Assign results\n    trajectory = res[\"Trajectory\"]\n    trajectory = (trajectory - np.min(trajectory)) / (\n        np.max(trajectory) - np.min(trajectory)\n    )\n    adata.obs['trajectory_wishbone'] = np.asarray(trajectory)\n\n    # branch_ = None\n    if branch:\n        branches = res[\"Branches\"].astype(int)\n        adata.obs['branch_wishbone'] = np.asarray(branches)", "idx": 89}
{"project": "Scanpy", "commit_id": "130_scanpy_0.0_scanpy_readwrite.py_postprocess_reading.py", "target": 1, "func": "def postprocess_reading(key, value):\n    if value.dtype.kind == 'S':\n        value = value.astype(str)\n    # get back dictionaries\n    if key.endswith('_ann'):\n        dd = {}\n        for row in value:\n            # the type is stored after the \"_\"\n            t = row[0].split('_')[-1]\n            k = '_'.join(row[0].split('_')[:-1])\n            dd[k] = row[1:].astype(t)\n        return key[:-4], dd\n    else:\n        return key, value", "idx": 95}
{"project": "Scanpy", "commit_id": "592_scanpy_1.9.0_test_neighbors.py_test_umap_connectivities_euclidean.py", "target": 0, "func": "def test_umap_connectivities_euclidean(neigh):\n    neigh.compute_neighbors(method='umap', n_neighbors=n_neighbors)\n    assert np.allclose(neigh.distances.toarray(), distances_euclidean)\n    assert np.allclose(neigh.connectivities.toarray(), connectivities_umap)\n    neigh.compute_transitions()\n    assert np.allclose(neigh.transitions_sym.toarray(), transitions_sym_umap)\n    assert np.allclose(neigh.transitions.toarray(), transitions_umap)", "idx": 101}
{"project": "Scanpy", "commit_id": "651_scanpy_1.2.0_scanpy_plotting_tools___init__.py_rank_genes_groups.py", "target": 1, "func": "def rank_genes_groups(adata, groups=None, n_genes=20, gene_symbols=None, key=None, fontsize=8, show=None, save=None, ext=None):\n    \"\"\"Plot ranking of genes.\n    Parameters\n    ----------\n    adata : :class:`~scanpy.api.AnnData`\n        Annotated data matrix.\n    groups : `str` or `list` of `str`\n        The groups for which to show the gene ranking.\n    gene_symbols : `str`\n        Key for field in `.var` that stores gene symbols if you do not want to\n        use `.var_names`.\n    n_genes : `int`, optional (default: 20)\n        Number of genes to show.\n    fontsize : `int`, optional (default: 8)\n        Fontsize for gene names.\n    show : `bool`, optional (default: `None`)\n        Show the plot, do not return axis.\n    save : `bool` or `str`, optional (default: `None`)\n        If `True` or a `str`, save the figure. A string is appended to the\n        default filename. Infer the filetype if ending on {{'.pdf', '.png', '.svg'}}.\n    ax : `matplotlib.Axes`, optional (default: `None`)\n        A `matplotlib.Axes` object.\n    \"\"\"\n    if key is None:\n        key = 'rank_genes_groups'\n    groups_key = str(adata.uns[key]['params']['groupby'])\n    reference = str(adata.uns[key]['params']['reference'])\n    group_names = (adata.uns[key]['names'].dtype.names\n                   if groups is None else groups)\n    # one panel for each group\n    n_panels = len(group_names)\n    # set up the figure\n    if n_panels <= 5:\n        n_panels_y = 1\n        n_panels_x = n_panels\n    else:\n        n_panels_y = 2\n        n_panels_x = int(n_panels/2+0.5)\n    from matplotlib import gridspec\n    fig = pl.figure(figsize=(n_panels_x * rcParams['figure.figsize'][0],\n                             n_panels_y * rcParams['figure.figsize'][1]))\n    left = 0.2/n_panels_x\n    bottom = 0.13/n_panels_y\n    gs = gridspec.GridSpec(nrows=n_panels_y,\n                           ncols=n_panels_x,\n                           left=left,\n                           right=1-(n_panels_x-1)*left-0.01/n_panels_x,\n                           bottom=bottom,\n                           top=1-(n_panels_y-1)*bottom-0.1/n_panels_y,\n                           wspace=0.18)\n    for count, group_name in enumerate(group_names):\n        pl.subplot(gs[count])\n        gene_names = adata.uns[key]['names'][group_name]\n        scores = adata.uns[key]['scores'][group_name]\n        for ig, g in enumerate(gene_names[:n_genes]):\n            gene_name = gene_names[ig]\n            pl.text(ig, scores[ig], gene_name if gene_symbols is None else adata.var[gene_symbols][gene_name],\n                    rotation='vertical', verticalalignment='bottom',\n                    horizontalalignment='center', fontsize=fontsize)\n        pl.title('{} vs. {}'.format(group_name, reference))\n        if n_panels <= 5 or count >= n_panels_x: pl.xlabel('ranking')\n        if count == 0 or count == n_panels_x: pl.ylabel('score')\n        ymin = np.min(scores)\n        ymax = np.max(scores)\n        ymax += 0.3*(ymax-ymin)\n        pl.ylim([ymin, ymax])\n        pl.xlim(-0.9, ig+1-0.1)\n    writekey = ('rank_genes_groups_'\n                + str(adata.uns[key]['params']['groupby']))\n    utils.savefig_or_show(writekey, show=show, save=save)", "idx": 102}
{"project": "Scanpy", "commit_id": "581_scanpy_1.0.4_scanpy_tools_rna_velocity.py_rna_velocity.py", "target": 1, "func": "def rna_velocity(adata, loomfile):\n\n    # this is n_genes x n_cells\n    ds = loompy.connect(self.loom_filepath)\n    X_spliced = ds.layer['spliced'][:, :]\n    X_unspliced = ds.layer['unspliced'][:, :]\n    # X_ambiguous = ds.layer['ambiguous'][:, :]\n\n    # for now, take non-normalized values\n    s = X_spliced\n    u = X_unspliced\n\n    # loop over genes\n    q = np.zeros(s.shape[0], dtype='float32')\n    gammas = np.zeros(s.shape[0], dtype='float32')\n    R2 = np.zeros(s.shape[0], dtype='float32')\n    for i in range(s.shape[0]):\n        m = opt.minimize(\n            lambda m: np.sum((-u[i] + s[i] * m[0] + m[1])**2), x0=(0.1, 1e-16),\n            method='L-BFGS-B', bounds=[(1e-8, 30), (0, 1.5)]).x\n        gammas[i] = m[0]\n        q[i] = m[1]\n        R2[i] = 1 - (np.sum((gammas[i] * s[i] + q[i] - u[i])**2)\n                     / np.sum((u[i].mean() - u[i])**2))\n\n    velocity = u - (gammas[:, None] * s + q[:, None])", "idx": 111}
{"project": "Scanpy", "commit_id": "1107_scanpy_1.9.0_scanpy_plotting__tools_paga.py_paga_compare.py", "target": 1, "func": "def paga_compare(\n    adata: AnnData,\n    basis=None,\n    edges=False,\n    color=None,\n    alpha=None,\n    groups=None,\n    components=None,\n    projection: Literal['2d', '3d'] = '2d',\n    legend_loc='on data',\n    legend_fontsize: Union[int, float, _FontSize, None] = None,\n    legend_fontweight: Union[int, _FontWeight] = 'bold',\n    legend_fontoutline=None,\n    color_map=None,\n    palette=None,\n    frameon=False,\n    size=None,\n    title=None,\n    right_margin=None,\n    left_margin=0.05,\n    show=None,\n    save=None,\n    title_graph=None,\n    groups_graph=None,\n    *,\n    pos=None,\n    **paga_graph_params,\n):\n    \"\"\"\\\n    Scatter and PAGA graph side-by-side.\n    Consists in a scatter plot and the abstracted graph. See\n    :func:`~scanpy.pl.paga` for all related parameters.\n    See :func:`~scanpy.pl.paga_path` for visualizing gene changes along paths\n    through the abstracted graph.\n    Additional parameters are as follows.\n    Parameters\n    ----------\n    adata\n        Annotated data matrix.\n    kwds_scatter\n        Keywords for :func:`~scanpy.pl.scatter`.\n    kwds_paga\n        Keywords for :func:`~scanpy.pl.paga`.\n    Returns\n    -------\n    A list of :class:`~matplotlib.axes.Axes` if `show` is `False`.\n    \"\"\"\n    axs, _, _, _ = _utils.setup_axes(\n        panels=[0, 1],\n        right_margin=right_margin,\n    )\n    if color is None:\n        color = adata.uns['paga']['groups']\n    suptitle = None  # common title for entire figure\n    if title_graph is None:\n        suptitle = color if title is None else title\n        title, title_graph = '', ''\n    if basis is None:\n        if 'X_draw_graph_fa' in adata.obsm:\n            basis = 'draw_graph_fa'\n        elif 'X_umap' in adata.obsm:\n            basis = 'umap'\n        elif 'X_tsne' in adata.obsm:\n            basis = 'tsne'\n        elif 'X_draw_graph_fr' in adata.obsm:\n            basis = 'draw_graph_fr'\n        else:\n            basis = 'umap'\n\n    from .scatterplots import embedding, _get_data_points\n\n    embedding(\n        adata,\n        ax=axs[0],\n        basis=basis,\n        color=color,\n        edges=edges,\n        alpha=alpha,\n        groups=groups,\n        components=components,\n        legend_loc=legend_loc,\n        legend_fontsize=legend_fontsize,\n        legend_fontweight=legend_fontweight,\n        legend_fontoutline=legend_fontoutline,\n        color_map=color_map,\n        palette=palette,\n        frameon=frameon,\n        size=size,\n        title=title,\n        show=False,\n        save=False,\n    )\n\n    if pos is None:\n        if color == adata.uns['paga']['groups']:\n            coords = _get_data_points(\n                adata, basis, projection=\"2d\", components=components, scale_factor=None\n            )[0][0]\n            pos = (\n                pd.DataFrame(coords, columns=[\"x\", \"y\"], index=adata.obs_names)\n                .groupby(adata.obs[color], observed=True)\n                .median()\n                .sort_index()\n            ).to_numpy()\n        else:\n            pos = adata.uns['paga']['pos']\n    xlim, ylim = axs[0].get_xlim(), axs[0].get_ylim()\n    axs[1].set_xlim(xlim)\n    axs[1].set_ylim(ylim)\n    if 'labels' in paga_graph_params:\n        labels = paga_graph_params.pop('labels')\n    else:\n        labels = groups_graph\n    if legend_fontsize is not None:\n        paga_graph_params['fontsize'] = legend_fontsize\n    if legend_fontweight is not None:\n        paga_graph_params['fontweight'] = legend_fontweight\n    if legend_fontoutline is not None:\n        paga_graph_params['fontoutline'] = legend_fontoutline\n    paga(\n        adata,\n        ax=axs[1],\n        show=False,\n        save=False,\n        title=title_graph,\n        labels=labels,\n        colors=color,\n        frameon=frameon,\n        pos=pos,\n        **paga_graph_params,\n    )\n    if suptitle is not None:\n        pl.suptitle(suptitle)\n    _utils.savefig_or_show('paga_compare', show=show, save=save)\n    if show is False:\n        return axs", "idx": 115}
{"project": "Scanpy", "commit_id": "671_scanpy_1.2.2_scanpy_preprocessing_simple.py_filter_genes.py", "target": 1, "func": "def filter_genes(data, min_counts=None, min_cells=None, max_counts=None,\n                 max_cells=None, copy=False):\n    \"\"\"Filter genes based on number of cells or counts.\n    Keep genes that have at least `min_counts` counts or are expressed in at\n    least `min_cells` cells or have at most `max_counts` counts or are expressed\n    in at most `max_cells` cells.\n    Only provide one of the optional parameters `min_counts`, `min_cells`,\n    `max_counts`, `max_cells` per call.\n    Parameters\n    ----------\n    data : :class:`~anndata.AnnData`, `np.ndarray`, `sp.spmatrix`\n        The (annotated) data matrix of shape `n_obs` \u00d7 `n_vars`. Rows correspond\n        to cells and columns to genes.\n    min_counts : `int`, optional (default: `None`)\n        Minimum number of counts required for a cell to pass filtering.\n    min_cells : `int`, optional (default: `None`)\n        Minimum number of cells expressed required for a cell to pass filtering.\n    max_counts : `int`, optional (default: `None`)\n        Maximum number of counts required for a cell to pass filtering.\n    max_cells : `int`, optional (default: `None`)\n        Maximum number of cells expressed required for a cell to pass filtering.\n    copy : `bool`, optional (default: `False`)\n        If an :class:`~anndata.AnnData` is passed, determines whether a copy\n        is returned.\n    Returns\n    -------\n    If `data` is an :class:`~anndata.AnnData`, filters the object and adds\\\n    either `n_cells` or `n_counts` to `adata.var`. Otherwise a tuple\n    gene_subset : `np.ndarray`\n        Boolean index mask that does filtering. `True` means that the gene is\n        kept. `False` means the gene is removed.\n    number_per_cell : `np.ndarray`\n        Either `n_counts` or `n_cells` per cell.\n    \"\"\"\n    n_given_options = sum(\n        option is not None for option in\n        [min_cells, min_counts, max_cells, max_counts])\n    if n_given_options != 1:\n        raise ValueError(\n            'Only provide one of the optional parameters `min_counts`,'\n            '`min_cells`, `max_counts`, `max_cells` per call.')\n    if isinstance(data, AnnData):\n        adata = data.copy() if copy else data\n        gene_subset, number = filter_genes(adata.X, min_cells=min_cells,\n                                           min_counts=min_counts, max_cells=max_cells,\n                                           max_counts=max_counts)\n        if min_cells is None and max_cells is None:\n            adata.var['n_counts'] = number\n        else:\n            adata.var['n_cells'] = number\n        adata._inplace_subset_var(gene_subset)\n        return adata if copy else None\n    X = data  # proceed with processing the data matrix\n    min_number = min_counts if min_cells is None else min_cells\n    max_number = max_counts if max_cells is None else max_cells\n    number_per_gene = np.sum(X if min_cells is None and max_cells is None\n                             else X > 0, axis=0)\n    if issparse(X):\n        number_per_gene = number_per_gene.A1\n    if min_number is not None:\n        gene_subset = number_per_gene >= min_number\n    if max_number is not None:\n        gene_subset = number_per_gene <= max_number\n\n    s = np.sum(~gene_subset)\n    logg.msg('filtered out {} genes that are detected'.format(s), end=' ', v=4)\n    if min_cells is not None or min_counts is not None:\n        logg.msg('in less than',\n               str(min_cells) + ' cells'\n               if min_counts is None else str(min_counts) + ' counts', v=4, no_indent=True)\n    if max_cells is not None or max_counts is not None:\n        logg.msg('in more than ',\n               str(max_cells) + ' cells'\n               if max_counts is None else str(max_counts) + ' counts', v=4, no_indent=True)\n    return gene_subset, number_per_gene", "idx": 122}
{"project": "Scanpy", "commit_id": "854_scanpy_1.4_scanpy_plotting__anndata.py_stacked_violin.py", "target": 1, "func": "def stacked_violin(adata, var_names, groupby=None, log=False, use_raw=None, num_categories=7,\n                   figsize=None,  dendrogram=False, gene_symbols=None,\n                   var_group_positions=None, var_group_labels=None, standard_scale=None,\n                   var_group_rotation=None, layer=None, stripplot=False, jitter=False, size=1,\n                   scale='width', order=None, swap_axes=False, show=None, save=None,\n                   row_palette='muted', **kwds):\n    \"\"\"\\\n    Stacked violin plots.\n    Makes a compact image composed of individual violin plots (from `seaborn.violinplot`)\n    stacked on top of each other. Useful to visualize gene expression per cluster.\n    Wraps `seaborn.violinplot` for :class:`~anndata.AnnData`.\n    Parameters\n    ----------\n    {common_plot_args}\n    stripplot : `bool` optional (default: `True`)\n        Add a stripplot on top of the violin plot.\n        See `seaborn.stripplot`.\n    jitter : `float` or `bool`, optional (default: `True`)\n        Add jitter to the stripplot (only when stripplot is True)\n        See `seaborn.stripplot`.\n    size : int, optional (default: 1)\n        Size of the jitter points.\n    order : list of str, optional (default: `True`)\n        Order in which to show the categories.\n    scale : {{'area', 'count', 'width'}}, optional (default: 'width')\n        The method used to scale the width of each violin. If 'area', each\n        violin will have the same area. If 'count', the width of the violins\n        will be scaled by the number of observations in that bin. If 'width',\n        each violin will have the same width.\n    row_palette: `str` (default: `muted`)\n        The row palette determines the colors to use in each of the stacked violin plots. The value\n        should be a valid seaborn palette name or a valic matplotlib colormap\n        (see https://seaborn.pydata.org/generated/seaborn.color_palette.html). Alternatively,\n        a single color name or hex value can be passed. E.g. 'red' or '#cc33ff'\n    standard_scale : {{'var', 'obs'}}, optional (default: None)\n        Whether or not to standardize that dimension between 0 and 1, meaning for each variable or observation,\n        subtract the minimum and divide each by its maximum.\n    swap_axes: `bool`, optional (default: `False`)\n         By default, the x axis contains `var_names` (e.g. genes) and the y axis the `groupby` categories.\n         By setting `swap_axes` then x are the `groupby` categories and y the `var_names`. When swapping\n         axes var_group_positions are no longer used\n    {show_save_ax}\n    **kwds : keyword arguments\n        Are passed to `seaborn.violinplot`.\n    Returns\n    -------\n    List of `matplotlib.Axes`\n    Examples\n    -------\n    >>> adata = sc.datasets.pbmc68k_reduced()\n    >>> sc.pl.stacked_violin(adata, ['C1QA', 'PSAP', 'CD79A', 'CD79B', 'CST3', 'LYZ'],\n    ...                      groupby='bulk_labels', dendrogram=True)\n    \"\"\"\n    if use_raw is None and adata.raw is not None: use_raw = True\n    has_var_groups = True if var_group_positions is not None and len(var_group_positions) > 0 else False\n    if isinstance(var_names, str):\n        var_names = [var_names]\n    categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories,\n                                              gene_symbols=gene_symbols, layer=layer)\n    if standard_scale == 'obs':\n        obs_tidy = obs_tidy.sub(obs_tidy.min(1), axis=0)\n        obs_tidy = obs_tidy.div(obs_tidy.max(1), axis=0).fillna(0)\n    elif standard_scale == 'var':\n        obs_tidy -= obs_tidy.min(0)\n        obs_tidy /= obs_tidy.max(0).fillna(0)\n    elif standard_scale is None:\n        pass\n    else:\n        logg.warn('Unknown type for standard_scale, ignored')\n    if 'color' in kwds:\n        row_palette = kwds['color']\n        # remove color from kwds in case is set to avoid an error caused by\n        # double parameters\n        del (kwds['color'])\n    if 'linewidth' not in kwds:\n        # for the tiny violin plots used, is best\n        # to use a thin lindwidth.\n        kwds['linewidth'] = 0.5\n    if groupby is None or len(categories) <= 1:\n        # dendrogram can only be computed  between groupby categories\n        dendrogram = False\n    if dendrogram:\n        dendro_data = _reorder_categories_after_dendrogram(adata, groupby, dendrogram,\n                                                           var_names=var_names,\n                                                           var_group_labels=var_group_labels,\n                                                           var_group_positions=var_group_positions)\n        var_group_labels = dendro_data['var_group_labels']\n        var_group_positions = dendro_data['var_group_positions']\n        # reorder obs_tidy\n        if dendro_data['var_names_idx_ordered'] is not None:\n            obs_tidy = obs_tidy.iloc[:, dendro_data['var_names_idx_ordered']]\n            var_names = [var_names[x] for x in dendro_data['var_names_idx_ordered']]\n        obs_tidy.index = obs_tidy.index.reorder_categories(\n            [categories[x] for x in dendro_data['categories_idx_ordered']], ordered=True)\n        categories = [categories[x] for x in dendro_data['categories_idx_ordered']]\n    global count\n    count = 0\n\n    # All columns should have a unique name, otherwise the\n    # pd.melt object that is passed to seaborn will merge non-unique columns.\n    # Here, I simply rename the columns using a count from 1..n using the\n    # mapping function `rename_cols_to_int` to solve the problem.\n    obs_tidy.rename(rename_cols_to_int, axis='columns', inplace=True)\n    if not swap_axes:\n        # plot image in which x = var_names and y = groupby categories\n        dendro_width = 1.4 if dendrogram else 0\n        if figsize is None:\n            height = len(categories) * 0.2 + 3\n            width = len(var_names) * 0.2 + 1 + dendro_width\n        else:\n            width, height = figsize\n        num_rows = len(categories)\n        height_ratios = None\n        if has_var_groups:\n            # add some space in case 'brackets' want to be plotted on top of the image\n            num_rows += 2  # +2 to add the row for the brackets and a spacer\n            height_ratios = [0.2, 0.05] + [float(height) / len(categories)] * len(categories)\n            categories = [None, None] + list(categories)\n        fig = pl.figure(figsize=(width, height))\n        # define a layout of nrows = len(categories) rows x 2 columns\n        # each row is one violin plot. Second column is reserved for dendrogram (if any)\n        # if var_group_positions is defined, a new row is added\n        axs = gridspec.GridSpec(nrows=num_rows, ncols=2, height_ratios=height_ratios,\n                                width_ratios=[width, dendro_width], wspace=0.08)\n        axs_list = []\n        if dendrogram:\n            first_plot_idx = 1 if has_var_groups else 0\n            dendro_ax = fig.add_subplot(axs[first_plot_idx:, 1])\n            _plot_dendrogram(dendro_ax, adata, groupby, dendrogram_key=dendrogram)\n            axs_list.append(dendro_ax)\n        ax0 = None\n        if is_color_like(row_palette):\n            row_colors = [row_palette] * len(categories)\n        else:\n            row_colors = sns.color_palette(row_palette, n_colors=len(categories))\n        for idx in range(num_rows)[::-1]:  # iterate in reverse to start on the bottom plot\n                                           # this facilitates adding the brackets plot (if\n                                           # needed) by sharing the x axis with a previous\n                                           # violing plot.\n            category = categories[idx]\n            if has_var_groups and idx <= 1:\n                # if var_group_positions is given, axs[0] and axs[1] are the location for the\n                # brackets and a spacer (axs[1])\n                if idx == 0:\n                    brackets_ax = fig.add_subplot(axs[0], sharex=ax0)\n                    _plot_gene_groups_brackets(brackets_ax, group_positions=var_group_positions,\n                                               group_labels=var_group_labels,\n                                               rotation=var_group_rotation)\n                continue\n            df = pd.melt(obs_tidy[obs_tidy.index == category], value_vars=obs_tidy.columns)\n            if ax0 is None:\n                ax = fig.add_subplot(axs[idx, 0])\n                ax0 = ax\n            else:\n                ax = fig.add_subplot(axs[idx, 0])\n            axs_list.append(ax)\n            ax = sns.violinplot('variable', y='value', data=df, inner=None, order=order,\n                                orient='vertical', scale=scale, ax=ax, color=row_colors[idx], **kwds)\n            if stripplot:\n                ax = sns.stripplot('variable', y='value', data=df, order=order,\n                                   jitter=jitter, color='black', size=size, ax=ax)\n            # remove the grids because in such a compact plot are unnecessary\n            ax.grid(False)\n            ax.tick_params(axis='y', left=False, right=True, labelright=True,\n                           labelleft=False, labelsize='x-small', length=1, pad=1)\n            ax.set_ylabel(category, rotation=0, fontsize='small', labelpad=8, ha='right', va='center')\n            ax.set_xlabel('')\n            if log:\n                ax.set_yscale('log')\n            if idx < num_rows - 1:\n                # remove the xticks labels except for the last processed plot (first from bottom-up).\n                # Because the plots share the x axis it is redundant and less compact to plot the\n                # axis ticks and labels for each plot\n                ax.set_xticklabels([])\n                ax.tick_params(axis='x', bottom=False, top=False, labeltop=False, labelbottom=False)\n            else:\n                ax.set_xticklabels(var_names)\n        ax0.tick_params(axis='x', labelrotation=90, labelsize='small')\n    else:\n        # plot image in which x = group by and y = var_names\n        dendro_height = 3 if dendrogram else 0\n        vargroups_width = 0.45 if has_var_groups else 0\n        if figsize is None:\n            height = len(var_names) * 0.3 + dendro_height\n            width = len(categories) * 0.4 + vargroups_width\n        else:\n            width, height = figsize\n        fig = pl.figure(figsize=(width, height))\n        # define a layout of nrows = var_names x 1 columns\n        # if plot dendrogram a row is added\n        # each row is one violin plot.\n        num_rows = len(var_names) + 1 # +1 to account for dendrogram\n        height_ratios = [dendro_height] + ([1] * len(var_names))\n        axs = gridspec.GridSpec(nrows=num_rows, ncols=2,\n                                height_ratios=height_ratios, wspace=0.2,\n                                width_ratios=[width - vargroups_width, vargroups_width])\n        axs_list = []\n        if dendrogram:\n            dendro_ax = fig.add_subplot(axs[0])\n            _plot_dendrogram(dendro_ax, adata, groupby, orientation='top', dendrogram_key=dendrogram)\n            axs_list.append(dendro_ax)\n        first_ax = None\n        if is_color_like(row_palette):\n            row_colors = [row_palette] * len(var_names)\n        else:\n            row_colors = sns.color_palette(row_palette, n_colors=len(var_names))\n        for idx, y in enumerate(obs_tidy.columns):\n            ax_idx = idx + 1  # +1 to account that idx 0 is the dendrogram\n            if first_ax is None:\n                ax = fig.add_subplot(axs[ax_idx, 0])\n                first_ax = ax\n            else:\n                ax = fig.add_subplot(axs[ax_idx, 0])\n            axs_list.append(ax)\n            ax = sns.violinplot(x=obs_tidy.index, y=y, data=obs_tidy, inner=None, order=order,\n                                orient='vertical', scale=scale, ax=ax, color=row_colors[idx], **kwds)\n            if stripplot:\n                ax = sns.stripplot(x=obs_tidy.index, y=y, data=obs_tidy, order=order,\n                                   jitter=jitter, color='black', size=size, ax=ax)\n            ax.set_ylabel(var_names[idx], rotation=0, fontsize='small', labelpad=8, ha='right', va='center')\n            # remove the grids because in such a compact plot are unnecessary\n            ax.grid(False)\n            ax.tick_params(axis='y', right=True, labelright=True, left=False,\n                           labelleft=False, labelrotation=0, labelsize='x-small')\n            ax.tick_params(axis='x', labelsize='small')\n            # remove the xticks labels except for the last processed plot (first from bottom-up).\n            # Because the plots share the x axis it is redundant and less compact to plot the\n            # axis for each plot\n            if idx < len(var_names) - 1:\n                ax.tick_params(labelbottom=False, labeltop=False, bottom=False, top=False)\n                ax.set_xlabel('')\n            if log:\n                ax.set_yscale('log')\n            if max([len(x) for x in categories]) > 1:\n                ax.tick_params(axis='x', labelrotation=90)\n        if has_var_groups:\n            start = 1 if dendrogram else 0\n            gene_groups_ax = fig.add_subplot(axs[start:, 1])\n            arr = []\n            for idx, pos in enumerate(var_group_positions):\n                arr += [idx] * (pos[1]+1 - pos[0])\n            _plot_gene_groups_brackets(gene_groups_ax, var_group_positions, var_group_labels,\n                                       left_adjustment=0.3, right_adjustment=0.7, orientation='right')\n            gene_groups_ax.set_ylim(len(var_names), 0)\n            axs_list.append(gene_groups_ax)\n    # remove the spacing between subplots\n    pl.subplots_adjust(wspace=0, hspace=0)\n    utils.savefig_or_show('stacked_violin', show=show, save=save)\n    return axs_list", "idx": 123}
{"project": "Scanpy", "commit_id": "341_scanpy_0.1_scanpy_preprocessing_simple.py_subsample.py", "target": 1, "func": "def subsample(data, subsample, seed=0, copy=False):\n    \"\"\"Subsample.\n    Parameters\n    ----------\n    data : AnnData or array-like\n        Annotated data matrix.\n    subsample : int\n        Subsample to a fraction of 1/subsample of the data.\n    seed : int\n        Random seed to change subsampling.\n    copy : bool (default: False)\n        If an AnnData is passed, determines whether a copy is returned.\n    Notes\n    -----\n    Returns X, smp_indices if data is array-like, otherwise subsamples the passed\n    AnnData (copy == False) or a copy of it (copy == True).\n    \"\"\"\n    from .. import utils\n    if not isinstance(data, AnnData):\n        X = data\n        return utils.subsample(X, subsample, seed)\n    adata = data.copy() if copy else data\n    _, smp_indices = utils.subsample(adata.X, subsample, seed)\n    adata.inplace_subset_smp(smp_indices)\n    for k in adata.smp_keys():\n        # TODO: this should also be taken into account when slicing\n        if k + '_masks' in adata.add:\n            adata.add[k + '_masks'] = adata[k + '_masks'][:, smp_indices]\n    return adata if copy else None", "idx": 125}
{"project": "Scanpy", "commit_id": "438_scanpy_1.9.0__utils.py__get_mean_var.py", "target": 0, "func": "def _get_mean_var(X, *, axis=0):\n    if sparse.issparse(X):\n        mean, var = sparse_mean_variance_axis(X, axis=axis)\n    else:\n        mean = np.mean(X, axis=axis, dtype=np.float64)\n        mean_sq = np.multiply(X, X).mean(axis=axis, dtype=np.float64)\n        var = mean_sq - mean**2\n    # enforce R convention (unbiased estimator) for variance\n    var *= X.shape[axis] / (X.shape[axis] - 1)\n    return mean, var", "idx": 126}
{"project": "Scanpy", "commit_id": "673_scanpy_1.9.0_test_preprocessing.py_test_normalize_per_cell.py", "target": 0, "func": "def test_normalize_per_cell():\n    adata = AnnData(np.array([[1, 0], [3, 0], [5, 6]]))\n    sc.pp.normalize_per_cell(adata, counts_per_cell_after=1, key_n_counts='n_counts2')\n    assert adata.X.sum(axis=1).tolist() == [1.0, 1.0, 1.0]\n    # now with copy option\n    adata = AnnData(np.array([[1, 0], [3, 0], [5, 6]]))\n    # note that sc.pp.normalize_per_cell is also used in\n    # pl.highest_expr_genes with parameter counts_per_cell_after=100\n    adata_copy = sc.pp.normalize_per_cell(adata, counts_per_cell_after=1, copy=True)\n    assert adata_copy.X.sum(axis=1).tolist() == [1.0, 1.0, 1.0]\n    # now sparse\n    adata = AnnData(np.array([[1, 0], [3, 0], [5, 6]]))\n    adata_sparse = AnnData(sp.csr_matrix([[1, 0], [3, 0], [5, 6]]))\n    sc.pp.normalize_per_cell(adata)\n    sc.pp.normalize_per_cell(adata_sparse)\n    assert adata.X.sum(axis=1).tolist() == adata_sparse.X.sum(axis=1).A1.tolist()", "idx": 140}
{"project": "Scanpy", "commit_id": "2_scanpy_1.9.0_conftest.py_pytest_collection_modifyitems.py", "target": 0, "func": "def pytest_collection_modifyitems(config, items):\n    run_internet = config.getoption(\"--internet-tests\")\n    skip_internet = pytest.mark.skip(reason=\"need --internet-tests option to run\")\n    for item in items:\n        # All tests marked with `pytest.mark.internet` get skipped unless\n        # `--run-internet` passed\n        if not run_internet and (\"internet\" in item.keywords):\n            item.add_marker(skip_internet)", "idx": 161}
{"project": "Scanpy", "commit_id": "659_scanpy_1.2.2_scanpy_plotting_anndata.py_heatmap.py", "target": 1, "func": "def heatmap(adata, var_names, groupby=None, use_raw=True, log=False, num_categories=7,\n            show=None, save=None, **kwargs):\n    \"\"\"Plot a heatmap of the expression values of `var_names`. If groupby is given, the heatmap\n    is ordered by the respective group. For example, a list of marker genes\n    can be plotted, ordered by clustering. If the groupby observation is not categorical\n    the observation is turn into a categorical by binning the data into the number\n    especified in `num_categories`.\n    Parameters\n    ----------\n    adata : :class:`~scanpy.api.AnnData`\n        Annotated data matrix.\n    var_names : `str` or list of `str`\n        var_names should be a valid subset of  `.var_names`.\n    groupby : `str` or `None`, optional (default: `None`)\n        The key of the observation grouping to consider. It is expected that groupby is\n        a categorical. If groupby is not a categorical observation, it would be\n        subdivided into `num_categories`.\n    log : `bool`, optional (default: `False`)\n        Use the log of the values\n    use_raw : `bool`, optional (default: `True`)\n        Use `raw` attribute of `adata` if present.\n    num_categories : `int`, optional (default: `7`)\n        Only used if groupby observation is not categorical. This value determines\n        the number of groups into which the groupby observation should be subdivided.\n    show : bool, optional (default: `None`)\n         Show the plot.\n    save : `bool` or `str`, optional (default: `None`)\n        If `True` or a `str`, save the figure. A string is appended to the\n        default filename. Infer the filetype if ending on {{'.pdf', '.png', '.svg'}}.\n    **kwargs : keyword arguments\n        Are passed to `seaborn.heatmap`.\n    Returns\n    -------\n    A list of `matplotlib.Axes` where the first ax is the groupby categories colorcode, the\n    second axis is the heatmap and the third axis is the colorbar.\n    \"\"\"\n    from scipy.sparse import issparse\n    sanitize_anndata(adata)\n    if isinstance(var_names, str):\n        var_names = [var_names]\n    if groupby is not None:\n        if groupby not in adata.obs_keys():\n            raise ValueError('groupby has to be a valid observation. Given value: {}, '\n                             'valid observations: {}'.format(groupby, adata.obs_keys()))\n    if use_raw:\n        matrix = adata.raw[:, var_names].X\n    else:\n        matrix = adata[:, var_names].X\n    if issparse(matrix):\n        matrix = matrix.toarray()\n    if log:\n        matrix = np.log1p(matrix)\n    obs_tidy = pd.DataFrame(matrix, columns=var_names)\n    if groupby is None:\n        x = 'variable'\n    else:\n        if not is_categorical_dtype(adata.obs[groupby]):\n            # if the groupby column is not categorical, turn it into one\n            # by subdividing into 7 categories\n            categorical = pd.cut(adata.obs[groupby], num_categories)\n        else:\n            categorical = adata.obs[groupby]\n        obs_tidy.set_index(categorical, groupby, inplace=True)\n        x = obs_tidy.index.categories\n    height = 10\n    heatmap_width = len(var_names) * 0.18\n    width = heatmap_width + 3  # +3 to account for the colorbar and labels\n    ax_frac2width = 0.25\n    fig, axs = pl.subplots(nrows=1, ncols=3, sharey=False,\n                           figsize=(width, height), gridspec_kw={'width_ratios': [ax_frac2width, width, ax_frac2width]})\n    groupby_ax = axs[0]\n    heatmap_ax = axs[1]\n    heatmap_cbar_ax = axs[2]\n    if groupby:\n        obs_tidy = obs_tidy.sort_index()\n    # determine groupby label positions\n    value_sum = 0\n    ticks = []\n    labels = []\n    label2code = {}\n    for code, (label, value) in enumerate(obs_tidy.index.value_counts(sort=False).iteritems()):\n        ticks.append(value_sum + (value / 2))\n        labels.append(label)\n        value_sum += value\n        label2code[label] = code\n    groupby_ax.imshow(np.matrix([label2code[lab] for lab in obs_tidy.index]).T, aspect='auto')\n    groupby_ax.set_yticks(ticks)\n    groupby_ax.set_yticklabels(labels)\n    # remove y ticks\n    groupby_ax.tick_params(axis='y', left=False)\n    # remove x ticks and labels\n    groupby_ax.tick_params(axis='x', bottom=False, labelbottom=False)\n    # remove surrounding lines\n    groupby_ax.spines['right'].set_visible(False)\n    groupby_ax.spines['top'].set_visible(False)\n    groupby_ax.spines['left'].set_visible(False)\n    groupby_ax.spines['bottom'].set_visible(False)\n\n    groupby_ax.set_ylabel(groupby)\n\n    sns.heatmap(obs_tidy, yticklabels='none', ax=heatmap_ax, cbar_ax=heatmap_cbar_ax, **kwargs)\n    heatmap_ax.tick_params(axis='y', left=False, labelleft=False)\n    heatmap_ax.set_ylabel('')\n    pl.subplots_adjust(wspace=0.03, hspace=0.01)\n    utils.savefig_or_show('heatmap', show=show, save=save)\n\n    return axs", "idx": 164}
{"project": "Scanpy", "commit_id": "113_scanpy_0.0_scanpy_readwrite.py_write_dict_to_file.py", "target": 1, "func": "def write_dict_to_file(filename, d, ext='h5'):\n    \"\"\"\n    Write content of dictionary to file.\n    Parameters\n    ----------\n    filename : str\n        Filename of data file.\n    d : dict\n        Dictionary storing keys with np.ndarray-like data or scalars.\n    ext : string\n        Determines file type, allowed are 'h5' (hdf5),\n        'xlsx' (Excel) [or 'csv' (comma separated value file)].\n    \"\"\"\n    directory = os.path.dirname(filename)\n    if not os.path.exists(directory):\n        sett.m(0, 'creating directory', directory + '/', 'for saving output files')\n        os.makedirs(directory)\n    if ext == 'h5':\n        with h5py.File(filename, 'w') as f:\n            for key, value in d.items():\n                key, value = prepare_writing(key, value, ext)\n                try:\n                    f.create_dataset(key, data=value)\n                except Exception as e:\n                    sett.m(0,'error creating dataset for key =', key)\n                    raise e\n    elif ext == 'csv' or ext == 'txt':\n        # here this is actually a directory that corresponds to the\n        # single hdf5 file\n        dirname = filename.replace('.' + ext, '')\n        if not os.path.exists(dirname):\n            os.makedirs(dirname)\n        for key, value in d.items():\n            key, value = prepare_writing(key, value, ext)\n            if value.dtype.kind == 'S':\n                value = value.astype('U')\n            if len(value.shape) > 0:\n                np.savetxt(dirname + '/' + key + '.' + ext, value,\n                           fmt = ('%.14e' if value.dtype.char == 'f'\n                                  else '%s' if value.dtype.char == 'U'\n                                  else '%d' if (value.dtype.char == 'b' or value.dtype.char == 'i')\n                                  else '%f'),\n                           delimiter=' ' if ext == 'txt' else ',')\n    elif ext == 'xlsx':\n        raise ValueError('TODO: this is broke.')\n        import pandas as pd\n        with pd.ExcelWriter(filename, engine='openpyxl') as writer:\n            for key, value in d.items():\n                pd.DataFrame(value).to_excel(writer,key)", "idx": 172}
{"project": "Scanpy", "commit_id": "735_scanpy_1.9.0_test_score_genes.py__create_random_gene_names.py", "target": 0, "func": "def _create_random_gene_names(n_genes, name_length):\n    \"\"\"\n    creates a bunch of random gene names (just CAPS letters)\n    \"\"\"\n    return np.array(\n        [\n            ''.join(map(chr, np.random.randint(65, 90, name_length)))\n            for _ in range(n_genes)", "idx": 174}
{"project": "Scanpy", "commit_id": "128_scanpy_1.9.0__normalization.py_normalize_pearson_residuals_pca.py", "target": 0, "func": "def normalize_pearson_residuals_pca(\n    adata: AnnData,\n    *,\n    theta: float = 100,\n    clip: Optional[float] = None,\n    n_comps: Optional[int] = 50,\n    random_state: Optional[float] = 0,\n    kwargs_pca: Optional[dict] = {},\n    use_highly_variable: Optional[bool] = None,\n    check_values: bool = True,\n    inplace: bool = True,\n) -> Optional[AnnData]:\n    \"\"\"\\\n    Applies analytic Pearson residual normalization and PCA, based on [Lause21]_.\n\n    The residuals are based on a negative binomial offset model with overdispersion\n    `theta` shared across genes. By default, residuals are clipped to `sqrt(n_obs)`,\n    overdispersion `theta=100` is used, and PCA is run with 50 components.\n\n    Operates on the subset of highly variable genes in `adata.var['highly_variable']`\n    by default. Expects raw count input.\n\n    Params\n    ------\n    {adata}\n    {dist_params}\n    {pca_chunk}\n    use_highly_variable\n        If `True`, uses gene selection present in `adata.var['highly_variable']` to\n        subset the data before normalizing (default). Otherwise, proceed on the full\n        dataset.\n    {check_values}\n    {inplace}\n\n    Returns\n    -------\n    If `inplace=False`, returns the Pearson residual-based PCA results (as :class:`~anndata.AnnData`\n    object). If `inplace=True`, updates `adata` with the following fields:\n\n    `.uns['pearson_residuals_normalization']['pearson_residuals_df']`\n         The subset of highly variable genes, normalized by Pearson residuals.\n    `.uns['pearson_residuals_normalization']['theta']`\n         The used value of the overdisperion parameter theta.\n    `.uns['pearson_residuals_normalization']['clip']`\n         The used value of the clipping parameter.\n\n    `.obsm['X_pca']`\n        PCA representation of data after gene selection (if applicable) and Pearson\n        residual normalization.\n    `.varm['PCs']`\n         The principal components containing the loadings. When `inplace=True` and\n         `use_highly_variable=True`, this will contain empty rows for the genes not\n         selected.\n    `.uns['pca']['variance_ratio']`\n         Ratio of explained variance.\n    `.uns['pca']['variance']`\n         Explained variance, equivalent to the eigenvalues of the covariance matrix.\n    \"\"\"\n\n    # check if HVG selection is there if user wants to use it\n    if use_highly_variable and 'highly_variable' not in adata.var_keys():\n        raise ValueError(\n            \"You passed `use_highly_variable=True`, but no HVG selection was found \"\n            \"(e.g., there was no 'highly_variable' column in adata.var).'\"\n        )\n\n    # default behavior: if there is a HVG selection, we will use it\n    if use_highly_variable is None and 'highly_variable' in adata.var_keys():\n        use_highly_variable = True\n\n    if use_highly_variable:\n        adata_sub = adata[:, adata.var['highly_variable']].copy()\n        adata_pca = AnnData(\n            adata_sub.X.copy(), obs=adata_sub.obs[[]], var=adata_sub.var[[]]\n        )\n    else:\n        adata_pca = AnnData(adata.X.copy(), obs=adata.obs[[]], var=adata.var[[]])\n\n    normalize_pearson_residuals(\n        adata_pca, theta=theta, clip=clip, check_values=check_values\n    )\n    pca(adata_pca, n_comps=n_comps, random_state=random_state, **kwargs_pca)\n\n    if inplace:\n        norm_settings = adata_pca.uns['pearson_residuals_normalization']\n        norm_dict = dict(**norm_settings, pearson_residuals_df=adata_pca.to_df())\n        if use_highly_variable:\n            adata.varm['PCs'] = np.zeros(shape=(adata.n_vars, n_comps))\n            adata.varm['PCs'][adata.var['highly_variable']] = adata_pca.varm['PCs']\n        else:\n            adata.varm['PCs'] = adata_pca.varm['PCs']\n        adata.uns['pca'] = adata_pca.uns['pca']\n        adata.uns['pearson_residuals_normalization'] = norm_dict\n        adata.obsm['X_pca'] = adata_pca.obsm['X_pca']\n        return None\n    else:\n        return adata_pca", "idx": 176}
{"project": "Scanpy", "commit_id": "323_scanpy_1.9.0__utils.py_hierarchy_sc.py", "target": 0, "func": "def hierarchy_sc(G, root, node_sets):\n    import networkx as nx\n\n    def make_sc_tree(sc_G, node=root, parent=None):\n        sc_G.add_node(node)\n        neighbors = G.neighbors(node)\n        if parent is not None:\n            sc_G.add_edge(parent, node)\n            neighbors.remove(parent)\n        old_node = node\n        for n in node_sets[int(node)]:\n            new_node = str(node) + '_' + str(n)\n            sc_G.add_node(new_node)\n            sc_G.add_edge(old_node, new_node)\n            old_node = new_node\n        for neighbor in neighbors:\n            sc_G = make_sc_tree(sc_G, neighbor, node)\n        return sc_G\n\n    return make_sc_tree(nx.Graph())", "idx": 181}
{"project": "Scanpy", "commit_id": "381_scanpy_1.9.0___init__.py_rank_genes_groups_stacked_violin.py", "target": 0, "func": "def rank_genes_groups_stacked_violin(\n    adata: AnnData,\n    groups: Union[str, Sequence[str]] = None,\n    n_genes: Optional[int] = None,\n    groupby: Optional[str] = None,\n    gene_symbols: Optional[str] = None,\n    *,\n    var_names: Optional[Union[Sequence[str], Mapping[str, Sequence[str]]]] = None,\n    min_logfoldchange: Optional[float] = None,\n    key: Optional[str] = None,\n    show: Optional[bool] = None,\n    save: Optional[bool] = None,\n    return_fig: Optional[bool] = False,\n    **kwds,\n):\n    \"\"\"\\\n    Plot ranking of genes using stacked_violin plot\n    (see :func:`~scanpy.pl.stacked_violin`)\n\n    Parameters\n    ----------\n    {params}\n    {show_save_ax}\n    return_fig\n        Returns :class:`StackedViolin` object. Useful for fine-tuning\n        the plot. Takes precedence over `show=False`.\n    **kwds\n        Are passed to :func:`~scanpy.pl.stacked_violin`.\n\n    Returns\n    -------\n    If `return_fig` is `True`, returns a :class:`StackedViolin` object,\n    else if `show` is false, return axes dict\n\n    Examples\n    --------\n    >>> import scanpy as sc\n    >>> adata = sc.datasets.pbmc68k_reduced()\n    >>> sc.tl.rank_genes_groups(adata, 'bulk_labels')\n\n    >>> sc.pl.rank_genes_groups_stacked_violin(adata, n_genes=4,\n    ... min_logfoldchange=4, figsize=(8,6))\n\n    \"\"\"\n\n    return _rank_genes_groups_plot(\n        adata,\n        plot_type='stacked_violin',\n        groups=groups,\n        n_genes=n_genes,\n        gene_symbols=gene_symbols,\n        groupby=groupby,\n        var_names=var_names,\n        key=key,\n        min_logfoldchange=min_logfoldchange,\n        show=show,\n        save=save,\n        return_fig=return_fig,\n        **kwds,", "idx": 188}
{"project": "Scanpy", "commit_id": "339_scanpy_0.1_scanpy_data_structs_ann_data.py_inplace_subset_smp.py", "target": 1, "func": "def inplace_subset_smp(self, index):\n    \"\"\"Inplace subsetting along variables dimension.\n    Same as adata = adata[index, :], but inplace.\n    \"\"\"\n    self.X = self.X[index, :]\n    self.smp = self.smp[index]\n    self.n_smps = self.X.shape[0]\n    return None", "idx": 194}
{"project": "Scanpy", "commit_id": "582_scanpy_1.0.4_scanpy_tools_rna_velocity.py_rna_velocity.py", "target": 1, "func": "def rna_velocity(adata, loomfile, copy=False):\n    \"\"\"Estimate RNA velocity. [LaManno17]_\n    This requires generating a loom file with Velocyto, which stores the counts\n    of spliced, unspliced and ambiguous RNA for every cell and every gene.\n    In contrast to Velocyto, here, we neither use RNA velocities for\n    extrapolation nor for constructing a Markov process. Instead, we directly\n    orient and weight edges in the nearest neighbor graph by computing\n    ``cosine_similarity((x_i - x_j), v_i)``, where `i` labels a cell, `j` a\n    neighbor of the cell, `x` a gene expression vector and `v` a velocity\n    vector.\n    \"\"\"\n    adata = adata.copy() if copy else adata\n\n    # this is n_genes x n_cells\n    ds = loompy.connect(self.loom_filepath)\n    X_spliced = ds.layer['spliced'][:, :]\n    X_unspliced = ds.layer['unspliced'][:, :]\n    # X_ambiguous = ds.layer['ambiguous'][:, :]\n    row_attrs = dict(ds.row_attrs.items())\n    gene_names = row_attrs['Gene']\n\n    # subset the spliced and unspliced matrices to the genes in adata\n    from anndata.base import _normalize_index\n    gene_index = _normalize_index(gene_names, adata.var_names)\n    X_spliced = X_spliced[gene_index]\n    X_unspliced = X_unspliced[gene_index]\n\n    # for now, take non-normalized values\n    from ..preprocessing.simple import normalize_per_cell\n    normalize_per_cell(X_spliced.T)\n    normalize_per_cell(X_unspliced.T)\n\n    # loop over genes\n    offset = np.zeros(s.shape[0], dtype='float32')\n    gamma = np.zeros(s.shape[0], dtype='float32')\n    for i in range(s.shape[0]):\n        gamma[i], offset[i] = opt.minimize(\n            lambda m: np.sum((-X_unspliced[i] + X_spliced[i] * m[0] + m[1]) ** 2),\n            x0=(0.1, 1e-16),\n            method='L-BFGS-B',\n            bounds=[(1e-8, 30), (0, 1.5)]).x\n    velocity = X_unspliced - (gamma[:, None] * X_spliced + offset[:, None])\n    from ..neighbors import Neighbors, get_indices_distances_from_sparse_matrix\n    neigh = Neighbors(adata)\n    knn_indices, knn_distances = get_indices_distances_from_sparse_matrix(\n        neigh.distances, self.n_neighbors)\n\n    n_obs = adata.n_obs\n    n_neighbors = neigh.n_neighbors\n\n    from scipy.sparse import dok_matrix\n    graph = dok_matrix((n_obs, n_obs), dtype='float32')\n    from scipy.spatial.distance import cosine\n    for i in range(knn_indices.shape[0]):\n        for j in range(n_neighbors):\n            if knn_indices[i, j] != i:\n                val = 1 - cosine((X_spliced[:, i] - X_spliced[:, j]), velocity[:, i])\n                if val > 0:\n                    # transition from i to j\n                    graph[j, i] = val\n                else:\n                    # transition from j to i\n                    graph[i, j] = val\n    graph = graph.tocoo().tocsr()\n    adata.uns['rna_velocity'] = {}\n    adata.uns['rna_velocity']['graph'] = graph\n    adata.var['rna_velocity_gamma'] = gamma\n    adata.var['rna_velocity_offset'] = offset\n    return adata if copy else None", "idx": 198}
{"project": "Scanpy", "commit_id": "106_scanpy_1.9.0__datasets.py_krumsiek11.py", "target": 0, "func": "def krumsiek11() -> ad.AnnData:\n    \"\"\"\\\n    Simulated myeloid progenitors [Krumsiek11]_.\n\n    The literature-curated boolean network from [Krumsiek11]_ was used to\n    simulate the data. It describes development to four cell fates: 'monocyte',\n    'erythrocyte', 'megakaryocyte' and 'neutrophil'.\n\n    See also the discussion of this data in [Wolf19]_.\n\n    Simulate via :func:`~scanpy.tl.sim`.\n\n    Returns\n    -------\n    Annotated data matrix.\n    \"\"\"\n    filename = HERE / 'krumsiek11.txt'\n    verbosity_save = settings.verbosity\n    settings.verbosity = 'error'  # suppress output...\n    adata = read(filename, first_column_names=True)\n    settings.verbosity = verbosity_save\n    adata.uns['iroot'] = 0\n    fate_labels = {0: 'Stem', 159: 'Mo', 319: 'Ery', 459: 'Mk', 619: 'Neu'}\n    adata.uns['highlights'] = fate_labels\n    cell_type = np.array(['progenitor' for i in range(adata.n_obs)])\n    cell_type[80:160] = 'Mo'\n    cell_type[240:320] = 'Ery'\n    cell_type[400:480] = 'Mk'\n    cell_type[560:640] = 'Neu'\n    adata.obs['cell_type'] = cell_type\n    _utils.sanitize_anndata(adata)\n    return adata", "idx": 201}
{"project": "Scanpy", "commit_id": "483_scanpy_1.9.0_test_combat.py_test_covariates.py", "target": 0, "func": "def test_covariates():\n    adata = sc.datasets.blobs()\n    key = 'blobs'\n\n    X1 = sc.pp.combat(adata, key=key, inplace=False)\n\n    np.random.seed(0)\n    adata.obs['cat1'] = np.random.binomial(3, 0.5, size=(adata.n_obs))\n    adata.obs['cat2'] = np.random.binomial(2, 0.1, size=(adata.n_obs))\n    adata.obs['num1'] = np.random.normal(size=(adata.n_obs))\n\n    X2 = sc.pp.combat(\n        adata, key=key, covariates=['cat1', 'cat2', 'num1'], inplace=False\n    )\n    sc.pp.combat(adata, key=key, covariates=['cat1', 'cat2', 'num1'], inplace=True)\n\n    assert X1.shape == X2.shape\n\n    df = adata.obs[['cat1', 'cat2', 'num1', key]]\n    batch_cats = adata.obs[key].cat.categories\n    design = _design_matrix(df, key, batch_cats)\n\n    assert len(design.columns) == 4 + len(batch_cats) - 1", "idx": 203}
{"project": "Scanpy", "commit_id": "849_scanpy_1.9.0__rank_genes_groups.py_filter_rank_genes_groups.py", "target": 0, "func": "def filter_rank_genes_groups(\n    adata: AnnData,\n    key=None,\n    groupby=None,\n    use_raw=None,\n    key_added='rank_genes_groups_filtered',\n    min_in_group_fraction=0.25,\n    min_fold_change=1,\n    max_out_group_fraction=0.5,\n    compare_abs=False,\n) -> None:\n    \"\"\"\\\n    Filters out genes based on log fold change and fraction of genes expressing the\n    gene within and outside the `groupby` categories.\n\n    See :func:`~scanpy.tl.rank_genes_groups`.\n\n    Results are stored in `adata.uns[key_added]`\n    (default: 'rank_genes_groups_filtered').\n\n    To preserve the original structure of adata.uns['rank_genes_groups'],\n    filtered genes are set to `NaN`.\n\n    Parameters\n    ----------\n    adata\n    key\n    groupby\n    use_raw\n    key_added\n    min_in_group_fraction\n    min_fold_change\n    max_out_group_fraction\n    compare_abs\n        If `True`, compare absolute values of log fold change with `min_fold_change`.\n\n    Returns\n    -------\n    Same output as :func:`scanpy.tl.rank_genes_groups` but with filtered genes names set to\n    `nan`\n\n    Examples\n    --------\n    >>> import scanpy as sc\n    >>> adata = sc.datasets.pbmc68k_reduced()\n    >>> sc.tl.rank_genes_groups(adata, 'bulk_labels', method='wilcoxon')\n    >>> sc.tl.filter_rank_genes_groups(adata, min_fold_change=3)\n    >>> # visualize results\n    >>> sc.pl.rank_genes_groups(adata, key='rank_genes_groups_filtered')\n    >>> # visualize results using dotplot\n    >>> sc.pl.rank_genes_groups_dotplot(adata, key='rank_genes_groups_filtered')\n    \"\"\"\n    if key is None:\n        key = 'rank_genes_groups'\n\n    if groupby is None:\n        groupby = adata.uns[key]['params']['groupby']\n\n    if use_raw is None:\n        use_raw = adata.uns[key]['params']['use_raw']\n\n    same_params = (\n        adata.uns[key]['params']['groupby'] == groupby\n        and adata.uns[key]['params']['reference'] == 'rest'\n        and adata.uns[key]['params']['use_raw'] == use_raw\n    )\n\n    use_logfolds = same_params and 'logfoldchanges' in adata.uns[key]\n    use_fraction = same_params and 'pts_rest' in adata.uns[key]\n\n    # convert structured numpy array into DataFrame\n    gene_names = pd.DataFrame(adata.uns[key]['names'])\n\n    fraction_in_cluster_matrix = pd.DataFrame(\n        np.zeros(gene_names.shape),\n        columns=gene_names.columns,\n        index=gene_names.index,\n    )\n    fraction_out_cluster_matrix = pd.DataFrame(\n        np.zeros(gene_names.shape),\n        columns=gene_names.columns,\n        index=gene_names.index,\n    )\n\n    if use_logfolds:\n        fold_change_matrix = pd.DataFrame(adata.uns[key]['logfoldchanges'])\n    else:\n        fold_change_matrix = pd.DataFrame(\n            np.zeros(gene_names.shape),\n            columns=gene_names.columns,\n            index=gene_names.index,\n        )\n\n        if 'log1p' in adata.uns_keys() and adata.uns['log1p']['base'] is not None:\n            expm1_func = lambda x: np.expm1(x * np.log(adata.uns['log1p']['base']))\n        else:\n            expm1_func = np.expm1\n\n    logg.info(\n        f\"Filtering genes using: \"\n        f\"min_in_group_fraction: {min_in_group_fraction} \"\n        f\"min_fold_change: {min_fold_change}, \"\n        f\"max_out_group_fraction: {max_out_group_fraction}\"\n    )\n\n    for cluster in gene_names.columns:\n        # iterate per column\n        var_names = gene_names[cluster].values\n\n        if not use_logfolds or not use_fraction:\n            sub_X = adata.raw[:, var_names].X if use_raw else adata[:, var_names].X\n            in_group = adata.obs[groupby] == cluster\n            X_in = sub_X[in_group]\n            X_out = sub_X[~in_group]\n\n        if use_fraction:\n            fraction_in_cluster_matrix.loc[:, cluster] = (\n                adata.uns[key]['pts'][cluster].loc[var_names].values\n            )\n            fraction_out_cluster_matrix.loc[:, cluster] = (\n                adata.uns[key]['pts_rest'][cluster].loc[var_names].values\n            )\n        else:\n            fraction_in_cluster_matrix.loc[:, cluster] = _calc_frac(X_in)\n            fraction_out_cluster_matrix.loc[:, cluster] = _calc_frac(X_out)\n\n        if not use_logfolds:\n            # compute mean value\n            mean_in_cluster = np.ravel(X_in.mean(0))\n            mean_out_cluster = np.ravel(X_out.mean(0))\n            # compute fold change\n            fold_change_matrix.loc[:, cluster] = np.log2(\n                (expm1_func(mean_in_cluster) + 1e-9)\n                / (expm1_func(mean_out_cluster) + 1e-9)\n            )\n\n    if compare_abs:\n        fold_change_matrix = fold_change_matrix.abs()\n    # filter original_matrix\n    gene_names = gene_names[\n        (fraction_in_cluster_matrix > min_in_group_fraction)\n        & (fraction_out_cluster_matrix < max_out_group_fraction)\n        & (fold_change_matrix > min_fold_change)\n    ]\n    # create new structured array using 'key_added'.\n    adata.uns[key_added] = adata.uns[key].copy()\n    adata.uns[key_added]['names'] = gene_names.to_records(index=False)", "idx": 207}
{"project": "Scanpy", "commit_id": "747_scanpy_1.9.0_test_score_genes.py_test_use_raw_None.py", "target": 0, "func": "def test_use_raw_None():\n    adata = _create_adata(100, 1000, p_zero=0, p_nan=0)\n    adata_raw = adata.copy()\n    adata_raw.var_names = [str(i) for i in range(adata_raw.n_vars)]\n    adata.raw = adata_raw\n\n    sc.tl.score_genes(adata, adata_raw.var_names[:3], use_raw=None)", "idx": 208}
{"project": "Scanpy", "commit_id": "895_scanpy_1.4.3_scanpy_tools__umap.py_umap.py", "target": 1, "func": "def umap(\n    adata,\n    min_dist=0.5,\n    spread=1.0,\n    n_components=2,\n    maxiter=None,\n    alpha=1.0,\n    gamma=1.0,\n    negative_sample_rate=5,\n    init_pos='spectral',\n    random_state=0,\n    a=None,\n    b=None,\n    copy=False,\n):\n    \"\"\"Embed the neighborhood graph using UMAP [McInnes18]_.\n    UMAP (Uniform Manifold Approximation and Projection) is a manifold learning\n    technique suitable for visualizing high-dimensional data. Besides tending to\n    be faster than tSNE, it optimizes the embedding such that it best reflects\n    the topology of the data, which we represent throughout Scanpy using a\n    neighborhood graph. tSNE, by contrast, optimizes the distribution of\n    nearest-neighbor distances in the embedding such that these best match the\n    distribution of distances in the high-dimensional space.  We use the\n    implementation of `umap-learn <https://github.com/lmcinnes/umap>`__\n    [McInnes18]_. For a few comparisons of UMAP with tSNE, see this `preprint\n    <https://doi.org/10.1101/298430>`__.\n    Parameters\n    ----------\n    adata : :class:`~anndata.AnnData`\n        Annotated data matrix.\n    min_dist : `float`, optional (default: 0.5)\n        The effective minimum distance between embedded points. Smaller values\n        will result in a more clustered/clumped embedding where nearby points on\n        the manifold are drawn closer together, while larger values will result\n        on a more even dispersal of points. The value should be set relative to\n        the ``spread`` value, which determines the scale at which embedded\n        points will be spread out. The default of in the `umap-learn` package is\n        0.1.\n    spread : `float` (optional, default 1.0)\n        The effective scale of embedded points. In combination with `min_dist`\n        this determines how clustered/clumped the embedded points are.\n    n_components : `int`, optional (default: 2)\n        The number of dimensions of the embedding.\n    maxiter : `int`, optional (default: `None`)\n        The number of iterations (epochs) of the optimization. Called `n_epochs`\n        in the original UMAP.\n    alpha : `float`, optional (default: 1.0)\n        The initial learning rate for the embedding optimization.\n    gamma : `float` (optional, default 1.0)\n        Weighting applied to negative samples in low dimensional embedding\n        optimization. Values higher than one will result in greater weight\n        being given to negative samples.\n    negative_sample_rate : `int` (optional, default 5)\n        The number of negative edge/1-simplex samples to use per positive\n        edge/1-simplex sample in optimizing the low dimensional embedding.\n    init_pos : `string` or `np.array`, optional (default: 'spectral')\n        How to initialize the low dimensional embedding. Called `init` in the\n        original UMAP.\n        Options are:\n        * Any key for `adata.obsm`.\n        * 'paga': positions from :func:`~scanpy.api.pl.paga`.\n        * 'spectral': use a spectral embedding of the graph.\n        * 'random': assign initial embedding positions at random.\n        * A numpy array of initial embedding positions.\n    random_state : `int`, `RandomState` or `None`, optional (default: 0)\n        If `int`, `random_state` is the seed used by the random number generator;\n        If `RandomState`, `random_state` is the random number generator;\n        If `None`, the random number generator is the `RandomState` instance used\n        by `np.random`.\n    a : `float` (optional, default `None`)\n        More specific parameters controlling the embedding. If `None` these\n        values are set automatically as determined by `min_dist` and\n        `spread`.\n    b : `float` (optional, default `None`)\n        More specific parameters controlling the embedding. If `None` these\n        values are set automatically as determined by `min_dist` and\n        `spread`.\n    copy : `bool` (default: `False`)\n        Return a copy instead of writing to adata.\n    Returns\n    -------\n    Depending on `copy`, returns or updates `adata` with the following fields.\n    **X_umap** : `adata.obsm` field\n        UMAP coordinates of data.\n    \"\"\"\n    adata = adata.copy() if copy else adata\n    if 'neighbors' not in adata.uns:\n        raise ValueError(\n            'Did not find \\'neighbors/connectivities\\'. Run `sc.pp.neighbors` first.')\n    start = logg.info('computing UMAP')\n    if ('params' not in adata.uns['neighbors']\n        or adata.uns['neighbors']['params']['method'] != 'umap'):\n        logg.warning('neighbors/connectivities have not been computed using umap')\n    from umap.umap_ import find_ab_params, simplicial_set_embedding\n    if a is None or b is None:\n        a, b = find_ab_params(spread, min_dist)\n    else:\n        a = a\n        b = b\n    if init_pos in adata.obsm.keys():\n        init_coords = adata.obsm[init_pos]\n    elif init_pos == 'paga':\n        init_coords = get_init_pos_from_paga(adata, random_state=random_state).astype(adata.X.dtype)\n    else:\n        init_coords = init_pos\n    from sklearn.utils import check_random_state\n    random_state = check_random_state(random_state)\n    n_epochs = 0 if maxiter is None else maxiter\n    neigh_params = adata.uns['neighbors']['params']\n    X = choose_representation(\n        adata, neigh_params.get('use_rep', None), neigh_params.get('n_pcs', None), silent=True)\n    # the data matrix X is really only used for determining the number of connected components\n    # for the init condition in the UMAP embedding\n    X_umap = simplicial_set_embedding(\n        X,\n        adata.uns['neighbors']['connectivities'].tocoo(),\n        n_components,\n        alpha,\n        a,\n        b,\n        gamma,\n        negative_sample_rate,\n        n_epochs,\n        init_coords,\n        random_state,\n        neigh_params.get('metric', 'euclidean'),\n        neigh_params.get('metric_kwds', {}),\n        verbose=settings.verbosity > 3,\n    )\n    adata.obsm['X_umap'] = X_umap  # annotate samples with UMAP coordinates\n    logg.info(\n        '    finished',\n        time=start,\n        deep=(\n            'added\\n'\n            \"    'X_umap', UMAP coordinates (adata.obsm)\"\n        ),\n    )\n    return adata if copy else None", "idx": 212}
{"project": "Scanpy", "commit_id": "305_scanpy_0.1_scanpy_plotting___init__.py_aga_tree.py", "target": 1, "func": "def aga_tree(\n        adata,\n        root=0,\n        colors=None,\n        names=None,\n        fontsize=None,\n        node_size=1,\n        ext='pdf',\n        show=None):\n    if colors is None and 'aga_groups_colors_original' in adata.add:\n        colors = adata.add['aga_groups_colors_original']\n    if names is None and 'aga_groups_names_original' in adata.add:\n        names = adata.add['aga_groups_names_original']\n    # plot the tree\n    if isinstance(adata, nx.Graph):\n        G = adata\n        colors = ['grey' for n in enumerate(G)]\n    else:\n        if colors is None:\n            if ('aga_groups_colors' not in adata.add\n                or len(adata.add['aga_groups_names']) != len(adata.add['aga_groups_colors'])):\n                utils.add_colors_for_categorical_sample_annotation(adata, 'aga_groups')\n            colors = adata.add['aga_groups_colors']\n        else: colors = colors\n        if names is None:\n            names = {i: n for i, n in enumerate(adata.add['aga_groups_names'])}\n        for iname, name in enumerate(adata.add['aga_groups_names']):\n            if name in sett._ignore_categories: colors[iname] = 'grey'\n        G = nx.Graph(adata.add['aga_adjacency'])\n    pos = utils.hierarchy_pos(G, root)\n    # pos = nx.spring_layout(G)\n    if len(pos) == 1: pos[0] = 0.5, 0.5\n    fig = pl.figure()\n    ax = pl.axes([0.08, 0.08, 0.9, 0.9], frameon=False)\n    labels = nx.get_edge_attributes(G, 'weight')\n    nx.draw_networkx_edges(G, pos, ax=ax)\n    edge_labels = {}\n    for n1, n2, label in G.edges(data=True):\n        edge_labels[(n1, n2)] = '{:.3f}'.format(label['weight'])\n    # nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, ax=ax, font_size=5)\n    trans = ax.transData.transform\n    trans2 = fig.transFigure.inverted().transform\n    pl.xticks([])\n    pl.yticks([])\n    piesize = 1/(np.sqrt(G.number_of_nodes()) + 10) * node_size\n    p2 = piesize/2.0\n    for n_cnt, n in enumerate(G):\n        xx, yy = trans(pos[n])     # figure coordinates\n        xa, ya = trans2((xx, yy))  # normalized coordinates\n        a = pl.axes([xa-p2, ya-p2, piesize, piesize])\n        if is_color_like(colors[n_cnt]):\n            fracs = [100]\n            color = [colors[n_cnt]]\n        else:\n            color = colors[n_cnt].keys()\n            fracs = [colors[n_cnt][c] for c in color]\n            if sum(fracs) < 1:\n                color = list(color)\n                color.append('grey')\n                fracs.append(1-sum(fracs))\n                # names[n_cnt] += '\\n?'\n        a.pie(fracs, colors=color)\n        if names is not None:\n            a.text(0.5, 0.5, names[n_cnt],\n                   verticalalignment='center',\n                   horizontalalignment='center',\n                   transform=a.transAxes, size=fontsize)\n    savefig_or_show('aga_tree', show, ext=ext)\n    return ax", "idx": 215}
{"project": "Scanpy", "commit_id": "965_scanpy_1.4.4_scanpy_plotting__tools_paga.py_paga.py", "target": 1, "func": "def paga(\n    adata: AnnData,\n    threshold: Optional[float] = None,\n    color: Optional[str] = None,\n    layout: Optional[_IGraphLayout] = None,\n    layout_kwds: Mapping[str, Any] = MappingProxyType({}),\n    init_pos: Optional[np.ndarray] = None,\n    root: Union[int, str, Sequence[int], None] = 0,\n    labels: Union[str, Sequence[str], Mapping[str, str], None] = None,\n    single_component: bool = False,\n    solid_edges: str = 'connectivities',\n    dashed_edges: Optional[str] = None,\n    transitions: Optional[str] = None,\n    fontsize: Optional[int] = None,\n    fontweight: str = 'bold',\n    fontoutline: Optional[int] = None,\n    text_kwds: Mapping[str, Any] = MappingProxyType({}),\n    node_size_scale: float = 1.,\n    node_size_power: float = 0.5,\n    edge_width_scale: float = 1.,\n    min_edge_width: Optional[float] = None,\n    max_edge_width: Optional[float] = None,\n    arrowsize: int = 30,\n    title: Optional[str] = None,\n    left_margin: float = 0.01,\n    random_state: Optional[int] = 0,\n    pos: Union[np.ndarray, str, Path, None] = None,\n    normalize_to_color: bool = False,\n    cmap: Union[str, Colormap]=None,\n    cax: Optional[Axes] = None,\n    colorbar=None,  # TODO: this seems to be unused\n    cb_kwds: Mapping[str, Any] = MappingProxyType({}),\n    frameon: Optional[bool] = None,\n    add_pos: bool = True,\n    export_to_gexf: bool = False,\n    use_raw: bool = True,\n    colors=None,   # backwards compat\n    groups=None,  # backwards compat\n    plot: bool = True,\n    show: Optional[bool] = None,\n    save: Union[bool, str, None] = None,\n    ax: Optional[Axes] = None,\n) -> Union[Axes, List[Axes], None]:\n    \"\"\"\\\n    Plot the PAGA graph through thresholding low-connectivity edges.\n    Compute a coarse-grained layout of the data. Reuse this by passing\n    `init_pos='paga'` to :func:`~scanpy.tl.umap` or\n    :func:`~scanpy.tl.draw_graph` and obtain embeddings with more meaningful\n    global topology [Wolf19]_.\n    This uses ForceAtlas2 or igraph's layout algorithms for most layouts [Csardi06]_.\n    Parameters\n    ----------\n    adata\n        Annotated data matrix.\n    threshold\n        Do not draw edges for weights below this threshold. Set to 0 if you want\n        all edges. Discarding low-connectivity edges helps in getting a much\n        clearer picture of the graph.\n    color\n        Gene name or `obs` annotation defining the node colors.\n        Also plots the degree of the abstracted graph when\n        passing {`'degree_dashed'`, `'degree_solid'`}.\n    labels\n        The node labels. If `None`, this defaults to the group labels stored in\n        the categorical for which :func:`~scanpy.tl.paga` has been computed.\n    pos\n        Two-column array-like storing the x and y coordinates for drawing.\n        Otherwise, path to a `.gdf` file that has been exported from Gephi or\n        a similar graph visualization software.\n    layout\n        Plotting layout that computes positions.\n        `'fa'` stands for \u201cForceAtlas2\u201d,\n        `'fr'` stands for \u201cFruchterman-Reingold\u201d,\n        `'rt'` stands for \u201cReingold-Tilford\u201d,\n        `'eq_tree'` stands for \u201ceqally spaced tree\u201d.\n        All but `'fa'` and `'eq_tree'` are igraph layouts.\n        All other igraph layouts are also permitted.\n        See also parameter `pos` and :func:`~scanpy.tl.draw_graph`.\n    layout_kwds\n        Keywords for the layout.\n    init_pos\n        Two-column array storing the x and y coordinates for initializing the\n        layout.\n    random_state\n        For layouts with random initialization like `'fr'`, change this to use\n        different intial states for the optimization. If `None`, the initial\n        state is not reproducible.\n    root\n        If choosing a tree layout, this is the index of the root node or a list\n        of root node indices. If this is a non-empty vector then the supplied\n        node IDs are used as the roots of the trees (or a single tree if the\n        graph is connected). If this is `None` or an empty list, the root\n        vertices are automatically calculated based on topological sorting.\n    transitions\n        Key for `.uns['paga']` that specifies the matrix that \u2013 for instance\n        `'transistions_confidence'` \u2013 that specifies the matrix that stores the\n        arrows.\n    solid_edges\n        Key for `.uns['paga']` that specifies the matrix that stores the edges\n        to be drawn solid black.\n    dashed_edges\n        Key for `.uns['paga']` that specifies the matrix that stores the edges\n        to be drawn dashed grey. If `None`, no dashed edges are drawn.\n    single_component\n        Restrict to largest connected component.\n    fontsize\n        Font size for node labels.\n    fontoutline\n        Width of the white outline around fonts.\n    text_kwds\n        Keywords for :meth:`~matplotlib.axes.Axes.text`.\n    node_size_scale\n        Increase or decrease the size of the nodes.\n    node_size_power\n        The power with which groups sizes influence the radius of the nodes.\n    edge_width_scale\n        Edge with scale in units of `rcParams['lines.linewidth']`.\n    min_edge_width\n        Min width of solid edges.\n    max_edge_width\n        Max width of solid and dashed edges.\n    arrowsize\n       For directed graphs, choose the size of the arrow head head's length and\n       width. See :py:class: `matplotlib.patches.FancyArrowPatch` for attribute\n       `mutation_scale` for more info.\n    export_to_gexf\n        Export to gexf format to be read by graph visualization programs such as\n        Gephi.\n    normalize_to_color\n        Whether to normalize categorical plots to `color` or the underlying\n        grouping.\n    cmap\n        The color map.\n    cax\n        A matplotlib axes object for a potential colorbar.\n    cb_kwds\n        Keyword arguments for :class:`~matplotlib.colorbar.ColorbarBase`,\n        for instance, `ticks`.\n    add_pos\n        Add the positions to `adata.uns['paga']`.\n    title\n        Provide a title.\n    frameon\n        Draw a frame around the PAGA graph.\n    plot\n        If `False`, do not create the figure, simply compute the layout.\n    save\n        If `True` or a `str`, save the figure.\n        A string is appended to the default filename.\n        Infer the filetype if ending on \\\\{`'.pdf'`, `'.png'`, `'.svg'`\\\\}.\n    ax\n        A matplotlib axes object.\n    Returns\n    -------\n    If `show==False`, one or more :class:`~matplotlib.axes.Axes` objects.\n    Adds `'pos'` to `adata.uns['paga']` if `add_pos` is `True`.\n    Notes\n    -----\n    When initializing the positions, note that \u2013 for some reason \u2013 igraph\n    mirrors coordinates along the x axis... that is, you should increase the\n    `maxiter` parameter by 1 if the layout is flipped.\n    .. currentmodule:: scanpy\n    See also\n    --------\n    tl.paga\n    pl.paga_compare\n    pl.paga_path\n    \"\"\"\n    if groups is not None:  # backwards compat\n        labels = groups\n        logg.warning('`groups` is deprecated in `pl.paga`: use `labels` instead')\n    if colors is None:\n        colors = color\n    groups_key = adata.uns['paga']['groups']\n\n    if is_flat(colors):\n        colors = [colors]\n    if frameon is None:\n        frameon = settings._frameon\n    # labels is a list that contains no lists\n    if is_flat(labels):\n        labels = [labels for _ in range(len(colors))]\n    if title is None and len(colors) > 1:\n        title = [c for c in colors]\n    elif isinstance(title, str):\n        title = [title for c in colors]\n    elif title is None:\n        title = [None for c in colors]\n    if colorbar is None:\n        var_names = adata.var_names if adata.raw is None else adata.raw.var_names\n        colorbars = [((c in adata.obs_keys() and adata.obs[c].dtype.name != 'category') or\n                      (c in var_names)) for c in colors]\n    else:\n        colorbars = [False for _ in colors]\n    if isinstance(root, str):\n        if root not in labels:\n            raise ValueError(\n                'If `root` is a string, '\n                f'it needs to be one of {labels} not {root!r}.'\n            )\n        root = list(labels).index(root)\n    if isinstance(root, cabc.Sequence) and root[0] in labels:\n        root = [list(labels).index(r) for r in root]\n    # define the adjacency matrices\n    adjacency_solid = adata.uns['paga'][solid_edges].copy()\n    adjacency_dashed = None\n    if threshold is None:\n        threshold = 0.01  # default threshold\n    if threshold > 0:\n        adjacency_solid.data[adjacency_solid.data < threshold] = 0\n        adjacency_solid.eliminate_zeros()\n    if dashed_edges is not None:\n        adjacency_dashed = adata.uns['paga'][dashed_edges].copy()\n        if threshold > 0:\n            adjacency_dashed.data[adjacency_dashed.data < threshold] = 0\n            adjacency_dashed.eliminate_zeros()\n    # compute positions\n    if pos is None:\n        adj_tree = None\n        if layout in {'rt', 'rt_circular', 'eq_tree'}:\n            adj_tree = adata.uns['paga']['connectivities_tree']\n        pos = _compute_pos(\n            adjacency_solid,\n            layout=layout, random_state=random_state, init_pos=init_pos,\n            layout_kwds=layout_kwds, adj_tree=adj_tree, root=root,\n        )\n    if plot:\n        axs, panel_pos, draw_region_width, figure_width = _utils.setup_axes(\n            ax=ax,\n            panels=colors,\n            colorbars=colorbars,\n        )\n        if len(colors) == 1 and not isinstance(axs, list):\n            axs = [axs]\n        for icolor, c in enumerate(colors):\n            if title[icolor] is not None:\n                axs[icolor].set_title(title[icolor])\n            sct = _paga_graph(\n                adata,\n                axs[icolor],\n                colors=c,\n                solid_edges=solid_edges,\n                dashed_edges=dashed_edges,\n                transitions=transitions,\n                threshold=threshold,\n                adjacency_solid=adjacency_solid,\n                adjacency_dashed=adjacency_dashed,\n                root=root,\n                labels=labels[icolor],\n                fontsize=fontsize,\n                fontweight=fontweight,\n                fontoutline=fontoutline,\n                text_kwds=text_kwds,\n                node_size_scale=node_size_scale,\n                node_size_power=node_size_power,\n                edge_width_scale=edge_width_scale,\n                min_edge_width=min_edge_width,\n                max_edge_width=max_edge_width,\n                normalize_to_color=normalize_to_color,\n                frameon=frameon,\n                cmap=cmap,\n                colorbar=colorbars[icolor],\n                cb_kwds=cb_kwds,\n                use_raw=use_raw,\n                title=title[icolor],\n                export_to_gexf=export_to_gexf,\n                single_component=single_component,\n                arrowsize=arrowsize,\n                pos=pos,\n            )\n            if colorbars[icolor]:\n                if cax is None:\n                    bottom = panel_pos[0][0]\n                    height = panel_pos[1][0] - bottom\n                    width = 0.006 * draw_region_width / len(colors)\n                    left = panel_pos[2][2*icolor+1] + 0.2 * width\n                    rectangle = [left, bottom, width, height]\n                    fig = pl.gcf()\n                    ax_cb = fig.add_axes(rectangle)\n                else:\n                    ax_cb = cax[icolor]\n                cb = pl.colorbar(\n                    sct,\n                    format=ticker.FuncFormatter(_utils.ticks_formatter),\n                    cax=ax_cb,\n                )\n    if add_pos:\n        adata.uns['paga']['pos'] = pos\n        logg.hint(\"added 'pos', the PAGA positions (adata.uns['paga'])\")\n    if plot:\n        _utils.savefig_or_show('paga', show=show, save=save)\n        if len(colors) == 1 and isinstance(axs, list): axs = axs[0]\n        return axs if not show else None", "idx": 217}
{"project": "Scanpy", "commit_id": "176_scanpy_1.9.0__sam.py_sam.py", "target": 0, "func": "def sam(\n    adata: AnnData,\n    max_iter: int = 10,\n    num_norm_avg: int = 50,\n    k: int = 20,\n    distance: str = 'correlation',\n    standardization: Literal['Normalizer', 'StandardScaler', 'None'] = 'StandardScaler',\n    weight_pcs: bool = False,\n    sparse_pca: bool = False,\n    n_pcs: Optional[int] = 150,\n    n_genes: Optional[int] = 3000,\n    projection: Literal['umap', 'tsne', 'None'] = 'umap',\n    inplace: bool = True,\n    verbose: bool = True,\n) -> Union[SAM, Tuple[SAM, AnnData]]:\n    \"\"\"\\\n    Self-Assembling Manifolds single-cell RNA sequencing analysis tool [Tarashansky19]_.\n\n    SAM iteratively rescales the input gene expression matrix to emphasize\n    genes that are spatially variable along the intrinsic manifold of the data.\n    It outputs the gene weights, nearest neighbor matrix, and a 2D projection.\n\n    The AnnData input should contain unstandardized, non-negative values.\n    Preferably, the data should be log-normalized and no genes should be filtered out.\n\n\n    Parameters\n    ----------\n\n    k\n        The number of nearest neighbors to identify for each cell.\n\n    distance\n        The distance metric to use when identifying nearest neighbors.\n        Can be any of the distance metrics supported by\n        :func:`~scipy.spatial.distance.pdist`.\n\n    max_iter\n        The maximum number of iterations SAM will run.\n\n    projection\n        If 'tsne', generates a t-SNE embedding. If 'umap', generates a UMAP\n        embedding. If 'None', no embedding will be generated.\n\n    standardization\n        If 'Normalizer', use sklearn.preprocessing.Normalizer, which\n        normalizes expression data prior to PCA such that each cell has\n        unit L2 norm. If 'StandardScaler', use\n        sklearn.preprocessing.StandardScaler, which normalizes expression\n        data prior to PCA such that each gene has zero mean and unit\n        variance. Otherwise, do not normalize the expression data. We\n        recommend using 'StandardScaler' for large datasets with many\n        expected cell types and 'Normalizer' otherwise. If 'None', no\n        transformation is applied.\n\n    num_norm_avg\n        The top 'num_norm_avg' dispersions are averaged to determine the\n        normalization factor when calculating the weights. This prevents\n        genes with large spatial dispersions from skewing the distribution\n        of weights.\n\n    weight_pcs\n        If True, scale the principal components by their eigenvalues. In\n        datasets with many expected cell types, setting this to False might\n        improve the resolution as these cell types might be encoded by lower-\n        variance principal components.\n\n    sparse_pca\n        If True, uses an implementation of PCA that accepts sparse inputs.\n        This way, we no longer need a temporary dense copy of the sparse data.\n        However, this implementation is slower and so is only worth using when\n        memory constraints become noticeable.\n\n    n_pcs\n        Determines the number of top principal components selected at each\n        iteration of the SAM algorithm. If None, this number is chosen\n        automatically based on the size of the dataset. If weight_pcs is\n        set to True, this parameter primarily affects the runtime of the SAM\n        algorithm (more PCs = longer runtime).\n\n    n_genes\n        Determines the number of top SAM-weighted genes to use at each iteration\n        of the SAM algorithm. If None, this number is chosen automatically\n        based on the size of the dataset. This parameter primarily affects\n        the runtime of the SAM algorithm (more genes = longer runtime). For\n        extremely homogeneous datasets, decreasing `n_genes` may improve\n        clustering resolution.\n\n    inplace\n        Set fields in `adata` if True. Otherwise, returns a copy.\n\n    verbose\n        If True, displays SAM log statements.\n\n    Returns\n    -------\n    sam_obj if inplace is True or (sam_obj,AnnData) otherwise\n\n    adata - AnnData\n        `.var['weights']`\n            SAM weights for each gene.\n        `.var['spatial_dispersions']`\n            Spatial dispersions for each gene (these are used to compute the\n            SAM weights)\n        `.uns['sam']`\n            Dictionary of SAM-specific outputs, such as the parameters\n            used for preprocessing ('preprocess_args') and running\n            ('run_args') SAM.\n        `.uns['neighbors']`\n            A dictionary with key 'connectivities' containing the kNN adjacency\n            matrix output by SAM. If built-in scanpy dimensionality reduction\n            methods are to be used using the SAM-output AnnData, users\n            should recompute the neighbors using `.obs['X_pca']` with\n            `scanpy.pp.neighbors`.\n        `.obsm['X_pca']`\n            The principal components output by SAM.\n        `.obsm['X_umap']`\n            The UMAP projection output by SAM.\n        `.layers['X_disp']`\n            The expression matrix used for nearest-neighbor averaging.\n        `.layers['X_knn_avg']`\n            The nearest-neighbor-averaged expression data used for computing the\n            spatial dispersions of genes.\n\n    Example\n    -------\n    >>> import scanpy.external as sce\n    >>> import scanpy as sc\n\n    *** Running SAM ***\n\n    Assuming we are given an AnnData object called `adata`, we can run the SAM\n    algorithm as follows:\n\n    >>> sam_obj = sce.tl.sam(adata,inplace=True)\n\n    The input AnnData object should contain unstandardized, non-negative\n    expression values. Preferably, the data should be log-normalized and no\n    genes should be filtered out.\n\n    Please see the documentation for a description of all available parameters.\n\n    For more detailed tutorials, please visit the original Github repository:\n    https://github.com/atarashansky/self-assembling-manifold/tree/master/tutorial\n\n    *** Plotting ***\n\n    To visualize the output, we can use:\n\n    >>> sce.pl.sam(adata,projection='X_umap')\n\n    `sce.pl.sam` accepts all keyword arguments used in the\n    `matplotlib.pyplot.scatter` function.\n\n    *** SAMGUI ***\n\n    SAM comes with the SAMGUI module, a graphical-user interface written with\n    `Plotly` and `ipythonwidgets` for interactively exploring and annotating\n    the scRNAseq data and running SAM.\n\n    Dependencies can be installed with Anaconda by following the instructions in\n    the self-assembling-manifold Github README:\n    https://github.com/atarashansky/self-assembling-manifold\n\n    In a Jupyter notebook, execute the following to launch the interface:\n\n    >>> from samalg.gui import SAMGUI\n    >>> sam_gui = SAMGUI(sam_obj) # sam_obj is your SAM object\n    >>> sam_gui.SamPlot\n\n    This can also be enabled in Jupyer Lab by following the instructions in the\n    self-assembling-manifold README.\n\n    \"\"\"\n\n    try:\n        from samalg import SAM\n    except ImportError:\n        raise ImportError(\n            '\\nplease install sam-algorithm: \\n\\n'\n            '\\tgit clone git://github.com/atarashansky/self-assembling-manifold.git\\n'\n            '\\tcd self-assembling-manifold\\n'\n            '\\tpip install .'\n        )\n\n    logg.info('Self-assembling manifold')\n\n    s = SAM(counts=adata, inplace=inplace)\n\n    logg.info('Running SAM')\n    s.run(\n        max_iter=max_iter,\n        num_norm_avg=num_norm_avg,\n        k=k,\n        distance=distance,\n        preprocessing=standardization,\n        weight_PCs=weight_pcs,\n        npcs=n_pcs,\n        n_genes=n_genes,\n        projection=projection,\n        sparse_pca=sparse_pca,\n        verbose=verbose,\n    )\n\n    s.adata.uns['sam'] = {}\n    for attr in ['nnm', 'preprocess_args', 'run_args', 'ranked_genes']:\n        s.adata.uns['sam'][attr] = s.adata.uns.pop(attr, None)\n\n    return s if inplace else (s, s.adata)", "idx": 221}
{"project": "Scanpy", "commit_id": "7_scanpy_0.0_scanpy_exs_builtin.py_paul15_raw.py", "target": 1, "func": "def paul15_raw():\n    filename = 'data/paul15/paul15.h5'\n    url = 'http://falexwolf.de/data/paul15.h5'\n    ddata = sc.read(filename, 'data.debatched', backup_url=url)\n    # the data has to be transposed (in the hdf5 and R files, each row\n    # corresponds to one gene, we use the opposite convention)\n    ddata = utils.transpose_ddata(ddata)\n    # define local variables to manipulate\n    X = ddata['X']\n    genenames = ddata['colnames']\n    # cluster assocations identified by Paul et al.\n    # groupnames_n = sc.read(filename,'cluster.id')['X']\n    infogenenames = sc.read(filename, 'info.genes_strings')['X']\n    sett.m(1,'the first 10 informative gene names are \\n',infogenenames[:10])\n    # just keep the first of the equivalent names for each gene\n    genenames = np.array([gn.split(';')[0] for gn in genenames])\n    sett.m(1,'the first 10 trunkated gene names are \\n',genenames[:10])\n    # mask array for the informative genes\n    infogenes_idcs = np.array([(True if gn in infogenenames else False)\n                                for gn in genenames])\n    # restrict data array to the 3451 informative genes\n    X = X[:, infogenes_idcs]\n    genenames = genenames[infogenes_idcs]\n    sett.m(1,'after selecting info genes, the first 10 gene names are \\n',\n             genenames[:10])\n    # write to dict\n    ddata['X'] = X\n    ddata['colnames'] = genenames\n    # set root cell as in Haghverdi et al. (2016)\n    ddata['xroot'] = X[840] # note that in Matlab/R, counting starts at 1\n    return ddata", "idx": 225}
{"project": "Scanpy", "commit_id": "425_scanpy_0.2.9_scanpy_exporting.py_save_spring_dir.py", "target": 1, "func": "def save_spring_dir(X, D, k, gene_list, project_directory,\n                    custom_colors={}, cell_groupings={}, use_genes=[]):\n    \"\"\"Builds a SPRING project directory.\n\n    This is based on a preprocessing function by Caleb Weinreb:\n    https://github.com/AllonKleinLab/SPRING/\n    Parameters\n    ----------\n    X : np.ndarray\n        Matrix of gene expression. Rows correspond to cells and columns\n        correspond to genes.\n    D : np.ndarray\n        Distance matrix for construction of knn graph. Any distance matrix can\n        be used as long as higher values correspond to greater distances.\n    k : int\n        Number of edges assigned to each node in knn graph\n    gene_list : list, np.ndarry-like\n        An ordered list of gene names with length X.shape[1].\n    project_directory : str\n        Path to a directory where SPRING readable files will be written. The\n        directory does not have to exist before running this function.\n    cell_groupings : dict\n        Dictionary with one key-value pair for each cell grouping.  The key is\n        the name of the grouping (e.g. \"SampleID\") and the value is a list of\n        labels (e.g. [\"sample1\",\"sample2\"...])  If there are N cells total\n        (i.e. X.shape[0] == N), then the list of labels should have N entries.\n    custom_colors : dict\n        Dictionary with one key-value pair for each custom color.  The key is\n        the name of the color track and the value is a list of scalar values\n        (i.e. color intensities). If there are N cells total (i.e. X.shape[0] ==\n        N), then the list of labels should have N entries.\n    \"\"\"\n    os.system('mkdir ' + project_directory)\n    if not project_directory[-1] == '/': project_directory += '/'\n    # Build graph\n    edges = get_knn_edges(D, k)\n\n    # save genesets\n    custom_colors['Uniform'] = np.zeros(X.shape[0])\n    write_color_tracks(custom_colors, project_directory+'color_data_gene_sets.csv')\n    all = []\n\n    # save gene colortracks\n    os.system('mkdir '+project_directory+'gene_colors')\n    II = len(gene_list) / 50 + 1\n    for j in range(50):\n        fname = project_directory+'/gene_colors/color_data_all_genes-' + repr(j) + '.csv'\n        if len(use_genes) > 0: all_gene_colors = {\n            g: X[:, i+II*j] for i, g in enumerate(gene_list[II*j: II*(j+1)]) if g in use_genes}\n        else:\n            all_gene_colors = {\n                g: X[:, i+II*j] for i, g in enumerate(\n                    gene_list[II*j: II*(j+1)]) if np.mean(X[:, i+II*j]) > 0.05}\n        write_color_tracks(all_gene_colors, fname)\n        all += all_gene_colors.keys()\n\n    # Create and save a dictionary of color profiles to be used by the visualizer\n    color_stats = {}\n    for i in range(X.shape[1]):\n        mean = np.mean(X[:, i])\n        std = np.std(X[:, i])\n        max = np.max(X[:, i])\n        centile = np.percentile(X[:, i], 99.6)\n        color_stats[gene_list[i]] = (mean, std, 0, max, centile)\n    for k, v in custom_colors.items():\n        color_stats[k] = (0, 1, np.min(v), np.max(v)+.01, np.percentile(v, 99))\n    json.dump(color_stats,\n              open(project_directory + '/color_stats.json', 'w'), indent=4, sort_keys=True)\n\n    # save cell labels\n    categorical_coloring_data = {}\n    from matplotlib.colors import cnames\n    for k, labels in cell_groupings.items():\n        label_colors = {l: frac_to_hex(float(i)/len(set(labels)))\n                                       for i, l in enumerate(list(set(labels)))}\n        if k == 'ctpaths':\n            label_colors['dontknow'] = cnames['grey']\n        if k == 'celltypes':\n            label_colors['no_gate'] = cnames['grey']\n        categorical_coloring_data[k] = {'label_colors': label_colors, 'label_list': labels}\n    json.dump(categorical_coloring_data, open(\n        project_directory + '/categorical_coloring_data.json', 'w'), indent=4)\n\n    nodes = [{'name': i, 'number': i} for i in range(X.shape[0])]\n    edges = [{'source': i, 'target': j, 'distance': 0} for i, j in edges]\n    out = {'nodes': nodes, 'links': edges}\n    open(project_directory + 'graph_data.json', 'w').write(\n        json.dumps(out, indent=4, separators=(',', ': ')))", "idx": 232}
{"project": "Scanpy", "commit_id": "320_scanpy_0.1_scanpy_data_structs_data_graph.py___getitem__.py", "target": 1, "func": "def __getitem__(self, index):\n    if isinstance(index, int) or isinstance(index, np.integer):\n        if self.restrict_array is None:\n            glob_index = index\n        else:\n            # map the index back to the global index\n            glob_index = self.restrict_array[index]\n        if glob_index not in self.rows:\n            self.rows[glob_index] = self.get_row(glob_index,\n                                                 DC_start=self.DC_start,\n                                                 DC_end=self.DC_end)\n        row = self.rows[glob_index]\n        if self.restrict_array is None:\n            return row\n        else:\n            return row[self.restrict_array]\n    else:\n        if self.restrict_array is None:\n            glob_index_0, glob_index_1 = index\n        else:\n            glob_index_0 = self.restrict_array[index[0]]\n            glob_index_1 = self.restrict_array[index[1]]\n        if glob_index_0 not in self.rows:\n            self.rows[glob_index_0] = self.get_row(glob_index_0,\n                                                   DC_start=self.DC_start,\n                                                   DC_end=self.DC_end)\n        return self.rows[glob_index_0][glob_index_1]", "idx": 238}
{"project": "Scanpy", "commit_id": "434_scanpy_0.2.9.1_scanpy_readwrite.py_write_anndata_to_file.py", "target": 1, "func": "def write_anndata_to_file(filename, d, ext='h5'):\n    \"\"\"Write dictionary to file.\n    Values need to be np.arrays or transformable to numpy arrays.\n    Parameters\n    ----------\n    filename : str, Path\n        Filename of data file.\n    d : dict\n        Dictionary storing keys with np.ndarray-like data or scalars.\n    ext : string\n        Determines file type, allowed are 'h5' (hdf5),\n        'xlsx' (Excel) [or 'csv' (comma separated value file)].\n    \"\"\"\n    filename = str(filename)  # allow passing pathlib.Path objects\n    directory = os.path.dirname(filename)\n    if not os.path.exists(directory):\n        logg.info('creating directory', directory + '/', 'for saving output files')\n        os.makedirs(directory)\n    # output the following at warning level, it's very important for the users\n    if ext in {'h5', 'npz'}: logg.info('writing', filename)\n    d_write = {}\n    from scipy.sparse import issparse\n    for key, value in d.items():\n        if issparse(value):\n            for k, v in save_sparse_csr(value, key=key).items():\n                d_write[k] = v\n        else:\n            key, value = preprocess_writing(key, value)\n            d_write[key] = value\n    # now open the file\n    if ext == 'h5':\n        with h5py.File(filename, 'w') as f:\n            for key, value in d_write.items():\n                try:\n                    f.create_dataset(key, data=value)\n                except TypeError:\n                    # try writing it as byte strings\n                    try:\n                        if value.dtype.names is None:\n                            f.create_dataset(key, data=value.astype('S'))\n                        else:\n                            new_dtype = [(dt[0], 'S{}'.format(int(dt[1][2:])*4))\n                                         for dt in value.dtype.descr]\n                            f.create_dataset(key, data=value.astype(new_dtype))\n                    except Exception as e:\n                        raise e\n                        logg.info(str(e))\n                        logg.warn('Could not save field with key = \"{}\" to h5 file.'\n                                  .format(key))\n    elif ext == 'npz':\n        np.savez(filename, **d_write)\n    elif ext == 'csv' or ext == 'txt':\n        # here this is actually a directory that corresponds to the\n        # single hdf5 file\n        dirname = filename.replace('.' + ext, '/')\n        # write the following at warning level, it's very important for the users\n        logg.info('writing', ext, 'files to', dirname)\n        if not os.path.exists(dirname): os.makedirs(dirname)\n        if not os.path.exists(dirname + 'add'): os.makedirs(dirname + 'add')\n        from pandas import DataFrame\n        not_yet_raised_data_graph_warning = True\n        for key, value in d_write.items():\n            if key.startswith('data_graph') and not_yet_raised_data_graph_warning:\n                logg.warn('Omitting to write neighborhood graph (`adata.add[\\'data_graph...\\']`).')\n                not_yet_raised_data_graph_warning = False\n                continue\n            filename = dirname\n            if key not in {'X', 'var', 'smp'}: filename += 'add/'\n            filename += key + '.' + ext\n            if value.dtype.names is None:\n                if value.dtype.char == 'S': value = value.astype('U')\n                try:\n                    df = DataFrame(value)\n                except ValueError:\n                    continue\n                df.to_csv(filename, sep=(' ' if ext == 'txt' else ','),\n                          header=False, index=False)\n            else:\n                if np.ndim(value) == 0: value = value[None]\n                df = DataFrame.from_records(value)\n                cols = list(df.select_dtypes(include=[object]).columns)\n                # convert to unicode string\n                df[cols] = df[cols].values.astype('U')\n                if key == 'var':\n                    df = df.T\n                    df.to_csv(filename,\n                              sep=(' ' if ext == 'txt' else ','),\n                              header=False)\n                else:\n                    df.to_csv(filename,\n                              sep=(' ' if ext == 'txt' else ','),\n                              index=False)", "idx": 249}
{"project": "Scanpy", "commit_id": "56_scanpy_0.0_scanpy_readwrite.py__read_softgz.py", "target": 1, "func": "def _read_softgz(filename):\n    \"\"\"\n    Read a SOFT format data file.\n\n    The SOFT format is documented here\n    http://www.ncbi.nlm.nih.gov/geo/info/soft2.html.\n    Returns\n    -------\n    ddata : dict, containing\n        X : np.ndarray\n            A d x n array of gene expression values.\n        colnames : np.ndarray\n            A list of gene identifiers of length d.\n        rownames : np.ndarray\n            A list of sample identifiers of length n.\n        groups : np.ndarray\n            A list of sample desriptions of length n.\n    Note\n    ----\n    The function is based on a script by Kerby Shedden.\n    http://dept.stat.lsa.umich.edu/~kshedden/Python-Workshop/gene_expression_comparison.html\n    \"\"\"\n    import gzip\n    with gzip.open(filename) as file:\n        # The header part of the file contains information about the\n        # samples. Read that information first.\n        samples_info = {}\n        for line in file:\n            line = line.decode(\"utf-8\")\n            if line.startswith(\"!dataset_table_begin\"):\n                break\n            elif line.startswith(\"!subset_description\"):\n                subset_description = line.split(\"=\")[1].strip()\n            elif line.startswith(\"!subset_sample_id\"):\n                subset_ids = line.split(\"=\")[1].split(\",\")\n                subset_ids = [x.strip() for x in subset_ids]\n                for k in subset_ids:\n                    samples_info[k] = subset_description\n        # Next line is the column headers (sample id's)\n        sample_names = file.readline().decode(\"utf-8\").split(\"\\t\")\n        # The column indices that contain gene expression data\n        I = [i for i, x in enumerate(sample_names) if x.startswith(\"GSM\")]\n        # Restrict the column headers to those that we keep\n        sample_names = [sample_names[i] for i in I]\n        # Get a list of sample labels\n        groups = [samples_info[k] for k in sample_names]\n        # Read the gene expression data as a list of lists, also get the gene\n        # identifiers\n        gene_names, X = [], []\n        for line in file:\n            line = line.decode(\"utf-8\")\n            # This is what signals the end of the gene expression data\n            # section in the file\n            if line.startswith(\"!dataset_table_end\"):\n                break\n            V = line.split(\"\\t\")\n            # Extract the values that correspond to gene expression measures\n            # and convert the strings to numbers\n            x = [float(V[i]) for i in I]\n            X.append(x)\n            gene_names.append(  # V[0] + \";\" + # only use the second gene name\n                V[1])\n    # Convert the Python list of lists to a Numpy array and transpose to match\n    # the Scanpy convention of storing samples in rows and variables in colums.\n    X = np.array(X).T\n    rownames = sample_names\n    colnames = gene_names\n    ddata = {'X': X, 'rownames': rownames, 'colnames': colnames,\n             'groups': groups}\n    return ddata", "idx": 254}
{"project": "Scanpy", "commit_id": "128_scanpy_0.0_scanpy_classes_data_graph.py___init__.py", "target": 1, "func": "def __init__(self, adata_or_X_or_Dsq, params):\n        \"\"\" \n        \"\"\"\n        isadata = isinstance(adata_or_X_or_Dsq, AnnData)\n        if isadata:\n            adata = adata_or_X_or_Dsq\n        else:\n            X_or_Dsq = adata_or_X_or_Dsq\n        if isadata:\n            self.Dsq = None\n            if 'X_pca' in adata and adata.X.shape[1] > 50:\n                self.X = adata['X_pca']\n                sett.m(0, '--> using X_pca for building graph')\n            else:\n                self.X = adata.X\n                sett.m(0, '--> using X for building graph')\n        else:\n            if X_or_Dsq.shape[0] == X_or_Dsq.shape[1]:\n                sett.m(0,'--> computing data graph from distance matrix')\n                self.Dsq = X_or_Dsq\n                self.X = None\n            else:\n                sett.m(0,'--> computing data graph from data matrix')\n                self.X = X_or_Dsq\n                self.Dsq = None\n        self.N = self.X.shape[0]\n        self.params = params\n        if self.params['sigma'] > 0:\n            self.params['method'] = 'global'\n        else:\n            self.params['method'] = 'local'", "idx": 258}
{"project": "Scanpy", "commit_id": "951_scanpy_1.4.4_scanpy_tools__score_genes.py_score_genes.py", "target": 1, "func": "def score_genes(\n    adata: AnnData,\n    gene_list: Sequence[str],\n    ctrl_size: int = 50,\n    gene_pool: Optional[Sequence[str]] = None,\n    n_bins: int = 25,\n    score_name: str = 'score',\n    random_state: Optional[Union[int, RandomState]] = 0,\n    copy: bool = False,\n    use_raw: bool = None,\n) -> Optional[AnnData]:\n    \"\"\"\\\n    Score a set of genes [Satija15]_.\n    The score is the average expression of a set of genes subtracted with the\n    average expression of a reference set of genes. The reference set is\n    randomly sampled from the `gene_pool` for each binned expression value.\n    This reproduces the approach in Seurat [Satija15]_ and has been implemented\n    for Scanpy by Davide Cittaro.\n    Parameters\n    ----------\n    adata\n        The annotated data matrix.\n    gene_list\n        The list of gene names used for score calculation.\n    ctrl_size\n        Number of reference genes to be sampled. If `len(gene_list)` is not too\n        low, you can set `ctrl_size=len(gene_list)`.\n    gene_pool\n        Genes for sampling the reference set. Default is all genes.\n    n_bins\n        Number of expression level bins for sampling.\n    score_name\n        Name of the field to be added in `.obs`.\n    random_state\n        The random seed for sampling.\n    copy\n        Copy `adata` or modify it inplace.\n    use_raw\n        Use `raw` attribute of `adata` if present.\n    Returns\n    -------\n    Depending on `copy`, returns or updates `adata` with an additional field\n    `score_name`.\n    Examples\n    --------\n    See this `notebook <https://github.com/theislab/scanpy_usage/tree/master/180209_cell_cycle>`__.\n    \"\"\"\n    start = logg.info(f'computing score {score_name!r}')\n    adata = adata.copy() if copy else adata\n    if random_state is not None:\n        np.random.seed(random_state)\n    gene_list_in_var = []\n    var_names = adata.raw.var_names if use_raw else adata.var_names\n    genes_to_ignore = []\n    for gene in gene_list:\n        if gene in var_names:\n            gene_list_in_var.append(gene)\n        else:\n            genes_to_ignore.append(gene)\n    if len(genes_to_ignore) > 0:\n        logg.warning(f'genes are not in var_names and ignored: {genes_to_ignore}')\n    gene_list = set(gene_list_in_var[:])\n    if len(gene_list) == 0:\n        logg.warning('provided gene list has length 0, scores as 0')\n        adata.obs[score_name] = 0\n        return adata if copy else None\n    if gene_pool is None:\n        gene_pool = list(var_names)\n    else:\n        gene_pool = [x for x in gene_pool if x in var_names]\n    # Trying here to match the Seurat approach in scoring cells.\n    # Basically we need to compare genes against random genes in a matched\n    # interval of expression.\n    if use_raw is None:\n        use_raw = True if adata.raw is not None else False\n    _adata = adata.raw if use_raw else adata\n    _adata_subset = _adata[:, gene_pool] if len(gene_pool) < len(_adata.var_names) else _adata\n    if issparse(_adata_subset.X):\n        obs_avg = pd.Series(\n            np.array(_adata_subset.X.mean(axis=0)).flatten(), index=gene_pool)  # average expression of genes\n    else:\n        obs_avg = pd.Series(\n            np.nanmean(_adata_subset.X, axis=0), index=gene_pool)  # average expression of genes\n    obs_avg = obs_avg[np.isfinite(obs_avg)] # Sometimes (and I don't know how) missing data may be there, with nansfor\n    n_items = int(np.round(len(obs_avg) / (n_bins - 1)))\n    obs_cut = obs_avg.rank(method='min') // n_items\n    control_genes = set()\n    # now pick `ctrl_size` genes from every cut\n    for cut in np.unique(obs_cut.loc[gene_list]):\n        r_genes = np.array(obs_cut[obs_cut == cut].index)\n        np.random.shuffle(r_genes)\n        # uses full r_genes if ctrl_size > len(r_genes)\n        control_genes.update(set(r_genes[:ctrl_size]))\n    # To index, we need a list \u2013 indexing implies an order.\n    control_genes = list(control_genes - gene_list)\n    gene_list = list(gene_list)\n    X_list = _adata[:, gene_list].X\n    if issparse(X_list): X_list = X_list.toarray()\n    X_control = _adata[:, control_genes].X\n    if issparse(X_control): X_control = X_control.toarray()\n    X_control = np.nanmean(X_control, axis=1)\n    if len(gene_list) == 0:\n        # We shouldn't even get here, but just in case\n        logg.hint(\n            f'could not add \\n'\n            f'    {score_name!r}, score of gene set (adata.obs)'\n        )\n        return adata if copy else None\n    elif len(gene_list) == 1:\n        score = _adata[:, gene_list].X - X_control\n    else:\n        score = np.nanmean(X_list, axis=1) - X_control\n\n    adata.obs[score_name] = pd.Series(np.array(score).ravel(), index=adata.obs_names)\n    logg.info(\n        '    finished',\n        time=start,\n        deep=(\n            'added\\n'\n            f'    {score_name!r}, score of gene set (adata.obs)'\n        ),\n    )\n    return adata if copy else None", "idx": 259}
{"project": "Scanpy", "commit_id": "191_scanpy_1.9.0__gearys_c.py__gearys_c_inner_sparse_x_sparsevec.py", "target": 0, "func": "def _gearys_c_inner_sparse_x_sparsevec(\n    g_data, g_indices, g_indptr, x_data, x_indices, N, W\n):\n    x = np.zeros(N, dtype=np.float_)\n    x[x_indices] = x_data\n    x_bar = np.sum(x_data) / N\n    total = 0.0\n    N = len(x)\n    for i in numba.prange(N):\n        s = slice(g_indptr[i], g_indptr[i + 1])\n        i_indices = g_indices[s]\n        i_data = g_data[s]\n        total += np.sum(i_data * ((x[i] - x[i_indices]) ** 2))\n    numer = (N - 1) * total\n    # Expanded from 2 * W * ((x_k - x_k_bar) ** 2).sum(), but uses sparsity\n    # to skip some calculations\n    # fmt: off\n    denom = (\n        2 * W\n        * (\n            np.sum(x_data ** 2)\n            - np.sum(x_data * x_bar * 2)\n            + (x_bar ** 2) * N\n        )\n    )\n    # fmt: on\n    C = numer / denom\n    return C", "idx": 260}
{"project": "Scanpy", "commit_id": "278_scanpy_1.9.0__dotplot.py_dotplot.py", "target": 0, "func": "def dotplot(\n    adata: AnnData,\n    var_names: Union[_VarNames, Mapping[str, _VarNames]],\n    groupby: Union[str, Sequence[str]],\n    use_raw: Optional[bool] = None,\n    log: bool = False,\n    num_categories: int = 7,\n    expression_cutoff: float = 0.0,\n    mean_only_expressed: bool = False,\n    cmap: str = 'Reds',\n    dot_max: Optional[float] = DotPlot.DEFAULT_DOT_MAX,\n    dot_min: Optional[float] = DotPlot.DEFAULT_DOT_MIN,\n    standard_scale: Optional[Literal['var', 'group']] = None,\n    smallest_dot: Optional[float] = DotPlot.DEFAULT_SMALLEST_DOT,\n    title: Optional[str] = None,\n    colorbar_title: Optional[str] = DotPlot.DEFAULT_COLOR_LEGEND_TITLE,\n    size_title: Optional[str] = DotPlot.DEFAULT_SIZE_LEGEND_TITLE,\n    figsize: Optional[Tuple[float, float]] = None,\n    dendrogram: Union[bool, str] = False,\n    gene_symbols: Optional[str] = None,\n    var_group_positions: Optional[Sequence[Tuple[int, int]]] = None,\n    var_group_labels: Optional[Sequence[str]] = None,\n    var_group_rotation: Optional[float] = None,\n    layer: Optional[str] = None,\n    swap_axes: Optional[bool] = False,\n    dot_color_df: Optional[pd.DataFrame] = None,\n    show: Optional[bool] = None,\n    save: Union[str, bool, None] = None,\n    ax: Optional[_AxesSubplot] = None,\n    return_fig: Optional[bool] = False,\n    vmin: Optional[float] = None,\n    vmax: Optional[float] = None,\n    vcenter: Optional[float] = None,\n    norm: Optional[Normalize] = None,\n    **kwds,\n) -> Union[DotPlot, dict, None]:\n    \"\"\"\\\n    Makes a *dot plot* of the expression values of `var_names`.\n\n    For each var_name and each `groupby` category a dot is plotted.\n    Each dot represents two values: mean expression within each category\n    (visualized by color) and fraction of cells expressing the `var_name` in the\n    category (visualized by the size of the dot). If `groupby` is not given,\n    the dotplot assumes that all data belongs to a single category.\n\n    .. note::\n       A gene is considered expressed if the expression value in the `adata` (or\n       `adata.raw`) is above the specified threshold which is zero by default.\n\n    An example of dotplot usage is to visualize, for multiple marker genes,\n    the mean value and the percentage of cells expressing the gene\n    across  multiple clusters.\n\n    This function provides a convenient interface to the :class:`~scanpy.pl.DotPlot`\n    class. If you need more flexibility, you should use :class:`~scanpy.pl.DotPlot`\n    directly.\n\n    Parameters\n    ----------\n    {common_plot_args}\n    {groupby_plots_args}\n    size_title\n        Title for the size legend. New line character (\\\\n) can be used.\n    expression_cutoff\n        Expression cutoff that is used for binarizing the gene expression and\n        determining the fraction of cells expressing given genes. A gene is\n        expressed only if the expression value is greater than this threshold.\n    mean_only_expressed\n        If True, gene expression is averaged only over the cells\n        expressing the given genes.\n    dot_max\n        If none, the maximum dot size is set to the maximum fraction value found\n        (e.g. 0.6). If given, the value should be a number between 0 and 1.\n        All fractions larger than dot_max are clipped to this value.\n    dot_min\n        If none, the minimum dot size is set to 0. If given,\n        the value should be a number between 0 and 1.\n        All fractions smaller than dot_min are clipped to this value.\n    smallest_dot\n        If none, the smallest dot has size 0.\n        All expression levels with `dot_min` are plotted with this size.\n    {show_save_ax}\n    {vminmax}\n    kwds\n        Are passed to :func:`matplotlib.pyplot.scatter`.\n\n    Returns\n    -------\n    If `return_fig` is `True`, returns a :class:`~scanpy.pl.DotPlot` object,\n    else if `show` is false, return axes dict\n\n    See also\n    --------\n    :class:`~scanpy.pl.DotPlot`: The DotPlot class can be used to to control\n        several visual parameters not available in this function.\n    :func:`~scanpy.pl.rank_genes_groups_dotplot`: to plot marker genes\n        identified using the :func:`~scanpy.tl.rank_genes_groups` function.\n\n    Examples\n    --------\n\n    Create a dot plot using the given markers and the PBMC example dataset grouped by\n    the category 'bulk_labels'.\n\n    .. plot::\n        :context: close-figs\n\n        import scanpy as sc\n        adata = sc.datasets.pbmc68k_reduced()\n        markers = ['C1QA', 'PSAP', 'CD79A', 'CD79B', 'CST3', 'LYZ']\n        sc.pl.dotplot(adata, markers, groupby='bulk_labels', dendrogram=True)\n\n    Using var_names as dict:\n\n    .. plot::\n        :context: close-figs\n\n        markers = {{'T-cell': 'CD3D', 'B-cell': 'CD79A', 'myeloid': 'CST3'}}\n        sc.pl.dotplot(adata, markers, groupby='bulk_labels', dendrogram=True)\n\n    Get DotPlot object for fine tuning\n\n    .. plot::\n        :context: close-figs\n\n        dp = sc.pl.dotplot(adata, markers, 'bulk_labels', return_fig=True)\n        dp.add_totals().style(dot_edge_color='black', dot_edge_lw=0.5).show()\n\n    The axes used can be obtained using the get_axes() method\n\n    .. code-block:: python\n\n        axes_dict = dp.get_axes()\n        print(axes_dict)\n\n    \"\"\"\n\n    # backwards compatibility: previous version of dotplot used `color_map`\n    # instead of `cmap`\n    cmap = kwds.get('color_map', cmap)\n    if 'color_map' in kwds:\n        del kwds['color_map']\n\n    dp = DotPlot(\n        adata,\n        var_names,\n        groupby,\n        use_raw=use_raw,\n        log=log,\n        num_categories=num_categories,\n        expression_cutoff=expression_cutoff,\n        mean_only_expressed=mean_only_expressed,\n        standard_scale=standard_scale,\n        title=title,\n        figsize=figsize,\n        gene_symbols=gene_symbols,\n        var_group_positions=var_group_positions,\n        var_group_labels=var_group_labels,\n        var_group_rotation=var_group_rotation,\n        layer=layer,\n        dot_color_df=dot_color_df,\n        ax=ax,\n        vmin=vmin,\n        vmax=vmax,\n        vcenter=vcenter,\n        norm=norm,\n        **kwds,\n    )\n\n    if dendrogram:\n        dp.add_dendrogram(dendrogram_key=dendrogram)\n    if swap_axes:\n        dp.swap_axes()\n\n    dp = dp.style(\n        cmap=cmap,\n        dot_max=dot_max,\n        dot_min=dot_min,\n        smallest_dot=smallest_dot,\n        dot_edge_lw=kwds.pop('linewidth', DotPlot.DEFAULT_DOT_EDGELW),\n    ).legend(colorbar_title=colorbar_title, size_title=size_title)\n\n    if return_fig:\n        return dp\n    else:\n        dp.make_figure()\n        savefig_or_show(DotPlot.DEFAULT_SAVE_PREFIX, show=show, save=save)\n        show = settings.autoshow if show is None else show\n        if not show:\n            return dp.get_axes()", "idx": 264}
{"project": "Scanpy", "commit_id": "550_scanpy_1.9.0_test_get.py_shared_key_adata.py", "target": 0, "func": "def shared_key_adata(request):\n    kind = request.param\n    adata = sc.AnnData(\n        np.arange(50).reshape((5, 10)),\n        obs=pd.DataFrame(np.zeros((5, 1)), columns=[\"var_id\"]),\n        var=pd.DataFrame(index=[\"var_id\"] + [f\"gene_{i}\" for i in range(1, 10)]),\n    )\n    if kind == \"obs_df\":\n        return (\n            adata,\n            sc.get.obs_df,\n            r\"'var_id'.* adata\\.obs .* adata.var_names\",\n        )\n    elif kind == \"var_df\":\n        return (\n            adata.T,\n            sc.get.var_df,\n            r\"'var_id'.* adata\\.var .* adata.obs_names\",\n        )\n    elif kind == \"obs_df:use_raw\":\n        adata.raw = adata\n        adata.var_names = [f\"gene_{i}\" for i in range(10)]\n        return (\n            adata,\n            partial(sc.get.obs_df, use_raw=True),\n            r\"'var_id'.* adata\\.obs .* adata\\.raw\\.var_names\",\n        )\n    elif kind == \"obs_df:gene_symbols\":\n        adata.var[\"gene_symbols\"] = adata.var_names\n        adata.var_names = [f\"gene_{i}\" for i in range(10)]\n        return (\n            adata,\n            partial(sc.get.obs_df, gene_symbols=\"gene_symbols\"),\n            r\"'var_id'.* adata\\.obs .* adata\\.var\\['gene_symbols'\\]\",\n        )\n    elif kind == \"obs_df:gene_symbols,use_raw\":\n        base = adata.copy()\n        adata.var[\"gene_symbols\"] = adata.var_names\n        adata.var_names = [f\"gene_{i}\" for i in range(10)]\n        base.raw = adata\n        return (\n            base,\n            partial(\n                sc.get.obs_df,\n                gene_symbols=\"gene_symbols\",\n                use_raw=True,\n            ),\n            r\"'var_id'.* adata\\.obs .* adata\\.raw\\.var\\['gene_symbols'\\]\",\n        )\n    else:\n        assert False", "idx": 268}
{"project": "Scanpy", "commit_id": "699_scanpy_1.9.0_test_preprocessing_distributed.py_test_filter_genes.py", "target": 0, "func": "def test_filter_genes(self, adata, adata_dist):\n        filter_genes(adata_dist, min_cells=2)\n        result = materialize_as_ndarray(adata_dist.X)\n        filter_genes(adata, min_cells=2)\n        assert result.shape == adata.shape\n        assert result.shape == (adata.n_obs, adata.n_vars)\n        npt.assert_allclose(result, adata.X)", "idx": 269}
{"project": "Scanpy", "commit_id": "236_scanpy_0.1_scanpy_readwrite.py_get_params_from_list.py", "target": 1, "func": "def get_params_from_list(params_list):\n    \"\"\"\n    Transform params list to dictionary.\n    \"\"\"\n    params = {}\n    for i in range(0, len(params_list)):\n        key_val = params_list[i].split('=')\n        if len(key_val) != 2:\n            raise ValueError('Need to provide parameters as list of the form `par1=value1 par2=value2 ...`.')\n        key, val = key_val\n        params[key] = convert_string(val)\n    return params", "idx": 270}
{"project": "Scanpy", "commit_id": "41_scanpy_1.9.0_logging.py_print_version_and_date.py", "target": 0, "func": "def print_version_and_date(*, file=None):\n    \"\"\"\\\n    Useful for starting a notebook so you see when you started working.\n    \"\"\"\n    from . import __version__\n\n    if file is None:\n        file = sys.stdout\n    print(\n        f'Running Scanpy {__version__}, ' f'on {datetime.now():%Y-%m-%d %H:%M}.',\n        file=file,", "idx": 279}
{"project": "Scanpy", "commit_id": "507_scanpy_1.9.0_test_embedding_density.py_test_embedding_density.py", "target": 0, "func": "def test_embedding_density():\n    # Test that density values are scaled\n    # Test that the highest value is in the middle for a grid layout\n    test_data = AnnData(X=np.ones((9, 10)))\n    test_data.obsm['X_test'] = np.array([[x, y] for x in range(3) for y in range(3)])\n    sc.tl.embedding_density(test_data, 'test')\n\n    max_dens = np.max(test_data.obs['test_density'])\n    min_dens = np.min(test_data.obs['test_density'])\n    max_idx = test_data.obs['test_density'].idxmax()\n\n    assert max_idx == '4'\n    assert max_dens == 1\n    assert min_dens == 0", "idx": 280}
{"project": "Scanpy", "commit_id": "573_scanpy_1.9.0_test_logging.py_test_logfile.py", "target": 0, "func": "def test_logfile(tmp_path, logging_state):\n    s.verbosity = Verbosity.hint\n\n    io = StringIO()\n    s.logfile = io\n    assert s.logfile is io\n    assert s.logpath is None\n    log.error('test!')\n    assert io.getvalue() == 'ERROR: test!\\n'\n\n    p = tmp_path / 'test.log'\n    s.logpath = p\n    assert s.logpath == p\n    assert s.logfile.name == str(p)\n    log.hint('test2')\n    log.debug('invisible')\n    assert s.logpath.read_text() == '--> test2\\n'", "idx": 286}
{"project": "Scanpy", "commit_id": "32_scanpy_0.0_scanpy_tools_drawg.py_drawg.py", "target": 1, "func": "def drawg(ddata, k=4, nr_comps=2):\n    \"\"\"\n    Visualize data using graph drawing algorithms.\n    In particular the force-directed Fruchterman-Reingold algorithm.\n    Parameters\n    ----------\n    ddata : dict containing\n        X : np.ndarray\n            Data array, rows store observations, columns covariates.\n    k : int\n        Number of nearest neighbors in graph.\n    nr_comps : int\n        Number of principal components in preprocessing PCA.\n    Returns\n    -------\n    d : dict containing\n        Y : np.ndarray\n            Fruchterman Reingold representation of data.\n    \"\"\"\n    sett.m(0,'draw knn graph')\n    X = ddata['X']\n    D = utils.comp_distance(X, metric='euclidean')\n    # deterimine the distance of the k nearest neighbors\n    indices = np.zeros((D.shape[0],k),dtype=np.int_)\n    for irow, row in enumerate(D):\n        # the last item is already in its sorted position as\n        # argpartition puts the (k-1)th element - starting to count from\n        # zero - in its sorted position\n        idcs = np.argpartition(row,k-1)[:k]\n        indices[irow] = idcs\n    # compute adjacency matrix\n    # make this float, as we might put a weight matrix here\n    Adj = np.zeros(D.shape, dtype=float)\n    for irow, row in enumerate(indices):\n        Adj[irow,row] = 1\n        # symmetrize as in DPT\n        # for j in row:\n        #    if irow not in indices[j]:\n        #        Adj[j,irow] = 1\n\n    # just sample initial positions, the rest is done by the plotting tool\n    np.random.seed(1)\n    Y = np.asarray(np.random.random((Adj.shape[0], 2)), dtype=Adj.dtype)\n    # compute first step\n    istep = 1\n    sett.mt(0, 'compute Fruchterman-Reingold layout: step', istep)\n    Y = fruchterman_reingold_layout(Adj, Yinit=Y, iterations=step_size)\n    sett.mt(0, 'finished')\n    return {'type': 'drawg', 'Y': Y, 'Adj': Adj, 'istep': istep}", "idx": 290}
{"project": "Scanpy", "commit_id": "994_scanpy_1.4.6_scanpy_tools__diffmap.py_diffmap.py", "target": 1, "func": "def diffmap(adata: AnnData, n_comps: int = 15, copy: bool = False):\n    \"\"\"\\\n    Diffusion Maps [Coifman05]_ [Haghverdi15]_ [Wolf18]_.\n    Diffusion maps [Coifman05]_ has been proposed for visualizing single-cell\n    data by [Haghverdi15]_. The tool uses the adapted Gaussian kernel suggested\n    by [Haghverdi16]_ in the implementation of [Wolf18]_.\n    The width (\"sigma\") of the connectivity kernel is implicitly determined by\n    the number of neighbors used to compute the single-cell graph in\n    :func:`~scanpy.pp.neighbors`. To reproduce the original implementation\n    using a Gaussian kernel, use `method=='gauss'` in\n    :func:`~scanpy.pp.neighbors`. To use an exponential kernel, use the default\n    `method=='umap'`. Differences between these options shouldn't usually be\n    dramatic.\n    Parameters\n    ----------\n    adata\n        Annotated data matrix.\n    n_comps\n        The number of dimensions of the representation.\n    copy\n        Return a copy instead of writing to adata.\n    Returns\n    -------\n    Depending on `copy`, returns or updates `adata` with the following fields.\n    `X_diffmap` : :class:`numpy.ndarray` (`adata.obsm`)\n        Diffusion map representation of data, which is the right eigen basis of\n        the transition matrix with eigenvectors as columns.\n    `diffmap_evals` : :class:`numpy.ndarray` (`adata.uns`)\n        Array of size (number of eigen vectors).\n        Eigenvalues of transition matrix.\n    \"\"\"\n    if 'neighbors' not in adata.uns:\n        raise ValueError(\n            'You need to run `pp.neighbors` first to compute a neighborhood graph.'\n        )\n    if n_comps <= 2:\n        raise ValueError('Provide any value greater than 2 for `n_comps`. ')\n    adata = adata.copy() if copy else adata\n    _diffmap(adata, n_comps=n_comps)\n    return adata if copy else None", "idx": 297}
{"project": "Scanpy", "commit_id": "597_scanpy_1.0.4_scanpy_plotting_tools___init__.py_rank_genes_groups_violin.py", "target": 1, "func": "def rank_genes_groups_violin(adata, groups=None, n_genes=20,\n                             use_raw=None,\n                             split=True,\n                             scale='width',\n                             strip=True, jitter=True, size=1,\n                             ax=None, show=None, save=None):\n    \"\"\"Plot ranking of genes for all tested comparisons.\n    Parameters\n    ----------\n    adata : :class:`~scanpy.api.AnnData`\n        Annotated data matrix.\n    groups : list of `str`, optional (default: `None`)\n        List of group names.\n    n_genes : `int`, optional (default: 20)\n        Number of genes to show.\n    gene_symbols : `str`\n        Key for field in `.var` that stores gene symbols if you do not want to\n        use `.var_names`.\n    use_raw : `bool`, optional (default: `None`)\n        Use `raw` attribute of `adata` if present. Defaults to the value that\n        was used in :func:`~scanpy.api.tl.rank_genes_groups`.\n    split : `bool`, optional (default: `True`)\n        Whether to split the violins or not.\n    scale : `str` (default: 'width')\n        See `seaborn.violinplot`.\n    strip : `bool` (default: `True`)\n        Show a strip plot on top of the violin plot.\n    jitter : `int`, `float`, `bool`, optional (default: `True`)\n        If set to 0, no points are drawn. See `seaborn.stripplot`.\n    size : `int`, optional (default: 1)\n        Size of the jitter points.\n    show : `bool`, optional (default: `None`)\n        Show the plot, do not return axis.\n    save : `bool` or `str`, optional (default: `None`)\n        If `True` or a `str`, save the figure. A string is appended to the\n        default filename. Infer the filetype if ending on {{'.pdf', '.png', '.svg'}}.\n    ax : `matplotlib.Axes`, optional (default: `None`)\n        A `matplotlib.Axes` object.\n    \"\"\"\n    from ..tools import rank_genes_groups\n    groups_key = str(adata.uns['rank_genes_groups']['params']['groupby'])\n    if use_raw is None:\n        use_raw = bool(adata.uns['rank_genes_groups']['params']['use_raw'])\n    reference = str(adata.uns['rank_genes_groups']['params']['reference'])\n    groups_names = (adata.uns['rank_genes_groups']['names'].dtype.names\n                    if groups is None else groups)\n    if isinstance(groups_names, str): groups_names = [groups_names]\n    for group_name in groups_names:\n        gene_names = adata.uns[\n            'rank_genes_groups']['names'][group_name][:n_genes]\n        keys = gene_names\n        # make a \"hue\" option!\n        df = pd.DataFrame()\n        new_keys = []\n        for key in keys:\n            if adata.raw is not None and use_raw:\n                X_col = adata.raw[:, key].X\n            else:\n                X_col = adata[:, key].X\n            if issparse(X_col): X_col = X_col.toarray().flatten()\n            key = key if gene_symbols is None else adata.var[gene_symbols][key]\n            new_keys.append(key)\n            df[key] = X_col\n        df['hue'] = adata.obs[groups_key].astype(str).values\n        if reference == 'rest':\n            df['hue'][df['hue'] != group_name] = 'rest'\n        else:\n            df['hue'][~df['hue'].isin([group_name, reference])] = np.nan\n        df['hue'] = df['hue'].astype('category')\n        df_tidy = pd.melt(df, id_vars='hue', value_vars=new_keys)\n        x = 'variable'\n        y = 'value'\n        hue_order = [group_name, reference]\n        import seaborn as sns\n        ax = sns.violinplot(x=x, y=y, data=df_tidy, inner=None,\n                            hue_order=hue_order, hue='hue', split=split,\n                            scale=scale, orient='vertical', ax=ax)\n        if strip:\n            ax = sns.stripplot(x=x, y=y, data=df_tidy,\n                               hue='hue', dodge=True, hue_order=hue_order,\n                               jitter=jitter, color='black', size=size, ax=ax)\n        ax.set_xlabel('genes')\n        ax.set_title('{} vs. {}'.format(group_name, reference))\n        ax.legend_.remove()\n        if computed_distribution: ax.set_ylabel('z-score w.r.t. to bulk mean')\n        else: ax.set_ylabel('expression')\n        ax.set_xticklabels(gene_names, rotation='vertical')\n        writekey = ('rank_genes_groups_'\n                    + str(adata.uns['rank_genes_groups']['params']['groupby'])\n                    + '_' + group_name)\n        utils.savefig_or_show(writekey, show=show, save=save)", "idx": 299}
{"project": "Scanpy", "commit_id": "612_scanpy_1.9.0_test_package_structure.py_test_function_headers.py", "target": 0, "func": "def test_function_headers(f):\n    name = f\"{f.__module__}.{f.__qualname__}\"\n    assert f.__doc__ is not None, f\"{name} has no docstring\"\n    lines = getattr(f, \"__orig_doc__\", f.__doc__).split(\"\\n\")\n    broken = [i for i, l in enumerate(lines) if l.strip() and not l.startswith(\"    \")]\n    if any(broken):\n        msg = f'''\\\nHeader of function `{name}`\u2019s docstring should start with one-line description\nand be consistently indented like this:\n\n\u2423\u2423\u2423\u2423\"\"\"\\\\\n\u2423\u2423\u2423\u2423My one-line\u2423description.\n\n\u2423\u2423\u2423\u2423\u2026\n\u2423\u2423\u2423\u2423\"\"\"\n\nThe displayed line is under-indented.\n'''\n        filename = inspect.getsourcefile(f)\n        _, lineno = inspect.getsourcelines(f)\n        text = f\">{lines[broken[0]]}<\"\n        raise SyntaxError(msg, (filename, lineno, 2, text))", "idx": 305}
{"project": "Scanpy", "commit_id": "1089_scanpy_1.8.0_scanpy_tools__rank_genes_groups.py_rank_genes_groups.py", "target": 1, "func": "def rank_genes_groups(\n    adata: AnnData,\n    groupby: str,\n    use_raw: Optional[bool] = None,\n    groups: Union[Literal['all'], Iterable[str]] = 'all',\n    reference: str = 'rest',\n    n_genes: Optional[int] = None,\n    rankby_abs: bool = False,\n    pts: bool = False,\n    key_added: Optional[str] = None,\n    copy: bool = False,\n    method: _Method = None,\n    corr_method: _CorrMethod = 'benjamini-hochberg',\n    tie_correct: bool = False,\n    layer: Optional[str] = None,\n    **kwds,\n) -> Optional[AnnData]:\n    \"\"\"\\\n    Rank genes for characterizing groups.\n    Expects logarithmized data.\n    Parameters\n    ----------\n    adata\n        Annotated data matrix.\n    groupby\n        The key of the observations grouping to consider.\n    use_raw\n        Use `raw` attribute of `adata` if present.\n    layer\n        Key from `adata.layers` whose value will be used to perform tests on.\n    groups\n        Subset of groups, e.g. [`'g1'`, `'g2'`, `'g3'`], to which comparison\n        shall be restricted, or `'all'` (default), for all groups.\n    reference\n        If `'rest'`, compare each group to the union of the rest of the group.\n        If a group identifier, compare with respect to this group.\n    n_genes\n        The number of genes that appear in the returned tables.\n        Defaults to all genes.\n    method\n        The default method is `'t-test'`,\n        `'t-test_overestim_var'` overestimates variance of each group,\n        `'wilcoxon'` uses Wilcoxon rank-sum,\n        `'logreg'` uses logistic regression. See [Ntranos18]_,\n        `here <https://github.com/theislab/scanpy/issues/95>`__ and `here\n        <http://www.nxn.se/valent/2018/3/5/actionable-scrna-seq-clusters>`__,\n        for why this is meaningful.\n    corr_method\n        p-value correction method.\n        Used only for `'t-test'`, `'t-test_overestim_var'`, and `'wilcoxon'`.\n    tie_correct\n        Use tie correction for `'wilcoxon'` scores.\n        Used only for `'wilcoxon'`.\n    rankby_abs\n        Rank genes by the absolute value of the score, not by the\n        score. The returned scores are never the absolute values.\n    pts\n        Compute the fraction of cells expressing the genes.\n    key_added\n        The key in `adata.uns` information is saved to.\n    **kwds\n        Are passed to test methods. Currently this affects only parameters that\n        are passed to :class:`sklearn.linear_model.LogisticRegression`.\n        For instance, you can pass `penalty='l1'` to try to come up with a\n        minimal set of genes that are good predictors (sparse solution meaning\n        few non-zero fitted coefficients).\n    Returns\n    -------\n    **names** : structured `np.ndarray` (`.uns['rank_genes_groups']`)\n        Structured array to be indexed by group id storing the gene\n        names. Ordered according to scores.\n    **scores** : structured `np.ndarray` (`.uns['rank_genes_groups']`)\n        Structured array to be indexed by group id storing the z-score\n        underlying the computation of a p-value for each gene for each\n        group. Ordered according to scores.\n    **logfoldchanges** : structured `np.ndarray` (`.uns['rank_genes_groups']`)\n        Structured array to be indexed by group id storing the log2\n        fold change for each gene for each group. Ordered according to\n        scores. Only provided if method is 't-test' like.\n        Note: this is an approximation calculated from mean-log values.\n    **pvals** : structured `np.ndarray` (`.uns['rank_genes_groups']`)\n        p-values.\n    **pvals_adj** : structured `np.ndarray` (`.uns['rank_genes_groups']`)\n        Corrected p-values.\n    **pts** : `pandas.DataFrame` (`.uns['rank_genes_groups']`)\n        Fraction of cells expressing the genes for each group.\n    **pts_rest** : `pandas.DataFrame` (`.uns['rank_genes_groups']`)\n        Only if `reference` is set to `'rest'`.\n        Fraction of cells from the union of the rest of each group\n        expressing the genes.\n    Notes\n    -----\n    There are slight inconsistencies depending on whether sparse\n    or dense data are passed. See `here <https://github.com/theislab/scanpy/blob/master/scanpy/tests/test_rank_genes_groups.py>`__.\n    Examples\n    --------\n    >>> import scanpy as sc\n    >>> adata = sc.datasets.pbmc68k_reduced()\n    >>> sc.tl.rank_genes_groups(adata, 'bulk_labels', method='wilcoxon')\n    >>> # to visualize the results\n    >>> sc.pl.rank_genes_groups(adata)\n    \"\"\"\n    if use_raw is None:\n        use_raw = adata.raw is not None\n    elif use_raw is True and adata.raw is not None:\n        raise ValueError(\"Received `use_raw=True`, but `adata.raw` is empty.\")\n\n    if method is None:\n        logg.warning(\n            \"Default of the method has been changed to 't-test' from 't-test_overestim_var'\"\n        )\n        method = 't-test'\n    if 'only_positive' in kwds:\n        rankby_abs = not kwds.pop('only_positive')  # backwards compat\n    start = logg.info('ranking genes')\n    avail_methods = {'t-test', 't-test_overestim_var', 'wilcoxon', 'logreg'}\n    if method not in avail_methods:\n        raise ValueError(f'Method must be one of {avail_methods}.')\n    avail_corr = {'benjamini-hochberg', 'bonferroni'}\n    if corr_method not in avail_corr:\n        raise ValueError(f'Correction method must be one of {avail_corr}.')\n    adata = adata.copy() if copy else adata\n    _utils.sanitize_anndata(adata)\n    # for clarity, rename variable\n    if groups == 'all':\n        groups_order = 'all'\n    elif isinstance(groups, (str, int)):\n        raise ValueError('Specify a sequence of groups')\n    else:\n        groups_order = list(groups)\n        if isinstance(groups_order[0], int):\n            groups_order = [str(n) for n in groups_order]\n        if reference != 'rest' and reference not in set(groups_order):\n            groups_order += [reference]\n    if reference != 'rest' and reference not in adata.obs[groupby].cat.categories:\n        cats = adata.obs[groupby].cat.categories.tolist()\n        raise ValueError(\n            f'reference = {reference} needs to be one of groupby = {cats}.'\n        )\n    if key_added is None:\n        key_added = 'rank_genes_groups'\n    adata.uns[key_added] = {}\n    adata.uns[key_added]['params'] = dict(\n        groupby=groupby,\n        reference=reference,\n        method=method,\n        use_raw=use_raw,\n        layer=layer,\n        corr_method=corr_method,\n    )\n    test_obj = _RankGenes(adata, groups_order, groupby, reference, use_raw, layer, pts)\n    if check_nonnegative_integers(test_obj.X) and method != 'logreg':\n        logg.warning(\n            \"It seems you use rank_genes_groups on the raw count data. \"\n            \"Please logarithmize your data before calling rank_genes_groups.\"\n        )\n    # for clarity, rename variable\n    n_genes_user = n_genes\n    # make sure indices are not OoB in case there are less genes than n_genes\n    # defaults to all genes\n    if n_genes_user is None or n_genes_user > test_obj.X.shape[1]:\n        n_genes_user = test_obj.X.shape[1]\n    logg.debug(f'consider {groupby!r} groups:')\n    logg.debug(f'with sizes: {np.count_nonzero(test_obj.groups_masks, axis=1)}')\n    test_obj.compute_statistics(\n        method, corr_method, n_genes_user, rankby_abs, tie_correct, **kwds\n    )\n    if test_obj.pts is not None:\n        groups_names = [str(name) for name in test_obj.groups_order]\n        adata.uns[key_added]['pts'] = pd.DataFrame(\n            test_obj.pts.T, index=test_obj.var_names, columns=groups_names\n        )\n    if test_obj.pts_rest is not None:\n        adata.uns[key_added]['pts_rest'] = pd.DataFrame(\n            test_obj.pts_rest.T, index=test_obj.var_names, columns=groups_names\n        )\n    test_obj.stats.columns = test_obj.stats.columns.swaplevel()\n    dtypes = {\n        'names': 'O',\n        'scores': 'float32',\n        'logfoldchanges': 'float32',\n        'pvals': 'float64',\n        'pvals_adj': 'float64',\n    }\n    for col in test_obj.stats.columns.levels[0]:\n        adata.uns[key_added][col] = test_obj.stats[col].to_records(\n            index=False, column_dtypes=dtypes[col]\n        )\n    logg.info(\n        '    finished',\n        time=start,\n        deep=(\n            f'added to `.uns[{key_added!r}]`\\n'\n            \"    'names', sorted np.recarray to be indexed by group ids\\n\"\n            \"    'scores', sorted np.recarray to be indexed by group ids\\n\"\n            + (\n                \"    'logfoldchanges', sorted np.recarray to be indexed by group ids\\n\"\n                \"    'pvals', sorted np.recarray to be indexed by group ids\\n\"\n                \"    'pvals_adj', sorted np.recarray to be indexed by group ids\"\n                if method in {'t-test', 't-test_overestim_var', 'wilcoxon'}\n                else ''\n            )\n        ),\n    )\n    return adata if copy else None", "idx": 307}
{"project": "Scanpy", "commit_id": "201_scanpy_1.9.0__morans_i.py__morans_i_vec_W.py", "target": 0, "func": "def _morans_i_vec_W(\n    g_data: np.ndarray,\n    g_indices: np.ndarray,\n    g_indptr: np.ndarray,\n    x: np.ndarray,\n    W: np.float_,\n) -> float:\n    z = x - x.mean()\n    z2ss = (z * z).sum()\n    N = len(x)\n    inum = 0.0\n\n    for i in prange(N):\n        s = slice(g_indptr[i], g_indptr[i + 1])\n        i_indices = g_indices[s]\n        i_data = g_data[s]\n        inum += (i_data * z[i_indices]).sum() * z[i]\n\n    return len(x) / W * inum / z2ss", "idx": 309}
{"project": "Scanpy", "commit_id": "604_scanpy_1.9.0_test_normalization.py_test_normalize_total_layers.py", "target": 0, "func": "def test_normalize_total_layers(typ, dtype):\n    adata = AnnData(typ(X_total), dtype=dtype)\n    adata.layers[\"layer\"] = adata.X.copy()\n    with pytest.warns(FutureWarning, match=r\".*layers.*deprecated\"):\n        sc.pp.normalize_total(adata, layers=[\"layer\"])\n    assert np.allclose(adata.layers[\"layer\"].sum(axis=1), [3.0, 3.0, 3.0])", "idx": 314}
{"project": "Scanpy", "commit_id": "522_scanpy_1.9.0_test_embedding_plots.py_test_dimensions_same_as_components.py", "target": 0, "func": "def test_dimensions_same_as_components(adata, tmpdir, check_same_image):\n    tmpdir = Path(tmpdir)\n    adata = adata.copy()\n    adata.obs[\"mean\"] = np.ravel(adata.X.mean(axis=1))\n\n    comp_pth = tmpdir / \"components_plot.png\"\n    dims_pth = tmpdir / \"dimension_plot.png\"\n\n    # TODO: Deprecate components kwarg\n    # with pytest.warns(FutureWarning, match=r\"components .* deprecated\"):\n    sc.pl.pca(\n        adata,\n        color=[\"mean\", \"label\"],\n        components=[\"1,2\", \"2,3\"],\n        show=False,\n    )\n    plt.savefig(comp_pth, dpi=40)\n    plt.close()\n\n    sc.pl.pca(\n        adata,\n        color=[\"mean\", \"mean\", \"label\", \"label\"],\n        dimensions=[(0, 1), (1, 2), (0, 1), (1, 2)],\n        show=False,\n    )\n    plt.savefig(dims_pth, dpi=40)\n    plt.close()\n\n    check_same_image(dims_pth, comp_pth, tol=5)", "idx": 316}
{"project": "Scanpy", "commit_id": "129_scanpy_0.0_scanpy_plotting.py_plot_tool.py", "target": 1, "func": "def plot_tool(adata,\n              toolkey,\n              basis='pca',\n              smp=None,\n              names=None,\n              comps=None,\n              cont=None,\n              layout='2d',\n              legendloc='right margin',\n              cmap=None,\n              pal=None,\n              right_margin=None,\n              size=3,\n              subtitles=None):\n    \"\"\"\n    Scatter plots.\n    Parameters\n    ----------\n    adata : AnnData\n        Annotated data matrix.\n    toolkey : str\n        Toolkey to use for generating filename.\n    basis : {'pca', 'tsne', 'diffmap'}\n        String that denotes a plotting tool.\n    smp : str, optional (default: first annotation)\n        Sample/Cell annotation for coloring in the form \"ann1,ann2,...\". String\n        annotation is plotted assuming categorical annotation, float and integer\n        annotation is plotted assuming continuous annoation. Option 'cont'\n        allows to switch between these default choices.\n    names : str, optional (default: all names in smp)\n        Allows to restrict groups in sample annotation (smp) to a few.\n    comps : str, optional (default: '1,2')\n         String in the form '1,2,3'.\n    cont : bool, None (default: None)\n        Switch on continuous layout, switch off categorical layout.\n    layout : {'2d', '3d', 'unfolded 3d'}, optional (default: '2d')\n         Layout of plot.\n    legendloc : see matplotlib.legend, optional (default: 'lower right')\n         Options for keyword argument 'loc'.\n    cmap : str (default: 'viridis')\n         String denoting matplotlib color map.\n    pal : list of str (default: matplotlib.rcParams['axes.prop_cycle'].by_key()['color'])\n         Colors cycle to use for categorical groups.\n    right_margin : float (default: None)\n         Adjust how far the plotting panel extends to the right.\n    size : float (default: 3)\n         Point size.\n    \"\"\"\n    # compute components\n    from numpy import array\n    if comps is None:\n        comps = '1,2' if '2d' in layout else '1,2,3'\n    comps = array(comps.split(',')).astype(int) - 1\n    smps = [None] if smp is None else smp.split(',') if isinstance(smp, str) else smp\n    names = None if names is None else names.split(',') if isinstance(names, str) else names\n    # highlights\n    highlights = adata['highlights'] if 'highlights' in adata else []\n    try:\n        Y = adata['X_' + basis][:, comps]\n    except KeyError:\n        sett.mi('--> compute the basis using plotting tool', basis, 'first')\n        raise\n    pal = default_pal(pal)\n    component_name = ('DC' if basis == 'diffmap'\n                      else 'Spring' if basis == 'spring'\n                      else 'tSNE' if basis == 'tsne'\n                      else 'PC')\n    colors = []\n    categoricals = []\n    colorbars = []\n    for ismp, smp in enumerate(smps):\n        c = 'grey'\n        categorical = False\n        continuous = False\n        if len(adata.smp_keys()) > 0:\n            if smp is None:\n                for smp_ in adata.smp_keys():\n                    if smp_ not in smps:\n                        smp = smp_\n                        smps[ismp] = smp\n            # test whether we have categorial or continuous annotation\n            if smp in adata.smp_keys():\n                if adata.smp[smp].dtype.char in ['S', 'U']:\n                    categorical = True\n                    if cont is True:\n                        c = adata.smp[smp]\n                else:\n                    continuous = True\n                    c = adata.smp[smp]\n                sett.m(0, '... coloring according to', smp)\n            # coloring according to gene expression\n            elif smp in adata.var_names:\n                c = adata.X[:, np.where(smp==adata.var_names)[0][0]]\n                continuous = True\n                sett.m(0, '... coloring according to expression of gene', smp)\n            else:\n                raise ValueError('\"' + smp + '\" is invalid!'\n                                 + ' specify valid sample annotation, one of '\n                                 + str(adata.smp_keys()) + ' or a gene name '\n                                 + str(adata.var_names))\n        if cont is not None:\n            categorical = not cont\n            continuous = cont\n        colors.append(c)\n        colorbars.append(True if continuous else False)\n        if categorical:\n            categoricals.append(ismp)\n\n    if right_margin is None and legendloc == 'right margin':\n        right_margin = 0.24\n    if subtitles is None and smps[0] is not None:\n        subtitles = [smp.replace('_', ' ') for smp in smps]\n\n    axs = scatter(Y,\n                  subtitles=subtitles,\n                  component_name=component_name,\n                  component_indexnames=comps + 1,\n                  layout=layout,\n                  c=colors,\n                  highlights=highlights,\n                  colorbars=colorbars,\n                  right_margin=right_margin,\n                  cmap='viridis' if cmap is None else cmap,\n                  s=size-2)\n    for ismp in categoricals:\n        smp = smps[ismp]\n        if (smp != 'groups' and 'groups_names' in adata\n            and len(np.setdiff1d(adata['groups_names'], adata[smp + '_names']))\n                < len(adata['groups_names'])):\n            # if there is a correspondence between smp and the 'groups' defined\n            # in adata, that is, if smp has corresponding categories with those\n            # in adata['groups_names']\n            adata[smp + '_colors'] = pal[:len(adata['groups_names'])].by_key()['color']\n        elif not smp + '_colors' in adata:\n            adata[smp + '_colors'] = pal[:len(adata[smp + '_names'])].by_key()['color']\n        for iname, name in enumerate(adata[smp + '_names']):\n            if (names is None or (names != None and name in names)):\n                group(axs[ismp], smp, iname, adata, Y, layout, size)\n        # for the last panel, let's put the legend outside panel\n        if legendloc == 'right margin':\n            axs[ismp].legend(frameon=False, loc='center left', bbox_to_anchor=(1, 0.5))\n        elif legendloc != 'none':\n            axs[ismp].legend(frameon=False, loc=legendloc)\n    writekey = sett.basekey + '_' + toolkey\n    writekey += '_' + ('-'.join(smps) if smps[0] is not None else '') + sett.plotsuffix\n    savefig(writekey)\n    if not sett.savefigs and sett.autoshow:\n        pl.show()", "idx": 318}
{"project": "Scanpy", "commit_id": "911_scanpy_1.4.4_scanpy_plotting__anndata.py__scatter_obs.py", "target": 1, "func": "def _scatter_obs(\n        adata,\n        x=None,\n        y=None,\n        color=None,\n        use_raw=None,\n        layers=None,\n        sort_order=True,\n        alpha=None,\n        basis=None,\n        groups=None,\n        components=None,\n        projection='2d',\n        legend_loc='right margin',\n        legend_fontsize=None,\n        legend_fontweight=None,\n        legend_fontoutline=None,\n        color_map=None,\n        palette=None,\n        frameon=None,\n        right_margin=None,\n        left_margin=None,\n        size=None,\n        title=None,\n        show=None,\n        save=None,\n        ax=None):\n    \"\"\"See docstring of scatter.\"\"\"\n    sanitize_anndata(adata)\n    from scipy.sparse import issparse\n    if use_raw is None and adata.raw is not None: use_raw = True\n    # Process layers\n    if (layers is None or layers == 'X' or layers in adata.layers.keys()):\n        layers = (layers, layers, layers)\n    elif isinstance(layers, (tuple, list)) and len(layers) == 3:\n        layers = tuple(layers)\n        for layer in layers:\n            if layer not in adata.layers.keys() and (layer != 'X' or layer is not None):\n                raise ValueError(\n                    '`layers` should have elements that are either None or in adata.layers.keys().')\n    else:\n        raise ValueError(f\"`layers` should be a string or a list/tuple of length 3, had value '{layers}'\")\n    if use_raw and (layers != ('X', 'X', 'X') or layers is not (None, None, None)):\n        ValueError('`use_raw` must be `False` if layers are used.')\n    if legend_loc not in VALID_LEGENDLOCS:\n        raise ValueError(\n            'Invalid `legend_loc`, need to be one of: {}.'.format(VALID_LEGENDLOCS))\n    if components is None: components = '1,2' if '2d' in projection else '1,2,3'\n    if isinstance(components, str): components = components.split(',')\n    components = np.array(components).astype(int) - 1\n    keys = ['grey'] if color is None else [color] if isinstance(color, str) else color\n    if title is not None and isinstance(title, str):\n        title = [title]\n    highlights = adata.uns['highlights'] if 'highlights' in adata.uns else []\n    if basis is not None:\n        try:\n            # ignore the '0th' diffusion component\n            if basis == 'diffmap': components += 1\n            Y = adata.obsm['X_' + basis][:, components]\n            # correct the component vector for use in labeling etc.\n            if basis == 'diffmap': components -= 1\n        except KeyError:\n            raise KeyError('compute coordinates using visualization tool {} first'\n                           .format(basis))\n    elif x is not None and y is not None:\n        if use_raw:\n            if x in adata.obs.columns:\n                x_arr = adata.obs_vector(x)\n            else:\n                x_arr = adata.raw.obs_vector(x)\n            if y in adata.obs.columns:\n                y_arr = adata.obs_vector(y)\n            else:\n                y_arr = adata.raw.obs_vector(y)\n        else:\n            x_arr = adata.obs_vector(x, layer=layers[0])\n            y_arr = adata.obs_vector(y, layer=layers[1])\n        Y = np.c_[x_arr, y_arr]\n    else:\n        raise ValueError('Either provide a `basis` or `x` and `y`.')\n    if size is None:\n        n = Y.shape[0]\n        size = 120000 / n\n    if legend_loc.startswith('on data') and legend_fontsize is None:\n        legend_fontsize = rcParams['legend.fontsize']\n    elif legend_fontsize is None:\n        legend_fontsize = rcParams['legend.fontsize']\n    palette_was_none = False\n    if palette is None: palette_was_none = True\n    if isinstance(palette, list):\n        if not is_color_like(palette[0]):\n            palettes = palette\n        else:\n            palettes = [palette]\n    else:\n        palettes = [palette for i in range(len(keys))]\n    for i, palette in enumerate(palettes):\n        palettes[i] = utils.default_palette(palette)\n    if basis is not None:\n        component_name = (\n            'DC' if basis == 'diffmap'\n            else 'tSNE' if basis == 'tsne'\n            else 'UMAP' if basis == 'umap'\n            else 'PC' if basis == 'pca'\n            else basis.replace('draw_graph_', '').upper() if 'draw_graph' in basis\n            else basis)\n    else:\n        component_name = None\n    axis_labels = (x, y) if component_name is None else None\n    show_ticks = True if component_name is None else False\n    # generate the colors\n    color_ids = []\n    categoricals = []\n    colorbars = []\n    for ikey, key in enumerate(keys):\n        c = 'white'\n        categorical = False  # by default, assume continuous or flat color\n        colorbar = None\n        # test whether we have categorial or continuous annotation\n        if key in adata.obs_keys():\n            if is_categorical_dtype(adata.obs[key]):\n                categorical = True\n            else:\n                c = adata.obs[key]\n        # coloring according to gene expression\n        elif (use_raw\n              and adata.raw is not None\n              and key in adata.raw.var_names):\n            c = adata.raw.obs_vector(key)\n        elif key in adata.var_names:\n            c = adata.raw.obs_vector(key, layer=layers[2])\n        elif is_color_like(key):  # a flat color\n            c = key\n            colorbar = False\n        else:\n            raise ValueError(\n                'key \\'{}\\' is invalid! pass valid observation annotation, '\n                'one of {} or a gene name {}'\n                .format(key, adata.obs_keys(), adata.var_names))\n        if colorbar is None:\n            colorbar = not categorical\n        colorbars.append(colorbar)\n        if categorical: categoricals.append(ikey)\n        color_ids.append(c)\n    if right_margin is None and len(categoricals) > 0:\n        if legend_loc == 'right margin': right_margin = 0.5\n    if title is None and keys[0] is not None:\n        title = [key.replace('_', ' ') if not is_color_like(key) else '' for key in keys]\n    axs = scatter_base(Y,\n                       title=title,\n                       alpha=alpha,\n                       component_name=component_name,\n                       axis_labels=axis_labels,\n                       component_indexnames=components + 1,\n                       projection=projection,\n                       colors=color_ids,\n                       highlights=highlights,\n                       colorbars=colorbars,\n                       right_margin=right_margin,\n                       left_margin=left_margin,\n                       sizes=[size for c in keys],\n                       color_map=color_map,\n                       show_ticks=show_ticks,\n                       ax=ax)\n\n    # loop over all categorical annotation and plot it\n    for i, ikey in enumerate(categoricals):\n        palette = palettes[i]\n        key = keys[ikey]\n        utils.add_colors_for_categorical_sample_annotation(\n            adata, key, palette, force_update_colors=not palette_was_none)\n        # actually plot the groups\n        mask_remaining = np.ones(Y.shape[0], dtype=bool)\n        centroids = {}\n        if groups is None:\n            for iname, name in enumerate(adata.obs[key].cat.categories):\n                if name not in settings.categories_to_ignore:\n                    mask = scatter_group(axs[ikey], key, iname,\n                                         adata, Y, projection, size=size, alpha=alpha)\n                    mask_remaining[mask] = False\n                    if legend_loc.startswith('on data'): add_centroid(centroids, name, Y, mask)\n        else:\n            groups = [groups] if isinstance(groups, str) else groups\n            for name in groups:\n                if name not in set(adata.obs[key].cat.categories):\n                    raise ValueError('\"' + name + '\" is invalid!'\n                                     + ' specify valid name, one of '\n                                     + str(adata.obs[key].cat.categories))\n                else:\n                    iname = np.flatnonzero(adata.obs[key].cat.categories.values == name)[0]\n                    mask = scatter_group(axs[ikey], key, iname,\n                                         adata, Y, projection, size=size, alpha=alpha)\n                    if legend_loc.startswith('on data'): add_centroid(centroids, name, Y, mask)\n                    mask_remaining[mask] = False\n        if mask_remaining.sum() > 0:\n            data = [Y[mask_remaining, 0], Y[mask_remaining, 1]]\n            if projection == '3d': data.append(Y[mask_remaining, 2])\n            axs[ikey].scatter(*data, marker='.', c='lightgrey', s=size,\n                                    edgecolors='none', zorder=-1)\n        legend = None\n        if legend_loc.startswith('on data'):\n            if legend_fontweight is None:\n                legend_fontweight = 'bold'\n            if legend_fontoutline is not None:\n                legend_fontoutline = [patheffects.withStroke(linewidth=legend_fontoutline,\n                                                             foreground='w')]\n            for name, pos in centroids.items():\n                axs[ikey].text(pos[0], pos[1], name,\n                               weight=legend_fontweight,\n                               verticalalignment='center',\n                               horizontalalignment='center',\n                               fontsize=legend_fontsize,\n                               path_effects=legend_fontoutline)\n            all_pos = np.zeros((len(adata.obs[key].cat.categories), 2))\n            for iname, name in enumerate(adata.obs[key].cat.categories):\n                if name in centroids:\n                    all_pos[iname] = centroids[name]\n                else:\n                    all_pos[iname] = [np.nan, np.nan]\n            utils._tmp_cluster_pos = all_pos\n            if legend_loc == 'on data export':\n                filename = settings.writedir / 'pos.csv'\n                logg.warning(f'exporting label positions to {filename}')\n                settings.writedir.mkdir(parents=True, exist_ok=True)\n                np.savetxt(filename, all_pos, delimiter=',')\n        elif legend_loc == 'right margin':\n            legend = axs[ikey].legend(\n                frameon=False, loc='center left',\n                bbox_to_anchor=(1, 0.5),\n                ncol=(1 if len(adata.obs[key].cat.categories) <= 14\n                      else 2 if len(adata.obs[key].cat.categories) <= 30 else 3),\n                fontsize=legend_fontsize)\n        elif legend_loc != 'none':\n            legend = axs[ikey].legend(\n                frameon=False, loc=legend_loc, fontsize=legend_fontsize)\n        if legend is not None:\n            for handle in legend.legendHandles: handle.set_sizes([300.0])\n    # draw a frame around the scatter\n    frameon = settings._frameon if frameon is None else frameon\n    if not frameon and x is None and y is None:\n        for ax in axs:\n            ax.set_xlabel('')\n            ax.set_ylabel('')\n            ax.set_frame_on(False)\n    utils.savefig_or_show('scatter' if basis is None else basis, show=show, save=save)\n    if show == False: return axs if len(keys) > 1 else axs[0]", "idx": 333}
{"project": "Scanpy", "commit_id": "778_scanpy_1.3.7_scanpy_readwrite.py__read_legacy_10x_h5.py", "target": 1, "func": "def _read_legacy_10x_h5(filename, genome=None):\n    \"\"\"\n    Read hdf5 file from Cell Ranger v2 or earlier versions.\n    \"\"\"\n    with tables.open_file(str(filename), 'r') as f:\n        try:\n            if not genome:\n                children = f.list_nodes(f.root)\n                if len(children) > 1:\n                    raise ValueError(\"This file contains more than one genome.\"\n                                    \" For legacy 10x h5 files you must specify\"\n                                    \" the genome if more than one is present.\")\n                genome = children[0]._v_name\n            dsets = {}\n            for node in f.walk_nodes('/' + genome, 'Array'):\n                dsets[node.name] = node.read()\n            # AnnData works with csr matrices\n            # 10x stores the transposed data, so we do the transposition right away\n            from scipy.sparse import csr_matrix\n            M, N = dsets['shape']\n            data = dsets['data']\n            if dsets['data'].dtype == np.dtype('int32'):\n                data = dsets['data'].view('float32')\n                data[:] = dsets['data']\n            matrix = csr_matrix((data, dsets['indices'], dsets['indptr']),\n                                shape=(N, M))\n            # the csc matrix is automatically the transposed csr matrix\n            # as scanpy expects it, so, no need for a further transpostion\n            adata = AnnData(matrix,\n                            {'obs_names': dsets['barcodes'].astype(str)},\n                            {'var_names': dsets['gene_names'].astype(str),\n                             'gene_ids': dsets['genes'].astype(str)})\n            logg.info(t=True)\n            return adata\n        except tables.NoSuchNodeError:\n            raise Exception('Genome %s does not exist in this file.' % genome)\n        except KeyError:\n            raise Exception('File is missing one or more required datasets.')", "idx": 337}
{"project": "Scanpy", "commit_id": "716_scanpy_1.3.2_scanpy_tools_rank_genes_groups.py_rank_genes_groups.py", "target": 1, "func": "def rank_genes_groups(\n        adata,\n        groupby,\n        use_raw=True,\n        groups='all',\n        reference='rest',\n        n_genes=100,\n        rankby_abs=False,\n        key_added=None,\n        copy=False,\n        method='t-test_overestim_var',\n        **kwds):\n    \"\"\"Rank genes for characterizing groups.\n    Parameters\n    ----------\n    adata : :class:`~anndata.AnnData`\n        Annotated data matrix.\n    groupby : `str`\n        The key of the observations grouping to consider.\n    use_raw : `bool`, optional (default: `True`)\n        Use `raw` attribute of `adata` if present.\n    groups : `str`, `list`, optional (default: `'all'`)\n        Subset of groups, e.g. `['g1', 'g2', 'g3']`, to which comparison shall\n        be restricted. If not passed, a ranking will be generated for all\n        groups.\n    reference : `str`, optional (default: `'rest'`)\n        If `'rest'`, compare each group to the union of the rest of the group.  If\n        a group identifier, compare with respect to this group.\n    n_genes : `int`, optional (default: 100)\n        The number of genes that appear in the returned tables.\n    method : {'logreg', 't-test', 'wilcoxon', 't-test_overestim_var'}, optional (default: 't-test_overestim_var')\n        If 't-test', uses t-test, if 'wilcoxon', uses Wilcoxon-Rank-Sum. If\n        't-test_overestim_var', overestimates variance of each group. If\n        'logreg' uses logistic regression, see [Ntranos18]_, `here\n        <https://github.com/theislab/scanpy/issues/95>`__ and `here\n        <http://www.nxn.se/valent/2018/3/5/actionable-scrna-seq-clusters>`__, for\n        why this is meaningful.\n    rankby_abs : `bool`, optional (default: `False`)\n        Rank genes by the absolute value of the score, not by the\n        score. The returned scores are never the absolute values.\n    **kwds : keyword parameters\n        Are passed to test methods. Currently this affects only parameters that\n        are passed to `sklearn.linear_model.LogisticRegression\n        <http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html>`__.\n        For instance, you can pass `penalty='l1'` to try to come up with a\n        minimal set of genes that are good predictors (sparse solution meaning\n        few non-zero fitted coefficients).\n    Returns\n    -------\n    Updates `adata` with the following fields.\n    names : structured `np.ndarray` (`.uns['rank_genes_groups']`)\n        Structured array to be indexed by group id storing the gene\n        names. Ordered according to scores.\n    scores : structured `np.ndarray` (`.uns['rank_genes_groups']`)\n        Structured array to be indexed by group id storing the score for each\n        gene for each group. Ordered according to scores.\n    logfoldchanges : structured `np.ndarray` (`.uns['rank_genes_groups']`)\n        Structured array to be indexed by group id storing the log2\n        fold change for each gene for each group. Ordered according to\n        scores. Only provided if method is 't-test' like.\n    \"\"\"\n    if 'only_positive' in kwds:\n        rankby_abs = not kwds.pop('only_positive')  # backwards compat\n\n    logg.info('ranking genes', r=True)\n    avail_methods = {'t-test', 't-test_overestim_var', 'wilcoxon', 'logreg'}\n    if method not in avail_methods:\n        raise ValueError('Method must be one of {}.'.format(avail_methods))\n\n    adata = adata.copy() if copy else adata\n    utils.sanitize_anndata(adata)\n    # for clarity, rename variable\n    groups_order = groups\n    if isinstance(groups_order, list) and isinstance(groups_order[0], int):\n        groups_order = [str(n) for n in groups_order]\n    if reference != 'rest' and reference not in set(groups_order):\n        groups_order += [reference]\n    if (reference != 'rest'\n            and reference not in set(adata.obs[groupby].cat.categories)):\n        raise ValueError('reference = {} needs to be one of groupby = {}.'\n                         .format(reference,\n                                 adata.obs[groupby].cat.categories.tolist()))\n\n    groups_order, groups_masks = utils.select_groups(\n        adata, groups_order, groupby)\n    if key_added is None:\n        key_added = 'rank_genes_groups'\n    adata.uns[key_added] = {}\n    adata.uns[key_added]['params'] = {\n        'groupby': groupby,\n        'reference': reference,\n        'method': method,\n        'use_raw': use_raw,\n    }\n    # adata_comp mocks an AnnData object if use_raw is True\n    # otherwise it's just the AnnData object\n    adata_comp = adata\n    if adata.raw is not None and use_raw:\n        adata_comp = adata.raw\n    X = adata_comp.X\n    # for clarity, rename variable\n    n_genes_user = n_genes\n    # make sure indices are not OoB in case there are less genes than n_genes\n    if n_genes_user > X.shape[1]:\n        n_genes_user = X.shape[1]\n    # in the following, n_genes is simply another name for the total number of genes\n    n_genes = X.shape[1]\n\n    n_groups = groups_masks.shape[0]\n    ns = np.zeros(n_groups, dtype=int)\n    for imask, mask in enumerate(groups_masks):\n        ns[imask] = np.where(mask)[0].size\n    logg.msg('consider \\'{}\\' groups:'.format(groupby), groups_order, v=4)\n    logg.msg('with sizes:', ns, v=4)\n    if reference != 'rest':\n        ireference = np.where(groups_order == reference)[0][0]\n    reference_indices = np.arange(adata_comp.n_vars, dtype=int)\n    rankings_gene_scores = []\n    rankings_gene_names = []\n    rankings_gene_logfoldchanges = []\n    rankings_gene_pvals = []\n    rankings_gene_pvals_adj = []\n\n    if method in {'t-test', 't-test_overestim_var'}:\n        from scipy import stats\n        # loop over all masks and compute means, variances and sample numbers\n        means = np.zeros((n_groups, n_genes))\n        vars = np.zeros((n_groups, n_genes))\n        for imask, mask in enumerate(groups_masks):\n            means[imask], vars[imask] = simple._get_mean_var(X[mask])\n        # test each either against the union of all other groups or against a\n        # specific group\n        for igroup in range(n_groups):\n            if reference == 'rest':\n                mask_rest = ~groups_masks[igroup]\n            else:\n                if igroup == ireference:\n                    continue\n                else:\n                    mask_rest = groups_masks[ireference]\n            mean_rest, var_rest = simple._get_mean_var(X[mask_rest])\n            ns_group = ns[igroup]  # number of observations in group\n            if method == 't-test':\n                ns_rest = np.where(mask_rest)[0].size\n            elif method == 't-test_overestim_var':\n                ns_rest = ns[igroup]  # hack for overestimating the variance for small groups\n            else:\n                raise ValueError('Method does not exist.')\n\n            denominator = np.sqrt(vars[igroup] / ns_group + var_rest / ns_rest)\n            denominator[np.flatnonzero(denominator == 0)] = np.nan\n            scores = (means[igroup] - mean_rest) / denominator  # Welch t-test\n            mean_rest[mean_rest == 0] = 1e-9  # set 0s to small value\n            foldchanges = (means[igroup] + 1e-9) / mean_rest\n            scores[np.isnan(scores)] = 0\n            # Get p-values\n            denominator_dof = (np.square(vars[igroup]) / (np.square(ns_group) * (ns_group - 1))) + (\n                (np.square(var_rest) / (np.square(ns_rest) * (ns_rest - 1))))\n            denominator_dof[np.flatnonzero(denominator_dof == 0)] = np.nan\n            dof = np.square(\n                vars[igroup] / ns_group + var_rest / ns_rest) / denominator_dof  # dof calculation for Welch t-test\n            dof[np.isnan(dof)] = 0\n            pvals = stats.t.sf(abs(scores), dof) * 2  # *2 because of two-tailed t-test\n            pvals_adj = pvals * n_genes\n            scores_sort = np.abs(scores) if rankby_abs else scores\n            partition = np.argpartition(scores_sort, -n_genes_user)[-n_genes_user:]\n            partial_indices = np.argsort(scores_sort[partition])[::-1]\n            global_indices = reference_indices[partition][partial_indices]\n            rankings_gene_scores.append(scores[global_indices])\n            rankings_gene_logfoldchanges.append(np.log2(np.abs(foldchanges[global_indices])))\n            rankings_gene_names.append(adata_comp.var_names[global_indices])\n            rankings_gene_pvals.append(pvals[global_indices])\n            rankings_gene_pvals_adj.append(pvals_adj[global_indices])\n\n    elif method == 'logreg':\n        # if reference is not set, then the groups listed will be compared to the rest\n        # if reference is set, then the groups listed will be compared only to the other groups listed\n        from sklearn.linear_model import LogisticRegression\n        reference = groups_order[0]\n        if len(groups) == 1:\n            raise Exception('Cannot perform logistic regression on a single cluster.')\n        adata_copy = adata[adata.obs[groupby].isin(groups_order)]\n        adata_comp = adata_copy\n        if adata.raw is not None and use_raw:\n            adata_comp = adata_copy.raw\n        X = adata_comp.X\n        clf = LogisticRegression(**kwds)\n        clf.fit(X, adata_copy.obs[groupby].cat.codes)\n        scores_all = clf.coef_\n        for igroup, group in enumerate(groups_order):\n            if len(groups_order) <= 2:  # binary logistic regression\n                scores = scores_all[0]\n            else:\n                scores = scores_all[igroup]\n            partition = np.argpartition(scores, -n_genes_user)[-n_genes_user:]\n            partial_indices = np.argsort(scores[partition])[::-1]\n            global_indices = reference_indices[partition][partial_indices]\n            rankings_gene_scores.append(scores[global_indices])\n            rankings_gene_names.append(adata_comp.var_names[global_indices])\n            if len(groups_order) <= 2:\n                break\n\n    elif method == 'wilcoxon':\n        CONST_MAX_SIZE = 10000000\n        ns_rest = np.zeros(n_groups, dtype=int)\n        # initialize space for z-scores\n        scores = np.zeros(n_genes)\n        # First loop: Loop over all genes\n        if reference != 'rest':\n            for imask, mask in enumerate(groups_masks):\n                if imask == ireference:\n                    continue\n                else:\n                    mask_rest = groups_masks[ireference]\n                ns_rest[imask] = np.where(mask_rest)[0].size\n                if ns_rest[imask] <= 25 or ns[imask] <= 25:\n                    logg.hint('Few observations in a group for '\n                              'normal approximation (<=25). Lower test accuracy.')\n                n_active = ns[imask]\n                m_active = ns_rest[imask]\n                # Now calculate gene expression ranking in chunkes:\n                chunk = []\n                # Calculate chunk frames\n                n_genes_max_chunk = floor(CONST_MAX_SIZE / (n_active + m_active))\n                if n_genes_max_chunk < n_genes - 1:\n                    chunk_index = n_genes_max_chunk\n                    while chunk_index < n_genes - 1:\n                        chunk.append(chunk_index)\n                        chunk_index = chunk_index + n_genes_max_chunk\n                    chunk.append(n_genes - 1)\n                else:\n                    chunk.append(n_genes - 1)\n                left = 0\n                # Calculate rank sums for each chunk for the current mask\n                for chunk_index, right in enumerate(chunk):\n                    # Check if issparse is true: AnnData objects are currently sparse.csr or ndarray.\n                    if issparse(X):\n                        df1 = pd.DataFrame(data=X[mask, left:right].todense())\n                        df2 = pd.DataFrame(data=X[mask_rest, left:right].todense(),\n                                           index=np.arange(start=n_active, stop=n_active + m_active))\n                    else:\n                        df1 = pd.DataFrame(data=X[mask, left:right])\n                        df2 = pd.DataFrame(data=X[mask_rest, left:right],\n                                           index=np.arange(start=n_active, stop=n_active + m_active))\n                    df1 = df1.append(df2)\n                    ranks = df1.rank()\n                    # sum up adjusted_ranks to calculate W_m,n\n                    scores[left:right] = np.sum(ranks.loc[0:n_active, :])\n                    left = right + 1\n                scores = (scores - (n_active * (n_active + m_active + 1) / 2)) / sqrt(\n                    (n_active * m_active * (n_active + m_active + 1) / 12))\n                scores = scores if not rankby_abs else np.abs(scores)\n                scores[np.isnan(scores)] = 0\n                partition = np.argpartition(scores, -n_genes_user)[-n_genes_user:]\n                partial_indices = np.argsort(scores[partition])[::-1]\n                global_indices = reference_indices[partition][partial_indices]\n                rankings_gene_scores.append(scores[global_indices])\n                rankings_gene_names.append(adata_comp.var_names[global_indices])\n        # If no reference group exists, ranking needs only to be done once (full mask)\n        else:\n            scores = np.zeros((n_groups, n_genes))\n            chunk = []\n            n_cells = X.shape[0]\n            n_genes_max_chunk = floor(CONST_MAX_SIZE / n_cells)\n            if n_genes_max_chunk < n_genes - 1:\n                chunk_index = n_genes_max_chunk\n                while chunk_index < n_genes - 1:\n                    chunk.append(chunk_index)\n                    chunk_index = chunk_index + n_genes_max_chunk\n                chunk.append(n_genes - 1)\n            else:\n                chunk.append(n_genes - 1)\n            left = 0\n            for chunk_index, right in enumerate(chunk):\n                # Check if issparse is true\n                if issparse(X):\n                    df1 = pd.DataFrame(data=X[:, left:right].todense())\n                else:\n                    df1 = pd.DataFrame(data=X[:, left:right])\n                ranks = df1.rank()\n                # sum up adjusted_ranks to calculate W_m,n\n                for imask, mask in enumerate(groups_masks):\n                    scores[imask, left:right] = np.sum(ranks.loc[mask, :])\n                left = right + 1\n\n            for imask, mask in enumerate(groups_masks):\n                scores[imask, :] = (scores[imask, :] - (ns[imask] * (n_cells + 1) / 2)) / sqrt(\n                    (ns[imask] * (n_cells - ns[imask]) * (n_cells + 1) / 12))\n                scores = scores if not rankby_abs else np.abs(scores)\n                scores[np.isnan(scores)] = 0\n                partition = np.argpartition(scores[imask, :], -n_genes_user)[-n_genes_user:]\n                partial_indices = np.argsort(scores[imask, partition])[::-1]\n                global_indices = reference_indices[partition][partial_indices]\n                rankings_gene_scores.append(scores[imask, global_indices])\n                rankings_gene_names.append(adata_comp.var_names[global_indices])\n\n    groups_order_save = [str(g) for g in groups_order]\n    if (reference != 'rest' and method != 'logreg') or (method == 'logreg' and len(groups) == 2):\n        groups_order_save = [g for g in groups_order if g != reference]\n    adata.uns[key_added]['scores'] = np.rec.fromarrays(\n        [n for n in rankings_gene_scores],\n        dtype=[(rn, 'float32') for rn in groups_order_save])\n    adata.uns[key_added]['names'] = np.rec.fromarrays(\n        [n for n in rankings_gene_names],\n        dtype=[(rn, 'U50') for rn in groups_order_save])\n\n    if method in {'t-test', 't-test_overestim_var'}:\n        adata.uns[key_added]['logfoldchanges'] = np.rec.fromarrays(\n            [n for n in rankings_gene_logfoldchanges],\n            dtype=[(rn, 'float32') for rn in groups_order_save])\n        adata.uns[key_added]['pvals'] = np.rec.fromarrays(\n            [n for n in rankings_gene_pvals],\n            dtype=[(rn, 'float64') for rn in groups_order_save])\n        adata.uns[key_added]['pvals_adj'] = np.rec.fromarrays(\n            [n for n in rankings_gene_pvals_adj],\n            dtype=[(rn, 'float64') for rn in groups_order_save])\n\n    logg.info('    finished', time=True, end=' ' if settings.verbosity > 2 else '\\n')\n    logg.hint(\n        'added to `.uns[\\'{}\\']`\\n'\n        '    \\'names\\', sorted np.recarray to be indexed by group ids\\n'\n        '    \\'scores\\', sorted np.recarray to be indexed by group ids\\n'\n        .format(key_added)\n        + ('    \\'logfoldchanges\\', sorted np.recarray to be indexed by group ids\\n'\n           '    \\'pvals\\', sorted np.recarray to be indexed by group ids\\n'\n           '    \\'pvals_adj\\', sorted np.recarray to be indexed by group ids'\n           if method in {'t-test', 't-test_overestim_var'} else ''))\n    return adata if copy else None", "idx": 350}
{"project": "Scanpy", "commit_id": "277_scanpy_0.1_scanpy_plotting_utils.py_scatter_base.py", "target": 1, "func": "def scatter_base(Y,\n                 colors='blue',\n                 highlights=[],\n                 title='',\n                 right_margin=None,\n                 layout='2d',\n                 titles=None,\n                 component_name='DC',\n                 component_indexnames=[1, 2, 3],\n                 axis_labels=None,\n                 colorbars=[False],\n                 sizes=[1],\n                 cmap='viridis',\n                 show_ticks=True):\n    \"\"\"Plot scatter plot of data.\n    Parameters\n    ----------\n    Y : np.ndarray\n        Data array.\n    layout : {'2d', '3d'}\n    Returns\n    -------\n    axs : matplotlib.axis or list of matplotlib.axis\n        Depending on whether supplying a single array or a list of arrays,\n        return a single axis or a list of axes.\n    \"\"\"\n    if '3d' in layout: from mpl_toolkits.mplot3d import Axes3D\n    if isinstance(highlights, dict):\n        highlights_indices = sorted(highlights)\n        highlights_labels = [highlights[i] for i in highlights_indices]\n        higlights = highlights_indices\n    else:\n        highlights_labels = []\n    # if we have a single array, transform it into a list with a single array\n    avail_layouts = {'2d', '3d'}\n    if layout not in avail_layouts:\n        raise ValueError('choose layout from', avail_layouts)\n    if type(colors) == str: colors = [colors]\n    if len(sizes) != len(colors):\n        if len(sizes) == 1:\n            sizes = [sizes[0] for i in range(len(colors))]\n    # grid of axes for plotting and legends/colorbars\n    if np.any(colorbars) and right_margin is None: right_margin = 0.25\n    elif right_margin is None: right_margin = 0.01\n    # make a list of right margins for each panel\n    if not isinstance(right_margin, list):\n        right_margin_list = [right_margin for i in range(len(colors))]\n    else:\n        right_margin_list = right_margin\n    # make a figure with len(colors) panels in a row side by side\n    top_offset = 1 - rcParams['figure.subplot.top']\n    bottom_offset = 0.15 if show_ticks else 0.08\n    left_offset = 1 if show_ticks else 0.3  # in units of base_height\n    base_height = rcParams['figure.figsize'][1]\n    height = base_height\n    base_width = rcParams['figure.figsize'][0]\n    if show_ticks: base_width *= 1.1\n    draw_region_width = base_width - left_offset - top_offset - 0.5  # this is kept constant throughout\n    right_margin_factor = sum([1 + right_margin for right_margin in right_margin_list])\n    width_without_offsets = right_margin_factor * draw_region_width  # this is the total width that keeps draw_region_width\n    right_offset = (len(colors) - 1) * left_offset\n    figure_width = width_without_offsets + left_offset + right_offset\n    draw_region_width_frac = draw_region_width / figure_width\n    left_offset_frac = left_offset / figure_width\n    right_offset_frac = 1 - (len(colors) - 1) * left_offset_frac\n    fig = pl.figure(figsize=(figure_width, height),\n                    subplotpars=sppars(left=0, right=1, bottom=bottom_offset))\n    fig.suptitle(title)\n    left_positions = [left_offset_frac, left_offset_frac + draw_region_width_frac]\n    for i in range(1, len(colors)):\n        right_margin = right_margin_list[i-1]\n        left_positions.append(left_positions[-1] + right_margin * draw_region_width_frac)\n        left_positions.append(left_positions[-1] + draw_region_width_frac)\n    panel_pos = [[bottom_offset], [1-top_offset], left_positions]\n    axs = []\n    for icolor, color in enumerate(colors):\n        left = panel_pos[2][2*icolor]\n        bottom = panel_pos[0][0]\n        width = draw_region_width / figure_width\n        height = panel_pos[1][0] - bottom\n        if layout == '2d':\n            ax = pl.axes([left, bottom, width, height])\n            data = Y[:, 0], Y[:, 1]\n        elif layout == '3d':\n            ax = pl.axes([left, bottom, width, height], projection='3d')\n            data = Y[:, 0], Y[:, 1], Y[:, 2]\n        if not isinstance(color, str) or color != 'white':\n            sct = ax.scatter(*data,\n                             marker='.',\n                             c=color,\n                             edgecolors='none',  # 'face',\n                             s=sizes[icolor],\n                             cmap=cmap)\n        if colorbars[icolor]:\n            width = 0.006 * draw_region_width\n            left = panel_pos[2][2*icolor+1] + (1 if layout == '3d' else 0.2) * width\n            rectangle = [left, bottom, width, height]\n            ax_cb = fig.add_axes(rectangle)\n            cb = pl.colorbar(sct, format=ticker.FuncFormatter(ticks_formatter),\n                             cax=ax_cb)\n        # set the titles\n        if titles is not None:\n            ax.set_title(titles[icolor])\n        # output highlighted data points\n        for iihighlight, ihighlight in enumerate(highlights):\n            data = [Y[ihighlight, 0]], [Y[ihighlight, 1]]\n            if '3d' in layout:\n                data = [Y[ihighlight, 0]], [Y[ihighlight, 1]], [Y[ihighlight, 2]]\n            ax.scatter(*data, c='black',\n                       facecolors='black', edgecolors='black',\n                       marker='x', s=40, zorder=20)\n            highlight = (highlights_labels[iihighlight] if\n                         len(highlights_labels) > 0\n                         else str(ihighlight))\n            # the following is a Python 2 compatibility hack\n            ax.text(*([d[0] for d in data] + [highlight]), zorder=20)\n        if not show_ticks:\n            ax.set_xticks([])\n            ax.set_yticks([])\n            if '3d' in layout: ax.set_zticks([])\n        # scale limits to match data\n        ax.autoscale_view()\n        axs.append(ax)\n    # set default axis_labels\n    if axis_labels is None:\n        axis_labels = [[component_name + str(i) for i in idcs]\n                       for idcs in\n                       [component_indexnames for iax in range(len(axs))]]\n    else:\n        axis_labels = [[axis_labels[0], axis_labels[1]] for i in range(len(axs))]\n    for iax, ax in enumerate(axs):\n        ax.set_xlabel(axis_labels[iax][0])\n        ax.set_ylabel(axis_labels[iax][1])\n        if '3d' in layout:\n            # shift the label closer to the axis\n            ax.set_zlabel(axis_labels[iax][2], labelpad=-7)\n    return axs", "idx": 357}
{"project": "Scanpy", "commit_id": "733_scanpy_1.3.2_scanpy_tools_rank_genes_groups.py_rank_genes_groups.py", "target": 1, "func": "def rank_genes_groups(\n        adata,\n        groupby,\n        use_raw=True,\n        groups='all',\n        reference='rest',\n        n_genes=100,\n        rankby_abs=False,\n        key_added=None,\n        copy=False,\n        method='t-test_overestim_var',\n        **kwds):\n    \"\"\"Rank genes for characterizing groups.\n    Parameters\n    ----------\n    adata : :class:`~anndata.AnnData`\n        Annotated data matrix.\n    groupby : `str`\n        The key of the observations grouping to consider.\n    use_raw : `bool`, optional (default: `True`)\n        Use `raw` attribute of `adata` if present.\n    groups : `str`, `list`, optional (default: `'all'`)\n        Subset of groups, e.g. `['g1', 'g2', 'g3']`, to which comparison shall\n        be restricted. If not passed, a ranking will be generated for all\n        groups.\n    reference : `str`, optional (default: `'rest'`)\n        If `'rest'`, compare each group to the union of the rest of the group.  If\n        a group identifier, compare with respect to this group.\n    n_genes : `int`, optional (default: 100)\n        The number of genes that appear in the returned tables.\n    method : {'logreg', 't-test', 'wilcoxon', 't-test_overestim_var'}, optional (default: 't-test_overestim_var')\n        If 't-test', uses t-test, if 'wilcoxon', uses Wilcoxon-Rank-Sum. If\n        't-test_overestim_var', overestimates variance of each group. If\n        'logreg' uses logistic regression, see [Ntranos18]_, `here\n        <https://github.com/theislab/scanpy/issues/95>`__ and `here\n        <http://www.nxn.se/valent/2018/3/5/actionable-scrna-seq-clusters>`__, for\n        why this is meaningful.\n    rankby_abs : `bool`, optional (default: `False`)\n        Rank genes by the absolute value of the score, not by the\n        score. The returned scores are never the absolute values.\n    **kwds : keyword parameters\n        Are passed to test methods. Currently this affects only parameters that\n        are passed to `sklearn.linear_model.LogisticRegression\n        <http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html>`__.\n        For instance, you can pass `penalty='l1'` to try to come up with a\n        minimal set of genes that are good predictors (sparse solution meaning\n        few non-zero fitted coefficients).\n    Returns\n    -------\n    Updates `adata` with the following fields.\n    names : structured `np.ndarray` (`.uns['rank_genes_groups']`)\n        Structured array to be indexed by group id storing the gene\n        names. Ordered according to scores.\n    scores : structured `np.ndarray` (`.uns['rank_genes_groups']`)\n        Structured array to be indexed by group id storing the score for each\n        gene for each group. Ordered according to scores.\n    logfoldchanges : structured `np.ndarray` (`.uns['rank_genes_groups']`)\n        Structured array to be indexed by group id storing the log2\n        fold change for each gene for each group. Ordered according to\n        scores. Only provided if method is 't-test' like.\n    \"\"\"\n    if 'only_positive' in kwds:\n        rankby_abs = not kwds.pop('only_positive')  # backwards compat\n\n    logg.info('ranking genes', r=True)\n    avail_methods = {'t-test', 't-test_overestim_var', 'wilcoxon', 'logreg'}\n    if method not in avail_methods:\n        raise ValueError('Method must be one of {}.'.format(avail_methods))\n\n    adata = adata.copy() if copy else adata\n    utils.sanitize_anndata(adata)\n    # for clarity, rename variable\n    groups_order = groups\n    if isinstance(groups_order, list) and isinstance(groups_order[0], int):\n        groups_order = [str(n) for n in groups_order]\n    if reference != 'rest' and reference not in set(groups_order):\n        groups_order += [reference]\n    if (reference != 'rest'\n            and reference not in set(adata.obs[groupby].cat.categories)):\n        raise ValueError('reference = {} needs to be one of groupby = {}.'\n                         .format(reference,\n                                 adata.obs[groupby].cat.categories.tolist()))\n\n    groups_order, groups_masks = utils.select_groups(\n        adata, groups_order, groupby)\n    if key_added is None:\n        key_added = 'rank_genes_groups'\n    adata.uns[key_added] = {}\n    adata.uns[key_added]['params'] = {\n        'groupby': groupby,\n        'reference': reference,\n        'method': method,\n        'use_raw': use_raw,\n    }\n\n    # adata_comp mocks an AnnData object if use_raw is True\n    # otherwise it's just the AnnData object\n    adata_comp = adata\n    if adata.raw is not None and use_raw:\n        adata_comp = adata.raw\n    X = adata_comp.X\n    # for clarity, rename variable\n    n_genes_user = n_genes\n    # make sure indices are not OoB in case there are less genes than n_genes\n    if n_genes_user > X.shape[1]:\n        n_genes_user = X.shape[1]\n    # in the following, n_genes is simply another name for the total number of genes\n    n_genes = X.shape[1]\n\n    n_groups = groups_masks.shape[0]\n    ns = np.zeros(n_groups, dtype=int)\n    for imask, mask in enumerate(groups_masks):\n        ns[imask] = np.where(mask)[0].size\n    logg.msg('consider \\'{}\\' groups:'.format(groupby), groups_order, v=4)\n    logg.msg('with sizes:', ns, v=4)\n    if reference != 'rest':\n        ireference = np.where(groups_order == reference)[0][0]\n    reference_indices = np.arange(adata_comp.n_vars, dtype=int)\n    rankings_gene_scores = []\n    rankings_gene_names = []\n    rankings_gene_logfoldchanges = []\n    rankings_gene_pvals = []\n    rankings_gene_pvals_adj = []\n\n    if method in {'t-test', 't-test_overestim_var'}:\n        from scipy import stats\n        # loop over all masks and compute means, variances and sample numbers\n        means = np.zeros((n_groups, n_genes))\n        vars = np.zeros((n_groups, n_genes))\n        for imask, mask in enumerate(groups_masks):\n            means[imask], vars[imask] = simple._get_mean_var(X[mask])\n        # test each either against the union of all other groups or against a\n        # specific group\n        for igroup in range(n_groups):\n            if reference == 'rest':\n                mask_rest = ~groups_masks[igroup]\n            else:\n                if igroup == ireference:\n                    continue\n                else:\n                    mask_rest = groups_masks[ireference]\n            mean_rest, var_rest = simple._get_mean_var(X[mask_rest])\n            ns_group = ns[igroup]  # number of observations in group\n            if method == 't-test':\n                ns_rest = np.where(mask_rest)[0].size\n            elif method == 't-test_overestim_var':\n                ns_rest = ns[igroup]  # hack for overestimating the variance for small groups\n            else:\n                raise ValueError('Method does not exist.')\n\n            denominator = np.sqrt(vars[igroup] / ns_group + var_rest / ns_rest)\n            denominator[np.flatnonzero(denominator == 0)] = np.nan\n            scores = (means[igroup] - mean_rest) / denominator  # Welch t-test\n            mean_rest[mean_rest == 0] = 1e-9  # set 0s to small value\n            foldchanges = (means[igroup] + 1e-9) / mean_rest\n            scores[np.isnan(scores)] = 0\n            # Get p-values\n            denominator_dof = (np.square(vars[igroup]) / (np.square(ns_group) * (ns_group - 1))) + (\n                (np.square(var_rest) / (np.square(ns_rest) * (ns_rest - 1))))\n            denominator_dof[np.flatnonzero(denominator_dof == 0)] = np.nan\n            dof = np.square(\n                vars[igroup] / ns_group + var_rest / ns_rest) / denominator_dof  # dof calculation for Welch t-test\n            dof[np.isnan(dof)] = 0\n            pvals = stats.t.sf(abs(scores), dof) * 2  # *2 because of two-tailed t-test\n            pvals_adj = pvals * n_genes\n            scores_sort = np.abs(scores) if rankby_abs else scores\n            partition = np.argpartition(scores_sort, -n_genes_user)[-n_genes_user:]\n            partial_indices = np.argsort(scores_sort[partition])[::-1]\n            global_indices = reference_indices[partition][partial_indices]\n            rankings_gene_scores.append(scores[global_indices])\n            rankings_gene_logfoldchanges.append(np.log2(np.abs(foldchanges[global_indices])))\n            rankings_gene_names.append(adata_comp.var_names[global_indices])\n            rankings_gene_pvals.append(pvals[global_indices])\n            rankings_gene_pvals_adj.append(pvals_adj[global_indices])\n\n    elif method == 'logreg':\n        # if reference is not set, then the groups listed will be compared to the rest\n        # if reference is set, then the groups listed will be compared only to the other groups listed\n        from sklearn.linear_model import LogisticRegression\n        reference = groups_order[0]\n        if len(groups) == 1:\n            raise Exception('Cannot perform logistic regression on a single cluster.')\n        adata_copy = adata[adata.obs[groupby].isin(groups_order)]\n        adata_comp = adata_copy\n        if adata.raw is not None and use_raw:\n            adata_comp = adata_copy.raw\n        X = adata_comp.X\n        clf = LogisticRegression(**kwds)\n        clf.fit(X, adata_copy.obs[groupby].cat.codes)\n        scores_all = clf.coef_\n        for igroup, group in enumerate(groups_order):\n            if len(groups_order) <= 2:  # binary logistic regression\n                scores = scores_all[0]\n            else:\n                scores = scores_all[igroup]\n            partition = np.argpartition(scores, -n_genes_user)[-n_genes_user:]\n            partial_indices = np.argsort(scores[partition])[::-1]\n            global_indices = reference_indices[partition][partial_indices]\n            rankings_gene_scores.append(scores[global_indices])\n            rankings_gene_names.append(adata_comp.var_names[global_indices])\n            if len(groups_order) <= 2:\n                break\n\n    elif method == 'wilcoxon':\n        CONST_MAX_SIZE = 10000000\n        ns_rest = np.zeros(n_groups, dtype=int)\n        # initialize space for z-scores\n        scores = np.zeros(n_genes)\n        # First loop: Loop over all genes\n        if reference != 'rest':\n            for imask, mask in enumerate(groups_masks):\n                if imask == ireference:\n                    continue\n                else:\n                    mask_rest = groups_masks[ireference]\n                ns_rest[imask] = np.where(mask_rest)[0].size\n                if ns_rest[imask] <= 25 or ns[imask] <= 25:\n                    logg.hint('Few observations in a group for '\n                              'normal approximation (<=25). Lower test accuracy.')\n                n_active = ns[imask]\n                m_active = ns_rest[imask]\n                # Now calculate gene expression ranking in chunkes:\n                chunk = []\n                # Calculate chunk frames\n                n_genes_max_chunk = floor(CONST_MAX_SIZE / (n_active + m_active))\n                if n_genes_max_chunk < n_genes - 1:\n                    chunk_index = n_genes_max_chunk\n                    while chunk_index < n_genes - 1:\n                        chunk.append(chunk_index)\n                        chunk_index = chunk_index + n_genes_max_chunk\n                    chunk.append(n_genes - 1)\n                else:\n                    chunk.append(n_genes - 1)\n                left = 0\n                # Calculate rank sums for each chunk for the current mask\n                for chunk_index, right in enumerate(chunk):\n                    # Check if issparse is true: AnnData objects are currently sparse.csr or ndarray.\n                    if issparse(X):\n                        df1 = pd.DataFrame(data=X[mask, left:right].todense())\n                        df2 = pd.DataFrame(data=X[mask_rest, left:right].todense(),\n                                           index=np.arange(start=n_active, stop=n_active + m_active))\n                    else:\n                        df1 = pd.DataFrame(data=X[mask, left:right])\n                        df2 = pd.DataFrame(data=X[mask_rest, left:right],\n                                           index=np.arange(start=n_active, stop=n_active + m_active))\n                    df1 = df1.append(df2)\n                    ranks = df1.rank()\n                    # sum up adjusted_ranks to calculate W_m,n\n                    scores[left:right] = np.sum(ranks.loc[0:n_active, :])\n                    left = right + 1\n                scores = (scores - (n_active * (n_active + m_active + 1) / 2)) / sqrt(\n                    (n_active * m_active * (n_active + m_active + 1) / 12))\n                scores = scores if not rankby_abs else np.abs(scores)\n                scores[np.isnan(scores)] = 0\n                partition = np.argpartition(scores, -n_genes_user)[-n_genes_user:]\n                partial_indices = np.argsort(scores[partition])[::-1]\n                global_indices = reference_indices[partition][partial_indices]\n                rankings_gene_scores.append(scores[global_indices])\n                rankings_gene_names.append(adata_comp.var_names[global_indices])\n        # If no reference group exists, ranking needs only to be done once (full mask)\n        else:\n            scores = np.zeros((n_groups, n_genes))\n            chunk = []\n            n_cells = X.shape[0]\n            n_genes_max_chunk = floor(CONST_MAX_SIZE / n_cells)\n            if n_genes_max_chunk < n_genes - 1:\n                chunk_index = n_genes_max_chunk\n                while chunk_index < n_genes - 1:\n                    chunk.append(chunk_index)\n                    chunk_index = chunk_index + n_genes_max_chunk\n                chunk.append(n_genes - 1)\n            else:\n                chunk.append(n_genes - 1)\n            left = 0\n            for chunk_index, right in enumerate(chunk):\n                # Check if issparse is true\n                if issparse(X):\n                    df1 = pd.DataFrame(data=X[:, left:right].todense())\n                else:\n                    df1 = pd.DataFrame(data=X[:, left:right])\n                ranks = df1.rank()\n                # sum up adjusted_ranks to calculate W_m,n\n                for imask, mask in enumerate(groups_masks):\n                    scores[imask, left:right] = np.sum(ranks.loc[mask, :])\n                left = right + 1\n\n            for imask, mask in enumerate(groups_masks):\n                scores[imask, :] = (scores[imask, :] - (ns[imask] * (n_cells + 1) / 2)) / sqrt(\n                    (ns[imask] * (n_cells - ns[imask]) * (n_cells + 1) / 12))\n                scores = scores if not rankby_abs else np.abs(scores)\n                scores[np.isnan(scores)] = 0\n                partition = np.argpartition(scores[imask, :], -n_genes_user)[-n_genes_user:]\n                partial_indices = np.argsort(scores[imask, partition])[::-1]\n                global_indices = reference_indices[partition][partial_indices]\n                rankings_gene_scores.append(scores[imask, global_indices])\n                rankings_gene_names.append(adata_comp.var_names[global_indices])\n\n    groups_order_save = [str(g) for g in groups_order]\n    if (reference != 'rest' and method != 'logreg') or (method == 'logreg' and len(groups) == 2):\n        groups_order_save = [g for g in groups_order if g != reference]\n    adata.uns[key_added]['scores'] = np.rec.fromarrays(\n        [n for n in rankings_gene_scores],\n        dtype=[(rn, 'float32') for rn in groups_order_save])\n    adata.uns[key_added]['names'] = np.rec.fromarrays(\n        [n for n in rankings_gene_names],\n        dtype=[(rn, 'U50') for rn in groups_order_save])\n\n    if method in {'t-test', 't-test_overestim_var'}:\n        adata.uns[key_added]['logfoldchanges'] = np.rec.fromarrays(\n            [n for n in rankings_gene_logfoldchanges],\n            dtype=[(rn, 'float32') for rn in groups_order_save])\n        adata.uns[key_added]['pvals'] = np.rec.fromarrays(\n            [n for n in rankings_gene_pvals],\n            dtype=[(rn, 'float64') for rn in groups_order_save])\n        adata.uns[key_added]['pvals_adj'] = np.rec.fromarrays(\n            [n for n in rankings_gene_pvals_adj],\n            dtype=[(rn, 'float64') for rn in groups_order_save])\n\n    logg.info('    finished', time=True, end=' ' if settings.verbosity > 2 else '\\n')\n    logg.hint(\n        'added to `.uns[\\'{}\\']`\\n'\n        '    \\'names\\', sorted np.recarray to be indexed by group ids\\n'\n        '    \\'scores\\', sorted np.recarray to be indexed by group ids\\n'\n        .format(key_added)\n        + ('    \\'logfoldchanges\\', sorted np.recarray to be indexed by group ids\\n'\n           '    \\'pvals\\', sorted np.recarray to be indexed by group ids\\n'\n           '    \\'pvals_adj\\', sorted np.recarray to be indexed by group ids'\n           if method in {'t-test', 't-test_overestim_var'} else ''))\n    return adata if copy else None", "idx": 359}
{"project": "Scanpy", "commit_id": "272_scanpy_1.9.0__baseplot_class.py_show.py", "target": 0, "func": "def show(self, return_axes: Optional[bool] = None):\n        \"\"\"\n        Show the figure\n\n        Parameters\n        ----------\n        return_axes\n             If true return a dictionary with the figure axes. When return_axes is true\n             then :func:`matplotlib.pyplot.show` is not called.\n\n        Returns\n        -------\n        If `return_axes=True`: Dict of :class:`matplotlib.axes.Axes`. The dict key\n        indicates the type of ax (eg. `mainplot_ax`)\n\n        See also\n        --------\n        `render()`: Renders the plot but does not call :func:`matplotlib.pyplot.show`\n        `savefig()`: Saves the plot.\n\n        Examples\n        -------\n        >>> adata = sc.datasets.pbmc68k_reduced()\n        >>> markers = ['C1QA', 'PSAP', 'CD79A', 'CD79B', 'CST3', 'LYZ']\n        >>> sc.pl.Plot(adata, markers, groupby='bulk_labels').show()\n\n        \"\"\"\n\n        self.make_figure()\n\n        if return_axes:\n            return self.ax_dict\n        else:\n            pl.show()", "idx": 370}
{"project": "Scanpy", "commit_id": "868_scanpy_1.9.0__sim.py_sim_model.py", "target": 0, "func": "def sim_model(self, tmax, X0, noiseDyn=0, restart=0):\n        \"\"\"Simulate the model.\"\"\"\n        self.noiseDyn = noiseDyn\n        #\n        X = np.zeros((tmax, self.dim))\n        X[0] = X0 + noiseDyn * np.random.randn(self.dim)\n        # run simulation\n        for t in range(1, tmax):\n            if self.modelType == 'hill':\n                Xdiff = self.Xdiff_hill(X[t - 1])\n            elif self.modelType == 'var':\n                Xdiff = self.Xdiff_var(X[t - 1])\n            else:\n                raise ValueError(f\"Unknown modelType {self.modelType!r}\")\n            X[t] = X[t - 1] + Xdiff\n            # add dynamic noise\n            X[t] += noiseDyn * np.random.randn(self.dim)\n        return X", "idx": 372}
{"project": "Scanpy", "commit_id": "208_scanpy_1.9.0___init__.py_compute_neighbors_umap.py", "target": 0, "func": "def compute_neighbors_umap(\n    X: Union[np.ndarray, csr_matrix],\n    n_neighbors: int,\n    random_state: AnyRandom = None,\n    metric: Union[_Metric, _MetricFn] = 'euclidean',\n    metric_kwds: Mapping[str, Any] = MappingProxyType({}),\n    angular: bool = False,\n    verbose: bool = False,\n):\n    \"\"\"This is from umap.fuzzy_simplicial_set [McInnes18]_.\n\n    Given a set of data X, a neighborhood size, and a measure of distance\n    compute the fuzzy simplicial set (here represented as a fuzzy graph in\n    the form of a sparse matrix) associated to the data. This is done by\n    locally approximating geodesic distance at each point, creating a fuzzy\n    simplicial set for each such point, and then combining all the local\n    fuzzy simplicial sets into a global one via a fuzzy union.\n\n    Parameters\n    ----------\n    X: array of shape (n_samples, n_features)\n        The data to be modelled as a fuzzy simplicial set.\n    n_neighbors\n        The number of neighbors to use to approximate geodesic distance.\n        Larger numbers induce more global estimates of the manifold that can\n        miss finer detail, while smaller values will focus on fine manifold\n        structure to the detriment of the larger picture.\n    random_state\n        A state capable being used as a numpy random state.\n    metric\n        The metric to use to compute distances in high dimensional space.\n        If a string is passed it must match a valid predefined metric. If\n        a general metric is required a function that takes two 1d arrays and\n        returns a float can be provided. For performance purposes it is\n        required that this be a numba jit'd function. Valid string metrics\n        include:\n            * euclidean\n            * manhattan\n            * chebyshev\n            * minkowski\n            * canberra\n            * braycurtis\n            * mahalanobis\n            * wminkowski\n            * seuclidean\n            * cosine\n            * correlation\n            * haversine\n            * hamming\n            * jaccard\n            * dice\n            * russelrao\n            * kulsinski\n            * rogerstanimoto\n            * sokalmichener\n            * sokalsneath\n            * yule\n        Metrics that take arguments (such as minkowski, mahalanobis etc.)\n        can have arguments passed via the metric_kwds dictionary. At this\n        time care must be taken and dictionary elements must be ordered\n        appropriately; this will hopefully be fixed in the future.\n    metric_kwds\n        Arguments to pass on to the metric, such as the ``p`` value for\n        Minkowski distance.\n    angular\n        Whether to use angular/cosine distance for the random projection\n        forest for seeding NN-descent to determine approximate nearest\n        neighbors.\n    verbose\n        Whether to report information on the current progress of the algorithm.\n\n    Returns\n    -------\n    **knn_indices**, **knn_dists** : np.arrays of shape (n_observations, n_neighbors)\n    \"\"\"\n    with warnings.catch_warnings():\n        # umap 0.5.0\n        warnings.filterwarnings(\"ignore\", message=r\"Tensorflow not installed\")\n        from umap.umap_ import nearest_neighbors\n\n    random_state = check_random_state(random_state)\n\n    knn_indices, knn_dists, forest = nearest_neighbors(\n        X,\n        n_neighbors,\n        random_state=random_state,\n        metric=metric,\n        metric_kwds=metric_kwds,\n        angular=angular,\n        verbose=verbose,\n    )\n\n    return knn_indices, knn_dists, forest", "idx": 384}
{"project": "Scanpy", "commit_id": "660_scanpy_1.2.2_scanpy_plotting_anndata.py_dotplot.py", "target": 1, "func": "def dotplot(adata, var_names, groupby=None, use_raw=True, log=False, num_categories=7,\n            show=None, save=None, **kwargs):\n    \"\"\"Makes a 'dot plot' of the expression values of `var_names`.\n    For each var_name and each groupby category a dot is plotted.\n    Each dot represents two values: mean expression within\n    each category (visualized by color) and fraction of cells expressing the\n    var_name in the category. (visualized by the size of the dot).\n    If groupby is not given, the dotplot assumes that all data belongs to a single\n    category. A gene is not considered expressed if the expression value in the adata\n    (or adata.raw) is equal to zero.\n    For example, for each marker gene, the mean value and the percentage of cells\n    expressing the gene can be visualized for each cluster.\n    Parameters\n    ----------\n    adata : :class:`~scanpy.api.AnnData`\n        Annotated data matrix.\n    var_names : `str` or list of `str`\n        var_names should be a valid subset of  `.var_names`.\n    groupby : `str` or `None`, optional (default: `None`)\n        The key of the observation grouping to consider. It is expected that groupby is\n        a categorical. If groupby is not a categorical observation, it would be\n        subdivided into `num_categories`.\n    log : `bool`, optional (default: `False`)\n        Use the log of the values\n    use_raw : `bool`, optional (default: `True`)\n        Use `raw` attribute of `adata` if present.\n    num_categories : `int`, optional (default: `7`)\n        Only used if groupby observation is not categorical. This value determines\n        the number of groups into which the groupby observation should be subdivided.\n    show : bool, optional (default: `None`)\n         Show the plot.\n    save : `bool` or `str`, optional (default: `None`)\n        If `True` or a `str`, save the figure. A string is appended to the\n        default filename. Infer the filetype if ending on {{'.pdf', '.png', '.svg'}}.\n    **kwargs : keyword arguments\n        Are passed to `seaborn.heatmap`.\n    Returns\n    -------\n    A list of `matplotlib.Axes` where the first ax is the groupby categories colorcode, the\n    second axis is the heatmap and the third axis is the colorbar.\n    \"\"\"\n    from scipy.sparse import issparse\n    sanitize_anndata(adata)\n    if isinstance(var_names, str):\n        var_names = [var_names]\n    if groupby is not None:\n        if groupby not in adata.obs_keys():\n            raise ValueError('groupby has to be a valid observation. Given value: {}, '\n                             'valid observations: {}'.format(groupby, adata.obs_keys()))\n    if use_raw:\n        matrix = adata.raw[:, var_names].X\n    else:\n        matrix = adata[:, var_names].X\n    if issparse(matrix):\n        matrix = matrix.toarray()\n    if log:\n        matrix = np.log1p(matrix)\n    obs_tidy = pd.DataFrame(matrix, columns=var_names)\n    if groupby is None:\n        # assign all data to a single category to allow groupby opeerations\n        categorical = pd.Series(np.repeat(0, obs_tidy.shape[0]), dtype='category')\n    else:\n        if not is_categorical_dtype(adata.obs[groupby]):\n            # if the groupby column is not categorical, turn it into one\n            # by subdividing into 7 categories\n            categorical = pd.cut(adata.obs[groupby], num_categories)\n        else:\n            categorical = adata.obs[groupby]\n    obs_tidy.set_index(categorical, groupby, inplace=True)\n    categories = obs_tidy.index.categories\n    # for if category defined by groupby (if any) compute for each var_name\n    # 1. the mean value over the category\n    # 2. the fraction of cells in the category having a value > 0\n    # 1. compute mean value\n    mean_obs = obs_tidy.groupby(level=0).mean()\n    # 2. compute fraction of cells having value >0\n    # transform obs_tidy into boolean matrix\n    obs_bool = obs_tidy.astype(bool)\n    # compute the sum per group which in the boolean matrix this is the number\n    # of values >0, and divide the result by the total number of values in the group\n    # (given by `count()`)\n    fraction_obs = obs_bool.groupby(level=0).sum() / obs_bool.groupby(level=0).count()\n    # set up axes\n    height = len(categories) * 0.5\n    # if the number of categories is small (eg 1 or 2) use\n    # a larger height\n    height = max([4, height])\n    heatmap_width = len(var_names) * 0.2\n    width = heatmap_width + 3  # +3 to account for the colorbar and labels\n    ax_frac2width = 0.25\n    size_legend_width = 0.5\n    fig, axs = pl.subplots(nrows=1, ncols=3, sharey=False,\n                           figsize=(width, height),\n                           gridspec_kw={'width_ratios': [width, ax_frac2width, size_legend_width]})\n    dot_ax = axs[0]\n    color_legend = axs[1]\n    size_legend = axs[2]\n    # make scatter plot in which\n    # x = var_names\n    # y = groupby category\n    # size = fraction\n    # color = mean expression\n    y, x = np.indices(mean_obs.shape)\n    y = y.flatten()\n    x = x.flatten()\n    frac = fraction_obs.values.flatten()\n    mean_flat = mean_obs.values.flatten()\n    cmap = pl.get_cmap('Reds')\n    size = (frac * 10) ** 2\n    import matplotlib.colors\n    normalize = matplotlib.colors.Normalize(vmin=min(mean_flat), vmax=max(mean_flat))\n    colors = [cmap(normalize(value)) for value in mean_flat]\n    dot_ax.scatter(x, y, color=colors, s=size, cmap=cmap, norm=None, edgecolor='none')\n    y_ticks = range(mean_obs.shape[0])\n    dot_ax.set_yticks(y_ticks)\n    dot_ax.set_yticklabels([mean_obs.index[idx] for idx in y_ticks])\n    x_ticks = range(mean_obs.shape[1])\n    dot_ax.set_xticks(x_ticks)\n    dot_ax.set_xticklabels([mean_obs.columns[idx] for idx in x_ticks], rotation=90)\n    dot_ax.grid(False)\n    dot_ax.set_xlim(-0.5, len(var_names) + 0.5)\n\n    # plot colorbar\n    import matplotlib.colorbar\n    matplotlib.colorbar.ColorbarBase(color_legend, cmap=cmap, norm=normalize)\n    # plot size bar\n    fracs_legend = np.array([0.25, 0.50, 0.75, 1])\n    size = (fracs_legend * 10) ** 2\n    color = [cmap(normalize(value)) for value in np.repeat(max(mean_flat) * 0.7, len(size))]\n    size_legend.scatter(np.repeat(0, len(size)), range(len(size)), s=size, color=color)\n    size_legend.set_yticks(range(len(size)))\n    size_legend.set_yticklabels([\"{:.0%}\".format(x) for x in fracs_legend])\n    size_legend.tick_params(axis='y', left=False, labelleft=False, labelright=True)\n    # remove x ticks and labels\n    size_legend.tick_params(axis='x', bottom=False, labelbottom=False)\n    size_legend.set_ylim(-10, len(size))\n    # remove surrounding lines\n    size_legend.spines['right'].set_visible(False)\n    size_legend.spines['top'].set_visible(False)\n    size_legend.spines['left'].set_visible(False)\n    size_legend.spines['bottom'].set_visible(False)\n    size_legend.grid(False)\n    pl.subplots_adjust(wspace=0.1, hspace=0.01)\n    utils.savefig_or_show('heatmap', show=show, save=save)\n    return axs", "idx": 396}
{"project": "Scanpy", "commit_id": "79_scanpy_1.9.0__metadata.py_refresh_entry_points.py", "target": 0, "func": "def refresh_entry_points():\n    \"\"\"\\\n    Under some circumstances, (e.g. when installing a PEP 517 package via pip),\n    pkg_resources.working_set.entries is stale. This tries to fix that.\n    See https://github.com/pypa/setuptools_scm/issues/513\n    \"\"\"\n    try:\n        import sys\n        import pkg_resources\n\n        ws: pkg_resources.WorkingSet = pkg_resources.working_set\n        for entry in sys.path:\n            ws.add_entry(entry)\n    except Exception:\n        pass", "idx": 401}
{"project": "Scanpy", "commit_id": "566_scanpy_1.0.2_scanpy_plotting_anndata.py_scatter.py", "target": 1, "func": "def scatter(\n        adata,\n        x=None,\n        y=None,\n        color=None,\n        use_raw=True,\n        sort_order=True,\n        alpha=None,\n        basis=None,\n        groups=None,\n        components=None,\n        projection='2d',\n        legend_loc='right margin',\n        legend_fontsize=None,\n        legend_fontweight=None,\n        color_map=None,\n        palette=None,\n        right_margin=None,\n        left_margin=None,\n        size=None,\n        title=None,\n        show=None,\n        save=None,\n        ax=None):\n    \"\"\"Scatter plot.\n    Color with annotation of observations (`.obs`) or expression of genes\n    (`.var_names`).\n    Parameters\n    ----------\n    adata : :class:`~scanpy.api.AnnData`\n        Annotated data matrix.\n    x : `str` or `None`\n        x coordinate.\n    y : `str` or `None`\n        y coordinate.\n    color : string or list of strings, optional (default: `None`)\n        Keys for observation/cell annotation `[\\'ann1\\', \\'ann2\\']`.\n    use_raw : `bool`, optional (default: `True`)\n        Use `raw` attribute of `adata` if present.\n    sort_order : `bool`, optional (default: `True`)\n        For continuous annotations used as color parameter, plot data points\n        with higher values on top of others.\n    basis : {'pca', 'tsne', 'umap', 'diffmap', 'draw_graph_fr', etc.}\n        String that denotes a plotting tool that computed coordinates.\n    groups : `str`, optional (default: all groups in color)\n        Allows to restrict categories in observation annotation to a subset.\n    components : `str` or list of `str`, optional (default: '1,2')\n         String of the form '1,2' or ['1,2', '2,3'].\n    projection : {'2d', '3d'}, optional (default: '2d')\n         Projection of plot.\n    legend_loc : `str`, optional (default: 'right margin')\n         Location of legend, either 'on data', 'right margin' or valid keywords\n         for `matplotlib.pyplot.legend\n         <https://matplotlib.org/api/_as_gen/matplotlib.pyplot.legend.html>`_.\n    legend_fontsize : `int` (default: `None`)\n         Legend font size.\n    legend_fontweight : {'normal', 'bold', ...} (default: `None`)\n         Legend font weight. Defaults to 'bold' if `legend_loc = 'on data'`,\n         otherwise to 'normal'. Available are `['light', 'normal', 'medium',\n         'semibold', 'bold', 'heavy', 'black']`.\n    color_map : `str` (default: 'RdBu_r')\n         String denoting matplotlib color map for continuous coloring.\n    palette : list of `str` (default: `None`)\n         Colors to use for plotting groups (categorical annotation).\n    right_margin : `float` (default: 0.3)\n         Adjust how far the plotting panel extends to the right.\n    size : float (default: None)\n         Point size. Observation-number dependent by default.\n    title : `str` or list of `str`, optional (default: `None`)\n         Provide title for panels either as `[\\'title1\\', ...]`.\n    show : `bool`, optional (default: `None`)\n         Show the plot.\n    save : `bool` or `str`, optional (default: `None`)\n        If `True` or a `str`, save the figure. A string is appended to the\n        default filename. Infer the filetype if ending on \\{'.pdf', '.png', '.svg'\\}.\n    ax : `matplotlib.Axes`\n         A `matplotlib.Axes` object.\n    Returns\n    -------\n    If `show==False`, a list of `matplotlib.Axis` objects. Every second element\n    corresponds to the 'right margin' drawing area for color bars and legends.\n    \"\"\"\n    sanitize_anndata(adata)\n    if legend_loc not in VALID_LEGENDLOCS:\n        raise ValueError(\n            'Invalid `legend_loc`, need to be one of: {}.'.format(VALID_LEGENDLOCS))\n    if components is None: components = '1,2' if '2d' in projection else '1,2,3'\n    if isinstance(components, str): components = components.split(',')\n    components = np.array(components).astype(int) - 1\n    keys = ['grey'] if color is None else [color] if isinstance(color, str) else color\n    if title is not None and isinstance(title, str):\n        title = [title]\n    highlights = adata.uns['highlights'] if 'highlights' in adata.uns else []\n    if basis is not None:\n        try:\n            # ignore the '0th' diffusion component\n            if basis == 'diffmap': components += 1\n            Y = adata.obsm['X_' + basis][:, components]\n            # correct the component vector for use in labeling etc.\n            if basis == 'diffmap': components -= 1\n        except KeyError:\n            raise KeyError('compute coordinates using visualization tool {} first'\n                           .format(basis))\n    elif x is not None and y is not None:\n        x_arr = adata._get_obs_array(x)\n        y_arr = adata._get_obs_array(y)\n        Y = np.c_[x_arr[:, None], y_arr[:, None]]\n    else:\n        raise ValueError('Either provide a `basis` or `x` and `y`.')\n    if size is None:\n        n = Y.shape[0]\n        size = 120000 / n\n    if legend_loc == 'on data' and legend_fontsize is None:\n        legend_fontsize = rcParams['legend.fontsize']\n    elif legend_fontsize is None:\n        legend_fontsize = rcParams['legend.fontsize']\n    palette_was_none = False\n    if palette is None: palette_was_none = True\n    if isinstance(palette, list):\n        if not is_color_like(palette[0]):\n            palettes = palette\n        else:\n            palettes = [palette]\n    else:\n        palettes = [palette for i in range(len(keys))]\n    for i, palette in enumerate(palettes):\n        palettes[i] = utils.default_palette(palette)\n    if basis is not None:\n        component_name = (\n            'DC' if basis == 'diffmap'\n            else 'tSNE' if basis == 'tsne'\n            else 'UMAP' if basis == 'umap'\n            else 'PC' if basis == 'pca'\n            else basis.replace('draw_graph_', '').upper() if 'draw_graph' in basis\n            else None)\n    else:\n        component_name = None\n    axis_labels = (x, y) if component_name is None else None\n    show_ticks = True if component_name is None else False\n    # the actual color ids, e.g. 'grey' or '#109482'\n    color_ids = [None if not is_color_like(key)\n                 else key for key in keys]\n    categoricals = []\n    colorbars = []\n    for ikey, key in enumerate(keys):\n        if color_ids[ikey] is not None:\n            c = color_ids[ikey]\n            continuous = True\n            categorical = False\n            colorbars.append(False)\n        else:\n            c = 'white' if projection == '2d' else 'white'\n            categorical = False\n            continuous = False\n            # test whether we have categorial or continuous annotation\n            if key in adata.obs_keys():\n                if is_categorical_dtype(adata.obs[key]):\n                    categorical = True\n                else:\n                    continuous = True\n                    c = adata.obs[key]\n            # coloring according to gene expression\n            elif (use_raw\n                  and adata.raw is not None\n                  and key in adata.raw.var_names):\n                c = adata.raw[:, key].X\n                continuous = True\n            elif key in adata.var_names:\n                c = adata[:, key].X\n                continuous = True\n            else:\n                raise ValueError(\n                    'key \\'{}\\' is invalid! pass valid observation annotation, '\n                    'one of {} or a gene name {}'\n                    .format(key, adata.obs_keys(), adata.var_names))\n            colorbars.append(True if continuous else False)\n        if categorical: categoricals.append(ikey)\n        color_ids[ikey] = c\n    if right_margin is None and len(categoricals) > 0:\n        if legend_loc == 'right margin': right_margin = 0.5\n    if title is None and keys[0] is not None:\n        title = [key.replace('_', ' ') if not is_color_like(key) else '' for key in keys]\n    axs = scatter_base(Y,\n                       title=title,\n                       alpha=alpha,\n                       component_name=component_name,\n                       axis_labels=axis_labels,\n                       component_indexnames=components + 1,\n                       projection=projection,\n                       colors=color_ids,\n                       highlights=highlights,\n                       colorbars=colorbars,\n                       right_margin=right_margin,\n                       left_margin=left_margin,\n                       sizes=[size for c in keys],\n                       color_map=color_map,\n                       show_ticks=show_ticks,\n                       ax=ax)\n\n    for i, ikey in enumerate(categoricals):\n        palette = palettes[i]\n        key = keys[ikey]\n        if (not key + '_colors' in adata.uns\n            or not palette_was_none\n            or len(adata.obs[key].cat.categories) != len(adata.uns[key + '_colors'])):\n            utils.add_colors_for_categorical_sample_annotation(adata, key, palette)\n        # actually plot the groups\n        mask_remaining = np.ones(Y.shape[0], dtype=bool)\n        centroids = {}\n        if groups is None:\n            for iname, name in enumerate(adata.obs[key].cat.categories):\n                if name not in settings.categories_to_ignore:\n                    mask = scatter_group(axs[ikey], key, iname,\n                                         adata, Y, projection, size=size, alpha=alpha)\n                    mask_remaining[mask] = False\n                    if legend_loc == 'on data': add_centroid(centroids, name, Y, mask)\n        else:\n            for name in groups:\n                if name not in set(adata.obs[key].cat.categories):\n                    raise ValueError('\"' + name + '\" is invalid!'\n                                     + ' specify valid name, one of '\n                                     + str(adata.obs[key].cat.categories))\n                else:\n                    iname = np.flatnonzero(adata.obs[key].cat.categories.values == name)[0]\n                    mask = scatter_group(axs[ikey], key, iname,\n                                         adata, Y, projection, size=size, alpha=alpha)\n                    if legend_loc == 'on data': add_centroid(centroids, name, Y, mask)\n                    mask_remaining[mask] = False\n        if mask_remaining.sum() > 0:\n            data = [Y[mask_remaining, 0], Y[mask_remaining, 1]]\n            if projection == '3d': data.append(Y[mask_remaining, 2])\n            axs[ikey].scatter(*data, marker='.', c='grey', s=size,\n                                    edgecolors='none', zorder=-1)\n        legend = None\n        if legend_loc == 'on data':\n            if legend_fontweight is None:\n                legend_fontweight = 'bold'\n            for name, pos in centroids.items():\n                axs[ikey].text(pos[0], pos[1], name,\n                               weight=legend_fontweight,\n                               verticalalignment='center',\n                               horizontalalignment='center',\n                               fontsize=legend_fontsize)\n        elif legend_loc == 'right margin':\n            legend = axs[ikey].legend(frameon=False, loc='center left',\n                                            bbox_to_anchor=(1, 0.5),\n                                            ncol=(1 if len(adata.obs[key].cat.categories) <= 14\n                                                  else 2 if len(adata.obs[key].cat.categories) <= 30 else 3),\n                                            fontsize=legend_fontsize)\n        elif legend_loc != 'none':\n            legend = axs[ikey].legend(frameon=False, loc=legend_loc,\n                                            fontsize=legend_fontsize)\n        if legend is not None:\n            for handle in legend.legendHandles: handle.set_sizes([300.0])\n    utils.savefig_or_show('scatter' if basis is None else basis, show=show, save=save)\n    if show == False: return axs", "idx": 404}
{"project": "Scanpy", "commit_id": "856_scanpy_1.4_scanpy_plotting__anndata.py_dotplot.py", "target": 1, "func": "def dotplot(adata, var_names, groupby=None, use_raw=None, log=False, num_categories=7,\n            expression_cutoff=0., mean_only_expressed=False, color_map='Reds', dot_max=None,\n            dot_min=None, figsize=None, dendrogram=False, gene_symbols=None,\n            var_group_positions=None, standard_scale=None, smallest_dot=0.,\n            var_group_labels=None, var_group_rotation=None, layer=None, show=None,\n            save=None, **kwds):\n    \"\"\"\\\n    Makes a *dot plot* of the expression values of `var_names`.\n    For each var_name and each `groupby` category a dot is plotted. Each dot\n    represents two values: mean expression within each category (visualized by\n    color) and fraction of cells expressing the var_name in the\n    category (visualized by the size of the dot).  If groupby is not given, the\n    dotplot assumes that all data belongs to a single category.\n\n    **Note**: A gene is considered expressed if the expression value in the adata\n    (or adata.raw) is above the specified threshold which is zero by default.\n    An example of dotplot usage is to visualize, for multiple marker genes,\n    the mean value and the percentage of cells expressing the gene accross multiple clusters.\n    Parameters\n    ----------\n    {common_plot_args}\n    expression_cutoff : `float` (default: `0.`)\n        Expression cutoff that is used for binarizing the gene expression and determining the fraction\n        of cells expressing given genes. A gene is expressed only if the expression value is greater than\n        this threshold.\n    mean_only_expressed : `bool` (default: `False`)\n        If True, gene expression is averaged only over the cells expressing the given genes.\n    color_map : `str`, optional (default: `Reds`)\n        String denoting matplotlib color map.\n    dot_max : `float` optional (default: `None`)\n        If none, the maximum dot size is set to the maximum fraction value found (e.g. 0.6). If given,\n        the value should be a number between 0 and 1. All fractions larger than dot_max are clipped to\n        this value.\n    dot_min : `float` optional (default: `None`)\n        If none, the minimum dot size is set to 0. If given,\n        the value should be a number between 0 and 1. All fractions smaller than dot_min are clipped to\n        this value.\n    standard_scale : {{'var', 'group'}}, optional (default: None)\n        Whether or not to standardize that dimension between 0 and 1, meaning for each variable or group,\n        subtract the minimum and divide each by its maximum.\n    smallest_dot : `float` optional (default: 0.)\n        If none, the smallest dot has size 0. All expression levels with `dot_min` are potted with\n        `smallest_dot` dot size.\n    {show_save_ax}\n    **kwds : keyword arguments\n        Are passed to `matplotlib.pyplot.scatter`.\n    Returns\n    -------\n    List of `matplotlib.Axes`\n    Examples\n    -------\n    >>> adata = sc.datasets.pbmc68k_reduced()\n    >>> sc.pl.dotplot(adata, ['C1QA', 'PSAP', 'CD79A', 'CD79B', 'CST3', 'LYZ'],\n    ...               groupby='bulk_labels', dendrogram=True)\n    \"\"\"\n    if use_raw is None and adata.raw is not None: use_raw = True\n    if isinstance(var_names, str):\n        var_names = [var_names]\n    categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories,\n                                              layer=layer, gene_symbols=gene_symbols)\n    # for if category defined by groupby (if any) compute for each var_name\n    # 1. the fraction of cells in the category having a value > expression_cutoff\n    # 2. the mean value over the category\n    # 1. compute fraction of cells having value > expression_cutoff\n    # transform obs_tidy into boolean matrix using the expression_cutoff\n    obs_bool = obs_tidy > expression_cutoff\n    # compute the sum per group which in the boolean matrix this is the number\n    # of values > expression_cutoff, and divide the result by the total number of values\n    # in the group (given by `count()`)\n    fraction_obs = obs_bool.groupby(level=0).sum() / obs_bool.groupby(level=0).count()\n    # 2. compute mean value\n    if mean_only_expressed:\n        mean_obs = obs_tidy.mask(~obs_bool).groupby(level=0).mean().fillna(0)\n    else:\n        mean_obs = obs_tidy.groupby(level=0).mean()\n    if standard_scale == 'group':\n        mean_obs = mean_obs.sub(mean_obs.min(1), axis=0)\n        mean_obs = mean_obs.div(mean_obs.max(1), axis=0).fillna(0)\n    elif standard_scale == 'var':\n        mean_obs -= mean_obs.min(0)\n        mean_obs /= mean_obs.max(0).fillna(0)\n    elif standard_scale is None:\n        pass\n    else:\n        logg.warn('Unknown type for standard_scale, ignored')\n    dendro_width = 0.8 if dendrogram else 0\n    colorbar_width = 0.2\n    colorbar_width_spacer = 0.5\n    size_legend_width = 0.25\n    if figsize is None:\n        height = len(categories) * 0.3 + 1  # +1 for labels\n        # if the number of categories is small (eg 1 or 2) use\n        # a larger height\n        height = max([1.5, height])\n        heatmap_width = len(var_names) * 0.35\n        width = heatmap_width + colorbar_width + size_legend_width + dendro_width + colorbar_width_spacer\n    else:\n        width, height = figsize\n        heatmap_width = width - (colorbar_width + size_legend_width + dendro_width + colorbar_width_spacer)\n    # colorbar ax width should not change with differences in the width of the image\n    # otherwise can become too small\n    if var_group_positions is not None and len(var_group_positions) > 0:\n        # add some space in case 'brackets' want to be plotted on top of the image\n        height_ratios = [0.5, 10]\n    else:\n        height_ratios = [0, 10.5]\n    # define a layout of 2 rows x 5 columns\n    # first row is for 'brackets' (if no brackets needed, the height of this row is zero)\n    # second row is for main content. This second row\n    # is divided into 4 axes:\n    #   first ax is for the main figure\n    #   second ax is for dendrogram (if present)\n    #   third ax is for the color bar legend\n    #   fourth ax is for an spacer that avoids the ticks\n    #             from the color bar to be hidden beneath the size lengend axis\n    #   fifth ax is to plot the size legend\n    fig = pl.figure(figsize=(width, height))\n    axs = gridspec.GridSpec(nrows=2, ncols=5, wspace=0.02, hspace=0.04,\n                            width_ratios=[heatmap_width, dendro_width, colorbar_width, colorbar_width_spacer,\n                                          size_legend_width],\n                            height_ratios=height_ratios)\n    if len(categories) < 4:\n        # when few categories are shown, the colorbar and size legend\n        # need to be larger than the main plot, otherwise they would look\n        # compressed. For this, the dotplot ax is split into two:\n        axs2 = gridspec.GridSpecFromSubplotSpec(2, 1, subplot_spec=axs[1, 0],\n                                                height_ratios=[len(categories) * 0.3, 1])\n        dot_ax = fig.add_subplot(axs2[0])\n    else:\n        dot_ax = fig.add_subplot(axs[1, 0])\n    color_legend = fig.add_subplot(axs[1, 2])\n    if groupby is None or len(categories) <= 1:\n        # dendrogram can only be computed  between groupby categories\n        dendrogram = False\n    if dendrogram:\n        dendro_data = _reorder_categories_after_dendrogram(adata, groupby, dendrogram,\n                                                           var_names=var_names,\n                                                           var_group_labels=var_group_labels,\n                                                           var_group_positions=var_group_positions)\n        var_group_labels = dendro_data['var_group_labels']\n        var_group_positions = dendro_data['var_group_positions']\n        # reorder matrix\n        if dendro_data['var_names_idx_ordered'] is not None:\n            # reorder columns (usually genes) if needed. This only happens when\n            # var_group_positions and var_group_labels is set\n            mean_obs = mean_obs.iloc[:, dendro_data['var_names_idx_ordered']]\n            fraction_obs = fraction_obs.iloc[:, dendro_data['var_names_idx_ordered']]\n        # reorder rows (categories) to match the dendrogram order\n        mean_obs = mean_obs.iloc[dendro_data['categories_idx_ordered'], :]\n        fraction_obs = fraction_obs.iloc[dendro_data['categories_idx_ordered'], :]\n        y_ticks = range(mean_obs.shape[0])\n        dendro_ax = fig.add_subplot(axs[1, 1], sharey=dot_ax)\n        _plot_dendrogram(dendro_ax, adata, groupby, dendrogram_key=dendrogram, ticks=y_ticks)\n    # to keep the size_legen of about the same height, irrespective\n    # of the number of categories, the fourth ax is subdivided into two parts\n    size_legend_height = min(1.3, height)\n    # wspace is proportional to the width but a constant value is\n    # needed such that the spacing is the same for thinner or wider images.\n    wspace = 10.5 / width\n    axs3 = gridspec.GridSpecFromSubplotSpec(2, 1, subplot_spec=axs[1, 4], wspace=wspace,\n                                            height_ratios=[size_legend_height / height,\n                                                           (height - size_legend_height) / height])\n    # make scatter plot in which\n    # x = var_names\n    # y = groupby category\n    # size = fraction\n    # color = mean expression\n    y, x = np.indices(mean_obs.shape)\n    y = y.flatten()\n    x = x.flatten()\n    frac = fraction_obs.values.flatten()\n    mean_flat = mean_obs.values.flatten()\n    cmap = pl.get_cmap(color_map)\n    if dot_max is None:\n        dot_max = np.ceil(max(frac) * 10) / 10\n    else:\n        if dot_max < 0 or dot_max > 1:\n            raise ValueError(\"`dot_max` value has to be between 0 and 1\")\n    if dot_min is None:\n        dot_min = 0\n    else:\n        if dot_min < 0 or dot_min > 1:\n            raise ValueError(\"`dot_min` value has to be between 0 and 1\")\n    if dot_min != 0 or dot_max != 1:\n        # clip frac between dot_min and  dot_max\n        frac = np.clip(frac, dot_min, dot_max)\n        old_range = dot_max - dot_min\n        # re-scale frac between 0 and 1\n        frac = ((frac - dot_min) / old_range)\n    size = (frac * 10) ** 2\n    size += smallest_dot\n    import matplotlib.colors\n    normalize = matplotlib.colors.Normalize(vmin=kwds.get('vmin'), vmax=kwds.get('vmax'))\n    colors = cmap(normalize(mean_flat))\n    dot_ax.scatter(x, y, color=colors, s=size, cmap=cmap, norm=None, edgecolor='none', **kwds)\n    y_ticks = range(mean_obs.shape[0])\n    dot_ax.set_yticks(y_ticks)\n    dot_ax.set_yticklabels([mean_obs.index[idx] for idx in y_ticks])\n    x_ticks = range(mean_obs.shape[1])\n    dot_ax.set_xticks(x_ticks)\n    dot_ax.set_xticklabels([mean_obs.columns[idx] for idx in x_ticks], rotation=90)\n    dot_ax.tick_params(axis='both', labelsize='small')\n    dot_ax.grid(False)\n    dot_ax.set_xlim(-0.5, len(var_names) + 0.5)\n    dot_ax.set_ylabel(groupby)\n    # to be consistent with the heatmap plot, is better to\n    # invert the order of the y-axis, such that the first group is on\n    # top\n    ymin, ymax = dot_ax.get_ylim()\n    dot_ax.set_ylim(ymax + 0.5, ymin - 0.5)\n    dot_ax.set_xlim(-1, len(var_names))\n    # plot group legends on top of dot_ax (if given)\n    if var_group_positions is not None and len(var_group_positions) > 0:\n        gene_groups_ax = fig.add_subplot(axs[0, 0], sharex=dot_ax)\n        _plot_gene_groups_brackets(gene_groups_ax, group_positions=var_group_positions,\n                                   group_labels=var_group_labels,\n                                   rotation=var_group_rotation)\n    # plot colorbar\n    import matplotlib.colorbar\n    matplotlib.colorbar.ColorbarBase(color_legend, cmap=cmap, norm=normalize)\n    # for the dot size legend, use step between dot_max and dot_min\n    # based on how different they are.\n    diff = dot_max - dot_min\n    if 0.3 < diff <= 0.6:\n        step = 0.1\n    elif diff <= 0.3:\n        step = 0.05\n    else:\n        step = 0.2\n    # a descending range that is afterwards inverted is used\n    # to guarantee that dot_max is in the legend.\n    fracs_legends = np.arange(dot_max, dot_min, step * -1)[::-1]\n    if dot_min != 0 or dot_max != 1:\n        fracs_values = ((fracs_legends - dot_min) / old_range)\n    else:\n        fracs_values = fracs_legends\n    size = (fracs_values * 10) ** 2\n    size += smallest_dot\n    color = [cmap(normalize(value)) for value in np.repeat(max(mean_flat) * 0.7, len(size))]\n    # plot size bar\n    size_legend = fig.add_subplot(axs3[0])\n    size_legend.scatter(np.repeat(0, len(size)), range(len(size)), s=size, color=color)\n    size_legend.set_yticks(range(len(size)))\n    labels = [\"{:.0%}\".format(x) for x in fracs_legends]\n    if dot_max < 1:\n        labels[-1] = \">\" + labels[-1]\n    size_legend.set_yticklabels(labels)\n    size_legend.set_yticklabels([\"{:.0%}\".format(x) for x in fracs_legends])\n    size_legend.tick_params(axis='y', left=False, labelleft=False, labelright=True)\n    # remove x ticks and labels\n    size_legend.tick_params(axis='x', bottom=False, labelbottom=False)\n    # remove surrounding lines\n    size_legend.spines['right'].set_visible(False)\n    size_legend.spines['top'].set_visible(False)\n    size_legend.spines['left'].set_visible(False)\n    size_legend.spines['bottom'].set_visible(False)\n    size_legend.grid(False)\n    ymin, ymax = size_legend.get_ylim()\n    size_legend.set_ylim(ymin, ymax + 0.5)\n    utils.savefig_or_show('dotplot', show=show, save=save)\n    return axs", "idx": 407}
{"project": "Scanpy", "commit_id": "887_scanpy_1.4.3_scanpy_tools__phate.py_phate.py", "target": 1, "func": "def phate(\n        adata,\n        n_components=2,\n        k=5,\n        a=15,\n        n_landmark=2000,\n        t='auto',\n        gamma=1,\n        n_pca=100,\n        knn_dist='euclidean',\n        mds_dist='euclidean',\n        mds='metric',\n        n_jobs=None,\n        random_state=None,\n        verbose=None,\n        copy=False,\n        **kwargs):\n    \"\"\"PHATE [Moon17]_.\n    Potential of Heat-diffusion for Affinity-based Trajectory Embedding (PHATE)\n    embeds high dimensional single-cell data into two or three dimensions for\n    visualization of biological progressions.\n    For more information and access to the object-oriented interface, read the\n    `PHATE documentation <https://phate.readthedocs.io/>`__.  For\n    tutorials, bug reports, and R/MATLAB implementations, visit the `PHATE\n    GitHub page <https://github.com/KrishnaswamyLab/PHATE/>`__. For help\n    using PHATE, go `here <https://krishnaswamylab.org/get-help>`__.\n    Parameters\n    ----------\n    adata : :class:`~anndata.AnnData`\n        Annotated data matrix.\n    n_components : `int`, optional (default: 2)\n        number of dimensions in which the data will be embedded\n    k : `int`, optional (default: 5)\n        number of nearest neighbors on which to build kernel\n    a : `int`, optional (default: 15)\n        sets decay rate of kernel tails.\n        If None, alpha decaying kernel is not used\n    n_landmark : `int`, optional (default: 2000)\n        number of landmarks to use in fast PHATE\n    t : `int` or 'auto', optional (default: 'auto')\n        power to which the diffusion operator is powered\n        sets the level of diffusion. If 'auto', t is selected\n        according to the knee point in the Von Neumann Entropy of\n        the diffusion operator\n    gamma : float, optional, default: 1\n        Informational distance constant between -1 and 1.\n        `gamma=1` gives the PHATE log potential, `gamma=0` gives\n        a square root potential.\n    n_pca : `int`, optional (default: 100)\n        Number of principal components to use for calculating\n        neighborhoods. For extremely large datasets, using\n        n_pca < 20 allows neighborhoods to be calculated in\n        log(n_samples) time.\n    knn_dist : string, optional (default: 'euclidean')\n        recommended values: 'euclidean' and 'cosine'\n        Any metric from `scipy.spatial.distance` can be used\n        distance metric for building kNN graph\n    mds_dist : string, optional (default: 'euclidean')\n        recommended values: 'euclidean' and 'cosine'\n        Any metric from `scipy.spatial.distance` can be used\n        distance metric for MDS\n    mds : {'classic', 'metric', 'nonmetric'}, optional (default: 'metric')\n        Selects which MDS algorithm is used for dimensionality reduction\n    n_jobs : `int` or `None`, optional (default: `sc.settings.n_jobs`)\n        The number of jobs to use for the computation.\n        If `None`, `sc.settings.n_jobs` is used.\n        If -1 all CPUs are used. If 1 is given, no parallel computing code is\n        used at all, which is useful for debugging.\n        For n_jobs below -1, (n_cpus + 1 + n_jobs) are used. Thus for\n        n_jobs = -2, all CPUs but one are used\n    random_state : `int`, `numpy.RandomState` or `None`, optional (default: `None`)\n        Random seed. Defaults to the global `numpy` random number generator\n    verbose : `bool`, `int` or `None`, optional (default: `sc.settings.verbosity`)\n        If `True` or an integer `>= 2`, print status messages.\n        If `None`, `sc.settings.verbosity` is used.\n    copy : `bool` (default: `False`)\n        Return a copy instead of writing to `adata`.\n    kwargs : additional arguments to `phate.PHATE`\n    Returns\n    -------\n    Depending on `copy`, returns or updates `adata` with the following fields.\n    **X_phate** : `np.ndarray`, (`adata.obs`, shape=[n_samples, n_components], dtype `float`)\n        PHATE coordinates of data.\n    Examples\n    --------\n    >>> import scanpy.api as sc\n    >>> import phate\n    >>> tree_data, tree_clusters = phate.tree.gen_dla(n_dim=100,\n                                                      n_branch=20,\n                                                      branch_length=100)\n    >>> tree_data.shape\n    (2000, 100)\n    >>> adata = sc.AnnData(tree_data)\n    >>> sc.tl.phate(adata, k=5, a=20, t=150)\n    >>> adata.obsm['X_phate'].shape\n    (2000, 2)\n    >>> sc.pl.phate(adata)\n    \"\"\"\n    start = logg.info('computing PHATE')\n    adata = adata.copy() if copy else adata\n    verbose = settings.verbosity if verbose is None else verbose\n    if isinstance(settings.verbosity, (str, int)):\n        verbose = _settings_verbosity_greater_or_equal_than(2)\n    n_jobs = settings.n_jobs if n_jobs is None else n_jobs\n    try:\n        import phate\n    except ImportError:\n        raise ImportError(\n            'You need to install the package `phate`: please run `pip install '\n            '--user phate` in a terminal.')\n    X_phate = phate.PHATE(\n        n_components=n_components,\n        k=k,\n        a=a,\n        n_landmark=n_landmark,\n        t=t,\n        gamma=gamma,\n        n_pca=n_pca,\n        knn_dist=knn_dist,\n        mds_dist=mds_dist,\n        mds=mds,\n        n_jobs=n_jobs,\n        random_state=random_state,\n        verbose=verbose,\n        **kwargs\n    ).fit_transform(adata)\n    # update AnnData instance\n    adata.obsm['X_phate'] = X_phate  # annotate samples with PHATE coordinates\n    logg.info(\n        '    finished',\n        time=start,\n        deep=(\n            'added\\n'\n            \"    'X_phate', PHATE coordinates (adata.obsm)\"\n        ),\n    )\n    return adata if copy else None", "idx": 411}
{"project": "Scanpy", "commit_id": "637_scanpy_1.1_scanpy_plotting_tools_paga.py__paga_graph.py", "target": 1, "func": "def _paga_graph(\n        adata,\n        ax,\n        layout=None,\n        layout_kwds={},\n        init_pos=None,\n        solid_edges=None,\n        dashed_edges=None,\n        transitions=None,\n        threshold=None,\n        threshold_arrows=None,\n        threshold_solid=None,\n        threshold_dashed=None,\n        root=0,\n        colors=None,\n        labels=None,\n        fontsize=None,\n        text_kwds=None,\n        node_size_scale=1,\n        node_size_power=0.5,\n        edge_width_scale=1,\n        title=None,\n        pos=None,\n        cmap=None,\n        frameon=True,\n        min_edge_width=None,\n        max_edge_width=None,\n        export_to_gexf=False,\n        cax=None,\n        colorbar=None,\n        cb_kwds={},\n        single_component=False,\n        random_state=0):\n    node_labels = labels  # rename for clarity\n    if (node_labels is not None\n        and isinstance(node_labels, str)\n        and node_labels != adata.uns['paga']['groups']):\n        raise ValueError('Provide a list of group labels for the PAGA groups {}, not {}.'\n                         .format(adata.uns['paga']['groups'], node_labels))\n    groups_key = adata.uns['paga']['groups']\n    if node_labels is None:\n        node_labels = adata.obs[groups_key].cat.categories\n    if colors is None and groups_key is not None:\n        if (groups_key + '_colors' not in adata.uns\n            or len(adata.obs[groups_key].cat.categories)\n               != len(adata.uns[groups_key + '_colors'])):\n            utils.add_colors_for_categorical_sample_annotation(adata, groups_key)\n        colors = adata.uns[groups_key + '_colors']\n        for iname, name in enumerate(adata.obs[groups_key].cat.categories):\n            if name in settings.categories_to_ignore: colors[iname] = 'grey'\n    if isinstance(root, str):\n        if root in node_labels:\n            root = list(node_labels).index(root)\n        else:\n            raise ValueError(\n                'If `root` is a string, it needs to be one of {} not \\'{}\\'.'\n                .format(node_labels.tolist(), root))\n    if isinstance(root, list) and root[0] in node_labels:\n        root = [list(node_labels).index(r) for r in root]\n    # define the objects\n    adjacency_solid = adata.uns['paga'][solid_edges].copy()\n    # set the the thresholds, either explicitly\n    if threshold is not None:\n        threshold_solid = threshold\n        threshold_dashed = threshold\n    # or to a default value\n    else:\n        if threshold_solid is None:\n            threshold_solid = 0.01  # default threshold\n        if threshold_dashed is None:\n            threshold_dashed = 0.01  # default treshold\n    if threshold_solid > 0:\n        adjacency_solid.data[adjacency_solid.data < threshold_solid] = 0\n        adjacency_solid.eliminate_zeros()\n    nx_g_solid = nx.Graph(adjacency_solid)\n    if dashed_edges is not None:\n        adjacency_dashed = adata.uns['paga'][dashed_edges].copy()\n        if threshold_dashed > 0:\n            adjacency_dashed.data[adjacency_dashed.data < threshold_dashed] = 0\n            adjacency_dashed.eliminate_zeros()\n        nx_g_dashed = nx.Graph(adjacency_dashed)\n    # uniform color\n    if isinstance(colors, str) and is_color_like(colors):\n        colors = [colors for c in range(len(node_labels))]\n    # color degree of the graph\n    if isinstance(colors, str) and colors.startswith('degree'):\n        # see also tools.paga.paga_degrees\n        if colors == 'degree_dashed':\n            colors = [d for _, d in nx_g_dashed.degree(weight='weight')]\n        elif colors == 'degree_solid':\n            colors = [d for _, d in nx_g_solid.degree(weight='weight')]\n        else:\n            raise ValueError('`degree` either \"degree_dashed\" or \"degree_solid\".')\n        colors = (np.array(colors) - np.min(colors)) / (np.max(colors) - np.min(colors))\n    # plot numeric colors\n    if isinstance(colors, Iterable) and not isinstance(colors[0], (str, dict)):\n        import matplotlib\n        norm = matplotlib.colors.Normalize()\n        colors = norm(colors)\n        if cmap is None: cmap = rcParams['image.cmap']\n        cmap = matplotlib.cm.get_cmap(cmap)\n        colors = [cmap(c) for c in colors]\n        colorbar = True if colorbar is None else colorbar\n    else:\n        colorbar = False\n    if len(colors) < len(node_labels):\n        print(node_labels, colors)\n        raise ValueError(\n            '`color` list need to be at least as long as `goups`/`node_labels` list.')\n    # count number of connected components\n    n_components, labels = scipy.sparse.csgraph.connected_components(adjacency_solid)\n    if n_components > 1 and not single_component:\n        logg.info(\n            'Graph has more than a single connected component. '\n            'To restrict to this component, pass `single_component=True`.')\n    if n_components > 1 and single_component:\n        component_sizes = np.bincount(labels)\n        largest_component = np.where(\n            component_sizes == component_sizes.max())[0][0]\n        adjacency_solid = adjacency_solid.tocsr()[labels == largest_component, :]\n        adjacency_solid = adjacency_solid.tocsc()[:, labels == largest_component]\n        colors = np.array(colors)[labels == largest_component]\n        node_labels = np.array(node_labels)[labels == largest_component]\n        logg.info(\n            'Restricting graph to largest connected component by dropping categories\\n'\n            '{}'.format(\n                adata.obs[groups_key].cat.categories[labels != largest_component].tolist()))\n        nx_g_solid = nx.Graph(adjacency_solid)\n        if dashed_edges is not None:\n            raise ValueError('`single_component` only if `dashed_edges` is `None`.')\n    # node positions from adjacency_solid\n    if pos is None:\n        if layout is None:\n            layout = 'fr'\n        if layout == 'fa':\n            try:\n                from fa2 import ForceAtlas2\n            except:\n                logg.warn('Package \\'fa2\\' is not installed, falling back to layout \\'fr\\'.'\n                          'To use the faster and better ForceAtlas2 layout, '\n                          'install package \\'fa2\\' (`pip install fa2`).')\n                layout = 'fr'\n        if layout == 'fa':\n            np.random.seed(random_state)\n            if init_pos is None:\n                init_coords = np.random.random((adjacency_solid.shape[0], 2))\n            else:\n                init_coords = init_pos.copy()\n            forceatlas2 = ForceAtlas2(\n                # Behavior alternatives\n                outboundAttractionDistribution=False,  # Dissuade hubs\n                linLogMode=False,  # NOT IMPLEMENTED\n                adjustSizes=False,  # Prevent overlap (NOT IMPLEMENTED)\n                edgeWeightInfluence=1.0,\n                # Performance\n                jitterTolerance=1.0,  # Tolerance\n                barnesHutOptimize=True,\n                barnesHutTheta=1.2,\n                multiThreaded=False,  # NOT IMPLEMENTED\n                # Tuning\n                scalingRatio=2.0,\n                strongGravityMode=False,\n                gravity=1.0,\n                # Log\n                verbose=False)\n            if 'maxiter' in layout_kwds:\n                iterations = layout_kwds['maxiter']\n            elif 'iterations' in layout_kwds:\n                iterations = layout_kwds['iterations']\n            else:\n                iterations = 500\n            pos_list = forceatlas2.forceatlas2(\n                adjacency_solid, pos=init_coords, iterations=iterations)\n            pos = {n: [p[0], -p[1]] for n, p in enumerate(pos_list)}\n        elif layout == 'eq_tree':\n            nx_g_tree = nx_g_solid\n            if solid_edges == 'connectivities':\n                adj_tree = adata.uns['paga']['confidence_tree']\n                nx_g_tree = nx.Graph(adj_tree)\n            pos = utils.hierarchy_pos(nx_g_tree, root)\n            if len(pos) < adjacency_solid.shape[0]:\n                raise ValueError('This is a forest and not a single tree. '\n                                 'Try another `layout`, e.g., {\\'fr\\'}.')\n        else:\n            # igraph layouts\n            from ... import utils as sc_utils\n            g = sc_utils.get_igraph_from_adjacency(adjacency_solid)\n            if 'rt' in layout:\n                g_tree = g\n                if solid_edges == 'connectivities':\n                    adj_tree = adata.uns['paga']['confidence_tree']\n                    g_tree = sc_utils.get_igraph_from_adjacency(adj_tree)\n                pos_list = g_tree.layout(\n                    layout, root=root if isinstance(root, list) else [root]).coords\n            elif layout == 'circle':\n                pos_list = g.layout(layout).coords\n            else:\n                # I don't know why this is necessary\n                np.random.seed(random_state)\n                if init_pos is None:\n                    init_coords = np.random.random((adjacency_solid.shape[0], 2)).tolist()\n                else:\n                    init_pos = init_pos.copy()\n                    # this is a super-weird hack that is necessary as igraphs layout function\n                    # seems to do some strange stuff, here\n                    init_pos[:, 1] *= -1\n                    init_coords = init_pos.tolist()\n                pos_list = g.layout(\n                    layout, seed=init_coords,\n                    weights='weight', **layout_kwds).coords\n            pos = {n: [p[0], -p[1]] for n, p in enumerate(pos_list)}\n        pos_array = np.array([pos[n] for count, n in enumerate(nx_g_solid)])\n    else:\n        if isinstance(pos, str):\n            if not pos.endswith('.gdf'):\n                raise ValueError('Currently only supporting reading positions from .gdf files.'\n                                 'Consider generating them using, for instance, Gephi.')\n            s = ''  # read the node definition from the file\n            with open(pos) as f:\n                f.readline()\n                for line in f:\n                    if line.startswith('edgedef>'):\n                        break\n                    s += line\n            from io import StringIO\n            df = pd.read_csv(StringIO(s), header=-1)\n            pos = df[[4, 5]].values\n        pos_array = pos\n        # convert to dictionary\n        pos = {n: [p[0], p[1]] for n, p in enumerate(pos)}\n    if len(pos) == 1: pos[0] = (0.5, 0.5)\n    # edge widths\n    base_edge_width = edge_width_scale * 5 * rcParams['lines.linewidth']\n    # draw dashed edges\n    if dashed_edges is not None:\n        widths = [x[-1]['weight'] for x in nx_g_dashed.edges(data=True)]\n        widths = base_edge_width * np.array(widths)\n        if max_edge_width is not None:\n            widths = np.clip(widths, None, max_edge_width)\n        nx.draw_networkx_edges(nx_g_dashed, pos, ax=ax, width=widths, edge_color='grey',\n                               style='dashed', alpha=0.5)\n    # draw solid edges\n    if transitions is None:\n        widths = [x[-1]['weight'] for x in nx_g_solid.edges(data=True)]\n        widths = base_edge_width * np.array(widths)\n        if min_edge_width is not None or max_edge_width is not None:\n            widths = np.clip(widths, min_edge_width, max_edge_width)\n        nx.draw_networkx_edges(nx_g_solid, pos, ax=ax, width=widths, edge_color='black')\n    # draw directed edges\n    else:\n        adjacency_transitions = adata.uns['paga'][transitions].copy()\n        if threshold_arrows is None:\n            threshold_arrows = 0.005\n        adjacency_transitions.data[adjacency_transitions.data < threshold_arrows] = 0\n        adjacency_transitions.eliminate_zeros()\n        g_dir = nx.DiGraph(adjacency_transitions.T)\n        widths = [x[-1]['weight'] for x in g_dir.edges(data=True)]\n        widths = 100 * base_edge_width * np.array(widths)\n        if min_edge_width is not None or max_edge_width is not None:\n            widths = np.clip(widths, min_edge_width, max_edge_width)\n        nx.draw_networkx_edges(g_dir, pos, ax=ax, width=widths, edge_color='black')\n    if export_to_gexf:\n        if isinstance(colors[0], tuple):\n            from matplotlib.colors import rgb2hex\n            colors = [rgb2hex(c) for c in colors]\n        for count, n in enumerate(nx_g_solid.nodes()):\n            nx_g_solid.node[count]['label'] = str(node_labels[count])\n            nx_g_solid.node[count]['color'] = str(colors[count])\n            nx_g_solid.node[count]['viz'] = {\n                'position': {'x': 1000*pos[count][0],\n                             'y': 1000*pos[count][1],\n                             'z': 0}}\n        filename = settings.writedir + 'paga_graph.gexf'\n        logg.msg('exporting to {}'.format(filename), v=1)\n        if settings.writedir != '' and not os.path.exists(settings.writedir):\n            os.makedirs(settings.writedir)\n        nx.write_gexf(nx_g_solid, settings.writedir + 'paga_graph.gexf')\n    # deal with empty graph\n    # ax.plot(pos_array[:, 0], pos_array[:, 1], '.', c='white')\n    # draw the nodes (pie charts)\n    trans = ax.transData.transform\n    bbox = ax.get_position().get_points()\n    ax_x_min = bbox[0, 0]\n    ax_x_max = bbox[1, 0]\n    ax_y_min = bbox[0, 1]\n    ax_y_max = bbox[1, 1]\n    ax_len_x = ax_x_max - ax_x_min\n    ax_len_y = ax_y_max - ax_y_min\n    # print([ax_x_min, ax_x_max, ax_y_min, ax_y_max])\n    # print([ax_len_x, ax_len_y])\n    trans2 = ax.transAxes.inverted().transform\n    ax.set_frame_on(frameon)\n    ax.set_xticks([])\n    ax.set_yticks([])\n    if (groups_key is not None and groups_key + '_sizes' in adata.uns):\n        groups_sizes = adata.uns[groups_key + '_sizes']\n    else:\n        groups_sizes = np.ones(len(node_labels))\n    base_scale_scatter = 2000\n    base_pie_size = (base_scale_scatter / (np.sqrt(adjacency_solid.shape[0]) + 10)\n                     * node_size_scale)\n    median_group_size = np.median(groups_sizes)\n    groups_sizes = base_pie_size * np.power(\n        groups_sizes / median_group_size, node_size_power)\n    # usual scatter plot\n    if is_color_like(colors[0]):\n        scatter = ax.scatter(pos_array[:, 0], pos_array[:, 1],\n                             c=colors, edgecolors='face', s=groups_sizes)\n        for count, group in enumerate(node_labels):\n            ax.text(pos_array[count, 0], pos_array[count, 1], group,\n                    verticalalignment='center',\n                     horizontalalignment='center', size=fontsize, **text_kwds)\n    # else pie chart plot\n    else:\n        force_labels_to_front = True  # TODO: solve this differently!\n        for count, n in enumerate(nx_g_solid.nodes()):\n            pie_size = groups_sizes[count] / base_scale_scatter\n            xx, yy = trans(pos[n])     # data coordinates\n            xa, ya = trans2((xx, yy))  # axis coordinates\n            xa = ax_x_min + (xa - pie_size/2) * ax_len_x\n            ya = ax_y_min + (ya - pie_size/2) * ax_len_y\n            # clip, the fruchterman layout sometimes places below figure\n            if ya < 0: ya = 0\n            if xa < 0: xa = 0\n            a = ax.axes([xa, ya, pie_size * ax_len_x, pie_size * ax_len_y])\n            if not isinstance(colors[count], dict):\n                raise ValueError('{} is neither a dict of valid matplotlib colors '\n                                 'nor a valid matplotlib color.'.format(colors[count]))\n            color_single = colors[count].keys()\n            fracs = [colors[count][c] for c in color_single]\n            if sum(fracs) < 1:\n                color_single = list(color_single)\n                color_single.append('grey')\n                fracs.append(1-sum(fracs))\n            a.pie(fracs, colors=color_single)\n            if not force_labels_to_front and node_labels is not None:\n                a.text(0.5, 0.5, node_labels[count],\n                       verticalalignment='center',\n                       horizontalalignment='center',\n                       transform=a.transAxes,\n                       size=fontsize)\n        # TODO: this is a terrible hack, but if we use the solution above (`not\n        # force_labels_to_front`), labels get hidden behind pies\n        if force_labels_to_front and node_labels is not None:\n            for count, n in enumerate(nx_g_solid.nodes()):\n                pie_size = groups_sizes[count] / base_scale_scatter\n                # all copy and paste from above\n                xx, yy = trans(pos[n])     # data coordinates\n                xa, ya = trans2((xx, yy))  # axis coordinates\n                # make sure a new axis is created\n                xa = ax_x_min + (xa - pie_size/2.0000001) * ax_len_x\n                ya = ax_y_min + (ya - pie_size/2.0000001) * ax_len_y\n                # clip, the fruchterman layout sometimes places below figure\n                if ya < 0: ya = 0\n                if xa < 0: xa = 0\n                a = pl.axes([xa, ya, pie_size * ax_len_x, pie_size * ax_len_y])\n                a.set_frame_on(False)\n                a.set_xticks([])\n                a.set_yticks([])\n                a.text(0.5, 0.5, node_labels[count],\n                       verticalalignment='center',\n                       horizontalalignment='center',\n                       transform=a.transAxes, size=fontsize)\n    if title is not None: ax.set_title(title)\n    cb = None\n    if colorbar:\n        cax = pl.axes([0.95, 0.1, 0.03, 0.8]) if cax is None else cax\n        cb = matplotlib.colorbar.ColorbarBase(cax, cmap=cmap,\n                                              norm=norm, **cb_kwds)\n    return pos_array, cb", "idx": 415}
{"project": "Scanpy", "commit_id": "408_scanpy_1.9.0__qc.py_calculate_qc_metrics.py", "target": 0, "func": "def calculate_qc_metrics(\n    adata: AnnData,\n    *,\n    expr_type: str = \"counts\",\n    var_type: str = \"genes\",\n    qc_vars: Collection[str] = (),\n    percent_top: Optional[Collection[int]] = (50, 100, 200, 500),\n    layer: Optional[str] = None,\n    use_raw: bool = False,\n    inplace: bool = False,\n    log1p: bool = True,\n    parallel: Optional[bool] = None,\n) -> Optional[Tuple[pd.DataFrame, pd.DataFrame]]:\n    \"\"\"\\\n    Calculate quality control metrics.\n\n    Calculates a number of qc metrics for an AnnData object, see section\n    `Returns` for specifics. Largely based on `calculateQCMetrics` from scater\n    [McCarthy17]_. Currently is most efficient on a sparse CSR or dense matrix.\n\n    Note that this method can take a while to compile on the first call. That\n    result is then cached to disk to be used later.\n\n    Parameters\n    ----------\n    {doc_adata_basic}\n    {doc_qc_metric_naming}\n    {doc_obs_qc_args}\n    {doc_expr_reps}\n    inplace\n        Whether to place calculated metrics in `adata`'s `.obs` and `.var`.\n    log1p\n        Set to `False` to skip computing `log1p` transformed annotations.\n\n    Returns\n    -------\n    Depending on `inplace` returns calculated metrics\n    (as :class:`~pandas.DataFrame`) or updates `adata`'s `obs` and `var`.\n\n    {doc_obs_qc_returns}\n\n    {doc_var_qc_returns}\n\n    Example\n    -------\n    Calculate qc metrics for visualization.\n\n    .. plot::\n        :context: close-figs\n\n        import scanpy as sc\n        import seaborn as sns\n\n        pbmc = sc.datasets.pbmc3k()\n        pbmc.var[\"mito\"] = pbmc.var_names.str.startswith(\"MT-\")\n        sc.pp.calculate_qc_metrics(pbmc, qc_vars=[\"mito\"], inplace=True)\n        sns.jointplot(\n            data=pbmc.obs,\n            x=\"log1p_total_counts\",\n            y=\"log1p_n_genes_by_counts\",\n            kind=\"hex\",\n        )\n\n    .. plot::\n        :context: close-figs\n\n        sns.histplot(pbmc.obs[\"pct_counts_mito\"])\n    \"\"\"\n    if parallel is not None:\n        warn(\n            \"Argument `parallel` is deprecated, and currently has no effect.\",\n            FutureWarning,\n        )\n    # Pass X so I only have to do it once\n    X = _choose_mtx_rep(adata, use_raw, layer)\n    if isspmatrix_coo(X):\n        X = csr_matrix(X)  # COO not subscriptable\n    if issparse(X):\n        X.eliminate_zeros()\n\n    obs_metrics = describe_obs(\n        adata,\n        expr_type=expr_type,\n        var_type=var_type,\n        qc_vars=qc_vars,\n        percent_top=percent_top,\n        inplace=inplace,\n        X=X,\n        log1p=log1p,\n    )\n    var_metrics = describe_var(\n        adata,\n        expr_type=expr_type,\n        var_type=var_type,\n        inplace=inplace,\n        X=X,\n        log1p=log1p,\n    )\n\n    if not inplace:\n        return obs_metrics, var_metrics", "idx": 416}
{"project": "Scanpy", "commit_id": "18_scanpy_1.9.0_param_police.py_show_param_warnings.py", "target": 0, "func": "def show_param_warnings(app, exception):\n    import inspect\n\n    for (fname, fun), params in param_warnings.items():\n        _, line = inspect.getsourcelines(fun)\n        file_name = inspect.getsourcefile(fun)\n        params_str = '\\n'.join(f'\\t{n}: {t}' for n, t in params)\n        warnings.warn_explicit(\n            f'\\nParameters in `{fname}` have types in docstring.\\n'\n            f'Replace them with type annotations.\\n{params_str}',\n            UserWarning,\n            file_name,\n            line,\n        )\n    if param_warnings:\n        raise RuntimeError('Encountered text parameter type. Use annotations.')", "idx": 418}
{"project": "Scanpy", "commit_id": "812_scanpy_1.4_scanpy_tools__rank_genes_groups.py_rank_genes_groups.py", "target": 1, "func": "def rank_genes_groups(\n        adata,\n        groupby,\n        use_raw=True,\n        groups: Union[str, Iterable[Union[str, int]]] = 'all',\n        reference='rest',\n        n_genes=100,\n        rankby_abs=False,\n        key_added=None,\n        copy=False,\n        method='t-test_overestim_var',\n        corr_method='benjamini-hochberg',\n        **kwds\n):\n    \"\"\"Rank genes for characterizing groups.\n    Parameters\n    ----------\n    adata : :class:`~anndata.AnnData`\n        Annotated data matrix.\n    groupby : `str`\n        The key of the observations grouping to consider.\n    use_raw : `bool`, optional (default: `True`)\n        Use `raw` attribute of `adata` if present.\n    groups\n        Subset of groups, e.g. `['g1', 'g2', 'g3']`, to which comparison shall\n        be restricted, or `'all'` (default), for all groups.\n    reference : `str`, optional (default: `'rest'`)\n        If `'rest'`, compare each group to the union of the rest of the group.  If\n        a group identifier, compare with respect to this group.\n    n_genes : `int`, optional (default: 100)\n        The number of genes that appear in the returned tables.\n    method : `{'logreg', 't-test', 'wilcoxon', 't-test_overestim_var'}`, optional (default: 't-test_overestim_var')\n        If 't-test', uses t-test, if 'wilcoxon', uses Wilcoxon-Rank-Sum. If\n        't-test_overestim_var', overestimates variance of each group. If\n        'logreg' uses logistic regression, see [Ntranos18]_, `here\n        <https://github.com/theislab/scanpy/issues/95>`__ and `here\n        <http://www.nxn.se/valent/2018/3/5/actionable-scrna-seq-clusters>`__, for\n        why this is meaningful.\n    corr_method : `{'benjamini-hochberg', 'bonferroni'}`, optional (default: 'benjamini-hochberg')\n        p-value correction method. Used only for 't-test', 't-test_overestim_var',\n        and 'wilcoxon' methods.\n    rankby_abs : `bool`, optional (default: `False`)\n        Rank genes by the absolute value of the score, not by the\n        score. The returned scores are never the absolute values.\n    **kwds : keyword parameters\n        Are passed to test methods. Currently this affects only parameters that\n        are passed to `sklearn.linear_model.LogisticRegression\n        <http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html>`__.\n        For instance, you can pass `penalty='l1'` to try to come up with a\n        minimal set of genes that are good predictors (sparse solution meaning\n        few non-zero fitted coefficients).\n    Returns\n    -------\n    names : structured `np.ndarray` (`.uns['rank_genes_groups']`)\n        Structured array to be indexed by group id storing the gene\n        names. Ordered according to scores.\n    scores : structured `np.ndarray` (`.uns['rank_genes_groups']`)\n        Structured array to be indexed by group id storing the z-score\n        underlying the computation of a p-value for each gene for each\n        group. Ordered according to scores.\n    logfoldchanges : structured `np.ndarray` (`.uns['rank_genes_groups']`)\n        Structured array to be indexed by group id storing the log2\n        fold change for each gene for each group. Ordered according to\n        scores. Only provided if method is 't-test' like.\n    pvals : structured `np.ndarray` (`.uns['rank_genes_groups']`)\n        p-values.\n    pvals_adj : structured `np.ndarray` (`.uns['rank_genes_groups']`)\n        Corrected p-values.\n    Notes\n    -----\n    There are slight inconsistencies depending on whether sparse\n    or dense data are passed. See `here <https://github.com/theislab/scanpy/blob/master/scanpy/tests/test_rank_genes_groups.py>`__.\n    \"\"\"\n    if 'only_positive' in kwds:\n        rankby_abs = not kwds.pop('only_positive')  # backwards compat\n\n    logg.info('ranking genes', r=True)\n    avail_methods = {'t-test', 't-test_overestim_var', 'wilcoxon', 'logreg'}\n    if method not in avail_methods:\n        raise ValueError('Method must be one of {}.'.format(avail_methods))\n    avail_corr = {'benjamini-hochberg', 'bonferroni'}\n    if corr_method not in avail_corr:\n        raise ValueError('Correction method must be one of {}.'.format(avail_corr))\n\n    adata = adata.copy() if copy else adata\n    utils.sanitize_anndata(adata)\n    # for clarity, rename variable\n    groups_order = groups if isinstance(groups, str) else list(groups)\n    if isinstance(groups_order, list) and isinstance(groups_order[0], int):\n        groups_order = [str(n) for n in groups_order]\n    if reference != 'rest' and reference not in set(groups_order):\n        groups_order += [reference]\n    if (reference != 'rest'\n            and reference not in set(adata.obs[groupby].cat.categories)):\n        raise ValueError('reference = {} needs to be one of groupby = {}.'\n                         .format(reference,\n                                 adata.obs[groupby].cat.categories.tolist()))\n\n    groups_order, groups_masks = utils.select_groups(\n        adata, groups_order, groupby)\n    if key_added is None:\n        key_added = 'rank_genes_groups'\n    adata.uns[key_added] = {}\n    adata.uns[key_added]['params'] = {\n        'groupby': groupby,\n        'reference': reference,\n        'method': method,\n        'use_raw': use_raw,\n        'corr_method': corr_method,\n    }\n    # adata_comp mocks an AnnData object if use_raw is True\n    # otherwise it's just the AnnData object\n    adata_comp = adata\n    if adata.raw is not None and use_raw:\n        adata_comp = adata.raw\n    X = adata_comp.X\n    # for clarity, rename variable\n    n_genes_user = n_genes\n    # make sure indices are not OoB in case there are less genes than n_genes\n    if n_genes_user > X.shape[1]:\n        n_genes_user = X.shape[1]\n    # in the following, n_genes is simply another name for the total number of genes\n    n_genes = X.shape[1]\n\n    n_groups = groups_masks.shape[0]\n    ns = np.zeros(n_groups, dtype=int)\n    for imask, mask in enumerate(groups_masks):\n        ns[imask] = np.where(mask)[0].size\n    logg.msg('consider \\'{}\\' groups:'.format(groupby), groups_order, v=4)\n    logg.msg('with sizes:', ns, v=4)\n    if reference != 'rest':\n        ireference = np.where(groups_order == reference)[0][0]\n    reference_indices = np.arange(adata_comp.n_vars, dtype=int)\n    rankings_gene_scores = []\n    rankings_gene_names = []\n    rankings_gene_logfoldchanges = []\n    rankings_gene_pvals = []\n    rankings_gene_pvals_adj = []\n\n    if method in {'t-test', 't-test_overestim_var'}:\n        from scipy import stats\n        from statsmodels.stats.multitest import multipletests\n        # loop over all masks and compute means, variances and sample numbers\n        means = np.zeros((n_groups, n_genes))\n        vars = np.zeros((n_groups, n_genes))\n        for imask, mask in enumerate(groups_masks):\n            means[imask], vars[imask] = _get_mean_var(X[mask])\n        # test each either against the union of all other groups or against a\n        # specific group\n        for igroup in range(n_groups):\n            if reference == 'rest':\n                mask_rest = ~groups_masks[igroup]\n            else:\n                if igroup == ireference:\n                    continue\n                else:\n                    mask_rest = groups_masks[ireference]\n            mean_rest, var_rest = _get_mean_var(X[mask_rest])\n            ns_group = ns[igroup]  # number of observations in group\n            if method == 't-test':\n                ns_rest = np.where(mask_rest)[0].size\n            elif method == 't-test_overestim_var':\n                ns_rest = ns[igroup]  # hack for overestimating the variance for small groups\n            else:\n                raise ValueError('Method does not exist.')\n\n            denominator = np.sqrt(vars[igroup] / ns_group + var_rest / ns_rest)\n            denominator[np.flatnonzero(denominator == 0)] = np.nan\n            scores = (means[igroup] - mean_rest) / denominator  # Welch t-test\n            mean_rest[mean_rest == 0] = 1e-9  # set 0s to small value\n            foldchanges = (means[igroup] + 1e-9) / mean_rest\n            scores[np.isnan(scores)] = 0\n            # Get p-values\n            denominator_dof = (np.square(vars[igroup]) / (np.square(ns_group) * (ns_group - 1))) + (\n                (np.square(var_rest) / (np.square(ns_rest) * (ns_rest - 1))))\n            denominator_dof[np.flatnonzero(denominator_dof == 0)] = np.nan\n            dof = np.square(\n                vars[igroup] / ns_group + var_rest / ns_rest) / denominator_dof  # dof calculation for Welch t-test\n            dof[np.isnan(dof)] = 0\n            pvals = stats.t.sf(abs(scores), dof) * 2  # *2 because of two-tailed t-test\n            if corr_method == 'benjamini-hochberg':\n                pvals[np.isnan(pvals)] = 1  # set Nan values to 1 to properly convert using Benhjamini Hochberg\n                _, pvals_adj, _, _ = multipletests(pvals, alpha=0.05, method='fdr_bh')\n            elif corr_method == 'bonferroni':\n                pvals_adj = np.minimum(pvals * n_genes, 1.0)\n            scores_sort = np.abs(scores) if rankby_abs else scores\n            partition = np.argpartition(scores_sort, -n_genes_user)[-n_genes_user:]\n            partial_indices = np.argsort(scores_sort[partition])[::-1]\n            global_indices = reference_indices[partition][partial_indices]\n            rankings_gene_scores.append(scores[global_indices])\n            rankings_gene_logfoldchanges.append(np.log2(np.abs(foldchanges[global_indices])))\n            rankings_gene_names.append(adata_comp.var_names[global_indices])\n            rankings_gene_pvals.append(pvals[global_indices])\n            rankings_gene_pvals_adj.append(pvals_adj[global_indices])\n\n    elif method == 'logreg':\n        # if reference is not set, then the groups listed will be compared to the rest\n        # if reference is set, then the groups listed will be compared only to the other groups listed\n        from sklearn.linear_model import LogisticRegression\n        reference = groups_order[0]\n        if len(groups) == 1:\n            raise Exception('Cannot perform logistic regression on a single cluster.')\n        adata_copy = adata[adata.obs[groupby].isin(groups_order)]\n        adata_comp = adata_copy\n        if adata.raw is not None and use_raw:\n            adata_comp = adata_copy.raw\n        X = adata_comp.X\n        clf = LogisticRegression(**kwds)\n        clf.fit(X, adata_copy.obs[groupby].cat.codes)\n        scores_all = clf.coef_\n        for igroup, group in enumerate(groups_order):\n            if len(groups_order) <= 2:  # binary logistic regression\n                scores = scores_all[0]\n            else:\n                scores = scores_all[igroup]\n            partition = np.argpartition(scores, -n_genes_user)[-n_genes_user:]\n            partial_indices = np.argsort(scores[partition])[::-1]\n            global_indices = reference_indices[partition][partial_indices]\n            rankings_gene_scores.append(scores[global_indices])\n            rankings_gene_names.append(adata_comp.var_names[global_indices])\n            if len(groups_order) <= 2:\n                break\n    elif method == 'wilcoxon':\n        from scipy import stats\n        from statsmodels.stats.multitest import multipletests\n        CONST_MAX_SIZE = 10000000\n        means = np.zeros((n_groups, n_genes))\n        vars = np.zeros((n_groups, n_genes))\n        # initialize space for z-scores\n        scores = np.zeros(n_genes)\n        # First loop: Loop over all genes\n        if reference != 'rest':\n            for imask, mask in enumerate(groups_masks):\n                means[imask], vars[imask] = _get_mean_var(X[mask])  # for fold-change\n                if imask == ireference:\n                    continue\n                else:\n                    mask_rest = groups_masks[ireference]\n                ns_rest = np.where(mask_rest)[0].size\n                mean_rest, var_rest = _get_mean_var(X[mask_rest])  # for fold-change\n                if ns_rest <= 25 or ns[imask] <= 25:\n                    logg.hint('Few observations in a group for '\n                              'normal approximation (<=25). Lower test accuracy.')\n                n_active = ns[imask]\n                m_active = ns_rest\n                # Now calculate gene expression ranking in chunkes:\n                chunk = []\n                # Calculate chunk frames\n                n_genes_max_chunk = floor(CONST_MAX_SIZE / (n_active + m_active))\n                if n_genes_max_chunk < n_genes - 1:\n                    chunk_index = n_genes_max_chunk\n                    while chunk_index < n_genes - 1:\n                        chunk.append(chunk_index)\n                        chunk_index = chunk_index + n_genes_max_chunk\n                    chunk.append(n_genes - 1)\n                else:\n                    chunk.append(n_genes - 1)\n\n                left = 0\n                # Calculate rank sums for each chunk for the current mask\n                for chunk_index, right in enumerate(chunk):\n                    # Check if issparse is true: AnnData objects are currently sparse.csr or ndarray.\n                    if issparse(X):\n                        df1 = pd.DataFrame(data=X[mask, left:right].todense())\n                        df2 = pd.DataFrame(data=X[mask_rest, left:right].todense(),\n                                           index=np.arange(start=n_active, stop=n_active + m_active))\n                    else:\n                        df1 = pd.DataFrame(data=X[mask, left:right])\n                        df2 = pd.DataFrame(data=X[mask_rest, left:right],\n                                           index=np.arange(start=n_active, stop=n_active + m_active))\n                    df1 = df1.append(df2)\n                    ranks = df1.rank()\n                    # sum up adjusted_ranks to calculate W_m,n\n                    scores[left:right] = np.sum(ranks.loc[0:n_active, :])\n                    left = right + 1\n\n                scores = (scores - (n_active * (n_active + m_active + 1) / 2)) / sqrt(\n                    (n_active * m_active * (n_active + m_active + 1) / 12))\n                scores[np.isnan(scores)] = 0\n                pvals = 2 * stats.distributions.norm.sf(np.abs(scores))\n                if corr_method == 'benjamini-hochberg':\n                    pvals[np.isnan(pvals)] = 1  # set Nan values to 1 to properly convert using Benhjamini Hochberg\n                    _, pvals_adj, _, _ = multipletests(pvals, alpha=0.05, method='fdr_bh')\n                elif corr_method == 'bonferroni':\n                    pvals_adj = np.minimum(pvals * n_genes, 1.0)\n                mean_rest[mean_rest == 0] = 1e-9  # set 0s to small value\n                foldchanges = (means[imask] + 1e-9) / mean_rest\n                scores_sort = np.abs(scores) if rankby_abs else scores\n                partition = np.argpartition(scores_sort, -n_genes_user)[-n_genes_user:]\n                partial_indices = np.argsort(scores_sort[partition])[::-1]\n                global_indices = reference_indices[partition][partial_indices]\n                rankings_gene_scores.append(scores[global_indices])\n                rankings_gene_names.append(adata_comp.var_names[global_indices])\n                rankings_gene_logfoldchanges.append(np.log2(np.abs(foldchanges[global_indices])))\n                rankings_gene_pvals.append(pvals[global_indices])\n                rankings_gene_pvals_adj.append(pvals_adj[global_indices])\n        # If no reference group exists, ranking needs only to be done once (full mask)\n        else:\n            scores = np.zeros((n_groups, n_genes))\n            chunk = []\n            n_cells = X.shape[0]\n            n_genes_max_chunk = floor(CONST_MAX_SIZE / n_cells)\n            if n_genes_max_chunk < n_genes - 1:\n                chunk_index = n_genes_max_chunk\n                while chunk_index < n_genes - 1:\n                    chunk.append(chunk_index)\n                    chunk_index = chunk_index + n_genes_max_chunk\n                chunk.append(n_genes - 1)\n            else:\n                chunk.append(n_genes - 1)\n            left = 0\n            for chunk_index, right in enumerate(chunk):\n                # Check if issparse is true\n                if issparse(X):\n                    df1 = pd.DataFrame(data=X[:, left:right].todense())\n                else:\n                    df1 = pd.DataFrame(data=X[:, left:right])\n                ranks = df1.rank()\n                # sum up adjusted_ranks to calculate W_m,n\n                for imask, mask in enumerate(groups_masks):\n                    scores[imask, left:right] = np.sum(ranks.loc[mask, :])\n                left = right + 1\n\n            for imask, mask in enumerate(groups_masks):\n                means[imask], vars[imask] = _get_mean_var(X[mask])  # for fold-change\n                mask_rest = ~groups_masks[imask]\n                mean_rest, var_rest = _get_mean_var(X[mask_rest])  # for fold-change\n                scores[imask, :] = (scores[imask, :] - (ns[imask] * (n_cells + 1) / 2)) / sqrt(\n                    (ns[imask] * (n_cells - ns[imask]) * (n_cells + 1) / 12))\n                scores[np.isnan(scores)] = 0\n                pvals = 2 * stats.distributions.norm.sf(np.abs(scores[imask, :]))\n                if corr_method == 'benjamini-hochberg':\n                    pvals[np.isnan(pvals)] = 1  # set Nan values to 1 to properly convert using Benhjamini Hochberg\n                    _, pvals_adj, _, _ = multipletests(pvals, alpha=0.05, method='fdr_bh')\n                elif corr_method == 'bonferroni':\n                    pvals_adj = np.minimum(pvals * n_genes, 1.0)\n                mean_rest[mean_rest == 0] = 1e-9  # set 0s to small value\n                foldchanges = (means[imask] + 1e-9) / mean_rest\n                scores_sort = np.abs(scores) if rankby_abs else scores\n                partition = np.argpartition(scores_sort[imask, :], -n_genes_user)[-n_genes_user:]\n                partial_indices = np.argsort(scores_sort[imask, partition])[::-1]\n                global_indices = reference_indices[partition][partial_indices]\n                rankings_gene_scores.append(scores[imask, global_indices])\n                rankings_gene_names.append(adata_comp.var_names[global_indices])\n                rankings_gene_logfoldchanges.append(np.log2(np.abs(foldchanges[global_indices])))\n                rankings_gene_pvals.append(pvals[global_indices])\n                rankings_gene_pvals_adj.append(pvals_adj[global_indices])\n    groups_order_save = [str(g) for g in groups_order]\n    if (reference != 'rest' and method != 'logreg') or (method == 'logreg' and len(groups) == 2):\n        groups_order_save = [g for g in groups_order if g != reference]\n    adata.uns[key_added]['scores'] = np.rec.fromarrays(\n        [n for n in rankings_gene_scores],\n        dtype=[(rn, 'float32') for rn in groups_order_save])\n    adata.uns[key_added]['names'] = np.rec.fromarrays(\n        [n for n in rankings_gene_names],\n        dtype=[(rn, 'U50') for rn in groups_order_save])\n    if method in {'t-test', 't-test_overestim_var', 'wilcoxon'}:\n        adata.uns[key_added]['logfoldchanges'] = np.rec.fromarrays(\n            [n for n in rankings_gene_logfoldchanges],\n            dtype=[(rn, 'float32') for rn in groups_order_save])\n        adata.uns[key_added]['pvals'] = np.rec.fromarrays(\n            [n for n in rankings_gene_pvals],\n            dtype=[(rn, 'float64') for rn in groups_order_save])\n        adata.uns[key_added]['pvals_adj'] = np.rec.fromarrays(\n            [n for n in rankings_gene_pvals_adj],\n            dtype=[(rn, 'float64') for rn in groups_order_save])\n\n    logg.info('    finished', time=True, end=' ' if _settings_verbosity_greater_or_equal_than(3) else '\\n')\n    logg.hint(\n        'added to `.uns[\\'{}\\']`\\n'\n        '    \\'names\\', sorted np.recarray to be indexed by group ids\\n'\n        '    \\'scores\\', sorted np.recarray to be indexed by group ids\\n'\n        .format(key_added)\n        + ('    \\'logfoldchanges\\', sorted np.recarray to be indexed by group ids\\n'\n           '    \\'pvals\\', sorted np.recarray to be indexed by group ids\\n'\n           '    \\'pvals_adj\\', sorted np.recarray to be indexed by group ids'\n           if method in {'t-test', 't-test_overestim_var', 'wilcoxon'} else ''))\n    return adata if copy else None", "idx": 429}
{"project": "Scanpy", "commit_id": "22_scanpy_0.0_scanpy___init__.py_run_args.py", "target": 1, "func": "def run_args(toolkey, args):\n    \"\"\"\n    Run specified tool, do preprocessing and read/write outfiles.\n    Output files store the dictionary returned by the tool. File type is\n    determined by variable sett.extd allowed are 'h5' (hdf5), 'xlsx' (Excel) or\n    'csv' (comma separated value file).\n    If called twice with the same settings the existing output file is returned.\n    Parameters\n    ----------\n    toolkey : str\n        Name of the tool.\n    args : dict containing\n        exkey : str\n            String that identifies the example use key.\n    Returns\n    -------\n    dfunc : dict of type toolkey\n    dadd : dict\n         Additional dict used for plotting in a later step.\n    \"\"\"\n    if args['plotparams']:\n        if args['plotparams'][0] == 'help':\n            from sys import exit\n            exit(get_tool(toolkey).plot.__doc__)\n    writekey = sett.basekey + '_' + toolkey + sett.fsig\n    resultfile = sett.writedir + writekey + '.' + sett.extd\n    paramsfile = sett.writedir + writekey + '_params.txt'\n    if args['logfile']:\n        logfile = sett.writedir + writekey + '_log.txt'\n        sett.logfile(logfile)\n    if toolkey == 'sim':\n        if args['paramsfile'] != '':\n            params = read_params(args['paramsfile'])\n        else:\n            paramsfile_sim = 'sim/' + args['exkey'] + '_params.txt'\n            params = read_params(paramsfile_sim)\n            sett.m(0, '--> you can specify your custom params file using the option\\n'\n                      '    \"--paramsfile\" or provide parameters directly via \"--params\"')\n        if 'writedir' not in params:\n            params['writedir'] = sett.writedir + sett.basekey + '_' + toolkey\n    else:\n        ddata, exmodule = example(args['exkey'], return_module=True)\n        params = {}\n        # try to load tool parameters from dexamples\n        try:\n            did_not_find_params_in_exmodule = False\n            dexample = exmodule.dexamples[args['exkey']]\n            params = {}\n            for key in dexample.keys():\n                if toolkey in key:\n                    params = dexample[key]\n        except:\n            did_not_find_params_in_exmodule = True\n            pass\n        # if parameters have been specified in a parameter file\n        # update the current param dict with these\n        if args['paramsfile'] != '':\n            add_params = read_params(args['paramsfile'])\n            params = utils.update_params(params, add_params)\n        # same if parameters have been specified on the command line\n        if args['params']:\n            add_params = utils.get_params_from_list(args['params'])\n            params = utils.update_params(params, add_params)\n        elif did_not_find_params_in_exmodule and args['paramsfile'] != '':\n            sett.m(0, 'using default parameters, change them using \"--params\"')\n    # subsampling\n    if args['subsample'] != 1:\n        ddata = subsample(ddata, args['subsample'])\n    # previous tool\n    if args['prev'] != '':\n        prevkey = sett.basekey + '_' + args['prev'] + sett.fsig\n        dprev = read(prevkey)\n    # all tools that require a previous tool\n    elif toolkey in ['difftest', 'scdg']:\n        print('Error: need to provide a tool to option --prev')\n        print('--> presumably one for identification of subgroups')\n        from sys import exit\n        exit(0)\n\n    # simply load resultfile\n    if os.path.exists(resultfile) and not sett.recompute:\n        dtool = read(writekey)\n    # call the tool resultfile\n    else:\n        # TODO: solve this in a nicer way, also get an ordered dict for params\n        from inspect import getcallargs\n        tool = get_tool(toolkey, func=True)\n        if toolkey == 'sim':\n            dtool = tool(**params)\n            params = getcallargs(tool, **params)\n        elif args['prev'] != '':\n            dtool = tool(dprev, ddata, **params)\n            params = getcallargs(tool, dprev, ddata, **params)\n            # TODO: Would be good to name the first argument dprev_or_ddata\n            #       in difftest, but this doesn't work\n            del params['dprev']\n            del params['ddata']\n        else:\n            dtool = tool(ddata, **params)\n            params = getcallargs(tool, ddata, **params)\n            del params['ddata']\n        dtool['writekey'] = writekey\n        write(writekey, dtool)\n        # save a copy of the parameters to a file\n        utils.write_params(paramsfile, params)\n\n    # plotting and postprocessing\n    plotparams = {}\n    if args['plotparams']:\n        plotparams = utils.get_params_from_list(args['plotparams'])\n    if toolkey == 'sim':\n        plot(dtool, plotparams)\n    else:\n        # post-processing specific to example and tool\n        postprocess = args['exkey'] + '_' + toolkey\n        if postprocess in dir(exmodule) and args['subsample'] == 1:\n            dtool = getattr(exmodule, postprocess)(dtool)\n            write(writekey, dtool)\n        if args['plotkey'] != '':\n            plotwritekey = sett.exkey + '_' + args['plotkey'] + sett.fsig\n            dplot = read(plotwritekey)\n            sett.m(0, '--> using result', plotwritekey, 'for plotting')\n            plotargs = [dtool, ddata, dplot]\n        else:\n            plotargs = [dtool, ddata]\n        if args['prev'] != '':\n            plotargs.append(dprev)\n        plot(*tuple(plotargs), **plotparams)", "idx": 431}
{"project": "Scanpy", "commit_id": "647_scanpy_1.1_scanpy_tools_paga.py__compute_connectivities_v1_2.py", "target": 1, "func": "def _compute_connectivities_v1_2(self):\n    import igraph\n    ones = self._neighbors.distances.copy()\n    ones.data = np.ones(len(ones.data))\n    # should be directed if we deal with distances\n    g = utils.get_igraph_from_adjacency(ones, directed=True)\n    vc = igraph.VertexClustering(\n        g, membership=self._adata.obs[self._groups_key].cat.codes.values)\n    ns = vc.sizes()\n    es_inner_cluster = [vc.subgraph(i).ecount() for i in range(len(ns))]\n    cg = vc.cluster_graph(combine_edges='sum')\n    inter_es = utils.get_sparse_from_igraph(cg, weight_attr='weight')\n    es = np.array(es_inner_cluster) + inter_es.sum(axis=1).A1\n    inter_es = inter_es + inter_es.T  # \\epsilon_i + \\epsilon_j\n    connectivities = inter_es.copy()\n    inter_es = inter_es.tocoo()\n    for i, j, v in zip(inter_es.row, inter_es.col, inter_es.data):\n        expected_random_null = (es[i] * ns[j] + es[j] * ns[i]) / (ns[i] + ns[j] - 1)\n        if expected_random_null != 0:\n            scaled_value = v / expected_random_null\n        else:\n            scaled_value = 1\n        if scaled_value > 1:\n            scaled_value = 1\n        connectivities[i, j] = scaled_value\n    # set attributes\n    self.ns = ns\n    self.connectivities = connectivities\n    self.connectivities_tree = self._get_connectivities_tree_v1_2()\n    return inter_es.tocsr(), connectivities", "idx": 443}
{"project": "Scanpy", "commit_id": "135_scanpy_1.9.0_exporting.py__write_graph.py", "target": 0, "func": "def _write_graph(filename, n_nodes, edges):\n    nodes = [{'name': int(i), 'number': int(i)} for i in range(n_nodes)]\n    edges = [{'source': int(i), 'target': int(j), 'distance': 0} for i, j in edges]\n    out = {'nodes': nodes, 'links': edges}\n    open(filename, 'w').write(json.dumps(out, indent=4, separators=(',', ': ')))", "idx": 451}
{"project": "Scanpy", "commit_id": "447_scanpy_1.9.0___init__.py_zscore_deprecated.py", "target": 0, "func": "def zscore_deprecated(X: np.ndarray) -> np.ndarray:\n    \"\"\"\\\n    Z-score standardize each variable/gene in X.\n\n    Use `scale` instead.\n\n    Reference: Weinreb et al. (2017).\n\n    Parameters\n    ----------\n    X\n        Data matrix. Rows correspond to cells and columns to genes.\n\n    Returns\n    -------\n    Z-score standardized version of the data matrix.\n    \"\"\"\n    means = np.tile(np.mean(X, axis=0)[None, :], (X.shape[0], 1))\n    stds = np.tile(np.std(X, axis=0)[None, :], (X.shape[0], 1))\n    return (X - means) / (stds + 0.0001)", "idx": 454}
{"project": "Scanpy", "commit_id": "524_scanpy_1.9.0_test_embedding_plots.py_test_visium_circles.py", "target": 0, "func": "def test_visium_circles(image_comparer):  # standard visium data\n    save_and_compare_images = image_comparer(ROOT, FIGS, tol=15)\n    adata = sc.read_visium(HERE / '_data' / 'visium_data' / '1.0.0')\n    adata.obs = adata.obs.astype({'array_row': 'str'})\n\n    sc.pl.spatial(\n        adata,\n        color=\"array_row\",\n        groups=[\"24\", \"33\"],\n        crop_coord=(100, 400, 400, 100),\n        alpha=0.5,\n        size=1.3,\n        show=False,\n    )\n\n    save_and_compare_images('master_spatial_visium')", "idx": 468}
{"project": "Scanpy", "commit_id": "674_scanpy_1.9.0_test_preprocessing.py_test_subsample.py", "target": 0, "func": "def test_subsample():\n    adata = AnnData(np.ones((200, 10)))\n    sc.pp.subsample(adata, n_obs=40)\n    assert adata.n_obs == 40\n    sc.pp.subsample(adata, fraction=0.1)\n    assert adata.n_obs == 4", "idx": 470}
{"project": "Scanpy", "commit_id": "106_scanpy_0.0_scanpy_utils.py_select_groups.py", "target": 1, "func": "def select_groups(dgroups, groups_names_subset='all', smp='groups'):\n    \"\"\"\n    Get groups from dgroups.\n    \"\"\"\n    groups_names = dgroups[smp + '_names']\n    groups_masks = dgroups[smp + '_masks']\n    groups_ids = list(range(len(groups_names)))\n    if groups_names_subset != 'all':\n        # get list from string\n        if isinstance(groups_names_subset, str):\n            groups_names_subset = groups_names_subset.split(',')\n        # set groups_names to subset\n        groups_names = np.array(groups_names_subset)\n        groups_ids = np.where(np.in1d(dgroups[smp + '_names'], groups_names))[0]\n        if not np.any(groups_ids):\n            sett.m(0, 'specify valid groups_names for testing, one of',\n                   dgroups[smp + '_names'])\n            from sys import exit\n            exit(0)\n        groups_masks = groups_masks[groups_ids]\n    return groups_names, groups_masks", "idx": 473}
{"project": "Scanpy", "commit_id": "745_scanpy_1.9.0_test_score_genes.py_test_missing_genes.py", "target": 0, "func": "def test_missing_genes():\n    adata = _create_adata(100, 1000, p_zero=0, p_nan=0)\n    # These genes have a different length of name\n    non_extant_genes = _create_random_gene_names(n_genes=3, name_length=7)\n\n    with pytest.raises(ValueError):\n        sc.tl.score_genes(adata, non_extant_genes)", "idx": 477}
{"project": "Scanpy", "commit_id": "85_scanpy_0.0_scanpy_readwrite.py_write_dict_to_file.py", "target": 1, "func": "def write_dict_to_file(filename, d, ext='h5'):\n    \"\"\"\n    Write content of dictionary to file.\n    Parameters\n    ----------\n    filename : str\n        Filename of data file.\n    d : dict\n        Dictionary storing keys with np.ndarray-like data or scalars.\n    ext : string\n        Determines file type, allowed are 'h5' (hdf5),\n        'xlsx' (Excel) [or 'csv' (comma separated value file)].\n    \"\"\"\n    directory = os.path.dirname(filename)\n    if not os.path.exists(directory):\n        os.makedirs(directory)\n    if ext == 'h5':\n        with h5py.File(filename, 'w') as f:\n            for key, value in d.items():\n                key, value = prepare_writing(key, value)\n                try:\n                    f.create_dataset(key, data=value)\n                except Exception as e:\n                    sett.m(0,'error creating dataset for key =', key)\n                    raise e\n    elif ext == 'csv' or ext == 'txt':\n        dirname = filename.replace('.' + ext, '')\n        if not os.path.exists(dirname):\n            os.makedirs(dirname)\n        for key, value in d.items():\n            key, value = prepare_writing(key, value)\n            if len(value.shape) > 0:\n                np.savetxt(dirname + '/' + key + '.' + ext, value,\n                           fmt = '%.18e' if value.dtype.char != 'S' else '%s',\n                           delimiter=' ' if ext == 'txt' else ',')\n    elif ext == 'xlsx':\n        raise ValueError('TODO: this is broke.')\n        import pandas as pd\n        with pd.ExcelWriter(filename, engine='openpyxl') as writer:\n            for key, value in d.items():\n                pd.DataFrame(value).to_excel(writer,key)", "idx": 488}
{"project": "Scanpy", "commit_id": "613_scanpy_1.9.0_test_package_structure.py_test_metadata.py", "target": 0, "func": "def test_metadata(tmp_path, in_project_dir):\n    import flit_core.buildapi\n\n    flit_core.buildapi.prepare_metadata_for_build_wheel(tmp_path)\n\n    metadata_path = next(tmp_path.glob('*.dist-info')) / 'METADATA'\n    metadata = email.message_from_bytes(metadata_path.read_bytes())\n    assert not metadata.defects", "idx": 495}
{"project": "Scanpy", "commit_id": "562_scanpy_1.9.0_test_highly_variable_genes.py_test_seurat_v3_mean_var_output_with_batchkey.py", "target": 0, "func": "def test_seurat_v3_mean_var_output_with_batchkey():\n    pbmc = pbmc3k()\n    pbmc.var_names_make_unique()\n    n_cells = pbmc.shape[0]\n    batch = np.zeros((n_cells), dtype=int)\n    batch[1500:] = 1\n    pbmc.obs[\"batch\"] = batch\n\n    # true_mean, true_var = _get_mean_var(pbmc.X)\n    true_mean = np.mean(pbmc.X.toarray(), axis=0)\n    true_var = np.var(pbmc.X.toarray(), axis=0, dtype=np.float64, ddof=1)\n\n    result_df = sc.pp.highly_variable_genes(\n        pbmc, batch_key='batch', flavor='seurat_v3', n_top_genes=4000, inplace=False\n    )\n    np.testing.assert_allclose(true_mean, result_df['means'], rtol=2e-05, atol=2e-05)\n    np.testing.assert_allclose(true_var, result_df['variances'], rtol=2e-05, atol=2e-05)", "idx": 497}
{"project": "Scanpy", "commit_id": "815_scanpy_1.9.0__ingest.py__pca.py", "target": 0, "func": "def _pca(self, n_pcs=None):\n        X = self._adata_new.X\n        X = X.toarray() if issparse(X) else X.copy()\n        if self._pca_use_hvg:\n            X = X[:, self._adata_ref.var['highly_variable']]\n        if self._pca_centered:\n            X -= X.mean(axis=0)\n        X_pca = np.dot(X, self._pca_basis[:, :n_pcs])\n        return X_pca", "idx": 501}
{"project": "Scanpy", "commit_id": "1013_scanpy_1.4.6_scanpy_plotting__tools_scatterplots.py__get_data_points.py", "target": 1, "func": "def _get_data_points(\n    adata, basis, projection, components, img_key, library_id\n) -> Tuple[List[np.ndarray], List[Tuple[int, int]]]:\n    \"\"\"\n    Returns the data points corresponding to the selected basis, projection and/or components.\n    Because multiple components are given (eg components=['1,2', '2,3'] the\n    returned data are lists, containing each of the components. When only one component is plotted\n    the list length is 1.\n    Returns\n    -------\n    data_points\n        Each entry is a numpy array containing the data points\n    components\n        The cleaned list of components. Eg. [(0,1)] or [(0,1), (1,2)]\n        for components = [1,2] and components=['1,2', '2,3'] respectively\n    \"\"\"\n    if basis in adata.obsm.keys():\n        basis_key = basis\n    elif f\"X_{basis}\" in adata.obsm.keys():\n        basis_key = f\"X_{basis}\"\n    else:\n        raise KeyError(\n            f\"Could not find entry in `obsm` for '{basis}'.\\n\"\n            f\"Available keys are: {list(adata.obsm.keys())}.\"\n        )\n    n_dims = 2\n    if projection == '3d':\n        # check if the data has a third dimension\n        if adata.obsm[basis_key].shape[1] == 2:\n            if settings._low_resolution_warning:\n                logg.warning(\n                    'Selected projections is \"3d\" but only two dimensions '\n                    'are available. Only these two dimensions will be plotted'\n                )\n        else:\n            n_dims = 3\n    if components == 'all':\n        from itertools import combinations\n        r_value = 3 if projection == '3d' else 2\n        _components_list = np.arange(adata.obsm[basis_key].shape[1]) + 1\n        components = [\n            \",\".join(map(str, x)) for x in combinations(_components_list, r=r_value)\n        ]\n    components_list = []\n    offset = 0\n    if basis == 'diffmap':\n        offset = 1\n    if components is not None:\n        # components have different formats, either a list with integers, a string\n        # or a list of strings.\n        if isinstance(components, str):\n            # eg: components='1,2'\n            components_list.append(\n                tuple(int(x.strip()) - 1 + offset for x in components.split(','))\n            )\n        elif isinstance(components, cabc.Sequence):\n            if isinstance(components[0], int):\n                # components=[1,2]\n                components_list.append(tuple(int(x) - 1 + offset for x in components))\n            else:\n                # in this case, the components are str\n                # eg: components=['1,2'] or components=['1,2', '2,3]\n                # More than one component can be given and is stored\n                # as a new item of components_list\n                for comp in components:\n                    components_list.append(\n                        tuple(int(x.strip()) - 1 + offset for x in comp.split(','))\n                    )\n        else:\n            raise ValueError(\n                \"Given components: '{}' are not valid. Please check. \"\n                \"A valid example is `components='2,3'`\"\n            )\n        # check if the components are present in the data\n        try:\n            data_points = []\n            for comp in components_list:\n                data_points.append(adata.obsm[basis_key][:, comp])\n        except:\n            raise ValueError(\n                \"Given components: '{}' are not valid. Please check. \"\n                \"A valid example is `components='2,3'`\"\n            )\n        if basis == 'diffmap':\n            # remove the offset added in the case of diffmap, such that\n            # plot_scatter can print the labels correctly.\n            components_list = [\n                tuple(number - 1 for number in comp) for comp in components_list\n            ]\n    else:\n        data_points = [adata.obsm[basis_key][:, offset : offset + n_dims]]\n        components_list = []\n\n    if img_key is not None:\n        spatial_data = adata.uns[\"spatial\"][library_id]\n        if f\"tissue_{img_key}_scalef\" in spatial_data['scalefactors'].keys():\n            scalef_key = f\"tissue_{img_key}_scalef\"\n            data_points[0] = np.multiply(\n                data_points[0], spatial_data['scalefactors'][scalef_key],\n            )\n        else:\n            raise KeyError(\n                f\"Could not find entry in `adata.uns[spatial][{library_id}]` for '{img_key}'.\\n\"\n                f\"Available keys are: {list(spatial_data['images'].keys())}.\"\n            )\n\n    return data_points, components_list", "idx": 504}
{"project": "Scanpy", "commit_id": "146_scanpy_1.9.0_pl.py_phate.py", "target": 0, "func": "def phate(adata, **kwargs) -> Union[List[Axes], None]:\n    \"\"\"\\\n    Scatter plot in PHATE basis.\n\n    Parameters\n    ----------\n    {adata_color_etc}\n    {edges_arrows}\n    {scatter_bulk}\n    {show_save_ax}\n\n    Returns\n    -------\n    If `show==False`, a list of :class:`~matplotlib.axes.Axes` objects.\n    Every second element corresponds to the 'right margin'\n    drawing area for color bars and legends.\n\n    Examples\n    --------\n    >>> from anndata import AnnData\n    >>> import scanpy.external as sce\n    >>> import phate\n    >>> data, branches = phate.tree.gen_dla(\n    ...     n_dim=100,\n    ...     n_branch=20,\n    ...     branch_length=100,\n    ... )\n    >>> data.shape\n    (2000, 100)\n    >>> adata = AnnData(data)\n    >>> adata.obs['branches'] = branches\n    >>> sce.tl.phate(adata, k=5, a=20, t=150)\n    >>> adata.obsm['X_phate'].shape\n    (2000, 2)\n    >>> sce.pl.phate(\n    ...     adata,\n    ...     color='branches',\n    ...     color_map='tab20',\n    ... )\n    \"\"\"\n    return embedding(adata, 'phate', **kwargs)", "idx": 506}
{"project": "Scanpy", "commit_id": "937_scanpy_1.4.4_scanpy_preprocessing__highly_variable_genes.py__highly_variable_genes_single_batch.py", "target": 1, "func": "def _highly_variable_genes_single_batch(\n    adata: AnnData,\n    min_disp=None,\n    max_disp=None,\n    min_mean=None,\n    max_mean=None,\n    n_top_genes=None,\n    n_bins=20,\n    flavor='seurat',\n) -> pd.DataFrame:\n    \"\"\"Internal function for annotating highly variable genes [Satija15]_ [Zheng17]_.\n    Expects logarithmized data.\n    Depending on `flavor`, this reproduces the R-implementations of Seurat\n    [Satija15]_ and Cell Ranger [Zheng17]_.\n    The normalized dispersion is obtained by scaling with the mean and standard\n    deviation of the dispersions for genes falling into a given bin for mean\n    expression of genes. This means that for each bin of mean expression, highly\n    variable genes are selected.\n    Parameters\n    ----------\n    adata\n        The annotated data matrix of shape `n_obs` \u00d7 `n_vars`. Rows correspond\n        to cells and columns to genes.\n    min_mean : `float`, optional (default: 0.0125)\n        If `n_top_genes` unequals `None`, this and all other cutoffs for the means and the\n        normalized dispersions are ignored.\n    max_mean : `float`, optional (default: 3)\n        If `n_top_genes` unequals `None`, this and all other cutoffs for the means and the\n        normalized dispersions are ignored.\n    min_disp : `float`, optional (default: 0.5)\n        If `n_top_genes` unequals `None`, this and all other cutoffs for the means and the\n        normalized dispersions are ignored.\n    max_disp : `float`, optional (default: `None`)\n        If `n_top_genes` unequals `None`, this and all other cutoffs for the means and the\n        normalized dispersions are ignored.\n    n_top_genes : `int` or `None`, optional (default: `None`)\n        Number of highly-variable genes to keep.\n    n_bins : `int`, optional (default: 20)\n        Number of bins for binning the mean gene expression. Normalization is\n        done with respect to each bin. If just a single gene falls into a bin,\n        the normalized dispersion is artificially set to 1. You'll be informed\n        about this if you set `settings.verbosity = 4`.\n    flavor : {`'seurat'`, `'cell_ranger'`}\n        Choose the flavor for computing normalized dispersion. In their default\n        workflows, Seurat passes the cutoffs whereas Cell Ranger passes\n        `n_top_genes`.\n    Returns\n    -------\n    highly_variable_data_frame\n        A DataFrame that contains colums highly_variable, means, dispersions and dispersions_norm.\n    \"\"\"\n\n    if n_top_genes is not None and not all([\n            min_disp is None, max_disp is None, min_mean is None, max_mean is None]):\n        logg.info('If you pass `n_top_genes`, all cutoffs are ignored.')\n\n    if min_disp is None: min_disp = 0.5\n    if min_mean is None: min_mean = 0.0125\n    if max_mean is None: max_mean = 3\n    if max_disp is None: max_disp = np.inf\n    X = np.expm1(adata.X) if flavor == 'seurat' else adata.X\n    mean, var = materialize_as_ndarray(_get_mean_var(X))\n    # now actually compute the dispersion\n    mean[mean == 0] = 1e-12  # set entries equal to zero to small value\n    dispersion = var / mean\n    if flavor == 'seurat':  # logarithmized mean as in Seurat\n        dispersion[dispersion == 0] = np.nan\n        dispersion = np.log(dispersion)\n        mean = np.log1p(mean)\n    # all of the following quantities are \"per-gene\" here\n    df = pd.DataFrame()\n    df['means'] = mean\n    df['dispersions'] = dispersion\n    if flavor == 'seurat':\n        df['mean_bin'] = pd.cut(df['means'], bins=n_bins)\n        disp_grouped = df.groupby('mean_bin')['dispersions']\n        disp_mean_bin = disp_grouped.mean()\n        disp_std_bin = disp_grouped.std(ddof=1)\n        # retrieve those genes that have nan std, these are the ones where\n        # only a single gene fell in the bin and implicitly set them to have\n        # a normalized disperion of 1\n        one_gene_per_bin = disp_std_bin.isnull()\n        gen_indices = np.where(one_gene_per_bin[df['mean_bin'].values])[0].tolist()\n        if len(gen_indices) > 0:\n            logg.debug(\n                f'Gene indices {gen_indices} fell into a single bin: their '\n                'normalized dispersion was set to 1.\\n    '\n                'Decreasing `n_bins` will likely avoid this effect.'\n            )\n        # Circumvent pandas 0.23 bug. Both sides of the assignment have dtype==float32,\n        # but there\u2019s still a dtype error without \u201c.value\u201d.\n        disp_std_bin[one_gene_per_bin.values] = disp_mean_bin[one_gene_per_bin.values].values\n        disp_mean_bin[one_gene_per_bin.values] = 0\n        # actually do the normalization\n        df['dispersions_norm'] = (\n            (\n                df['dispersions'].values  # use values here as index differs\n                - disp_mean_bin[df['mean_bin'].values].values\n            ) / disp_std_bin[df['mean_bin'].values].values\n        )\n    elif flavor == 'cell_ranger':\n        from statsmodels import robust\n        df['mean_bin'] = pd.cut(df['means'], np.r_[\n            -np.inf,\n            np.percentile(df['means'], np.arange(10, 105, 5)),\n            np.inf\n        ])\n        disp_grouped = df.groupby('mean_bin')['dispersions']\n        disp_median_bin = disp_grouped.median()\n        # the next line raises the warning: \"Mean of empty slice\"\n        with warnings.catch_warnings():\n            warnings.simplefilter('ignore')\n            disp_mad_bin = disp_grouped.apply(robust.mad)\n            df['dispersions_norm'] = (df['dispersions'].values\n                - disp_median_bin[df['mean_bin'].values].values\n                ) / disp_mad_bin[df['mean_bin'].values].values\n    else:\n        raise ValueError('`flavor` needs to be \"seurat\" or \"cell_ranger\"')\n    dispersion_norm = df['dispersions_norm'].values.astype('float32')\n    if n_top_genes is not None:\n        dispersion_norm = dispersion_norm[~np.isnan(dispersion_norm)]\n        dispersion_norm[::-1].sort()  # interestingly, np.argpartition is slightly slower\n        disp_cut_off = dispersion_norm[n_top_genes-1]\n        gene_subset = np.nan_to_num(df['dispersions_norm'].values) >= disp_cut_off\n        logg.debug(\n            f'the {n_top_genes} top genes correspond to a '\n            f'normalized dispersion cutoff of {disp_cut_off}'\n        )\n    else:\n        dispersion_norm[np.isnan(dispersion_norm)] = 0  # similar to Seurat\n        gene_subset = np.logical_and.reduce((\n            mean > min_mean, mean < max_mean,\n            dispersion_norm > min_disp,\n            dispersion_norm < max_disp,\n        ))\n    df['highly_variable'] = gene_subset\n    return df", "idx": 509}
{"project": "Scanpy", "commit_id": "247_scanpy_1.9.0__anndata.py_dendrogram.py", "target": 0, "func": "def dendrogram(\n    adata: AnnData,\n    groupby: str,\n    *,\n    dendrogram_key: Optional[str] = None,\n    orientation: Literal['top', 'bottom', 'left', 'right'] = 'top',\n    remove_labels: bool = False,\n    show: Optional[bool] = None,\n    save: Union[str, bool, None] = None,\n    ax: Optional[Axes] = None,\n):\n    \"\"\"\\\n    Plots a dendrogram of the categories defined in `groupby`.\n\n    See :func:`~scanpy.tl.dendrogram`.\n\n    Parameters\n    ----------\n    adata\n        Annotated data matrix.\n    groupby\n        Categorical data column used to create the dendrogram\n    dendrogram_key\n        Key under with the dendrogram information was stored.\n        By default the dendrogram information is stored under\n        `.uns[f'dendrogram_{{groupby}}']`.\n    orientation\n        Origin of the tree. Will grow into the opposite direction.\n    remove_labels\n        Don\u2019t draw labels. Used e.g. by :func:`scanpy.pl.matrixplot`\n        to annotate matrix columns/rows.\n    {show_save_ax}\n\n    Returns\n    -------\n    :class:`matplotlib.axes.Axes`\n\n    Examples\n    --------\n    .. plot::\n        :context: close-figs\n\n        import scanpy as sc\n        adata = sc.datasets.pbmc68k_reduced()\n        sc.tl.dendrogram(adata, 'bulk_labels')\n        sc.pl.dendrogram(adata, 'bulk_labels')\n\n    .. currentmodule:: scanpy\n\n    \"\"\"\n    if ax is None:\n        _, ax = pl.subplots()\n    _plot_dendrogram(\n        ax,\n        adata,\n        groupby,\n        dendrogram_key=dendrogram_key,\n        remove_labels=remove_labels,\n        orientation=orientation,\n    )\n    _utils.savefig_or_show('dendrogram', show=show, save=save)\n    return ax", "idx": 510}
{"project": "Scanpy", "commit_id": "108_scanpy_0.0_scanpy_classes_ann_data.py_from_ddata.py", "target": 1, "func": "def from_ddata(self, ddata):\n    smp, var = OrderedDict(), OrderedDict()\n    meta = ddata.copy()\n    del ddata\n    X = meta['X']\n    del meta['X']\n    if 'row_names' in meta:\n        smp['smp_names'] = meta['row_names']\n        del meta['row_names']\n    elif 'smp_names' in meta:\n        smp['smp_names'] = meta['smp_names']\n        del meta['smp_names']\n    if 'col_names' in meta:\n        var['var_names'] = meta['col_names']\n        del meta['col_names']\n    elif 'var_names' in meta:\n        var['var_names'] = meta['var_names']\n        del meta['var_names']\n\n    smp = odict_merge(smp, meta.get('row', {}), meta.get('smp', {}))\n    var = odict_merge(var, meta.get('col', {}), meta.get('var', {}))\n    for n in {'row', 'smp', 'col', 'var'} & meta.keys():\n        del meta[n]\n\n    return X, smp, var, meta", "idx": 512}
{"project": "Scanpy", "commit_id": "922_scanpy_1.4.4_scanpy_tools__rank_genes_groups.py_rank_genes_groups.py", "target": 1, "func": "def rank_genes_groups(\n    adata: AnnData,\n    groupby: str,\n    use_raw: bool = True,\n    groups: Union[str, Iterable[str]] = 'all',\n    reference: str = 'rest',\n    n_genes: int = 100,\n    rankby_abs: bool = False,\n    key_added: Optional[str] = None,\n    copy: bool = False,\n    method: str = 't-test_overestim_var',\n    corr_method: str = 'benjamini-hochberg',\n    **kwds\n):\n    \"\"\"Rank genes for characterizing groups.\n    Parameters\n    ----------\n    adata\n        Annotated data matrix.\n    groupby\n        The key of the observations grouping to consider.\n    use_raw\n        Use `raw` attribute of `adata` if present.\n    groups\n        Subset of groups, e.g. [`'g1'`, `'g2'`, `'g3'`], to which comparison\n        shall be restricted, or `'all'` (default), for all groups.\n    reference\n        If `'rest'`, compare each group to the union of the rest of the group.\n        If a group identifier, compare with respect to this group.\n    n_genes\n        The number of genes that appear in the returned tables.\n    method: {`'logreg'`, `'t-test'`, `'wilcoxon'`, `'t-test_overestim_var'`}`\n        The default 't-test_overestim_var' overestimates variance of each group,\n        `'t-test'` uses t-test, `'wilcoxon'` uses Wilcoxon rank-sum,\n        `'logreg'` uses logistic regression. See [Ntranos18]_,\n        `here <https://github.com/theislab/scanpy/issues/95>`__ and `here\n        <http://www.nxn.se/valent/2018/3/5/actionable-scrna-seq-clusters>`__,\n        for why this is meaningful.\n    corr_method: {`'benjamini-hochberg'`, `'bonferroni'`}\n        p-value correction method.\n        Used only for `'t-test'`, `'t-test_overestim_var'`, and `'wilcoxon'`.\n    rankby_abs\n        Rank genes by the absolute value of the score, not by the\n        score. The returned scores are never the absolute values.\n    key_added\n        The key in `adata.uns` information is saved to.\n    **kwds\n        Are passed to test methods. Currently this affects only parameters that\n        are passed to `sklearn.linear_model.LogisticRegression\n        <http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html>`__.\n        For instance, you can pass `penalty='l1'` to try to come up with a\n        minimal set of genes that are good predictors (sparse solution meaning\n        few non-zero fitted coefficients).\n    Returns\n    -------\n    **names** : structured `np.ndarray` (`.uns['rank_genes_groups']`)\n        Structured array to be indexed by group id storing the gene\n        names. Ordered according to scores.\n    **scores** : structured `np.ndarray` (`.uns['rank_genes_groups']`)\n        Structured array to be indexed by group id storing the z-score\n        underlying the computation of a p-value for each gene for each\n        group. Ordered according to scores.\n    **logfoldchanges** : structured `np.ndarray` (`.uns['rank_genes_groups']`)\n        Structured array to be indexed by group id storing the log2\n        fold change for each gene for each group. Ordered according to\n        scores. Only provided if method is 't-test' like.\n        Note: this is an approximation calculated from mean-log values.\n    **pvals** : structured `np.ndarray` (`.uns['rank_genes_groups']`)\n        p-values.\n    **pvals_adj** : structured `np.ndarray` (`.uns['rank_genes_groups']`)\n        Corrected p-values.\n    Notes\n    -----\n    There are slight inconsistencies depending on whether sparse\n    or dense data are passed. See `here <https://github.com/theislab/scanpy/blob/master/scanpy/tests/test_rank_genes_groups.py>`__.\n    Examples\n    --------\n    >>> adata = sc.datasets.pbmc68k_reduced()\n    >>> sc.tl.rank_genes_groups(adata, 'bulk_labels', method='wilcoxon')\n    # to visualize the results\n    >>> sc.pl.rank_genes_groups(adata)\n    \"\"\"\n    if 'only_positive' in kwds:\n        rankby_abs = not kwds.pop('only_positive')  # backwards compat\n    start = logg.info('ranking genes')\n    avail_methods = {'t-test', 't-test_overestim_var', 'wilcoxon', 'logreg'}\n    if method not in avail_methods:\n        raise ValueError('Method must be one of {}.'.format(avail_methods))\n    avail_corr = {'benjamini-hochberg', 'bonferroni'}\n    if corr_method not in avail_corr:\n        raise ValueError('Correction method must be one of {}.'.format(avail_corr))\n    adata = adata.copy() if copy else adata\n    utils.sanitize_anndata(adata)\n    # for clarity, rename variable\n    if groups == 'all':\n        groups_order = 'all'\n    elif isinstance(groups, (str, int)):\n        raise ValueError('Specify a sequence of groups')\n    else:\n        groups_order = list(groups)\n        if isinstance(groups_order[0], int):\n            groups_order = [str(n) for n in groups_order]\n        if reference != 'rest' and reference not in set(groups_order):\n            groups_order += [reference]\n    if (\n        reference != 'rest'\n        and reference not in set(adata.obs[groupby].cat.categories)\n    ):\n        cats = adata.obs[groupby].cat.categories.tolist()\n        raise ValueError(\n            f'reference = {reference} needs to be one of groupby = {cats}.'\n        )\n    groups_order, groups_masks = utils.select_groups(\n        adata, groups_order, groupby)\n    if key_added is None:\n        key_added = 'rank_genes_groups'\n    adata.uns[key_added] = {}\n    adata.uns[key_added]['params'] = {\n        'groupby': groupby,\n        'reference': reference,\n        'method': method,\n        'use_raw': use_raw,\n        'corr_method': corr_method,\n    }\n\n    # adata_comp mocks an AnnData object if use_raw is True\n    # otherwise it's just the AnnData object\n    adata_comp = adata\n    if adata.raw is not None and use_raw:\n        adata_comp = adata.raw\n    X = adata_comp.X\n\n    # for clarity, rename variable\n    n_genes_user = n_genes\n    # make sure indices are not OoB in case there are less genes than n_genes\n    if n_genes_user > X.shape[1]:\n        n_genes_user = X.shape[1]\n    # in the following, n_genes is simply another name for the total number of genes\n    n_genes = X.shape[1]\n    n_groups = groups_masks.shape[0]\n    ns = np.zeros(n_groups, dtype=int)\n    for imask, mask in enumerate(groups_masks):\n        ns[imask] = np.where(mask)[0].size\n    logg.debug(f'consider {groupby!r} groups:')\n    logg.debug(f'with sizes: {ns}')\n    if reference != 'rest':\n        ireference = np.where(groups_order == reference)[0][0]\n    reference_indices = np.arange(adata_comp.n_vars, dtype=int)\n    rankings_gene_scores = []\n    rankings_gene_names = []\n    rankings_gene_logfoldchanges = []\n    rankings_gene_pvals = []\n    rankings_gene_pvals_adj = []\n    if method in {'t-test', 't-test_overestim_var'}:\n        from scipy import stats\n        from statsmodels.stats.multitest import multipletests\n        # loop over all masks and compute means, variances and sample numbers\n        means = np.zeros((n_groups, n_genes))\n        vars = np.zeros((n_groups, n_genes))\n        for imask, mask in enumerate(groups_masks):\n            means[imask], vars[imask] = _get_mean_var(X[mask])\n        # test each either against the union of all other groups or against a\n        # specific group\n        for igroup in range(n_groups):\n            if reference == 'rest':\n                mask_rest = ~groups_masks[igroup]\n            else:\n                if igroup == ireference: continue\n                else: mask_rest = groups_masks[ireference]\n            mean_group, var_group = means[igroup], vars[igroup]\n            mean_rest, var_rest = _get_mean_var(X[mask_rest])\n            ns_group = ns[igroup]  # number of observations in group\n            if method == 't-test': ns_rest = np.where(mask_rest)[0].size\n            elif method == 't-test_overestim_var': ns_rest = ns[igroup]  # hack for overestimating the variance for small groups\n            else: raise ValueError('Method does not exist.')\n            # TODO: Come up with better solution. Mask unexpressed genes?\n            # See https://github.com/scipy/scipy/issues/10269\n            with np.errstate(invalid=\"ignore\"):\n                scores, pvals = stats.ttest_ind_from_stats(\n                    mean1=mean_group, std1=np.sqrt(var_group), nobs1=ns_group,\n                    mean2=mean_rest,  std2=np.sqrt(var_rest),  nobs2=ns_rest,\n                    equal_var=False  # Welch's\n                )\n            # Fold change\n            foldchanges = (np.expm1(mean_group) + 1e-9) / (np.expm1(mean_rest) + 1e-9)  # add small value to remove 0's\n            scores[np.isnan(scores)] = 0  # I think it's only nan when means are the same and vars are 0\n            pvals[np.isnan(pvals)] = 1  # This also has to happen for Benjamini Hochberg\n            if corr_method == 'benjamini-hochberg':\n                _, pvals_adj, _, _ = multipletests(pvals, alpha=0.05, method='fdr_bh')\n            elif corr_method == 'bonferroni':\n                pvals_adj = np.minimum(pvals * n_genes, 1.0)\n            scores_sort = np.abs(scores) if rankby_abs else scores\n            partition = np.argpartition(scores_sort, -n_genes_user)[-n_genes_user:]\n            partial_indices = np.argsort(scores_sort[partition])[::-1]\n            global_indices = reference_indices[partition][partial_indices]\n            rankings_gene_scores.append(scores[global_indices])\n            rankings_gene_logfoldchanges.append(np.log2(foldchanges[global_indices]))\n            rankings_gene_names.append(adata_comp.var_names[global_indices])\n            rankings_gene_pvals.append(pvals[global_indices])\n            rankings_gene_pvals_adj.append(pvals_adj[global_indices])\n    elif method == 'logreg':\n        # if reference is not set, then the groups listed will be compared to the rest\n        # if reference is set, then the groups listed will be compared only to the other groups listed\n        from sklearn.linear_model import LogisticRegression\n        reference = groups_order[0]\n        if len(groups) == 1:\n            raise Exception('Cannot perform logistic regression on a single cluster.')\n        adata_copy = adata[adata.obs[groupby].isin(groups_order)]\n        adata_comp = adata_copy\n        if adata.raw is not None and use_raw:\n            adata_comp = adata_copy.raw\n        X = adata_comp.X\n\n        clf = LogisticRegression(**kwds)\n        clf.fit(X, adata_copy.obs[groupby].cat.codes)\n        scores_all = clf.coef_\n        for igroup, group in enumerate(groups_order):\n            if len(groups_order) <= 2:  # binary logistic regression\n                scores = scores_all[0]\n            else:\n                scores = scores_all[igroup]\n            partition = np.argpartition(scores, -n_genes_user)[-n_genes_user:]\n            partial_indices = np.argsort(scores[partition])[::-1]\n            global_indices = reference_indices[partition][partial_indices]\n            rankings_gene_scores.append(scores[global_indices])\n            rankings_gene_names.append(adata_comp.var_names[global_indices])\n            if len(groups_order) <= 2:\n                break\n    elif method == 'wilcoxon':\n        from scipy import stats\n        from statsmodels.stats.multitest import multipletests\n        CONST_MAX_SIZE = 10000000\n        means = np.zeros((n_groups, n_genes))\n        vars = np.zeros((n_groups, n_genes))\n        # initialize space for z-scores\n        scores = np.zeros(n_genes)\n        # First loop: Loop over all genes\n        if reference != 'rest':\n            for imask, mask in enumerate(groups_masks):\n                means[imask], vars[imask] = _get_mean_var(X[mask])  # for fold-change only\n                if imask == ireference: continue\n                else: mask_rest = groups_masks[ireference]\n                ns_rest = np.where(mask_rest)[0].size\n                mean_rest, var_rest = _get_mean_var(X[mask_rest]) # for fold-change only\n                if ns_rest <= 25 or ns[imask] <= 25:\n                    logg.hint('Few observations in a group for '\n                              'normal approximation (<=25). Lower test accuracy.')\n                n_active = ns[imask]\n                m_active = ns_rest\n                # Now calculate gene expression ranking in chunkes:\n                chunk = []\n                # Calculate chunk frames\n                n_genes_max_chunk = floor(CONST_MAX_SIZE / (n_active + m_active))\n                if n_genes_max_chunk < n_genes:\n                    chunk_index = n_genes_max_chunk\n                    while chunk_index < n_genes:\n                        chunk.append(chunk_index)\n                        chunk_index = chunk_index + n_genes_max_chunk\n                    chunk.append(n_genes)\n                else:\n                    chunk.append(n_genes)\n                left = 0\n                # Calculate rank sums for each chunk for the current mask\n                for chunk_index, right in enumerate(chunk):\n                    # Check if issparse is true: AnnData objects are currently sparse.csr or ndarray.\n                    if issparse(X):\n                        df1 = pd.DataFrame(data=X[mask, left:right].todense())\n                        df2 = pd.DataFrame(data=X[mask_rest, left:right].todense(),\n                                           index=np.arange(start=n_active, stop=n_active + m_active))\n                    else:\n                        df1 = pd.DataFrame(data=X[mask, left:right])\n                        df2 = pd.DataFrame(data=X[mask_rest, left:right],\n                                           index=np.arange(start=n_active, stop=n_active + m_active))\n                    df1 = df1.append(df2)\n                    ranks = df1.rank()\n                    # sum up adjusted_ranks to calculate W_m,n\n                    scores[left:right] = np.sum(ranks.loc[0:n_active, :])\n                    left = right\n                scores = (scores - (n_active * (n_active + m_active + 1) / 2)) / sqrt(\n                    (n_active * m_active * (n_active + m_active + 1) / 12))\n                scores[np.isnan(scores)] = 0\n                pvals = 2 * stats.distributions.norm.sf(np.abs(scores))\n                if corr_method == 'benjamini-hochberg':\n                    pvals[np.isnan(pvals)] = 1  # set Nan values to 1 to properly convert using Benhjamini Hochberg\n                    _, pvals_adj, _, _ = multipletests(pvals, alpha=0.05, method='fdr_bh')\n                elif corr_method == 'bonferroni':\n                    pvals_adj = np.minimum(pvals * n_genes, 1.0)\n                # Fold change\n                foldchanges = (np.expm1(means[imask]) + 1e-9) / (np.expm1(mean_rest) + 1e-9)  # add small value to remove 0's\n                scores_sort = np.abs(scores) if rankby_abs else scores\n                partition = np.argpartition(scores_sort, -n_genes_user)[-n_genes_user:]\n                partial_indices = np.argsort(scores_sort[partition])[::-1]\n                global_indices = reference_indices[partition][partial_indices]\n                rankings_gene_scores.append(scores[global_indices])\n                rankings_gene_names.append(adata_comp.var_names[global_indices])\n                rankings_gene_logfoldchanges.append(np.log2(foldchanges[global_indices]))\n                rankings_gene_pvals.append(pvals[global_indices])\n                rankings_gene_pvals_adj.append(pvals_adj[global_indices])\n        # If no reference group exists, ranking needs only to be done once (full mask)\n        else:\n            scores = np.zeros((n_groups, n_genes))\n            chunk = []\n            n_cells = X.shape[0]\n            n_genes_max_chunk = floor(CONST_MAX_SIZE / n_cells)\n            if n_genes_max_chunk < n_genes:\n                chunk_index = n_genes_max_chunk\n                while chunk_index < n_genes:\n                    chunk.append(chunk_index)\n                    chunk_index = chunk_index + n_genes_max_chunk\n                chunk.append(n_genes)\n            else:\n                chunk.append(n_genes)\n            left = 0\n            for chunk_index, right in enumerate(chunk):\n                # Check if issparse is true\n                if issparse(X):\n                    df1 = pd.DataFrame(data=X[:, left:right].todense())\n                else:\n                    df1 = pd.DataFrame(data=X[:, left:right])\n                ranks = df1.rank()\n                # sum up adjusted_ranks to calculate W_m,n\n                for imask, mask in enumerate(groups_masks):\n                    scores[imask, left:right] = np.sum(ranks.loc[mask, :])\n                left = right\n            for imask, mask in enumerate(groups_masks):\n                mask_rest = ~groups_masks[imask]\n                means[imask], vars[imask] = _get_mean_var(X[mask]) #for fold-change\n                mean_rest, var_rest = _get_mean_var(X[mask_rest])  # for fold-change\n                scores[imask, :] = (scores[imask, :] - (ns[imask] * (n_cells + 1) / 2)) / sqrt(\n                    (ns[imask] * (n_cells - ns[imask]) * (n_cells + 1) / 12))\n                scores[np.isnan(scores)] = 0\n                pvals = 2 * stats.distributions.norm.sf(np.abs(scores[imask,:]))\n                if corr_method == 'benjamini-hochberg':\n                    pvals[np.isnan(pvals)] = 1  # set Nan values to 1 to properly convert using Benhjamini Hochberg\n                    _, pvals_adj, _, _ = multipletests(pvals, alpha=0.05, method='fdr_bh')\n                elif corr_method == 'bonferroni':\n                    pvals_adj = np.minimum(pvals * n_genes, 1.0)\n                # Fold change\n                foldchanges = (np.expm1(means[imask]) + 1e-9) / (np.expm1(mean_rest) + 1e-9)  # add small value to remove 0's\n                scores_sort = np.abs(scores) if rankby_abs else scores\n                partition = np.argpartition(scores_sort[imask, :], -n_genes_user)[-n_genes_user:]\n                partial_indices = np.argsort(scores_sort[imask, partition])[::-1]\n                global_indices = reference_indices[partition][partial_indices]\n                rankings_gene_scores.append(scores[imask, global_indices])\n                rankings_gene_names.append(adata_comp.var_names[global_indices])\n                rankings_gene_logfoldchanges.append(np.log2(foldchanges[global_indices]))\n                rankings_gene_pvals.append(pvals[global_indices])\n                rankings_gene_pvals_adj.append(pvals_adj[global_indices])\n    groups_order_save = [str(g) for g in groups_order]\n    if (reference != 'rest' and method != 'logreg') or (method == 'logreg' and len(groups) == 2):\n        groups_order_save = [g for g in groups_order if g != reference]\n    adata.uns[key_added]['scores'] = np.rec.fromarrays(\n        [n for n in rankings_gene_scores],\n        dtype=[(rn, 'float32') for rn in groups_order_save])\n    adata.uns[key_added]['names'] = np.rec.fromarrays(\n        [n for n in rankings_gene_names],\n        dtype=[(rn, 'U50') for rn in groups_order_save])\n    if method in {'t-test', 't-test_overestim_var', 'wilcoxon'}:\n        adata.uns[key_added]['logfoldchanges'] = np.rec.fromarrays(\n            [n for n in rankings_gene_logfoldchanges],\n            dtype=[(rn, 'float32') for rn in groups_order_save])\n        adata.uns[key_added]['pvals'] = np.rec.fromarrays(\n            [n for n in rankings_gene_pvals],\n            dtype=[(rn, 'float64') for rn in groups_order_save])\n        adata.uns[key_added]['pvals_adj'] = np.rec.fromarrays(\n            [n for n in rankings_gene_pvals_adj],\n            dtype=[(rn, 'float64') for rn in groups_order_save])\n    logg.info(\n        '    finished',\n        time=start,\n        deep=(\n            f'added to `.uns[{key_added!r}]`\\n'\n            \"    'names', sorted np.recarray to be indexed by group ids\\n\"\n            \"    'scores', sorted np.recarray to be indexed by group ids\\n\"\n            + (\n                \"    'logfoldchanges', sorted np.recarray to be indexed by group ids\\n\"\n                \"    'pvals', sorted np.recarray to be indexed by group ids\\n\"\n                \"    'pvals_adj', sorted np.recarray to be indexed by group ids\"\n                if method in {'t-test', 't-test_overestim_var', 'wilcoxon'} else\n                ''\n            )\n        ),\n    )\n    return adata if copy else None", "idx": 516}
{"project": "Scanpy", "commit_id": "160_scanpy_1.9.0__magic.py_magic.py", "target": 0, "func": "def magic(\n    adata: AnnData,\n    name_list: Union[Literal['all_genes', 'pca_only'], Sequence[str], None] = None,\n    *,\n    knn: int = 5,\n    decay: Optional[float] = 1,\n    knn_max: Optional[int] = None,\n    t: Union[Literal['auto'], int] = 3,\n    n_pca: Optional[int] = 100,\n    solver: Literal['exact', 'approximate'] = 'exact',\n    knn_dist: str = 'euclidean',\n    random_state: AnyRandom = None,\n    n_jobs: Optional[int] = None,\n    verbose: bool = False,\n    copy: Optional[bool] = None,\n    **kwargs,\n) -> Optional[AnnData]:\n    \"\"\"\\\n    Markov Affinity-based Graph Imputation of Cells (MAGIC) API [vanDijk18]_.\n\n    MAGIC is an algorithm for denoising and transcript recover of single cells\n    applied to single-cell sequencing data. MAGIC builds a graph from the data\n    and uses diffusion to smooth out noise and recover the data manifold.\n\n    The algorithm implemented here has changed primarily in two ways\n    compared to the algorithm described in [vanDijk18]_. Firstly, we use\n    the adaptive kernel described in Moon et al, 2019 [Moon17]_ for\n    improved stability. Secondly, data diffusion is applied\n    in the PCA space, rather than the data space, for speed and\n    memory improvements.\n\n    More information and bug reports\n    `here <https://github.com/KrishnaswamyLab/MAGIC>`__. For help, visit\n    <https://krishnaswamylab.org/get-help>.\n\n    Parameters\n    ----------\n    adata\n        An anndata file with `.raw` attribute representing raw counts.\n    name_list\n        Denoised genes to return. The default `'all_genes'`/`None`\n        may require a large amount of memory if the input data is sparse.\n        Another possibility is `'pca_only'`.\n    knn\n        number of nearest neighbors on which to build kernel.\n    decay\n        sets decay rate of kernel tails.\n        If None, alpha decaying kernel is not used.\n    knn_max\n        maximum number of nearest neighbors with nonzero connection.\n        If `None`, will be set to 3 * `knn`.\n    t\n        power to which the diffusion operator is powered.\n        This sets the level of diffusion. If 'auto', t is selected\n        according to the Procrustes disparity of the diffused data.\n    n_pca\n        Number of principal components to use for calculating\n        neighborhoods. For extremely large datasets, using\n        n_pca < 20 allows neighborhoods to be calculated in\n        roughly log(n_samples) time. If `None`, no PCA is performed.\n    solver\n        Which solver to use. \"exact\" uses the implementation described\n        in van Dijk et al. (2018) [vanDijk18]_. \"approximate\" uses a faster\n        implementation that performs imputation in the PCA space and then\n        projects back to the gene space. Note, the \"approximate\" solver may\n        return negative values.\n    knn_dist\n        recommended values: 'euclidean', 'cosine', 'precomputed'\n        Any metric from `scipy.spatial.distance` can be used\n        distance metric for building kNN graph. If 'precomputed',\n        `data` should be an n_samples x n_samples distance or\n        affinity matrix.\n    random_state\n        Random seed. Defaults to the global `numpy` random number generator.\n    n_jobs\n        Number of threads to use in training. All cores are used by default.\n    verbose\n        If `True` or an integer `>= 2`, print status messages.\n        If `None`, `sc.settings.verbosity` is used.\n    copy\n        If true, a copy of anndata is returned. If `None`, `copy` is True if\n        `genes` is not `'all_genes'` or `'pca_only'`. `copy` may only be False\n        if `genes` is `'all_genes'` or `'pca_only'`, as the resultant data\n        will otherwise have different column names from the input data.\n    kwargs\n        Additional arguments to `magic.MAGIC`.\n\n    Returns\n    -------\n    If `copy` is True, AnnData object is returned.\n\n    If `subset_genes` is not `all_genes`, PCA on MAGIC values of cells are\n    stored in `adata.obsm['X_magic']` and `adata.X` is not modified.\n\n    The raw counts are stored in `.raw` attribute of AnnData object.\n\n    Examples\n    --------\n    >>> import scanpy as sc\n    >>> import scanpy.external as sce\n    >>> adata = sc.datasets.paul15()\n    >>> sc.pp.normalize_per_cell(adata)\n    >>> sc.pp.sqrt(adata)  # or sc.pp.log1p(adata)\n    >>> adata_magic = sce.pp.magic(adata, name_list=['Mpo', 'Klf1', 'Ifitm1'], knn=5)\n    >>> adata_magic.shape\n    (2730, 3)\n    >>> sce.pp.magic(adata, name_list='pca_only', knn=5)\n    >>> adata.obsm['X_magic'].shape\n    (2730, 100)\n    >>> sce.pp.magic(adata, name_list='all_genes', knn=5)\n    >>> adata.X.shape\n    (2730, 3451)\n    \"\"\"\n\n    try:\n        from magic import MAGIC, __version__\n    except ImportError:\n        raise ImportError(\n            'Please install magic package via `pip install --user '\n            'git+git://github.com/KrishnaswamyLab/MAGIC.git#subdirectory=python`'\n        )\n    else:\n        if not version.parse(__version__) >= version.parse(MIN_VERSION):\n            raise ImportError(\n                'scanpy requires magic-impute >= '\n                f'v{MIN_VERSION} (detected: v{__version__}). '\n                'Please update magic package via `pip install --user '\n                '--upgrade magic-impute`'\n            )\n\n    start = logg.info('computing MAGIC')\n    all_or_pca = isinstance(name_list, (str, type(None)))\n    if all_or_pca and name_list not in {\"all_genes\", \"pca_only\", None}:\n        raise ValueError(\n            \"Invalid string value for `name_list`: \"\n            \"Only `'all_genes'` and `'pca_only'` are allowed.\"\n        )\n    if copy is None:\n        copy = not all_or_pca\n    elif not all_or_pca and not copy:\n        raise ValueError(\n            \"Can only perform MAGIC in-place with `name_list=='all_genes' or \"\n            f\"`name_list=='pca_only'` (got {name_list}). Consider setting \"\n            \"`copy=True`\"\n        )\n    adata = adata.copy() if copy else adata\n    n_jobs = settings.n_jobs if n_jobs is None else n_jobs\n\n    X_magic = MAGIC(\n        knn=knn,\n        decay=decay,\n        knn_max=knn_max,\n        t=t,\n        n_pca=n_pca,\n        solver=solver,\n        knn_dist=knn_dist,\n        random_state=random_state,\n        n_jobs=n_jobs,\n        verbose=verbose,\n        **kwargs,\n    ).fit_transform(adata, genes=name_list)\n    logg.info(\n        '    finished',\n        time=start,\n        deep=(\n            \"added\\n    'X_magic', PCA on MAGIC coordinates (adata.obsm)\"\n            if name_list == \"pca_only\"\n            else ''\n        ),\n    )\n    # update AnnData instance\n    if name_list == \"pca_only\":\n        # special case \u2013 update adata.obsm with smoothed values\n        adata.obsm[\"X_magic\"] = X_magic.X\n    elif copy:\n        # just return X_magic\n        X_magic.raw = adata\n        adata = X_magic\n    else:\n        # replace data with smoothed data\n        adata.raw = adata\n        adata.X = X_magic.X\n\n    if copy:\n        return adata", "idx": 518}
{"project": "Scanpy", "commit_id": "880_scanpy_1.9.0__sim.py_coupl_model6.py", "target": 0, "func": "def coupl_model6(self):\n        \"\"\"Variant of toggle switch.\"\"\"\n        self.Coupl = 0.5 * self.Adj_signed", "idx": 526}
{"project": "Scanpy", "commit_id": "236_scanpy_1.9.0___init__.py__set_pseudotime.py", "target": 0, "func": "def _set_pseudotime(self):\n        \"\"\"Return pseudotime with respect to root point.\"\"\"\n        self.pseudotime = self.distances_dpt[self.iroot].copy()\n        self.pseudotime /= np.max(self.pseudotime[self.pseudotime < np.inf])", "idx": 529}
{"project": "Scanpy", "commit_id": "578_scanpy_1.0.4_scanpy_plotting_tools___init__.py_rank_genes_groups_violin.py", "target": 1, "func": "def rank_genes_groups_violin(adata, groups=None, n_genes=20,\n                             use_raw=None,\n                             split=True,\n                             scale='width',\n                             strip=True, jitter=True, size=1,\n                             computed_distribution=False,\n                             ax=None, show=None, save=None):\n    \"\"\"Plot ranking of genes for all tested comparisons.\n    Parameters\n    ----------\n    adata : :class:`~scanpy.api.AnnData`\n        Annotated data matrix.\n    groups : list of `str`, optional (default: `None`)\n        List of group names.\n    n_genes : `int`, optional (default: 20)\n        Number of genes to show.\n    use_raw : `bool`, optional (default: `None`)\n        Use `raw` attribute of `adata` if present. Defaults to the value that\n        was used in :func:`~scanpy.api.tl.rank_genes_groups`.\n    split : `bool`, optional (default: `True`)\n        Whether to split the violins or not.\n    scale : `str` (default: 'width')\n        See `seaborn.violinplot`.\n    strip : `bool` (default: `True`)\n        Show a strip plot on top of the violin plot.\n    jitter : `int`, `float`, `bool`, optional (default: `True`)\n        If set to 0, no points are drawn. See `seaborn.stripplot`.\n    size : `int`, optional (default: 1)\n        Size of the jitter points.\n    computed_distribution : `bool`, optional (default: `False`)\n        Set to `True` if you want to use the scaled and shifted distribution\n        previously computed with the `compute_distribution` in\n        :func:`scanpy.api.tl.rank_genes_groups`\n    show : `bool`, optional (default: `None`)\n        Show the plot, do not return axis.\n    save : `bool` or `str`, optional (default: `None`)\n        If `True` or a `str`, save the figure. A string is appended to the\n        default filename. Infer the filetype if ending on \\{'.pdf', '.png', '.svg'\\}.\n    ax : `matplotlib.Axes`, optional (default: `None`)\n        A `matplotlib.Axes` object.\n    \"\"\"\n    from ..tools import rank_genes_groups\n    groups_key = str(adata.uns['rank_genes_groups']['params']['groupby'])\n    if use_raw is None:\n        use_raw = bool(adata.uns['rank_genes_groups']['params']['use_raw'])\n    reference = str(adata.uns['rank_genes_groups']['params']['reference'])\n    groups_names = (adata.uns['rank_genes_groups']['names'].dtype.names\n                    if groups is None else groups)\n    if isinstance(groups_names, str): groups_names = [groups_names]\n    for group_name in groups_names:\n        keys = []\n        gene_names = adata.uns[\n            'rank_genes_groups']['names'][group_name][:n_genes]\n        if computed_distribution:\n            for gene_counter, gene_name in enumerate(gene_names):\n                identifier = rank_genes_groups._build_identifier(\n                    groups_key, group_name, gene_counter, gene_name)\n                if compute_distribution and identifier not in set(adata.obs_keys()):\n                    raise ValueError(\n                        'You need to set `compute_distribution=True` in '\n                        '`sc.tl.rank_genes_groups()`.')\n                keys.append(identifier)\n        else:\n            keys = gene_names\n        # make a \"hue\" option!\n        df = pd.DataFrame()\n        for key in keys:\n            if adata.raw is not None and use_raw:\n                X_col = adata.raw[:, key].X\n            else:\n                X_col = adata[:, key].X\n            if issparse(X_col): X_col = X_col.toarray().flatten()\n            df[key] = X_col\n        df['hue'] = adata.obs[groups_key].astype(str).values\n        if reference == 'rest':\n            df['hue'][df['hue'] != group_name] = 'rest'\n        else:\n            df['hue'][~df['hue'].isin([group_name, reference])] = np.nan\n        df['hue'] = df['hue'].astype('category')\n        df_tidy = pd.melt(df, id_vars='hue', value_vars=keys)\n        x = 'variable'\n        y = 'value'\n        hue_order = [group_name, reference]\n        import seaborn as sns\n        ax = sns.violinplot(x=x, y=y, data=df_tidy, inner=None,\n                            hue_order=hue_order, hue='hue', split=split,\n                            scale=scale, orient='vertical', ax=ax)\n        if strip:\n            ax = sns.stripplot(x=x, y=y, data=df_tidy,\n                               hue='hue', dodge=True, hue_order=hue_order,\n                               jitter=jitter, color='black', size=size, ax=ax)\n        ax.set_xlabel('genes')\n        ax.set_title('{} vs. {}'.format(group_name, reference))\n        ax.legend_.remove()\n        if computed_distribution: ax.set_ylabel('z-score w.r.t. to bulk mean')\n        else: ax.set_ylabel('expression')\n        ax.set_xticklabels(gene_names, rotation='vertical')\n        writekey = ('rank_genes_groups_'\n                    + str(adata.uns['rank_genes_groups']['params']['groupby'])\n                    + '_' + group_name)\n        utils.savefig_or_show(writekey, show=show, save=save)", "idx": 532}
{"project": "Scanpy", "commit_id": "356_scanpy_0.2.3.5_scanpy_tools_tsne.py_tsne.py", "target": 1, "func": "def tsne(adata, random_state=0, n_pcs=50, perplexity=30, n_jobs=None, copy=False):\n    \"\"\"tSNE\n    Parameters\n    ----------\n    adata : AnnData\n        Annotated data matrix, optionally with adata.smp['X_pca'], which is\n        written when running sc.pca(adata). Is directly used for tSNE.\n    random_state : unsigned int or -1, optional (default: 0)\n        Change to use different intial states for the optimization, if -1, use\n        default behavior of implementation (sklearn uses np.random.seed,\n        Multicore-TSNE produces a new plot at every call).\n    n_pcs : int, optional (default: 50)\n        Number of principal components in preprocessing PCA.\n    perplexity : float, optional (default: 30)\n        The perplexity is related to the number of nearest neighbors that\n        is used in other manifold learning algorithms. Larger datasets\n        usually require a larger perplexity. Consider selecting a value\n        between 5 and 50. The choice is not extremely critical since t-SNE\n        is quite insensitive to this parameter.\n    n_jobs : int or None (default: None)\n        Use the multicore implementation, if it is installed. Defaults to\n        sett.n_jobs.\n    Returns\n    -------\n    Returns or updates adata depending on `copy` with\n        \"X_tsne\", tSNE coordinates of data (adata.smp)\n    Reference\n    ---------\n    L.J.P. van der Maaten and G.E. Hinton.\n    Visualizing High-Dimensional Data Using t-SNE.\n    Journal of Machine Learning Research 9(Nov):2579-2605, 2008.\n    \"\"\"\n    logg.m('compute tSNE', r=True)\n    adata = adata.copy() if copy else adata\n    # preprocessing by PCA\n    if 'X_pca' in adata.smp and adata.smp['X_pca'].shape[1] >= n_pcs:\n        X = adata.smp['X_pca'][:, :n_pcs]\n        logg.m('    using X_pca for tSNE')\n        logg.m('    using', n_pcs, 'principal components')\n    else:\n        if n_pcs > 0 and adata.X.shape[1] > n_pcs:\n            logg.m('    preprocess using PCA with', n_pcs, 'PCs')\n            logg.m('avoid this by setting n_pcs = 0', v='hint')\n            X = pca(adata.X, random_state=random_state, n_comps=n_pcs)\n            adata.smp['X_pca'] = X\n            logg.m('    using', n_pcs, 'principal components')\n        else:\n            X = adata.X\n            logg.m('    using data matrix X directly (no PCA)')\n    # params for sklearn\n    params_sklearn = {'perplexity': perplexity,\n                      'random_state': None if random_state == -1 else random_state,\n                      'verbose': max(0, sett.verbosity-3),\n                      'learning_rate': 200,\n                      'early_exaggeration': 12,\n                      # 'method': 'exact'\n                      }\n    n_jobs = sett.n_jobs if n_jobs is None else n_jobs\n    # deal with different tSNE implementations\n    multicore_failed = False\n    if n_jobs > 1:\n        try:\n            from MulticoreTSNE import MulticoreTSNE as TSNE\n            tsne = TSNE(n_jobs=n_jobs, **params_sklearn)\n            logg.m('    using package MulticoreTSNE')\n            X_tsne = tsne.fit_transform(X.astype(np.float64))\n        except ImportError:\n            multicore_failed = True\n            logg.m('did not find package MulticoreTSNE: to speed up the computation, install it from\\n'\n                   '    https://github.com/DmitryUlyanov/Multicore-TSNE', v='hint')\n    if n_jobs == 1 or multicore_failed:\n        from sklearn.manifold import TSNE\n        tsne = TSNE(**params_sklearn)\n        logg.warn('Consider installing the package MulticoreTSNE.\\n'\n                  '    https://github.com/DmitryUlyanov/Multicore-TSNE\\n'\n                  'Even for `n_jobs=1` this speeds up the computation considerably.')\n        logg.info('    using sklearn.manifold.TSNE')\n        X_tsne = tsne.fit_transform(X)\n    # update AnnData instance\n    adata.smp['X_tsne'] = X_tsne\n    logg.m('    finished', t=True, end=' ')\n    logg.m('and added\\n'\n           '    \"X_tsne\", tSNE coordinates (adata.smp)')\n    return adata if copy else None", "idx": 535}
{"project": "Scanpy", "commit_id": "773_scanpy_1.3.7_scanpy_readwrite.py_read_10x_h5.py", "target": 1, "func": "def read_10x_h5(filename, genome='mm10', gex_only=True):\n    \"\"\"Read 10x-Genomics-formatted hdf5 file.\n    Parameters\n    ----------\n    filename : `str` | :class:`~pathlib.Path`\n        Filename.\n    genome : `str`, optional (default: ``'mm10'``)\n        Genome group in hdf5 file.\n    gex_only : `bool`, optional (default: `True`)\n        Only keep 'Gene Expression' data and ignore other feature types,\n        e.g. 'Antibody Capture', 'CRISPR Guide Capture', or 'Custom'\n    Returns\n    -------\n    adata : :class:`~anndata.AnnData`\n        Annotated data matrix, where obsevations/cells are named by their\n        barcode and variables/genes by gene name. The data matrix is stored in\n        `adata.X`, cell names in `adata.obs_names` and gene names in\n        `adata.var_names`. The gene IDs are stored in `adata.var['gene_ids']`.\n        The feature types are stored in `adata.var['feature_types']`\n    \"\"\"\n    logg.info('reading', filename, r=True, end=' ')\n    with tables.open_file(str(filename), 'r') as f:\n        if '/matrix' in f:\n            adata = _read_v3_10x_h5(filename)\n            if not gex_only:\n                return adata    # ignore the `genome` argument\n            else:\n                adata = adata[:, list(map(lambda x: x == 'Gene Expression', adata.var['feature_types']))]\n                adata = adata[:, list(map(lambda x: x == str(genome), adata.var['genome']))]\n                return adata\n        else:\n            return _read_legacy_10x_h5(filename, genome=genome)", "idx": 538}
{"project": "Scanpy", "commit_id": "149_scanpy_1.9.0_pl.py_sam.py", "target": 0, "func": "def sam(\n    adata: AnnData,\n    projection: Union[str, np.ndarray] = 'X_umap',\n    c: Optional[Union[str, np.ndarray]] = None,\n    cmap: str = 'Spectral_r',\n    linewidth: float = 0.0,\n    edgecolor: str = 'k',\n    axes: Optional[Axes] = None,\n    colorbar: bool = True,\n    s: float = 10.0,\n    **kwargs: Any,\n) -> Axes:\n    \"\"\"\\\n    Scatter plot using the SAM projection or another input projection.\n\n    Parameters\n    ----------\n    projection\n        A case-sensitive string indicating the projection to display (a key\n        in adata.obsm) or a 2D numpy array with cell coordinates. If None,\n        projection defaults to UMAP.\n    c\n        Cell color values overlaid on the projection. Can be a string from adata.obs\n        to overlay cluster assignments / annotations or a 1D numpy array.\n    axes\n        Plot output to the specified, existing axes. If None, create new\n        figure window.\n    kwargs\n        all keyword arguments in matplotlib.pyplot.scatter are eligible.\n    \"\"\"\n\n    if isinstance(projection, str):\n        try:\n            dt = adata.obsm[projection]\n        except KeyError:\n            raise ValueError(\n                'Please create a projection first using run_umap or run_tsne'\n            )\n    else:\n        dt = projection\n\n    if axes is None:\n        plt.figure()\n        axes = plt.gca()\n\n    if c is None:\n        axes.scatter(\n            dt[:, 0], dt[:, 1], s=s, linewidth=linewidth, edgecolor=edgecolor, **kwargs\n        )\n        return axes\n\n    if isinstance(c, str):\n        try:\n            c = np.array(list(adata.obs[c]))\n        except KeyError:\n            pass\n\n    if isinstance(c[0], (str, np.str_)) and isinstance(c, (np.ndarray, list)):\n        import samalg.utilities as ut\n\n        i = ut.convert_annotations(c)\n        ui, ai = np.unique(i, return_index=True)\n        cax = axes.scatter(\n            dt[:, 0],\n            dt[:, 1],\n            c=i,\n            cmap=cmap,\n            s=s,\n            linewidth=linewidth,\n            edgecolor=edgecolor,\n            **kwargs,\n        )\n\n        if colorbar:\n            cbar = plt.colorbar(cax, ax=axes, ticks=ui)\n            cbar.ax.set_yticklabels(c[ai])\n    else:\n        if not isinstance(c, (np.ndarray, list)):\n            colorbar = False\n        i = c\n\n        cax = axes.scatter(\n            dt[:, 0],\n            dt[:, 1],\n            c=i,\n            cmap=cmap,\n            s=s,\n            linewidth=linewidth,\n            edgecolor=edgecolor,\n            **kwargs,\n        )\n\n        if colorbar:\n            plt.colorbar(cax, ax=axes)\n    return axes", "idx": 540}
{"project": "Scanpy", "commit_id": "857_scanpy_1.9.0__score_genes.py_score_genes.py", "target": 0, "func": "def score_genes(\n    adata: AnnData,\n    gene_list: Sequence[str],\n    ctrl_size: int = 50,\n    gene_pool: Optional[Sequence[str]] = None,\n    n_bins: int = 25,\n    score_name: str = 'score',\n    random_state: AnyRandom = 0,\n    copy: bool = False,\n    use_raw: Optional[bool] = None,\n) -> Optional[AnnData]:\n    \"\"\"\\\n    Score a set of genes [Satija15]_.\n\n    The score is the average expression of a set of genes subtracted with the\n    average expression of a reference set of genes. The reference set is\n    randomly sampled from the `gene_pool` for each binned expression value.\n\n    This reproduces the approach in Seurat [Satija15]_ and has been implemented\n    for Scanpy by Davide Cittaro.\n\n    Parameters\n    ----------\n    adata\n        The annotated data matrix.\n    gene_list\n        The list of gene names used for score calculation.\n    ctrl_size\n        Number of reference genes to be sampled from each bin. If `len(gene_list)` is not too\n        low, you can set `ctrl_size=len(gene_list)`.\n    gene_pool\n        Genes for sampling the reference set. Default is all genes.\n    n_bins\n        Number of expression level bins for sampling.\n    score_name\n        Name of the field to be added in `.obs`.\n    random_state\n        The random seed for sampling.\n    copy\n        Copy `adata` or modify it inplace.\n    use_raw\n        Whether to use `raw` attribute of `adata`. Defaults to `True` if `.raw` is present.\n\n        .. versionchanged:: 1.4.5\n           Default value changed from `False` to `None`.\n\n    Returns\n    -------\n    Depending on `copy`, returns or updates `adata` with an additional field\n    `score_name`.\n\n    Examples\n    --------\n    See this `notebook <https://github.com/theislab/scanpy_usage/tree/master/180209_cell_cycle>`__.\n    \"\"\"\n    start = logg.info(f'computing score {score_name!r}')\n    adata = adata.copy() if copy else adata\n    use_raw = _check_use_raw(adata, use_raw)\n\n    if random_state is not None:\n        np.random.seed(random_state)\n\n    gene_list_in_var = []\n    var_names = adata.raw.var_names if use_raw else adata.var_names\n    genes_to_ignore = []\n    for gene in gene_list:\n        if gene in var_names:\n            gene_list_in_var.append(gene)\n        else:\n            genes_to_ignore.append(gene)\n    if len(genes_to_ignore) > 0:\n        logg.warning(f'genes are not in var_names and ignored: {genes_to_ignore}')\n    gene_list = set(gene_list_in_var[:])\n\n    if len(gene_list) == 0:\n        raise ValueError(\"No valid genes were passed for scoring.\")\n\n    if gene_pool is None:\n        gene_pool = list(var_names)\n    else:\n        gene_pool = [x for x in gene_pool if x in var_names]\n    if not gene_pool:\n        raise ValueError(\"No valid genes were passed for reference set.\")\n\n    # Trying here to match the Seurat approach in scoring cells.\n    # Basically we need to compare genes against random genes in a matched\n    # interval of expression.\n\n    _adata = adata.raw if use_raw else adata\n    _adata_subset = (\n        _adata[:, gene_pool] if len(gene_pool) < len(_adata.var_names) else _adata\n    )\n    if issparse(_adata_subset.X):\n        obs_avg = pd.Series(\n            np.array(_sparse_nanmean(_adata_subset.X, axis=0)).flatten(),\n            index=gene_pool,\n        )  # average expression of genes\n    else:\n        obs_avg = pd.Series(\n            np.nanmean(_adata_subset.X, axis=0), index=gene_pool\n        )  # average expression of genes\n\n    obs_avg = obs_avg[\n        np.isfinite(obs_avg)\n    ]  # Sometimes (and I don't know how) missing data may be there, with nansfor\n\n    n_items = int(np.round(len(obs_avg) / (n_bins - 1)))\n    obs_cut = obs_avg.rank(method='min') // n_items\n    control_genes = set()\n\n    # now pick `ctrl_size` genes from every cut\n    for cut in np.unique(obs_cut.loc[gene_list]):\n        r_genes = np.array(obs_cut[obs_cut == cut].index)\n        np.random.shuffle(r_genes)\n        # uses full r_genes if ctrl_size > len(r_genes)\n        control_genes.update(set(r_genes[:ctrl_size]))\n\n    # To index, we need a list \u2013 indexing implies an order.\n    control_genes = list(control_genes - gene_list)\n    gene_list = list(gene_list)\n\n    X_list = _adata[:, gene_list].X\n    if issparse(X_list):\n        X_list = np.array(_sparse_nanmean(X_list, axis=1)).flatten()\n    else:\n        X_list = np.nanmean(X_list, axis=1, dtype='float64')\n\n    X_control = _adata[:, control_genes].X\n    if issparse(X_control):\n        X_control = np.array(_sparse_nanmean(X_control, axis=1)).flatten()\n    else:\n        X_control = np.nanmean(X_control, axis=1, dtype='float64')\n\n    score = X_list - X_control\n\n    adata.obs[score_name] = pd.Series(\n        np.array(score).ravel(), index=adata.obs_names, dtype='float64'\n    )\n\n    logg.info(\n        '    finished',\n        time=start,\n        deep=(\n            'added\\n'\n            f'    {score_name!r}, score of gene set (adata.obs).\\n'\n            f'    {len(control_genes)} total control genes are used.'\n        ),\n    )\n    return adata if copy else None", "idx": 541}
{"project": "Scanpy", "commit_id": "502_scanpy_1.9.0_test_embedding.py_test_tsne.py", "target": 0, "func": "def test_tsne():\n    pbmc = pbmc68k_reduced()\n\n    euclidean1 = sc.tl.tsne(pbmc, metric=\"euclidean\", copy=True)\n    with pytest.warns(UserWarning, match=\"In previous versions of scanpy\"):\n        euclidean2 = sc.tl.tsne(pbmc, metric=\"euclidean\", n_jobs=2, copy=True)\n    cosine = sc.tl.tsne(pbmc, metric=\"cosine\", copy=True)\n\n    # Reproducibility\n    np.testing.assert_equal(euclidean1.obsm[\"X_tsne\"], euclidean2.obsm[\"X_tsne\"])\n    # Metric has some effect\n    assert not np.array_equal(euclidean1.obsm[\"X_tsne\"], cosine.obsm[\"X_tsne\"])\n\n    # Params are recorded\n    assert euclidean1.uns[\"tsne\"][\"params\"][\"n_jobs\"] == 1\n    assert euclidean2.uns[\"tsne\"][\"params\"][\"n_jobs\"] == 2\n    assert cosine.uns[\"tsne\"][\"params\"][\"n_jobs\"] == 1\n    assert euclidean1.uns[\"tsne\"][\"params\"][\"metric\"] == \"euclidean\"\n    assert euclidean2.uns[\"tsne\"][\"params\"][\"metric\"] == \"euclidean\"\n    assert cosine.uns[\"tsne\"][\"params\"][\"metric\"] == \"cosine\"", "idx": 545}
{"project": "Scanpy", "commit_id": "109_scanpy_1.9.0__datasets.py_toggleswitch.py", "target": 0, "func": "def toggleswitch() -> ad.AnnData:\n    \"\"\"\\\n    Simulated toggleswitch.\n\n    Data obtained simulating a simple toggleswitch [Gardner00]_\n\n    Simulate via :func:`~scanpy.tl.sim`.\n\n    Returns\n    -------\n    Annotated data matrix.\n    \"\"\"\n    filename = HERE / 'toggleswitch.txt'\n    adata = read(filename, first_column_names=True)\n    adata.uns['iroot'] = 0\n    return adata", "idx": 546}
{"project": "Scanpy", "commit_id": "864_scanpy_1.4.1_scanpy_neighbors___init__.py_get_sparse_matrix_from_indices_distances_umap.py", "target": 1, "func": "def get_sparse_matrix_from_indices_distances_umap(knn_indices, knn_dists, n_obs, n_neighbors):\n    rows = np.zeros((n_obs * n_neighbors), dtype=np.int64)\n    cols = np.zeros((n_obs * n_neighbors), dtype=np.int64)\n    vals = np.zeros((n_obs * n_neighbors), dtype=np.float64)\n    for i in range(knn_indices.shape[0]):\n        for j in range(n_neighbors):\n            if knn_indices[i, j] == -1:\n                continue  # We didn't get the full knn for i\n            if knn_indices[i, j] == i:\n                val = 0.0\n            else:\n                val = knn_dists[i, j]\n            rows[i * n_neighbors + j] = i\n            cols[i * n_neighbors + j] = knn_indices[i, j]\n            vals[i * n_neighbors + j] = val\n\n    result = coo_matrix((vals, (rows, cols)),\n                                      shape=(n_obs, n_obs))\n    return result.tocsr()", "idx": 547}
{"project": "Scanpy", "commit_id": "816_scanpy_1.4_scanpy_plotting__top_genes_visual.py__tail_var_estimate.py", "target": 1, "func": "def _tail_var_estimate(adata, mask, model='rough'):\n    # Method ZINB will be implemented soon\n    if model not in {'rough', 'zinb'}:\n        model = 'rough'\n        logg.warn('Model should be either rough or zinb (zero-inflated negative binomial)')\n    X = adata.X[mask, :]\n    n_cells = X.shape[0]\n    n_genes = X.shape[1]\n    variances = np.zeros((n_genes,))\n    if model is 'rough':\n        if issparse(X):\n            n_nonzero_elements = X.getnnz(axis=0)\n            # More efficient to use in flattened form, use indexing. Since highly sparsified, no memory issue\n            # Note that fulldata is flattened\n            fulldata = X.data\n            left = 0\n            right = 0\n            for i, j in enumerate(n_nonzero_elements):\n                right = right + j\n                variances[i] = np.var(fulldata[left:right])\n                left = right\n        else:\n            # non-sparse version\n            n_nonzero_elements = np.count_nonzero(X, axis=0)\n            variances = np.var(X, axis=0)\n    else:\n        # ZINB will be implemented soon\n        return 0\n    return variances", "idx": 558}
{"project": "Scanpy", "commit_id": "3_scanpy_0.0_scanpy_exs___init__.py_dexdata.py", "target": 1, "func": "def dexdata():\n    \"\"\" Example data. \"\"\"\n    all_dex = utils.merge_dicts(builtin.dexdata, user.dexdata)\n    try:\n        # additional possibility to add example module\n        from . import user_private\n        all_dex = utils.merge_dicts(all_dex, user_private.dexdata)\n    except:\n        pass\n    return all_dex", "idx": 565}
{"project": "Scanpy", "commit_id": "1060_scanpy_1.6.1_scanpy_plotting__anndata.py_heatmap.py", "target": 1, "func": "def heatmap(\n    adata: AnnData,\n    var_names: Union[_VarNames, Mapping[str, _VarNames]],\n    groupby: Union[str, Sequence[str]],\n    use_raw: Optional[bool] = None,\n    log: bool = False,\n    num_categories: int = 7,\n    dendrogram: Union[bool, str] = False,\n    gene_symbols: Optional[str] = None,\n    var_group_positions: Optional[Sequence[Tuple[int, int]]] = None,\n    var_group_labels: Optional[Sequence[str]] = None,\n    var_group_rotation: Optional[float] = None,\n    layer: Optional[str] = None,\n    standard_scale: Optional[Literal['var', 'obs']] = None,\n    swap_axes: bool = False,\n    show_gene_labels: Optional[bool] = None,\n    show: Optional[bool] = None,\n    save: Union[str, bool, None] = None,\n    figsize: Optional[Tuple[float, float]] = None,\n    **kwds,\n):\n    \"\"\"\\\n    Heatmap of the expression values of genes.\n    If `groupby` is given, the heatmap is ordered by the respective group. For\n    example, a list of marker genes can be plotted, ordered by clustering. If\n    the `groupby` observation annotation is not categorical the observation\n    annotation is turned into a categorical by binning the data into the number\n    specified in `num_categories`.\n    Parameters\n    ----------\n    {common_plot_args}\n    standard_scale\n        Whether or not to standardize that dimension between 0 and 1, meaning for each variable or observation,\n        subtract the minimum and divide each by its maximum.\n    swap_axes\n         By default, the x axis contains `var_names` (e.g. genes) and the y axis the `groupby`\n         categories (if any). By setting `swap_axes` then x are the `groupby` categories and y the `var_names`.\n    show_gene_labels\n         By default gene labels are shown when there are 50 or less genes. Otherwise the labels are removed.\n    {show_save_ax}\n    **kwds\n        Are passed to :func:`matplotlib.pyplot.imshow`.\n    Returns\n    -------\n    List of :class:`~matplotlib.axes.Axes`\n    Examples\n    -------\n    >>> import scanpy as sc\n    >>> adata = sc.datasets.pbmc68k_reduced()\n    >>> markers = ['C1QA', 'PSAP', 'CD79A', 'CD79B', 'CST3', 'LYZ']\n    >>> sc.pl.heatmap(adata, markers, groupby='bulk_labels', dendrogram=True, swap_axes=True)\n    Using var_names as dict:\n    >>> markers = {{'T-cell': 'CD3D', 'B-cell': 'CD79A', 'myeloid': 'CST3'}}\n    >>> sc.pl.heatmap(adata, markers, groupby='bulk_labels', dendrogram=True)\n    See also\n    --------\n    rank_genes_groups_heatmap: to plot marker genes identified using the :func:`~scanpy.tl.rank_genes_groups` function.\n    \"\"\"\n    if use_raw is None and adata.raw is not None:\n        use_raw = True\n    var_names, var_group_labels, var_group_positions = _check_var_names_type(\n        var_names, var_group_labels, var_group_positions\n    )\n    categories, obs_tidy = _prepare_dataframe(\n        adata,\n        var_names,\n        groupby,\n        use_raw,\n        log,\n        num_categories,\n        gene_symbols=gene_symbols,\n        layer=layer,\n    )\n\n    if standard_scale == 'obs':\n        obs_tidy = obs_tidy.sub(obs_tidy.min(1), axis=0)\n        obs_tidy = obs_tidy.div(obs_tidy.max(1), axis=0).fillna(0)\n    elif standard_scale == 'var':\n        obs_tidy -= obs_tidy.min(0)\n        obs_tidy = (obs_tidy / obs_tidy.max(0)).fillna(0)\n    elif standard_scale is None:\n        pass\n    else:\n        logg.warning('Unknown type for standard_scale, ignored')\n    if groupby is None or len(categories) <= 1:\n        categorical = False\n        # dendrogram can only be computed  between groupby categories\n        dendrogram = False\n    else:\n        categorical = True\n        # get categories colors:\n        if groupby + \"_colors\" in adata.uns:\n            groupby_colors = adata.uns[groupby + \"_colors\"]\n        else:\n            groupby_colors = None\n    if dendrogram:\n        dendro_data = _reorder_categories_after_dendrogram(\n            adata,\n            groupby,\n            dendrogram,\n            var_names=var_names,\n            var_group_labels=var_group_labels,\n            var_group_positions=var_group_positions,\n            categories=categories,\n        )\n        var_group_labels = dendro_data['var_group_labels']\n        var_group_positions = dendro_data['var_group_positions']\n        # reorder obs_tidy\n        if dendro_data['var_names_idx_ordered'] is not None:\n            obs_tidy = obs_tidy.iloc[:, dendro_data['var_names_idx_ordered']]\n            var_names = [var_names[x] for x in dendro_data['var_names_idx_ordered']]\n        obs_tidy.index = obs_tidy.index.reorder_categories(\n            [categories[x] for x in dendro_data['categories_idx_ordered']],\n            ordered=True,\n        )\n        # reorder groupby colors\n        if groupby_colors is not None:\n            groupby_colors = [\n                groupby_colors[x] for x in dendro_data['categories_idx_ordered']\n            ]\n    if show_gene_labels is None:\n        if len(var_names) <= 50:\n            show_gene_labels = True\n        else:\n            show_gene_labels = False\n            logg.warning(\n                'Gene labels are not shown when more than 50 genes are visualized. '\n                'To show gene labels set `show_gene_labels=True`'\n            )\n    if categorical:\n        obs_tidy = obs_tidy.sort_index()\n    colorbar_width = 0.2\n    if not swap_axes:\n        # define a layout of 2 rows x 4 columns\n        # first row is for 'brackets' (if no brackets needed, the height of this row is zero)\n        # second row is for main content. This second row is divided into three axes:\n        #   first ax is for the categories defined by `groupby`\n        #   second ax is for the heatmap\n        #   third ax is for the dendrogram\n        #   fourth ax is for colorbar\n        dendro_width = 1 if dendrogram else 0\n        groupby_width = 0.2 if categorical else 0\n        if figsize is None:\n            height = 6\n            if show_gene_labels:\n                heatmap_width = len(var_names) * 0.3\n            else:\n                heatmap_width = 8\n            width = heatmap_width + dendro_width + groupby_width\n        else:\n            width, height = figsize\n            heatmap_width = width - (dendro_width + groupby_width)\n        if var_group_positions is not None and len(var_group_positions) > 0:\n            # add some space in case 'brackets' want to be plotted on top of the image\n            height_ratios = [0.15, height]\n        else:\n            height_ratios = [0, height]\n        width_ratios = [\n            groupby_width,\n            heatmap_width,\n            dendro_width,\n            colorbar_width,\n        ]\n        fig = pl.figure(figsize=(width, height))\n        axs = gridspec.GridSpec(\n            nrows=2,\n            ncols=4,\n            width_ratios=width_ratios,\n            wspace=0.15 / width,\n            hspace=0.13 / height,\n            height_ratios=height_ratios,\n        )\n        heatmap_ax = fig.add_subplot(axs[1, 1])\n        kwds.setdefault('interpolation', 'nearest')\n        im = heatmap_ax.imshow(obs_tidy.values, aspect='auto', **kwds)\n        heatmap_ax.set_ylim(obs_tidy.shape[0] - 0.5, -0.5)\n        heatmap_ax.set_xlim(-0.5, obs_tidy.shape[1] - 0.5)\n        heatmap_ax.tick_params(axis='y', left=False, labelleft=False)\n        heatmap_ax.set_ylabel('')\n        heatmap_ax.grid(False)\n        # sns.heatmap(obs_tidy, yticklabels=\"auto\", ax=heatmap_ax, cbar_ax=heatmap_cbar_ax, **kwds)\n        if show_gene_labels:\n            heatmap_ax.tick_params(axis='x', labelsize='small')\n            heatmap_ax.set_xticks(np.arange(len(var_names)))\n            heatmap_ax.set_xticklabels(var_names, rotation=90)\n        else:\n            heatmap_ax.tick_params(axis='x', labelbottom=False, bottom=False)\n\n        # plot colorbar\n        _plot_colorbar(im, fig, axs[1, 3])\n\n        if categorical:\n            groupby_ax = fig.add_subplot(axs[1, 0])\n            ticks, labels, groupby_cmap, norm = _plot_categories_as_colorblocks(\n                groupby_ax, obs_tidy, colors=groupby_colors, orientation='left'\n            )\n\n            # add lines to main heatmap\n            line_positions = (\n                np.cumsum(obs_tidy.index.value_counts(sort=False))[:-1] - 0.5\n            )\n            heatmap_ax.hlines(\n                line_positions,\n                -0.73,\n                len(var_names) - 0.5,\n                lw=0.6,\n                zorder=10,\n                clip_on=False,\n            )\n        if dendrogram:\n            dendro_ax = fig.add_subplot(axs[1, 2], sharey=heatmap_ax)\n            _plot_dendrogram(\n                dendro_ax, adata, groupby, ticks=ticks, dendrogram_key=dendrogram\n            )\n        # plot group legends on top of heatmap_ax (if given)\n        if var_group_positions is not None and len(var_group_positions) > 0:\n            gene_groups_ax = fig.add_subplot(axs[0, 1], sharex=heatmap_ax)\n            _plot_gene_groups_brackets(\n                gene_groups_ax,\n                group_positions=var_group_positions,\n                group_labels=var_group_labels,\n                rotation=var_group_rotation,\n                left_adjustment=-0.3,\n                right_adjustment=0.3,\n            )\n    # swap axes case\n    else:\n        # define a layout of 3 rows x 3 columns\n        # The first row is for the dendrogram (if not dendrogram height is zero)\n        # second row is for main content. This col is divided into three axes:\n        #   first ax is for the heatmap\n        #   second ax is for 'brackets' if any (othwerise width is zero)\n        #   third ax is for colorbar\n        dendro_height = 0.8 if dendrogram else 0\n        groupby_height = 0.13 if categorical else 0\n        if figsize is None:\n            if show_gene_labels:\n                heatmap_height = len(var_names) * 0.18\n            else:\n                heatmap_height = 4\n            width = 10\n            height = heatmap_height + dendro_height + groupby_height\n        else:\n            width, height = figsize\n            heatmap_height = height - (dendro_height + groupby_height)\n        height_ratios = [dendro_height, heatmap_height, groupby_height]\n        if var_group_positions is not None and len(var_group_positions) > 0:\n            # add some space in case 'brackets' want to be plotted on top of the image\n            width_ratios = [width, 0.14, colorbar_width]\n        else:\n            width_ratios = [width, 0, colorbar_width]\n        fig = pl.figure(figsize=(width, height))\n        axs = gridspec.GridSpec(\n            nrows=3,\n            ncols=3,\n            wspace=0.25 / width,\n            hspace=0.3 / height,\n            width_ratios=width_ratios,\n            height_ratios=height_ratios,\n        )\n        # plot heatmap\n        heatmap_ax = fig.add_subplot(axs[1, 0])\n        kwds.setdefault('interpolation', 'nearest')\n        im = heatmap_ax.imshow(obs_tidy.T.values, aspect='auto', **kwds)\n        heatmap_ax.set_xlim(0, obs_tidy.shape[0])\n        heatmap_ax.set_ylim(obs_tidy.shape[1] - 0.5, -0.5)\n        heatmap_ax.tick_params(axis='x', bottom=False, labelbottom=False)\n        heatmap_ax.set_xlabel('')\n        heatmap_ax.grid(False)\n        if show_gene_labels:\n            heatmap_ax.tick_params(axis='y', labelsize='small', length=1)\n            heatmap_ax.set_yticks(np.arange(len(var_names)))\n            heatmap_ax.set_yticklabels(var_names, rotation=0)\n        else:\n            heatmap_ax.tick_params(axis='y', labelleft=False, left=False)\n\n        if categorical:\n            groupby_ax = fig.add_subplot(axs[2, 0])\n            ticks, labels, groupby_cmap, norm = _plot_categories_as_colorblocks(\n                groupby_ax, obs_tidy, colors=groupby_colors, orientation='bottom'\n            )\n            # add lines to main heatmap\n            line_positions = (\n                np.cumsum(obs_tidy.index.value_counts(sort=False))[:-1] - 0.5\n            )\n            heatmap_ax.vlines(\n                line_positions,\n                -0.5,\n                len(var_names) + 0.35,\n                lw=0.6,\n                zorder=10,\n                clip_on=False,\n            )\n        if dendrogram:\n            dendro_ax = fig.add_subplot(axs[0, 0], sharex=heatmap_ax)\n            _plot_dendrogram(\n                dendro_ax,\n                adata,\n                groupby,\n                dendrogram_key=dendrogram,\n                ticks=ticks,\n                orientation='top',\n            )\n        # plot group legends next to the heatmap_ax (if given)\n        if var_group_positions is not None and len(var_group_positions) > 0:\n            gene_groups_ax = fig.add_subplot(axs[1, 1])\n            arr = []\n            for idx, pos in enumerate(var_group_positions):\n                arr += [idx] * (pos[1] + 1 - pos[0])\n\n            gene_groups_ax.imshow(\n                np.array([arr]).T, aspect='auto', cmap=groupby_cmap, norm=norm\n            )\n            gene_groups_ax.axis('off')\n        # plot colorbar\n        _plot_colorbar(im, fig, axs[1, 2])\n    return_ax_dict = {'heatmap_ax': heatmap_ax}\n    if categorical:\n        return_ax_dict['groupby_ax'] = groupby_ax\n    if dendrogram:\n        return_ax_dict['dendrogram_ax'] = dendro_ax\n    if var_group_positions is not None and len(var_group_positions) > 0:\n        return_ax_dict['gene_groups_ax'] = gene_groups_ax\n    _utils.savefig_or_show('heatmap', show=show, save=save)\n    show = settings.autoshow if show is None else show\n    if not show:\n        return return_ax_dict", "idx": 575}
{"project": "Scanpy", "commit_id": "29_scanpy_1.9.0_cli.py___delitem__.py", "target": 0, "func": "def __delitem__(self, k: str) -> None:\n        del self.parser_map[k]", "idx": 583}
{"project": "Scanpy", "commit_id": "156_scanpy_1.9.0__hashsolo.py__calculate_log_likelihoods.py", "target": 0, "func": "def _calculate_log_likelihoods(data, number_of_noise_barcodes):\n    \"\"\"Calculate log likelihoods for each hypothesis, negative, singlet, doublet\n\n    Parameters\n    ----------\n    data : np.ndarray\n        cells by hashing counts matrix\n    number_of_noise_barcodes : int,\n        number of barcodes to used to calculated noise distribution\n\n    Returns\n    -------\n    log_likelihoods_for_each_hypothesis : np.ndarray\n        a 2d np.array log likelihood of each hypothesis\n    all_indices\n    counter_to_barcode_combo\n    \"\"\"\n\n    def gaussian_updates(data, mu_o, std_o):\n        \"\"\"Update parameters of your gaussian\n        https://www.cs.ubc.ca/~murphyk/Papers/bayesGauss.pdf\n\n        Parameters\n        ----------\n        data : np.array\n            1-d array of counts\n        mu_o : float,\n            global mean for hashing count distribution\n        std_o : float,\n            global std for hashing count distribution\n\n        Returns\n        -------\n        float\n            mean of gaussian\n        float\n            std of gaussian\n        \"\"\"\n        lam_o = 1 / (std_o**2)\n        n = len(data)\n        lam = 1 / np.var(data) if len(data) > 1 else lam_o\n        lam_n = lam_o + n * lam\n        mu_n = (\n            (np.mean(data) * n * lam + mu_o * lam_o) / lam_n if len(data) > 0 else mu_o\n        )\n        return mu_n, (1 / (lam_n / (n + 1))) ** (1 / 2)\n\n    eps = 1e-15\n    # probabilites for negative, singlet, doublets\n    log_likelihoods_for_each_hypothesis = np.zeros((data.shape[0], 3))\n\n    all_indices = np.empty(data.shape[0])\n    num_of_barcodes = data.shape[1]\n    number_of_non_noise_barcodes = (\n        num_of_barcodes - number_of_noise_barcodes\n        if number_of_noise_barcodes is not None\n        else 2\n    )\n\n    num_of_noise_barcodes = num_of_barcodes - number_of_non_noise_barcodes\n\n    # assume log normal\n    data = np.log(data + 1)\n    data_arg = np.argsort(data, axis=1)\n    data_sort = np.sort(data, axis=1)\n\n    # global signal and noise counts useful for when we have few cells\n    # barcodes with the highest number of counts are assumed to be a true signal\n    # barcodes with rank < k are considered to be noise\n    global_signal_counts = np.ravel(data_sort[:, -1])\n    global_noise_counts = np.ravel(data_sort[:, :-number_of_non_noise_barcodes])\n    global_mu_signal_o, global_sigma_signal_o = (\n        np.mean(global_signal_counts),\n        np.std(global_signal_counts),\n    )\n    global_mu_noise_o, global_sigma_noise_o = (\n        np.mean(global_noise_counts),\n        np.std(global_noise_counts),\n    )\n\n    noise_params_dict = {}\n    signal_params_dict = {}\n\n    # for each barcode get  empirical noise and signal distribution parameterization\n    for x in np.arange(num_of_barcodes):\n        sample_barcodes = data[:, x]\n        sample_barcodes_noise_idx = np.where(data_arg[:, :num_of_noise_barcodes] == x)[\n            0\n        ]\n        sample_barcodes_signal_idx = np.where(data_arg[:, -1] == x)\n\n        # get noise and signal counts\n        noise_counts = sample_barcodes[sample_barcodes_noise_idx]\n        signal_counts = sample_barcodes[sample_barcodes_signal_idx]\n\n        # get parameters of distribution, assuming lognormal do update from global values\n        noise_param = gaussian_updates(\n            noise_counts, global_mu_noise_o, global_sigma_noise_o\n        )\n        signal_param = gaussian_updates(\n            signal_counts, global_mu_signal_o, global_sigma_signal_o\n        )\n        noise_params_dict[x] = noise_param\n        signal_params_dict[x] = signal_param\n\n    counter_to_barcode_combo = {}\n    counter = 0\n\n    # for each combination of noise and signal barcode calculate probiltiy of in silico and real cell hypotheses\n    for noise_sample_idx, signal_sample_idx in product(\n        np.arange(num_of_barcodes), np.arange(num_of_barcodes)\n    ):\n        signal_subset = data_arg[:, -1] == signal_sample_idx\n        noise_subset = data_arg[:, -2] == noise_sample_idx\n        subset = signal_subset & noise_subset\n        if sum(subset) == 0:\n            continue\n\n        indices = np.where(subset)[0]\n        barcode_combo = \"_\".join([str(noise_sample_idx), str(signal_sample_idx)])\n        all_indices[np.where(subset)[0]] = counter\n        counter_to_barcode_combo[counter] = barcode_combo\n        counter += 1\n        noise_params = noise_params_dict[noise_sample_idx]\n        signal_params = signal_params_dict[signal_sample_idx]\n\n        # calculate probabilties for each hypothesis for each cell\n        data_subset = data[subset]\n        log_signal_signal_probs = np.log(\n            norm.pdf(\n                data_subset[:, signal_sample_idx],\n                *signal_params[:-2],\n                loc=signal_params[-2],\n                scale=signal_params[-1],\n            )\n            + eps\n        )\n        signal_noise_params = signal_params_dict[noise_sample_idx]\n        log_noise_signal_probs = np.log(\n            norm.pdf(\n                data_subset[:, noise_sample_idx],\n                loc=signal_noise_params[-2],\n                scale=signal_noise_params[-1],\n            )\n            + eps\n        )\n\n        log_noise_noise_probs = np.log(\n            norm.pdf(\n                data_subset[:, noise_sample_idx],\n                loc=noise_params[-2],\n                scale=noise_params[-1],\n            )\n            + eps\n        )\n        log_signal_noise_probs = np.log(\n            norm.pdf(\n                data_subset[:, signal_sample_idx],\n                loc=noise_params[-2],\n                scale=noise_params[-1],\n            )\n            + eps\n        )\n\n        probs_of_negative = np.sum(\n            [log_noise_noise_probs, log_signal_noise_probs], axis=0\n        )\n        probs_of_singlet = np.sum(\n            [log_noise_noise_probs, log_signal_signal_probs], axis=0\n        )\n        probs_of_doublet = np.sum(\n            [log_noise_signal_probs, log_signal_signal_probs], axis=0\n        )\n        log_probs_list = [probs_of_negative, probs_of_singlet, probs_of_doublet]\n\n        # each cell and each hypothesis probability\n        for prob_idx, log_prob in enumerate(log_probs_list):\n            log_likelihoods_for_each_hypothesis[indices, prob_idx] = log_prob\n    return (\n        log_likelihoods_for_each_hypothesis,\n        all_indices,\n        counter_to_barcode_combo,", "idx": 588}
{"project": "Scanpy", "commit_id": "901_scanpy_1.4.3_scanpy_datasets__ebi_expression_atlas.py_read_expression_from_archive.py", "target": 1, "func": "def read_expression_from_archive(archive: ZipFile) -> anndata.AnnData:\n    info = archive.infolist()\n    assert len(info) == 3\n    mtx_data_info = next(i for i in info if i.filename.endswith(\".mtx\"))\n    mtx_rows_info = next(i for i in info if i.filename.endswith(\".mtx_rows\"))\n    mtx_cols_info = next(i for i in info if i.filename.endswith(\".mtx_cols\"))\n    with archive.open(mtx_data_info, \"r\") as f:\n        expr = read_mtx_from_stream(f)\n    with archive.open(mtx_rows_info, \"r\") as f:\n        varname = pd.read_csv(f, sep=\"\\t\", header=None)[1]\n    with archive.open(mtx_cols_info, \"r\") as f:\n        obsname = pd.read_csv(f, sep=\"\\t\", header=None)[1]\n    adata = anndata.AnnData(expr)\n    adata.var_names = varname\n    adata.obs_names = obsname\n    return adata", "idx": 591}
{"project": "Scanpy", "commit_id": "496_scanpy_0.4.2_scanpy_tools_rank_genes_groups.py_rank_genes_groups.py", "target": 1, "func": "def rank_genes_groups(\n        adata,\n        group_by,\n        use_raw=True,\n        groups='all',\n        reference='rest',\n        n_genes=100,\n        compute_distribution=False,\n        only_positive=True,\n        copy=False,\n        test_type='t-test_overestim_var'):\n    \"\"\"Rank genes according to differential expression [Wolf17]_.\n    Rank genes by differential expression. By default, a t-test-like ranking is\n    used, in which means are normalized with variances.\n    Parameters\n    ----------\n    adata : :class:`~scanpy.api.AnnData`\n        Annotated data matrix.\n    group_by : `str`\n        The key of the sample grouping to consider.\n    use_raw : `bool`, optional (default: `True`)\n        Use `raw` attribute of `adata` if present.\n    groups : `str`, `list`, optional (default: `'all'`)\n        Subset of groups, e.g. `['g1', 'g2', 'g3']`, to which comparison shall\n        be restricted. If not passed, a ranking will be generated for all\n        groups.\n    reference : `str`, optional (default: `'rest'`)\n        If `'rest'`, compare each group to the union of the rest of the group.  If\n        a group identifier, compare with respect to this group.\n    n_genes : `int`, optional (default: 100)\n        How many genes to rank.\n    test_type : {'t-test_overestim_var', 't-test', 'wilcoxon'}, optional (default: 't-test_overestim_var')\n        If 't-test', use t-test to calculate test statistics. If 'wilcoxon', use\n        Wilcoxon-Rank-Sum to calculate test statistic. If\n        't-test_overestim_var', overestimate variance.\n    only_positive : bool, optional (default: `True`)\n        Only consider positive differences.\n    Returns\n    -------\n    rank_genes_groups_gene_scores : structured `np.ndarray` (adata.uns)\n        Structured array to be indexed by group id of shape storing the zscore\n        for each gene for each group.\n    rank_genes_groups_gene_names : structured `np.ndarray` (adata.uns)\n        Structured array to be indexed by group id for storing the gene names.\n    \"\"\"\n    logg.info('rank differentially expressed genes', r=True)\n    adata = adata.copy() if copy else adata\n    utils.sanitize_anndata(adata)\n    if compute_distribution:\n        logg.warn('`compute_distribution` is deprecated, as it requires storing'\n                  'a shifted and rescaled disribution for each gene'\n                  'You can now run `sc.pl.rank_genes_groups_violin` without it, '\n                  'which will show the original distribution of the gene.')\n    # for clarity, rename variable\n    n_genes_user = n_genes\n    groups_order = groups\n    if isinstance(groups_order, list) and isinstance(groups_order[0], int):\n        groups_order = [str(n) for n in groups_order]\n    if reference != 'rest' and reference not in set(groups_order):\n        groups_order += [reference]\n    if (reference != 'rest'\n        and reference not in set(adata.obs[group_by].cat.categories)):\n        raise ValueError('reference = {} needs to be one of group_by = {}.'\n                         .format(reference,\n                                 adata.obs[group_by].cat.categories.tolist()))\n    groups_order, groups_masks = utils.select_groups(\n        adata, groups_order, group_by)\n    adata.uns['rank_genes_groups_params'] = np.array(\n        (group_by, reference, test_type, use_raw),\n        dtype=[('group_by', 'U50'), ('reference', 'U50'), ('test_type', 'U50'), ('use_raw', np.bool_)])\n    # adata_comp mocks an AnnData object if use_raw is True\n    # otherwise it's just the AnnData object\n    adata_comp = adata\n    if adata.raw is not None and use_raw:\n        adata_comp = adata.raw\n    X = adata_comp.X\n\n    # Make sure indices are not OoB in case there are less genes than n_genes\n    if n_genes > X.shape[1]:\n        n_genes = X.shape[1]\n\n    rankings_gene_zscores = []\n    rankings_gene_names = []\n    n_groups = groups_masks.shape[0]\n    n_genes = X.shape[1]\n    ns = np.zeros(n_groups, dtype=int)\n    for imask, mask in enumerate(groups_masks):\n        ns[imask] = np.where(mask)[0].size\n    logg.info('    consider \\'{}\\':'.format(group_by), groups_order,\n              'with sample numbers', ns)\n    if reference != 'rest':\n        ireference = np.where(groups_order == reference)[0][0]\n    reference_indices = np.arange(adata_comp.n_vars, dtype=int)\n\n    avail_tests = {'t-test', 't-test_overestim_var', 'wilcoxon'}\n    if test_type not in avail_tests:\n        raise ValueError('test_type should be one of {}.'\n                         '\"t-test_overestim_var\" is being used as default.'\n                         .format(avail_tests))\n\n    if test_type in {'t-test', 't-test_overestim_var'}:\n        # loop over all masks and compute means, variances and sample numbers\n        means = np.zeros((n_groups, n_genes))\n        vars = np.zeros((n_groups, n_genes))\n        for imask, mask in enumerate(groups_masks):\n            means[imask], vars[imask] = simple._get_mean_var(X[mask])\n        # test each either against the union of all other groups or against a\n        # specific group\n        for igroup in range(n_groups):\n            if reference == 'rest':\n                mask_rest = ~groups_masks[igroup]\n            else:\n                if igroup == ireference: continue\n                else: mask_rest = groups_masks[ireference]\n            mean_rest, var_rest = simple._get_mean_var(X[mask_rest])\n            if test_type == 't-test':\n                ns_rest = np.where(mask_rest)[0].size\n            else:  # hack for overestimating the variance\n                ns_rest = ns[igroup]\n            denominator = np.sqrt(vars[igroup]/ns[igroup] + var_rest/ns_rest)\n            denominator[np.flatnonzero(denominator == 0)] = np.nan\n            zscores = (means[igroup] - mean_rest) / denominator\n            zscores[np.isnan(zscores)] = 0\n            zscores = zscores if only_positive else np.abs(zscores)\n            partition = np.argpartition(zscores, -n_genes_user)[-n_genes_user:]\n            partial_indices = np.argsort(zscores[partition])[::-1]\n            global_indices = reference_indices[partition][partial_indices]\n            rankings_gene_zscores.append(zscores[global_indices])\n            rankings_gene_names.append(adata_comp.var_names[global_indices])\n            if compute_distribution:\n                mask = groups_masks[igroup]\n                for gene_counter in range(n_genes_user):\n                    gene_idx = global_indices[gene_counter]\n                    X_col = X[mask, gene_idx]\n                    if issparse(X): X_col = X_col.toarray()[:, 0]\n                    identifier = _build_identifier(group_by, groups_order[igroup],\n                                                   gene_counter, adata_comp.var_names[gene_idx])\n                    full_col = np.empty(adata.n_obs)\n                    full_col[:] = np.nan\n                    full_col[mask] = (X_col - mean_rest[gene_idx]) / denominator[gene_idx]\n                    adata.obs[identifier] = full_col\n    elif test_type == 'wilcoxon':\n        # Wilcoxon-rank-sum test is usually more powerful in detecting marker genes\n        # Limit maximal RAM that is required by the calculation. Currently set fixed to roughly 100 MByte\n        CONST_MAX_SIZE = 10000000\n        ns_rest = np.zeros(n_groups, dtype=int)\n        # initialize space for z-scores\n        zscores = np.zeros(n_genes)\n        # First loop: Loop over all genes\n        if reference != 'rest':\n            for imask, mask in enumerate(groups_masks):\n                if imask == ireference: continue\n                else: mask_rest = groups_masks[ireference]\n                ns_rest[imask] = np.where(mask_rest)[0].size\n                if ns_rest[imask] <= 25 or ns[imask] <= 25:\n                    logg.hint('Few observations in a group for '\n                              'normal approximation (<=25). Lower test accuracy.')\n                n_active = ns[imask]\n                m_active = ns_rest[imask]\n                # Now calculate gene expression ranking in chunkes:\n                chunk = []\n                # Calculate chunk frames\n                n_genes_max_chunk = floor(CONST_MAX_SIZE / (n_active + m_active))\n                if n_genes_max_chunk < n_genes - 1:\n                    chunk_index = n_genes_max_chunk\n                    while chunk_index < n_genes - 1:\n                        chunk.append(chunk_index)\n                        chunk_index = chunk_index + n_genes_max_chunk\n                    chunk.append(n_genes - 1)\n                else:\n                    chunk.append(n_genes - 1)\n                left = 0\n                # Calculate rank sums for each chunk for the current mask\n                for chunk_index, right in enumerate(chunk):\n                    # Check if issparse is true: AnnData objects are currently sparse.csr or ndarray.\n                    if issparse(X):\n                        df1 = pd.DataFrame(data=X[mask, left:right].todense())\n                        df2 = pd.DataFrame(data=X[mask_rest, left:right].todense(),\n                                           index=np.arange(start=n_active, stop=n_active + m_active))\n                    else:\n                        df1 = pd.DataFrame(data=X[mask, left:right])\n                        df2 = pd.DataFrame(data=X[mask_rest, left:right],\n                                           index=np.arange(start=n_active, stop=n_active + m_active))\n                    df1 = df1.append(df2)\n                    ranks = df1.rank()\n                    # sum up adjusted_ranks to calculate W_m,n\n                    zscores[left:right] = np.sum(ranks.loc[0:n_active, :])\n                    left = right + 1\n                zscores = (zscores - (n_active * (n_active + m_active + 1) / 2)) / sqrt(\n                    (n_active * m_active * (n_active + m_active + 1) / 12))\n                zscores = zscores if only_positive else np.abs(zscores)\n                zscores[np.isnan(zscores)] = 0\n                partition = np.argpartition(zscores, -n_genes_user)[-n_genes_user:]\n                partial_indices = np.argsort(zscores[partition])[::-1]\n                global_indices = reference_indices[partition][partial_indices]\n                rankings_gene_zscores.append(zscores[global_indices])\n                rankings_gene_names.append(adata_comp.var_names[global_indices])\n                if compute_distribution:\n                    # Add calculation of means, var: (Unnecessary for wilcoxon if compute distribution=False)\n                    mean, vars = simple._get_mean_var(X[mask])\n                    mean_rest, var_rest = simple._get_mean_var(X[mask_rest])\n                    denominator = np.sqrt(vars / ns[imask] + var_rest / ns_rest[imask])\n                    denominator[np.flatnonzero(denominator == 0)] = np.nan\n                    for gene_counter in range(n_genes_user):\n                        gene_idx = global_indices[gene_counter]\n                        X_col = X[mask, gene_idx]\n                        if issparse(X): X_col = X_col.toarray()[:, 0]\n                        identifier = _build_identifier(group_by, groups_order[imask],\n                                                       gene_counter, adata_comp.var_names[gene_idx])\n                        full_col = np.empty(adata.n_obs)\n                        full_col[:] = np.nan\n                        full_col[mask] = (X_col - mean_rest[gene_idx]) / denominator[gene_idx]\n                        adata.obs[identifier] = full_col\n        # If no reference group exists, ranking needs only to be done once (full mask)\n        else:\n            zscores = np.zeros((n_groups, n_genes))\n            chunk = []\n            n_cells = X.shape[0]\n            n_genes_max_chunk = floor(CONST_MAX_SIZE / n_cells)\n            if n_genes_max_chunk < n_genes - 1:\n                chunk_index = n_genes_max_chunk\n                while chunk_index < n_genes - 1:\n                    chunk.append(chunk_index)\n                    chunk_index = chunk_index + n_genes_max_chunk\n                chunk.append(n_genes - 1)\n            else:\n                chunk.append(n_genes - 1)\n            left = 0\n            for chunk_index, right in enumerate(chunk):\n                # Check if issparse is true\n                if issparse(X):\n                    df1 = pd.DataFrame(data=X[:, left:right].todense())\n                else:\n                    df1 = pd.DataFrame(data=X[:, left:right])\n                ranks = df1.rank()\n                # sum up adjusted_ranks to calculate W_m,n\n                for imask, mask in enumerate(groups_masks):\n                    zscores[imask, left:right] = np.sum(ranks.loc[mask, :])\n                left = right + 1\n            for imask, mask in enumerate(groups_masks):\n                zscores[imask, :] = (zscores[imask, :] - (ns[imask] * (n_cells + 1) / 2)) / sqrt(\n                    (ns[imask] * (n_cells - ns[imask]) * (n_cells + 1) / 12))\n                zscores = zscores if only_positive else np.abs(zscores)\n                zscores[np.isnan(zscores)] = 0\n                partition = np.argpartition(zscores[imask, :], -n_genes_user)[-n_genes_user:]\n                partial_indices = np.argsort(zscores[imask, partition])[::-1]\n                global_indices = reference_indices[partition][partial_indices]\n                rankings_gene_zscores.append(zscores[imask, global_indices])\n                rankings_gene_names.append(adata_comp.var_names[global_indices])\n                if compute_distribution:\n                    mean, vars = simple._get_mean_var(X[mask])\n                    mean_rest, var_rest = simple._get_mean_var(X[~mask])\n                    denominator = np.sqrt(vars / ns[imask] + var_rest / (n_cells-ns[imask]))\n                    denominator[np.flatnonzero(denominator == 0)] = np.nan\n                    for gene_counter in range(n_genes_user):\n                        gene_idx = global_indices[gene_counter]\n                        X_col = X[mask, gene_idx]\n                        if issparse(X): X_col = X_col.toarray()[:, 0]\n                        identifier = _build_identifier(group_by, groups_order[imask],\n                                                       gene_counter, adata_comp.var_names[gene_idx])\n                        full_col = np.empty(adata.n_obs)\n                        full_col[:] = np.nan\n                        full_col[mask] = (X_col - mean_rest[gene_idx]) / denominator[gene_idx]\n                        adata.obs[identifier] = full_col\n    groups_order_save = [str(g) for g in groups_order]\n    if reference != 'rest':\n        groups_order_save = [g for g in groups_order if g != reference]\n    adata.uns['rank_genes_groups_gene_scores'] = np.rec.fromarrays(\n        [n for n in rankings_gene_zscores],\n        dtype=[(rn, 'float32') for rn in groups_order_save])\n    adata.uns['rank_genes_groups_gene_names'] = np.rec.fromarrays(\n        [n for n in rankings_gene_names],\n        dtype=[(rn, 'U50') for rn in groups_order_save])\n    logg.info('    finished', time=True, end=' ' if settings.verbosity > 2 else '\\n')\n    logg.hint('added\\n'\n           '    \\'rank_genes_groups_gene_names\\', np.recarray to be indexed by group ids (adata.uns)\\n'\n           '    \\'rank_genes_groups_gene_scores\\', np.recarray to be indexed by group ids (adata.uns)')\n    return adata if copy else None", "idx": 592}
{"project": "Scanpy", "commit_id": "1002_scanpy_1.4.6_scanpy_preprocessing__simple.py_pca.py", "target": 1, "func": "def pca(\n    data: Union[AnnData, np.ndarray, spmatrix],\n    n_comps: Optional[int] = None,\n    zero_center: Optional[bool] = True,\n    svd_solver: str = 'arpack',\n    random_state: AnyRandom = 0,\n    return_info: bool = False,\n    use_highly_variable: Optional[bool] = None,\n    dtype: str = 'float32',\n    copy: bool = False,\n    chunked: bool = False,\n    chunk_size: Optional[int] = None,\n    pca_sparse: bool = False\n) -> Union[AnnData, np.ndarray, spmatrix]:\n    \"\"\"\\\n    Principal component analysis [Pedregosa11]_.\n    Computes PCA coordinates, loadings and variance decomposition.\n    Uses the implementation of *scikit-learn* [Pedregosa11]_.\n    Parameters\n    ----------\n    data\n        The (annotated) data matrix of shape `n_obs` \u00d7 `n_vars`.\n        Rows correspond to cells and columns to genes.\n    n_comps\n        Number of principal components to compute. Defaults to 50, or 1 - minimum\n        dimension size of selected representation.\n    zero_center\n        If `True`, compute standard PCA from covariance matrix.\n        If `False`, omit zero-centering variables\n        (uses :class:`~sklearn.decomposition.TruncatedSVD`),\n        which allows to handle sparse input efficiently.\n        Passing `None` decides automatically based on sparseness of the data.\n    svd_solver\n        SVD solver to use:\n        `'arpack'`\n          for the ARPACK wrapper in SciPy (:func:`~scipy.sparse.linalg.svds`)\n        `'randomized'`\n          for the randomized algorithm due to Halko (2009).\n        `'auto'` (the default)\n          chooses automatically depending on the size of the problem.\n        .. versionchanged:: 1.4.5\n           Default value changed from `'auto'` to `'arpack'`.\n        If `pca_sparse` is `True`, automatically uses the `'arpack'` solver.\n    random_state\n        Change to use different initial states for the optimization.\n    return_info\n        Only relevant when not passing an :class:`~anndata.AnnData`:\n        see \u201c**Returns**\u201d.\n    use_highly_variable\n        Whether to use highly variable genes only, stored in\n        `.var['highly_variable']`.\n        By default uses them if they have been determined beforehand.\n    dtype\n        Numpy data type string to which to convert the result.\n    copy\n        If an :class:`~anndata.AnnData` is passed, determines whether a copy\n        is returned. Is ignored otherwise.\n    chunked\n        If `True`, perform an incremental PCA on segments of `chunk_size`.\n        The incremental PCA automatically zero centers and ignores settings of\n        `random_seed` and `svd_solver`. If `False`, perform a full PCA.\n    chunk_size\n        Number of observations to include in each chunk.\n        Required if `chunked=True` was passed.\n    pca_sparse\n        If `True`, uses the `'arpack'` solver in `scipy.sparse.linalg.svds`\n        with implicit mean centering on the sparse data.\n    Returns\n    -------\n    X_pca : :class:`~scipy.sparse.spmatrix`, :class:`~numpy.ndarray`\n        If `data` is array-like and `return_info=False` was passed,\n        this function only returns `X_pca`\u2026\n    adata : anndata.AnnData\n        \u2026otherwise if `copy=True` it returns or else adds fields to `adata`:\n        `.obsm['X_pca']`\n             PCA representation of data.\n        `.varm['PCs']`\n             The principal components containing the loadings.\n        `.uns['pca']['variance_ratio']`\n             Ratio of explained variance.\n        `.uns['pca']['variance']`\n             Explained variance, equivalent to the eigenvalues of the\n             covariance matrix.\n    \"\"\"\n    # chunked calculation is not randomized, anyways\n    if svd_solver in {'auto', 'randomized'} and not chunked:\n        logg.info(\n            'Note that scikit-learn\\'s randomized PCA might not be exactly '\n            'reproducible across different computational platforms. For exact '\n            'reproducibility, choose `svd_solver=\\'arpack\\'.`'\n        )\n    data_is_AnnData = isinstance(data, AnnData)\n    if data_is_AnnData:\n        adata = data.copy() if copy else data\n    else:\n        adata = AnnData(data)\n    if use_highly_variable is True and 'highly_variable' not in adata.var.keys():\n        raise ValueError('Did not find adata.var[\\'highly_variable\\']. '\n                         'Either your data already only consists of highly-variable genes '\n                         'or consider running `pp.highly_variable_genes` first.')\n    if use_highly_variable is None:\n        use_highly_variable = True if 'highly_variable' in adata.var.keys() else False\n    if use_highly_variable:\n        logg.info('    on highly variable genes')\n    adata_comp = adata[:, adata.var['highly_variable']] if use_highly_variable else adata\n    if n_comps is None:\n        min_dim = min(adata_comp.n_vars, adata_comp.n_obs)\n        if N_PCS >= min_dim:\n            n_comps = min_dim - 1\n        else:\n            n_comps = N_PCS\n    random_state = check_random_state(random_state)\n    start = logg.info(f'computing PCA with n_comps = {n_comps}')\n    if chunked:\n        if not zero_center or random_state or svd_solver != 'arpack':\n            logg.debug('Ignoring zero_center, random_state, svd_solver')\n        from sklearn.decomposition import IncrementalPCA\n        X_pca = np.zeros((adata_comp.X.shape[0], n_comps), adata_comp.X.dtype)\n        pca_ = IncrementalPCA(n_components=n_comps)\n        for chunk, _, _ in adata_comp.chunked_X(chunk_size):\n            chunk = chunk.toarray() if issparse(chunk) else chunk\n            pca_.partial_fit(chunk)\n        for chunk, start, end in adata_comp.chunked_X(chunk_size):\n            chunk = chunk.toarray() if issparse(chunk) else chunk\n            X_pca[start:end] = pca_.transform(chunk)\n    elif (not pca_sparse or not issparse(adata_comp.X)):\n        if zero_center is None:\n            zero_center = not issparse(adata_comp.X)\n        if zero_center:\n            from sklearn.decomposition import PCA\n            if issparse(adata_comp.X):\n                logg.debug(\n                    '    as `zero_center=True`, '\n                    'sparse input is densified and may '\n                    'lead to huge memory consumption',\n                )\n                X = adata_comp.X.toarray()  # Copying the whole adata_comp.X here, could cause memory problems\n            else:\n                X = adata_comp.X\n            pca_ = PCA(n_components=n_comps, svd_solver=svd_solver, random_state=random_state)\n        else:\n            from sklearn.decomposition import TruncatedSVD\n            logg.debug(\n                '    without zero-centering: \\n'\n                '    the explained variance does not correspond to the exact statistical defintion\\n'\n                '    the first component, e.g., might be heavily influenced by different means\\n'\n                '    the following components often resemble the exact PCA very closely'\n            )\n            pca_ = TruncatedSVD(n_components=n_comps, random_state=random_state,\n                                algorithm=svd_solver)\n            X = adata_comp.X\n        X_pca = pca_.fit_transform(X)\n    else:\n        from sklearn.decomposition import PCA\n        if svd_solver not in ['lobpcg','arpack']:\n            logg.info('solver {svd_solver} is not a valid option for '\n                      '`scipy.sparse.linalg.svds`. Defaulting to `\\'arpack\\'`')\n            svd_solver = 'arpack'\n\n        X = adata_comp.X\n        X_pca,components = _pca_with_sparse(X,n_comps,solver = svd_solver,random_state=random_state)\n        #this is just a wrapper for the results\n        pca_ = PCA(n_components=n_comps,svd_solver=svd_solver)\n        pca_.components_ = components\n        pca_.explained_variance_ = X_pca.var(0)\n        pca_.explained_variance_ratio_ = (pca_.explained_variance_ /\n                                          pca_.explained_variance_.sum())\n\n    if X_pca.dtype.descr != np.dtype(dtype).descr: X_pca = X_pca.astype(dtype)\n\n    if data_is_AnnData:\n        adata.obsm['X_pca'] = X_pca\n        adata.uns['pca'] = {}\n        adata.uns['pca']['params'] = {\n            'zero_center': zero_center,\n            'use_highly_variable': use_highly_variable,\n            'pca_sparse': pca_sparse\n        }\n        if use_highly_variable:\n            adata.varm['PCs'] = np.zeros(shape=(adata.n_vars, n_comps))\n            adata.varm['PCs'][adata.var['highly_variable']] = pca_.components_.T\n        else:\n            adata.varm['PCs'] = pca_.components_.T\n        adata.uns['pca']['variance'] = pca_.explained_variance_\n        adata.uns['pca']['variance_ratio'] = pca_.explained_variance_ratio_\n        logg.info('    finished', time=start)\n        logg.debug(\n            'and added\\n'\n            '    \\'X_pca\\', the PCA coordinates (adata.obs)\\n'\n            '    \\'PC1\\', \\'PC2\\', ..., the loadings (adata.var)\\n'\n            '    \\'pca_variance\\', the variance / eigenvalues (adata.uns)\\n'\n            '    \\'pca_variance_ratio\\', the variance ratio (adata.uns)'\n        )\n        return adata if copy else None\n    else:\n        logg.info('    finished', time=start)\n        if return_info:\n            return X_pca, pca_.components_, pca_.explained_variance_ratio_, pca_.explained_variance_\n        else:\n            return X_pca", "idx": 595}
{"project": "Scanpy", "commit_id": "977_scanpy_1.4.5.1_scanpy_datasets___init__.py_visium_sge.py", "target": 1, "func": "def visium_sge(sample_ID='V1_Breast_Cancer_Block_A_Section_1') -> AnnData:\n    \"\"\"Processed Visium Spatial Gene Expression data from 10x Genomics.\n    Database: https://support.10xgenomics.com/spatial-gene-expression/datasets\n    Parameters\n    ----------\n    sample_ID\n        The ID of data sample in 10x spatial data base.\n        It should be one of the followings:\n        'V1_Breast_Cancer_Block_A_Section_1'\n        'V1_Breast_Cancer_Block_A_Section_2'\n        'V1_Human_Heart'\n        'V1_Human_Lymph_Node'\n        'V1_Mouse_Kidney'\n        'V1_Adult_Mouse_Brain'\n        'V1_Mouse_Brain_Sagittal_Posterior'\n        'V1_Mouse_Brain_Sagittal_Posterior_Section_2'\n        'V1_Mouse_Brain_Sagittal_Anterior'\n        'V1_Mouse_Brain_Sagittal_Anterior_Section_2'\n        By defaults, It is set to:\n        'V1_Breast_Cancer_Block_A_Section_1'\n    Returns\n    -------\n    Annotated data matrix.\n    \"\"\"\n    # setting filenames, tarfilenames, backup_rurls\n    # tarfilenames and backup_urls will be used for downloading data base\n    from urllib.parse import urljoin\n\n    files = dict(\n        counts=settings.datasetdir / f'{sample_ID}_raw_feature_bc_matrix.h5',\n        tissue_positions_file=settings.datasetdir / 'spatial/tissue_positions_list.csv',\n        scalefactors_json_file=settings.datasetdir / 'spatial/scalefactors_json.json',\n        hires_image=settings.datasetdir / 'spatial/tissue_hires_image.png',\n        lowres_image=settings.datasetdir / 'spatial/tissue_lowres_image.png',\n    )\n    tarfiles = dict(\n        tissue_positions_file=settings.datasetdir / f'{sample_ID}_spatial.tar',\n        scalefactors_json_file=settings.datasetdir / f'{sample_ID}_spatial.tar',\n        hires_image=settings.datasetdir / f'{sample_ID}_spatial.tar',\n        lowres_image=settings.datasetdir / f'{sample_ID}_spatial.tar',\n    )\n    _sge10x_url = 'http://cf.10xgenomics.com/samples/spatial-exp/1.0.0/'\n\n    backup_urls = dict(\n        counts=urljoin(\n            _sge10x_url, f'{sample_ID}/{sample_ID}_raw_feature_bc_matrix.h5'\n        ),\n        tissue_positions_file=urljoin(\n            _sge10x_url, f'{sample_ID}/{sample_ID}_spatial.tar.gz'\n        ),\n        scalefactors_json_file=urljoin(\n            _sge10x_url, f'{sample_ID}/{sample_ID}_spatial.tar.gz'\n        ),\n        hires_image=urljoin(_sge10x_url, f'{sample_ID}/{sample_ID}_spatial.tar.gz'),\n        lowres_image=urljoin(_sge10x_url, f'{sample_ID}/{sample_ID}_spatial.tar.gz'),\n    )\n    # Downloading or untar files if it's necessary\n    for _f in files:\n        if _f in tarfiles:\n            _utils.check_presence_download_untar(\n                filename=files[_f], tarfilename=tarfiles[_f], backup_url=backup_urls[_f]\n            )\n        else:\n            _utils.check_presence_download(\n                filename=files[_f], backup_url=backup_urls[_f]\n            )\n    # reading h5 file\n    from ..readwrite import read_10x_h5\n    adata = read_10x_h5(files['counts'])\n    adata.var_names_make_unique()\n    # reading images\n    # imread can only get string and not a Path object\n    import matplotlib.image as img\n    adata.uns['images'] = dict()\n    adata.uns['images']['hires'] = img.imread(str(files['hires_image']))\n    adata.uns['images']['lowres'] = img.imread(str(files['lowres_image']))\n    # reading json scalefactors\n    import json\n    adata.uns['scalefactors'] = json.loads(files['scalefactors_json_file'].read_bytes())\n    # reading coordinates\n    positions = pd.read_csv(\n        settings.datasetdir / 'spatial/tissue_positions_list.csv', header=None,\n    )\n    positions.columns = ['index', '0', '1', '2', 'X2_coord', 'X1_coord']\n    positions.set_index('index')\n\n    positions = positions.join(adata.obs, how='right')\n\n    adata.obsm['X_spatial'] = positions[['X2_coord', 'X1_coord']].to_numpy()\n\n    return adata", "idx": 609}
{"project": "Scanpy", "commit_id": "825_scanpy_1.4_scanpy_preprocessing_normalization.py_normalize_quantile.py", "target": 1, "func": "def normalize_quantile(data, counts_per_cell_after=None, quantile=1, min_counts=1,\n                       key_n_counts=None, copy=False, layers=[], use_rep=None):\n    if quantile < 0 or quantile > 1:\n        raise ValueError('Choose quantile between 0 and 1.')\n    adata = data.copy() if copy else data\n    if quantile < 1:\n        X = adata.X\n        counts_per_cell = X.sum(1).A1 if issparse(X) else X.sum(1)\n        gene_subset = (X>counts_per_cell[:, None]*quantile).sum(0).A1 == 0\n        adata._inplace_subset_var(gene_subset)\n        del X\n    X = adata.X\n    counts_per_cell = X.sum(1).A1 if issparse(X) else X.sum(1)\n    cell_subset = counts_per_cell >= min_counts\n    adata._inplace_subset_obs(cell_subset)\n    del X\n    counts_per_cell = counts_per_cell[cell_subset]\n    _normalize_data(adata.X, counts_per_cell_after, counts_per_cell)\n    layers = adata.layers.keys() if layers == 'all' else layers\n    if use_rep == 'after':\n        after = counts_per_cell_after\n    elif use_rep == 'X':\n        after = np.median(counts_per_cell)\n    elif use_rep is None:\n        after = None\n    for layer in layers:\n        L = adata.layers[layer]\n        counts = L.sum(1).A1 if issparse(L) else L.sum(1)\n        adata.layers[layer] = _normalize_data(L, after, counts, copy=True)", "idx": 611}
{"project": "Scanpy", "commit_id": "925_scanpy_1.9.0___init__.py_subsample.py", "target": 0, "func": "def subsample(\n    X: np.ndarray,\n    subsample: int = 1,\n    seed: int = 0,\n) -> Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\\\n    Subsample a fraction of 1/subsample samples from the rows of X.\n\n    Parameters\n    ----------\n    X\n        Data array.\n    subsample\n        1/subsample is the fraction of data sampled, n = X.shape[0]/subsample.\n    seed\n        Seed for sampling.\n\n    Returns\n    -------\n    Xsampled\n        Subsampled X.\n    rows\n        Indices of rows that are stored in Xsampled.\n    \"\"\"\n    if subsample == 1 and seed == 0:\n        return X, np.arange(X.shape[0], dtype=int)\n    if seed == 0:\n        # this sequence is defined simply by skipping rows\n        # is faster than sampling\n        rows = np.arange(0, X.shape[0], subsample, dtype=int)\n        n = rows.size\n        Xsampled = np.array(X[rows])\n    else:\n        if seed < 0:\n            raise ValueError(f'Invalid seed value < 0: {seed}')\n        n = int(X.shape[0] / subsample)\n        np.random.seed(seed)\n        Xsampled, rows = subsample_n(X, n=n)\n    logg.debug(f'... subsampled to {n} of {X.shape[0]} data points')\n    return Xsampled, rows", "idx": 612}
{"project": "Scanpy", "commit_id": "732_scanpy_1.9.0_test_read_10x.py_test_read_visium_counts.py", "target": 0, "func": "def test_read_visium_counts():\n    # Test that checks the read_visium function\n    visium_pth = ROOT / '../visium_data/1.0.0'\n    spec_genome_v3 = sc.read_visium(visium_pth, genome='GRCh38')\n    nospec_genome_v3 = sc.read_visium(visium_pth)\n    assert_anndata_equal(spec_genome_v3, nospec_genome_v3)", "idx": 616}
{"project": "Scanpy", "commit_id": "448_scanpy_0.2.9.1_docs_conf.py_modurl.py", "target": 1, "func": "def modurl(qualname):\n    \"\"\"Get the full GitHub URL for some object\u2019s qualname\"\"\"\n    obj, module = get_obj_module(qualname)\n    path = Path(module.__file__).relative_to(project_dir)\n    start, end = get_linenos(obj)\n    fragment = '#L{}-L{}'.format(start, end) if start and end else ''\n    return '{}/{}{}'.format(github_url, path, fragment)", "idx": 627}
{"project": "Scanpy", "commit_id": "1056_scanpy_1.6.1_scanpy_get.py_obs_df.py", "target": 1, "func": "def obs_df(\n    adata: AnnData,\n    keys: Iterable[str] = (),\n    obsm_keys: Iterable[Tuple[str, int]] = (),\n    *,\n    layer: str = None,\n    gene_symbols: str = None,\n    use_raw: bool = False,\n) -> pd.DataFrame:\n    \"\"\"\\\n    Return values for observations in adata.\n    Params\n    ------\n    adata\n        AnnData object to get values from.\n    keys\n        Keys from either `.var_names`, `.var[gene_symbols]`, or `.obs.columns`.\n    obsm_keys\n        Tuple of `(key from obsm, column index of obsm[key])`.\n    layer\n        Layer of `adata` to use as expression values.\n    gene_symbols\n        Column of `adata.var` to search for `keys` in.\n    use_raw\n        Whether to get expression values from `adata.raw`.\n    Returns\n    -------\n    A dataframe with `adata.obs_names` as index, and values specified by `keys`\n    and `obsm_keys`.\n    Examples\n    --------\n    Getting value for plotting:\n    >>> pbmc = sc.datasets.pbmc68k_reduced()\n    >>> plotdf = sc.get.obs_df(\n            pbmc,\n            keys=[\"CD8B\", \"n_genes\"],\n            obsm_keys=[(\"X_umap\", 0), (\"X_umap\", 1)]\n        )\n    >>> plotdf.plot.scatter(\"X_umap0\", \"X_umap1\", c=\"CD8B\")\n    Calculating mean expression for marker genes by cluster:\n    >>> pbmc = sc.datasets.pbmc68k_reduced()\n    >>> marker_genes = ['CD79A', 'MS4A1', 'CD8A', 'CD8B', 'LYZ']\n    >>> genedf = sc.get.obs_df(\n            pbmc,\n            keys=[\"louvain\", *marker_genes]\n        )\n    >>> grouped = genedf.groupby(\"louvain\")\n    >>> mean, var = grouped.mean(), grouped.var()\n    \"\"\"\n    if use_raw:\n        assert (\n            layer is None\n        ), \"Cannot specify use_raw=True and a layer at the same time.\"\n        if gene_symbols is not None:\n            gene_names = pd.Series(\n                adata.raw.var_names, index=adata.raw.var[gene_symbols]\n            )\n        else:\n            gene_names = pd.Series(adata.raw.var_names, index=adata.raw.var_names)\n    else:\n        if gene_symbols is not None:\n            gene_names = pd.Series(adata.var_names, index=adata.var[gene_symbols])\n        else:\n            gene_names = pd.Series(adata.var_names, index=adata.var_names)\n    obs_names = []\n    var_names = []\n    var_symbol = []\n    not_found = []\n    for key in keys:\n        if key in adata.obs.columns:\n            obs_names.append(key)\n        elif key in gene_names.index:\n            var_names.append(gene_names[key])\n            var_symbol.append(key)\n        else:\n            not_found.append(key)\n    if len(not_found) > 0:\n        if use_raw:\n            if gene_symbols is None:\n                gene_error = \"`adata.raw.var_names`\"\n            else:\n                gene_error = \"gene_symbols column `adata.raw.var[{}].values`\".format(\n                    gene_symbols\n                )\n        else:\n            if gene_symbols is None:\n                gene_error = \"`adata.var_names`\"\n            else:\n                gene_error = \"gene_symbols column `adata.var[{}].values`\".format(\n                    gene_symbols\n                )\n        raise KeyError(\n            f\"Could not find keys '{not_found}' in columns of `adata.obs` or in\"\n            f\" {gene_error}.\"\n        )\n    # Make df\n    df = pd.DataFrame(index=adata.obs.index)\n    # add var values\n    if len(var_names) > 0:\n        X = _get_obs_rep(adata, layer=layer, use_raw=use_raw)\n        if use_raw:\n            var_bool = adata.raw.var.index.isin(var_names)\n            print(var_names, var_symbol)\n        else:\n            var_bool = adata.var.index.isin(var_names)\n        matrix = X[:, var_bool]\n        from scipy.sparse import issparse\n        if issparse(matrix):\n            matrix = matrix.toarray()\n        df = df.join(pd.DataFrame(matrix, columns=var_symbol, index=adata.obs.index))\n\n    # add obs values\n    if len(obs_names) > 0:\n        df = df.join(adata.obs[obs_names])\n    # reorder columns to given order\n    df = df[keys]\n    for k, idx in obsm_keys:\n        added_k = f\"{k}-{idx}\"\n        val = adata.obsm[k]\n        if isinstance(val, np.ndarray):\n            df[added_k] = np.ravel(val[:, idx])\n        elif isinstance(val, spmatrix):\n            df[added_k] = np.ravel(val[:, idx].toarray())\n        elif isinstance(val, pd.DataFrame):\n            df[added_k] = val.loc[:, idx]\n    return df", "idx": 637}
{"project": "Scanpy", "commit_id": "518_scanpy_1.9.0_test_embedding_plots.py_test_missing_values_categorical.py", "target": 0, "func": "def test_missing_values_categorical(\n    fixture_request,\n    image_comparer,\n    adata,\n    plotfunc,\n    na_color,\n    na_in_legend,\n    legend_loc,\n    groupsfunc,\n):\n    save_and_compare_images = image_comparer(\n        MISSING_VALUES_ROOT, MISSING_VALUES_FIGS, tol=15\n    )\n    base_name = fixture_request.node.name\n\n    # Passing through a dict so it's easier to use default values\n    kwargs = {}\n    kwargs[\"legend_loc\"] = legend_loc\n    kwargs[\"groups\"] = groupsfunc(adata.obs[\"label\"])\n    if na_color is not None:\n        kwargs[\"na_color\"] = na_color\n    kwargs[\"na_in_legend\"] = na_in_legend\n\n    plotfunc(adata, color=[\"label\", \"label_missing\"], **kwargs)\n\n    save_and_compare_images(base_name)", "idx": 640}
{"project": "Scanpy", "commit_id": "443_scanpy_0.2.9.1_scanpy_plotting_ann_data.py_scatter.py", "target": 1, "func": "def scatter(\n        adata,\n        x=None,\n        y=None,\n        color='grey',\n        alpha=None,\n        basis=None,\n        groups=None,\n        components=None,\n        projection='2d',\n        legend_loc='right margin',\n        legend_fontsize=None,\n        legend_fontweight=None,\n        color_map=None,\n        palette=None,\n        right_margin=None,\n        left_margin=None,\n        size=None,\n        title=None,\n        show=None,\n        save=None,\n        ax=None):\n    \"\"\"Scatter plot.\n    Color with sample annotation (`color in adata.smp_keys()`) or gene\n    expression (`color in adata.var_names`).\n    Parameters\n    ----------\n    adata : AnnData\n        Annotated data matrix.\n    x : str or None\n        x coordinate.\n    y : str or None\n        y coordinate.\n    color : string or list of strings, optional (default: None)\n        Keys for sample/cell annotation either as list `[\"ann1\", \"ann2\"]` or\n        string `\"ann1,ann2,...\"`.\n    basis : {'pca', 'tsne', 'diffmap', 'draw_graph_fr', etc.}\n        String that denotes a plotting tool that computed coordinates.\n    groups : str, optional (default: all groups in color)\n        Allows to restrict categories in sample annotation to a subset.\n    components : str or list of str, optional (default: '1,2')\n         String of the form '1,2' or ['1,2', '2,3'].\n    projection : {'2d', '3d'}, optional (default: '2d')\n         Projection of plot.\n    legend_loc : str, optional (default: 'right margin')\n         Location of legend, either 'on data', 'right margin' or valid keywords\n         for matplotlib.legend.\n    legend_fontsize : int (default: None)\n         Legend font size.\n    legend_fontweight : int (default: None)\n         Legend font weight.\n    color_map : str (default: 'viridis')\n         String denoting matplotlib color map for continuous coloring.\n    palette : list of str (default: None)\n         Colors to use for plotting groups (categorical annotation).\n    right_margin : float (default: None)\n         Adjust how far the plotting panel extends to the right.\n    size : float (default: None)\n         Point size. Sample-number dependent by default.\n    title : str, optional (default: None)\n         Provide title for panels either as `[\"title1\", \"title2\", ...]` or\n         `\"title1,title2,...\"`.\n    show : bool, optional (default: None)\n         Show the plot.\n    save : bool or str, optional (default: None)\n         If True or a str, save the figure. A string is appended to the\n         default filename.\n    ax : matplotlib.Axes\n         A matplotlib axes object.\n    Returns\n    -------\n    A list of matplotlib.Axis objects.\n    \"\"\"\n    sanitize_anndata(adata)\n    if components is None: components = '1,2' if '2d' in projection else '1,2,3'\n    if isinstance(components, str): components = components.split(',')\n    components = np.array(components).astype(int) - 1\n    keys = ['grey'] if color is None else color.split(',') if isinstance(color, str) else color\n    groups = None if groups is None else groups.split(',') if isinstance(groups, str) else groups\n    highlights = adata.add['highlights'] if 'highlights' in adata.add else []\n    if basis is not None:\n        try:\n            Y = adata.smpm['X_' + basis][:, components]\n        except KeyError:\n            raise KeyError('compute coordinates using visualization tool {} first'\n                           .format(basis))\n    elif x is not None and y is not None:\n        x_arr = adata.get_smp_array(x)\n        y_arr = adata.get_smp_array(y)\n        Y = np.c_[x_arr[:, None], y_arr[:, None]]\n    else:\n        raise ValueError('Either provide keys for a `basis` or for `x` and `y`.')\n    if size is None:\n        n = Y.shape[0]\n        size = 120000 / n\n    if legend_loc == 'on data' and legend_fontsize is None:\n        legend_fontsize = rcParams['legend.fontsize']\n    elif legend_fontsize is None:\n        legend_fontsize = rcParams['legend.fontsize']\n    palette_was_none = False\n    if palette is None: palette_was_none = True\n    if isinstance(palette, list):\n        if not is_color_like(palette[0]):\n            palettes = palette\n        else:\n            palettes = [palette]\n    else:\n        palettes = [palette for i in range(len(keys))]\n    for i, palette in enumerate(palettes):\n        palettes[i] = utils.default_palette(palette)\n    if basis is not None:\n        component_name = ('DC' if basis == 'diffmap'\n                          else basis.replace('draw_graph_', '').upper() if 'draw_graph' in basis\n                          else 'tSNE' if basis == 'tsne'\n                          else 'PC' if basis == 'pca'\n                          else 'Spring' if basis == 'spring'\n                          else None)\n    else:\n        component_name = None\n    axis_labels = (x, y) if component_name is None else None\n    show_ticks = True if component_name is None else False\n    # the actual color ids, e.g. 'grey' or '#109482'\n    color_ids = [None if not is_color_like(key)\n                 else key for key in keys]\n    categoricals = []\n    colorbars = []\n    for ikey, key in enumerate(keys):\n        if color_ids[ikey] is not None:\n            c = color_ids[ikey]\n            continuous = True\n            categorical = False\n            colorbars.append(False)\n        else:\n            c = 'white' if projection == '2d' else 'white'\n            categorical = False\n            continuous = False\n            # test whether we have categorial or continuous annotation\n            if key in adata.smp_keys():\n                if is_categorical_dtype(adata.smp[key]):\n                    categorical = True\n                else:\n                    continuous = True\n                    c = adata.smp[key]\n            # coloring according to gene expression\n            elif key in set(adata.var_names):\n                c = adata[:, key].X\n                continuous = True\n            else:\n                raise ValueError('\"' + key + '\" is invalid!'\n                                 + ' specify valid sample annotation, one of '\n                                 + str(adata.smp_keys()) + ' or a gene name '\n                                 + str(adata.var_names))\n            colorbars.append(True if continuous else False)\n        if categorical: categoricals.append(ikey)\n        color_ids[ikey] = c\n    if right_margin is None and len(categoricals) > 0:\n        if legend_loc == 'right margin': right_margin = 0.5\n    if title is None and keys[0] is not None:\n        title = [key.replace('_', ' ') if not is_color_like(key) else '' for key in keys]\n    axs = scatter_base(Y,\n                       title=title,\n                       alpha=alpha,\n                       component_name=component_name,\n                       axis_labels=axis_labels,\n                       component_indexnames=components + 1,\n                       projection=projection,\n                       colors=color_ids,\n                       highlights=highlights,\n                       colorbars=colorbars,\n                       right_margin=right_margin,\n                       left_margin=left_margin,\n                       sizes=[size for c in keys],\n                       color_map=color_map,\n                       show_ticks=show_ticks,\n                       ax=ax)\n\n    for i, ikey in enumerate(categoricals):\n        palette = palettes[i]\n        key = keys[ikey]\n        if (not key + '_colors' in adata.add\n            or not palette_was_none\n            or len(adata.smp[key].cat.categories) != len(adata.add[key + '_colors'])):\n            utils.add_colors_for_categorical_sample_annotation(adata, key, palette)\n        # actually plot the groups\n        mask_remaining = np.ones(Y.shape[0], dtype=bool)\n        centroids = {}\n        if groups is None:\n            for iname, name in enumerate(adata.smp[key].cat.categories):\n                if name not in settings._ignore_categories:\n                    mask = scatter_group(axs[ikey], key, iname,\n                                         adata, Y, projection, size=size, alpha=alpha)\n                    mask_remaining[mask] = False\n                    if legend_loc == 'on data': add_centroid(centroids, name, Y, mask)\n        else:\n            for name in groups:\n                if name not in set(adata.smp[key].cat.categories):\n                    raise ValueError('\"' + name + '\" is invalid!'\n                                     + ' specify valid name, one of '\n                                     + str(adata.smp[key].cat.categories))\n                else:\n                    iname = np.flatnonzero(adata.smp[key].cat.categories.values == name)[0]\n                    mask = scatter_group(axs[ikey], key, iname,\n                                         adata, Y, projection, size=size, alpha=alpha)\n                    if legend_loc == 'on data': add_centroid(centroids, name, Y, mask)\n                    mask_remaining[mask] = False\n        if mask_remaining.sum() > 0:\n            data = [Y[mask_remaining, 0], Y[mask_remaining, 1]]\n            if projection == '3d': data.append(Y[mask_remaining, 2])\n            axs[ikey].scatter(*data, marker='.', c='grey', s=size,\n                                    edgecolors='none', zorder=-1)\n        legend = None\n        if legend_loc == 'on data':\n            for name, pos in centroids.items():\n                axs[ikey].text(pos[0], pos[1], name,\n                                     weight=legend_fontweight,\n                                     verticalalignment='center',\n                                     horizontalalignment='center',\n                                     fontsize=legend_fontsize)\n        elif legend_loc == 'right margin':\n            legend = axs[ikey].legend(frameon=False, loc='center left',\n                                            bbox_to_anchor=(1, 0.5),\n                                            ncol=(1 if len(adata.smp[key].cat.categories) <= 14\n                                                  else 2 if len(adata.smp[key].cat.categories) <= 30 else 3),\n                                            fontsize=legend_fontsize)\n        elif legend_loc != 'none':\n            legend = axs[ikey].legend(frameon=False, loc=legend_loc,\n                                            fontsize=legend_fontsize)\n        if legend is not None:\n            for handle in legend.legendHandles: handle.set_sizes([300.0])\n    utils.savefig_or_show('scatter' if basis is None else basis, show=show, save=save)\n    return axs", "idx": 648}
{"project": "Scanpy", "commit_id": "912_scanpy_1.9.0___init__.py__check_use_raw.py", "target": 0, "func": "def _check_use_raw(adata: AnnData, use_raw: Union[None, bool]) -> bool:\n    \"\"\"\n    Normalize checking `use_raw`.\n\n    My intentention here is to also provide a single place to throw a deprecation warning from in future.\n    \"\"\"\n    if use_raw is not None:\n        return use_raw\n    else:\n        if adata.raw is not None:\n            return True\n        else:\n            return False", "idx": 649}
{"project": "Scanpy", "commit_id": "1049_scanpy_1.6.1_scanpy_readwrite.py__download.py", "target": 1, "func": "def _download(url: str, path: Path):\n    try:\n        import ipywidgets\n        from tqdm.auto import tqdm\n    except ModuleNotFoundError:\n        from tqdm import tqdm\n    from urllib.request import urlopen, Request\n\n    blocksize = 1024 * 8\n    blocknum = 0\n\n    try:\n        # This is a modified urllib.request.urlretrieve, but that won't let us\n        # set headers.\n        # The '\\'s are ugly, but parenthesis are not allowed here\n        # fmt: off\n        with \\\n            tqdm(unit='B', unit_scale=True, miniters=1, desc=path.name) as t, \\\n            path.open(\"wb\") as f, \\\n            urlopen(Request(url, headers={\"User-agent\": \"scanpy-user\"})) as resp:\n\n            t.total = int(resp.info().get(\"content-length\", 0))\n\n            block = resp.read(blocksize)\n            while block:\n                f.write(block)\n                blocknum += 1\n                t.update(blocknum * blocksize - t.n)\n                block = resp.read(blocksize)\n        # fmt: on\n    except Exception:\n        # Make sure file doesn\u2019t exist half-downloaded\n        if path.is_file():\n            path.unlink()\n        raise", "idx": 650}
{"project": "Scanpy", "commit_id": "491_scanpy_0.4.1_scanpy_plotting_ann_data.py_clustermap.py", "target": 1, "func": "def clustermap(\n        adata, obs_keys=None, use_raw=True, show=None, save=None, **kwargs):\n    \"\"\"Hierarchically-clustered heatmap [Waskom16]_.\n    Wraps `seaborn.clustermap <https://seaborn.pydata.org/generated/seaborn.clustermap.html>`_ for :class:`~scanpy.api.AnnData`.\n    Parameters\n    ----------\n    adata : :class:`~scanpy.api.AnnData`\n        Annotated data matrix.\n    obs_keys : `str`\n        Categorical annotation to plot with a different color map.\n        Currently, only a single key is supported.\n    use_raw : `bool`, optional (default: `True`)\n        Use `raw` attribute of `adata` if present.\n    show : bool, optional (default: `None`)\n         Show the plot.\n    save : `bool` or `str`, optional (default: `None`)\n        If `True` or a `str`, save the figure. A string is appended to the\n        default filename. Infer the filetype if ending on \\{'.pdf', '.png', '.svg'\\}.\n    **kwargs : keyword arguments\n        Keyword arguments passed to `seaborn.clustermap <https://seaborn.pydata.org/generated/seaborn.clustermap.html>`_.\n    Returns\n    -------\n    If `not show`, a `seaborn.ClusterGrid` object.\n    Notes\n    -----\n    The returned object has a savefig() method that should be used if you want\n    to save the figure object without clipping the dendrograms.\n    To access the reordered row indices, use:\n    clustergrid.dendrogram_row.reordered_ind\n    Column indices, use: clustergrid.dendrogram_col.reordered_ind\n    Examples\n    --------\n    Soon to come with figures. In the meanwile, see\n    https://seaborn.pydata.org/generated/seaborn.clustermap.html.\n    >>> import scanpy.api as sc\n    >>> adata = sc.datasets.krumsiek11()\n    >>> sc.pl.clustermap(adata, obs_keys='cell_type')\n    \"\"\"\n    if not isinstance(obs_keys, str):\n        raise ValueError('Currently, only a single key is supported.')\n    sanitize_anndata(adata)\n    X = adata.raw.X if use_raw and adata.raw is not None else adata.X\n    df = pd.DataFrame(X, index=adata.obs_names, columns=adata.var_names)\n    if obs_keys is not None:\n        row_colors = adata.obs[obs_keys]\n        utils.add_colors_for_categorical_sample_annotation(adata, obs_keys)\n        # do this more efficiently... just a quick solution\n        lut = dict(zip(\n            row_colors.cat.categories,\n            adata.uns[obs_keys + '_colors']))\n        row_colors = adata.obs[obs_keys].map(lut)\n        g = sns.clustermap(df, row_colors=row_colors, **kwargs)\n    else:\n        g = sns.clustermap(df, **kwargs)\n    show = settings.autoshow if show is None else show\n    if show: pl.show()\n    else: return g", "idx": 655}
{"project": "Scanpy", "commit_id": "1009_scanpy_1.4.6_scanpy_tests_test_neighbors.py_test_restore_n_neighbors.py", "target": 1, "func": "def test_restore_n_neighbors(neigh, conv):\n    neigh.compute_neighbors(method='gauss', n_neighbors=n_neighbors)\n\n    ad = AnnData(np.array(X))\n    ad.uns['neighbors'] = dict(connectivities=conv(neigh.connectivities))\n    neigh_restored = Neighbors(ad)\n    assert neigh_restored.n_neighbors == 1", "idx": 660}
{"project": "Scanpy", "commit_id": "116_scanpy_1.9.0__ebi_expression_atlas.py_sniff_url.py", "target": 0, "func": "def sniff_url(accession: str):\n    # Note that data is downloaded from gxa/sc/experiment, not experiments\n    base_url = f\"https://www.ebi.ac.uk/gxa/sc/experiments/{accession}/\"\n    try:\n        with urlopen(base_url):  # Check if server up/ dataset exists\n            pass\n    except HTTPError as e:\n        e.msg = f\"{e.msg} ({base_url})\"  # Report failed url\n        raise", "idx": 669}
{"project": "Scanpy", "commit_id": "282_scanpy_0.1_scanpy_plotting___init__.py_ega_tree.py", "target": 1, "func": "def ega_tree(adata, root=0, colors=None, names=None, show=None, fontsize=None):\n    # plot the tree\n    if isinstance(adata, nx.Graph):\n        G = adata\n        colors = ['grey' for n in enumerate(G)]\n    else:\n        if colors is None:\n            if ('ega_groups_colors' not in adata.add\n                or len(adata.add['ega_groups_names']) != len(adata.add['ega_groups_colors'])):\n                utils.add_colors_for_categorical_sample_annotation(adata, 'ega_groups')\n            colors = adata.add['ega_groups_colors']\n        else: colors = colors\n        if names is None:\n            names = {i: n for i, n in enumerate(adata.add['ega_groups_names'])}\n        for iname, name in enumerate(adata.add['ega_groups_names']):\n            if name in sett._ignore_categories: colors[iname] = 'grey'\n        G = nx.Graph(adata.add['ega_groups_adjacency'])\n    pos = utils.hierarchy_pos(G, root)\n    # pos = nx.spring_layout(G)\n    if len(pos) == 1: pos[0] = 0.5, 0.5\n    fig = pl.figure()\n    ax = pl.axes([0.08, 0.08, 0.9, 0.9], frameon=False)\n    labels = nx.get_edge_attributes(G, 'weight')\n    nx.draw_networkx_edges(G, pos, ax=ax)  #, edge_labels=labels)\n    trans = ax.transData.transform\n    trans2 = fig.transFigure.inverted().transform\n    pl.xticks([])\n    pl.yticks([])\n    piesize = 1/(np.sqrt(G.number_of_nodes()) + 10)\n    p2 = piesize/2.0\n    for n_cnt, n in enumerate(G):\n        xx, yy = trans(pos[n])     # figure coordinates\n        xa, ya = trans2((xx, yy))  # normalized coordinates\n        a = pl.axes([xa-p2, ya-p2, piesize, piesize])\n        if is_color_like(colors[n_cnt]):\n            fracs = [100]\n            color = [colors[n_cnt]]\n        else:\n            color = colors[n_cnt].keys()\n            fracs = [colors[n_cnt][c] for c in color]\n            if sum(fracs) < 1:\n                color = list(color)\n                color.append('grey')\n                fracs.append(1-sum(fracs))\n                names[n_cnt] += '\\n?'\n        a.pie(fracs, colors=color)\n        if names is not None:\n            a.text(0.5, 0.5, names[n_cnt],\n                   verticalalignment='center',\n                   horizontalalignment='center',\n                   transform=a.transAxes, size=fontsize)\n    savefig_or_show('ega_tree', show)\n    return ax", "idx": 670}
{"project": "Scanpy", "commit_id": "7_scanpy_1.9.0_conf.py_setup.py", "target": 0, "func": "def setup(app):\n    app.warningiserror = on_rtd", "idx": 683}
{"project": "Scanpy", "commit_id": "479_scanpy_1.9.0_test_clustering.py_test_clustering_subset.py", "target": 0, "func": "def test_clustering_subset(adata_neighbors, clustering, key):\n    if clustering == sc.tl.louvain:\n        pytest.importorskip(\"louvain\")\n\n    clustering(adata_neighbors, key_added=key)\n\n    for c in adata_neighbors.obs[key].unique():\n        print('Analyzing cluster ', c)\n        cells_in_c = adata_neighbors.obs[key] == c\n        ncells_in_c = adata_neighbors.obs[key].value_counts().loc[c]\n        key_sub = str(key) + '_sub'\n        clustering(\n            adata_neighbors,\n            restrict_to=(key, [c]),\n            key_added=key_sub,\n        )\n        # Get new clustering labels\n        new_partition = adata_neighbors.obs[key_sub]\n\n        cat_counts = new_partition[cells_in_c].value_counts()\n\n        # Only original cluster's cells assigned to new categories\n        assert cat_counts.sum() == ncells_in_c\n\n        # Original category's cells assigned only to new categories\n        nonzero_cat = cat_counts[cat_counts > 0].index\n        common_cat = nonzero_cat.intersection(adata_neighbors.obs[key].cat.categories)\n        assert len(common_cat) == 0", "idx": 685}
{"project": "Scanpy", "commit_id": "43_scanpy_1.9.0_logging.py_error.py", "target": 0, "func": "def error(self, msg, *, time=None, deep=None, extra=None) -> datetime:\n        return self.log(ERROR, msg, time=time, deep=deep, extra=extra)", "idx": 689}
{"project": "Scanpy", "commit_id": "24_scanpy_1.9.0_cli.py_main.py", "target": 0, "func": "def main(\n    argv: Optional[Sequence[str]] = None, *, check: bool = True, **runargs\n) -> Optional[CompletedProcess]:\n    \"\"\"\\\n    Run a builtin scanpy command or a scanpy-* subcommand.\n\n    Uses :func:`subcommand.run` for the latter:\n    `~run(['scanpy', *argv], **runargs)`\n    \"\"\"\n    parser = ArgumentParser(\n        description=(\n            \"There are a few packages providing commands. \"\n            \"Try e.g. `pip install scanpy-scripts`!\"\n        )\n    )\n    parser.set_defaults(func=parser.print_help)\n\n    subparsers: _DelegatingSubparsersAction = parser.add_subparsers(\n        action=_DelegatingSubparsersAction,\n        _command='scanpy',\n        _runargs={**runargs, 'check': check},\n    )\n\n    parser_settings = subparsers.add_parser('settings')\n    parser_settings.set_defaults(func=_cmd_settings)\n\n    args = parser.parse_args(argv)\n    return args.func()", "idx": 690}
{"project": "Scanpy", "commit_id": "885_scanpy_1.9.0__sim.py_branch_init_model1.py", "target": 0, "func": "def branch_init_model1(self, tmax=100):\n        # check whether we can define trajectories\n        Xfix = np.array([self.Coupl[0, 1] / self.Coupl[0, 0], 1])\n        if Xfix[0] > 0.97 or Xfix[0] < 0.03:\n            settings.m(\n                0,\n                '... either no fixed point in [0,1]^2! \\n'\n                + '    or fixed point is too close to bounds',\n            )\n            return None\n        #\n        XbackUp = self.sim_model_backwards(\n            tmax=tmax / 3, X0=Xfix + np.array([0.02, -0.02])\n        )\n        XbackDo = self.sim_model_backwards(\n            tmax=tmax / 3, X0=Xfix + np.array([-0.02, -0.02])\n        )\n        #\n        Xup = self.sim_model(tmax=tmax, X0=XbackUp[0])\n        Xdo = self.sim_model(tmax=tmax, X0=XbackDo[0])\n        # compute mean\n        X0mean = 0.5 * (Xup[0] + Xdo[0])\n        #\n        if np.min(X0mean) < 0.025 or np.max(X0mean) > 0.975:\n            settings.m(0, '... initial point is too close to bounds')\n            return None\n        if self.show and self.verbosity > 1:\n            pl.figure()  # noqa: F821  TODO Fix me\n            pl.plot(XbackUp[:, 0], '.b', XbackUp[:, 1], '.g')  # noqa: F821  TODO Fix me\n            pl.plot(XbackDo[:, 0], '.b', XbackDo[:, 1], '.g')  # noqa: F821  TODO Fix me\n            pl.plot(Xup[:, 0], 'b', Xup[:, 1], 'g')  # noqa: F821  TODO Fix me\n            pl.plot(Xdo[:, 0], 'b', Xdo[:, 1], 'g')  # noqa: F821  TODO Fix me\n        return X0mean", "idx": 693}
{"project": "Scanpy", "commit_id": "840_scanpy_1.4_scanpy_readwrite.py__read.py", "target": 1, "func": "def _read(filename, backed=False, sheet=None, ext=None, delimiter=None,\n          first_column_names=None, backup_url=None, cache=False,\n          suppress_cache_warning=False, **kwargs):\n    if ext is not None and ext not in avail_exts:\n        raise ValueError('Please provide one of the available extensions.\\n'\n                         + avail_exts)\n    else:\n        ext = is_valid_filename(filename, return_ext=True)\n    is_present = check_datafile_present_and_download(filename,\n                                                     backup_url=backup_url)\n    if not is_present: logg.msg('... did not find original file', filename)\n    # read hdf5 files\n    if ext in {'h5', 'h5ad'}:\n        if sheet is None:\n            return read_h5ad(filename, backed=backed)\n        else:\n            logg.msg('reading sheet', sheet, 'from file', filename, v=4)\n            return read_hdf(filename, sheet)\n    # read other file types\n    filename_cache = (settings.cachedir + filename.lstrip(\n        './').replace('/', '-').replace('.' + ext, '.h5ad'))\n    if filename_cache.endswith('.gz'): filename_cache = filename_cache[:-3]\n    if filename_cache.endswith('.bz2'): filename_cache = filename_cache[:-4]\n    if cache and os.path.exists(filename_cache):\n        logg.info('... reading from cache file', filename_cache)\n        adata = read_h5ad(filename_cache, backed=False)\n    else:\n        if not is_present:\n            raise FileNotFoundError('Did not find file {}.'.format(filename))\n        logg.msg('reading', filename, v=4)\n        if not cache and not suppress_cache_warning:\n            logg.hint('This might be very slow. Consider passing `cache=True`, '\n                      'which enables much faster reading from a cache file.')\n        # do the actual reading\n        if ext == 'xlsx' or ext == 'xls':\n            if sheet is None:\n                raise ValueError(\n                    'Provide `sheet` parameter when reading \\'.xlsx\\' files.')\n            else:\n                adata = read_excel(filename, sheet)\n        elif ext in {'mtx', 'mtx.gz'}:\n            adata = read_mtx(filename)\n        elif ext == 'csv':\n            adata = read_csv(filename, first_column_names=first_column_names)\n        elif ext in {'txt', 'tab', 'data', 'tsv'}:\n            if ext == 'data':\n                logg.msg('... assuming \\'.data\\' means tab or white-space '\n                         'separated text file', v=3)\n                logg.hint('change this by passing `ext` to sc.read')\n            adata = read_text(filename, delimiter, first_column_names)\n        elif ext == 'soft.gz':\n            adata = _read_softgz(filename)\n        elif ext == 'loom':\n            adata = read_loom(filename=filename, **kwargs)\n        else:\n            raise ValueError('Unkown extension {}.'.format(ext))\n        if cache:\n            logg.info('... writing an', settings.file_format_data,\n                      'cache file to speedup reading next time')\n            if not os.path.exists(os.path.dirname(filename_cache)):\n                os.makedirs(os.path.dirname(filename_cache))\n            # write for faster reading when calling the next time\n            adata.write(filename_cache)\n    return adata", "idx": 703}
{"project": "Scanpy", "commit_id": "593_scanpy_1.0.4_scanpy_plotting_tools_paga.py__paga_graph.py", "target": 1, "func": "def _paga_graph(\n        adata,\n        ax,\n        solid_edges=None,\n        dashed_edges=None,\n        threshold=None,\n        threshold_solid=None,\n        threshold_dashed=None,\n        root=0,\n        rootlevel=None,\n        color=None,\n        groups=None,\n        fontsize=None,\n        node_size_scale=1,\n        node_size_power=0.5,\n        edge_width_scale=1,\n        title=None,\n        layout=None,\n        pos=None,\n        cmap=None,\n        frameon=True,\n        min_edge_width=None,\n        max_edge_width=None,\n        export_to_gexf=False,\n        random_state=0):\n    node_labels = groups\n    if (node_labels is not None\n        and isinstance(node_labels, str)\n        and node_labels != adata.uns['paga']['groups']):\n        raise ValueError('Provide a list of group labels for the PAGA groups {}, not {}.'\n                         .format(adata.uns['paga']['groups'], node_labels))\n    groups_key = adata.uns['paga']['groups']\n    if node_labels is None:\n        node_labels = adata.obs[groups_key].cat.categories\n    if color is None and groups_key is not None:\n        if (groups_key + '_colors' not in adata.uns\n            or len(adata.obs[groups_key].cat.categories)\n               != len(adata.uns[groups_key + '_colors'])):\n            utils.add_colors_for_categorical_sample_annotation(adata, groups_key)\n        color = adata.uns[groups_key + '_colors']\n        for iname, name in enumerate(adata.obs[groups_key].cat.categories):\n            if name in settings.categories_to_ignore: color[iname] = 'grey'\n    if isinstance(root, str):\n        if root in node_labels:\n            root = list(node_labels).index(root)\n        else:\n            raise ValueError(\n                'If `root` is a string, it needs to be one of {} not \\'{}\\'.'\n                .format(node_labels.tolist(), root))\n    if isinstance(root, list) and root[0] in node_labels:\n        root = [list(node_labels).index(r) for r in root]\n    # define the objects\n    adjacency_solid = adata.uns['paga'][solid_edges].copy()\n    # set the the thresholds, either explicitly\n    if threshold is not None:\n        threshold_solid = threshold\n        threshold_dashed = threshold\n    # or to a default value\n    else:\n        if threshold_solid is None:\n            threshold_solid = 0.01\n        if threshold_dashed is None:\n            threshold_dashed = 0.01\n    if threshold_solid is not None:\n        adjacency_solid[adjacency_solid < threshold_solid] = 0\n    nx_g_solid = nx.Graph(adjacency_solid)\n    if dashed_edges is not None:\n        adjacency_dashed = adata.uns['paga'][dashed_edges].copy()\n        if threshold_dashed is not None:\n            adjacency_dashed[adjacency_dashed < threshold_dashed] = 0\n        nx_g_dashed = nx.Graph(adjacency_dashed)\n    # degree of the graph for coloring\n    if isinstance(color, str) and color.startswith('degree'):\n        # see also tools.paga.paga_degrees\n        if color == 'degree_dashed':\n            color = [d for _, d in nx_g_dashed.degree(weight='weight')]\n        elif color == 'degree_solid':\n            color = [d for _, d in nx_g_solid.degree(weight='weight')]\n        else:\n            raise ValueError('`degree` either \"degree_dashed\" or \"degree_solid\".')\n        color = (np.array(color) - np.min(color)) / (np.max(color) - np.min(color))\n    # plot numeric colors\n    colorbar = False\n    if isinstance(color, (list, np.ndarray)) and not isinstance(color[0], (str, dict)):\n        import matplotlib\n        norm = matplotlib.colors.Normalize()\n        color = norm(color)\n        if cmap is None: cmap = rcParams['image.cmap']\n        cmap = matplotlib.cm.get_cmap(cmap)\n        color = [cmap(c) for c in color]\n        colorbar = True\n    if len(color) < len(node_labels):\n        print(node_labels, color)\n        raise ValueError('`color` list need to be at least as long as `node_labels` list.')\n    # node positions from adjacency_solid\n    if pos is None:\n        if layout is None:\n            layout = 'fr'\n        # igraph layouts\n        if layout != 'eq_tree':\n            from ... import utils as sc_utils\n            adj_solid_weights = adjacency_solid\n            g = sc_utils.get_igraph_from_adjacency(adj_solid_weights)\n            if 'rt' in layout:\n                g_tree = g\n                if solid_edges != 'confidence_tree':\n                    adj_tree = adata.uns['paga']['confidence_tree']\n                    g_tree = sc_utils.get_igraph_from_adjacency(adj_tree)\n                pos_list = g_tree.layout(\n                    layout, root=root if isinstance(root, list) else [root],\n                    rootlevel=rootlevel).coords\n            elif layout == 'circle':\n                pos_list = g.layout(layout).coords\n            else:\n                np.random.seed(random_state)\n                init_coords = np.random.random((adjacency_solid.shape[0], 2)).tolist()\n                pos_list = g.layout(layout, seed=init_coords, weights='weight').coords\n            pos = {n: [p[0], -p[1]] for n, p in enumerate(pos_list)}\n        # equally-spaced tree\n        else:\n            nx_g_tree = nx_g_solid\n            if solid_edges != 'confidence_tree':\n                adj_tree = adata.uns['paga']['confidence_tree']\n                nx_g_tree = nx.Graph(adj_tree)\n            pos = utils.hierarchy_pos(nx_g_tree, root)\n            if len(pos) < adjacency_solid.shape[0]:\n                raise ValueError('This is a forest and not a single tree. '\n                                 'Try another `layout`, e.g., {\\'fr\\'}.')\n        pos_array = np.array([pos[n] for count, n in enumerate(nx_g_solid)])\n    else:\n        if isinstance(pos, str):\n            if not pos.endswith('.gdf'):\n                raise ValueError('Currently only supporting reading positions from .gdf files.'\n                                 'Consider generating them using, for instance, Gephi.')\n            s = ''  # read the node definition from the file\n            with open(pos) as f:\n                f.readline()\n                for line in f:\n                    if line.startswith('edgedef>'):\n                        break\n                    s += line\n            from io import StringIO\n            df = pd.read_csv(StringIO(s), header=-1)\n            pos = df[[4, 5]].values\n        pos_array = pos\n        # convert to dictionary\n        pos = {n: [p[0], p[1]] for n, p in enumerate(pos)}\n    if len(pos) == 1: pos[0] = (0.5, 0.5)\n    # edge widths\n    base_edge_width = edge_width_scale * 5 * rcParams['lines.linewidth']\n    # draw dashed edges\n    if dashed_edges is not None:\n        widths = [x[-1]['weight'] for x in nx_g_dashed.edges(data=True)]\n        widths = base_edge_width * np.array(widths)\n        if max_edge_width is not None:\n            widths = np.clip(widths, None, max_edge_width)\n        nx.draw_networkx_edges(nx_g_dashed, pos, ax=ax, width=widths, edge_color='grey',\n                               style='dashed', alpha=0.5)\n\n    # draw solid edges\n    widths = [x[-1]['weight'] for x in nx_g_solid.edges(data=True)]\n    widths = base_edge_width * np.array(widths)\n    if min_edge_width is not None or max_edge_width is not None:\n        widths = np.clip(widths, min_edge_width, max_edge_width)\n    nx.draw_networkx_edges(nx_g_solid, pos, ax=ax, width=widths, edge_color='black')\n\n    if export_to_gexf:\n        for count, n in enumerate(nx_g_solid.nodes()):\n            nx_g_solid.node[count]['label'] = node_labels[count]\n            nx_g_solid.node[count]['color'] = color[count]\n            nx_g_solid.node[count]['viz'] = {\n                'position': {'x': 1000*pos[count][0],\n                             'y': 1000*pos[count][1],\n                             'z': 0}}\n        logg.msg('exporting to {}'.format(settings.writedir + 'paga_graph.gexf'), v=1)\n        nx.write_gexf(nx_g_solid, settings.writedir + 'paga_graph.gexf')\n    # deal with empty graph\n    # ax.plot(pos_array[:, 0], pos_array[:, 1], '.', c='white')\n    # draw the nodes (pie charts)\n    trans = ax.transData.transform\n    bbox = ax.get_position().get_points()\n    ax_x_min = bbox[0, 0]\n    ax_x_max = bbox[1, 0]\n    ax_y_min = bbox[0, 1]\n    ax_y_max = bbox[1, 1]\n    ax_len_x = ax_x_max - ax_x_min\n    ax_len_y = ax_y_max - ax_y_min\n    # print([ax_x_min, ax_x_max, ax_y_min, ax_y_max])\n    # print([ax_len_x, ax_len_y])\n    trans2 = ax.transAxes.inverted().transform\n    ax.set_frame_on(frameon)\n    ax.set_xticks([])\n    ax.set_yticks([])\n    if (groups_key is not None and groups_key + '_sizes' in adata.uns):\n        groups_sizes = adata.uns[groups_key + '_sizes']\n    else:\n        groups_sizes = np.ones(len(node_labels))\n    base_scale_scatter = 2000\n    base_pie_size = (base_scale_scatter / (np.sqrt(adjacency_solid.shape[0]) + 10)\n                     * node_size_scale)\n    median_group_size = np.median(groups_sizes)\n    groups_sizes = base_pie_size * np.power(\n        groups_sizes / median_group_size, node_size_power)\n    # usual scatter plot\n    if is_color_like(color[0]):\n        ax.scatter(pos_array[:, 0], pos_array[:, 1],\n                   c=color, edgecolors='face', s=groups_sizes)\n        for count, group in enumerate(node_labels):\n            ax.text(pos_array[count, 0], pos_array[count, 1], group,\n                verticalalignment='center',\n                horizontalalignment='center', size=fontsize)\n    # else pie chart plot\n    else:\n        force_labels_to_front = True  # TODO: solve this differently!\n        for count, n in enumerate(nx_g_solid.nodes()):\n            pie_size = groups_sizes[count] / base_scale_scatter\n            xx, yy = trans(pos[n])     # data coordinates\n            xa, ya = trans2((xx, yy))  # axis coordinates\n            xa = ax_x_min + (xa - pie_size/2) * ax_len_x\n            ya = ax_y_min + (ya - pie_size/2) * ax_len_y\n            # clip, the fruchterman layout sometimes places below figure\n            if ya < 0: ya = 0\n            if xa < 0: xa = 0\n            a = pl.axes([xa, ya, pie_size * ax_len_x, pie_size * ax_len_y])\n            if not isinstance(color[count], dict):\n                raise ValueError('{} is neither a dict of valid matplotlib colors '\n                                 'nor a valid matplotlib color.'.format(color[count]))\n            color_single = color[count].keys()\n            fracs = [color[count][c] for c in color_single]\n            if sum(fracs) < 1:\n                color_single = list(color_single)\n                color_single.append('grey')\n                fracs.append(1-sum(fracs))\n            a.pie(fracs, colors=color_single)\n            if not force_labels_to_front and node_labels is not None:\n                a.text(0.5, 0.5, node_labels[count],\n                       verticalalignment='center',\n                       horizontalalignment='center',\n                       transform=a.transAxes,\n                       size=fontsize)\n        # TODO: this is a terrible hack, but if we use the solution above (`not\n        # force_labels_to_front`), labels get hidden behind pies\n        if force_labels_to_front and node_labels is not None:\n            for count, n in enumerate(nx_g_solid.nodes()):\n                pie_size = groups_sizes[count] / base_scale_scatter\n                # all copy and paste from above\n                xx, yy = trans(pos[n])     # data coordinates\n                xa, ya = trans2((xx, yy))  # axis coordinates\n                # make sure a new axis is created\n                xa = ax_x_min + (xa - pie_size/2.0000001) * ax_len_x\n                ya = ax_y_min + (ya - pie_size/2.0000001) * ax_len_y\n                # clip, the fruchterman layout sometimes places below figure\n                if ya < 0: ya = 0\n                if xa < 0: xa = 0\n                a = pl.axes([xa, ya, pie_size * ax_len_x, pie_size * ax_len_y])\n                a.set_frame_on(False)\n                a.set_xticks([])\n                a.set_yticks([])\n                a.text(0.5, 0.5, node_labels[count],\n                       verticalalignment='center',\n                       horizontalalignment='center',\n                       transform=a.transAxes, size=fontsize)\n    if title is not None: ax.set_title(title)\n    if colorbar:\n        ax1 = pl.axes([0.95, 0.1, 0.03, 0.7])\n        cb = matplotlib.colorbar.ColorbarBase(ax1, cmap=cmap,\n                                              norm=norm)\n    return pos_array", "idx": 717}
{"project": "Scanpy", "commit_id": "709_scanpy_1.9.0_test_qc_metrics.py_test_layer_raw.py", "target": 0, "func": "def test_layer_raw(anndata):\n    adata = anndata.copy()\n    adata.raw = adata.copy()\n    adata.layers[\"counts\"] = adata.X.copy()\n    obs_orig, var_orig = sc.pp.calculate_qc_metrics(adata)\n    sc.pp.log1p(adata)  # To be sure they aren't reusing it\n    obs_layer, var_layer = sc.pp.calculate_qc_metrics(adata, layer=\"counts\")\n    obs_raw, var_raw = sc.pp.calculate_qc_metrics(adata, use_raw=True)\n    assert np.allclose(obs_orig, obs_layer)\n    assert np.allclose(obs_orig, obs_raw)\n    assert np.allclose(var_orig, var_layer)\n    assert np.allclose(var_orig, var_raw)", "idx": 719}
{"project": "Scanpy", "commit_id": "625_scanpy_1.0.4_scanpy_plotting_tools_paga.py__paga_graph.py", "target": 1, "func": "def _paga_graph(\n        adata,\n        ax,\n        layout=None,\n        layout_kwds={},\n        init_pos=None,\n        solid_edges=None,\n        dashed_edges=None,\n        transitions=None,\n        threshold=None,\n        threshold_arrows=None,\n        threshold_solid=None,\n        threshold_dashed=None,\n        root=0,\n        colors=None,\n        labels=None,\n        fontsize=None,\n        text_kwds=None,\n        node_size_scale=1,\n        node_size_power=0.5,\n        edge_width_scale=1,\n        title=None,\n        pos=None,\n        cmap=None,\n        frameon=True,\n        min_edge_width=None,\n        max_edge_width=None,\n        export_to_gexf=False,\n        cax=None,\n        cb_kwds={},\n        single_component=False,\n        random_state=0):\n    node_labels = labels  # rename for clarity\n    if (node_labels is not None\n        and isinstance(node_labels, str)\n        and node_labels != adata.uns['paga']['groups']):\n        raise ValueError('Provide a list of group labels for the PAGA groups {}, not {}.'\n                         .format(adata.uns['paga']['groups'], node_labels))\n    groups_key = adata.uns['paga']['groups']\n    if node_labels is None:\n        node_labels = adata.obs[groups_key].cat.categories\n    if colors is None and groups_key is not None:\n        if (groups_key + '_colors' not in adata.uns\n            or len(adata.obs[groups_key].cat.categories)\n               != len(adata.uns[groups_key + '_colors'])):\n            utils.add_colors_for_categorical_sample_annotation(adata, groups_key)\n        colors = adata.uns[groups_key + '_colors']\n        for iname, name in enumerate(adata.obs[groups_key].cat.categories):\n            if name in settings.categories_to_ignore: colors[iname] = 'grey'\n    if isinstance(root, str):\n        if root in node_labels:\n            root = list(node_labels).index(root)\n        else:\n            raise ValueError(\n                'If `root` is a string, it needs to be one of {} not \\'{}\\'.'\n                .format(node_labels.tolist(), root))\n    if isinstance(root, list) and root[0] in node_labels:\n        root = [list(node_labels).index(r) for r in root]\n    # define the objects\n    adjacency_solid = adata.uns['paga'][solid_edges].copy()\n    # set the the thresholds, either explicitly\n    if threshold is not None:\n        threshold_solid = threshold\n        threshold_dashed = threshold\n    # or to a default value\n    else:\n        if threshold_solid is None:\n            threshold_solid = 0.01  # default threshold\n        if threshold_dashed is None:\n            threshold_dashed = 0.01  # default treshold\n    if threshold_solid > 0:\n        adjacency_solid[adjacency_solid < threshold_solid] = 0\n        adjacency_solid.eliminate_zeros()\n    nx_g_solid = nx.Graph(adjacency_solid)\n    if dashed_edges is not None:\n        adjacency_dashed = adata.uns['paga'][dashed_edges].copy()\n        if threshold_dashed > 0:\n            adjacency_dashed[adjacency_dashed < threshold_dashed] = 0\n            adjacency_dashed.eliminate_zeros()\n        nx_g_dashed = nx.Graph(adjacency_dashed)\n    # color degree of the graph\n    if isinstance(colors, str) and colors.startswith('degree'):\n        # see also tools.paga.paga_degrees\n        if colors == 'degree_dashed':\n            colors = [d for _, d in nx_g_dashed.degree(weight='weight')]\n        elif colors == 'degree_solid':\n            colors = [d for _, d in nx_g_solid.degree(weight='weight')]\n        else:\n            raise ValueError('`degree` either \"degree_dashed\" or \"degree_solid\".')\n        colors = (np.array(colors) - np.min(colors)) / (np.max(colors) - np.min(colors))\n    # plot numeric colors\n    colorbar = False\n    if isinstance(colors, Iterable) and not isinstance(colors[0], (str, dict)):\n        import matplotlib\n        norm = matplotlib.colors.Normalize()\n        colors = norm(colors)\n        if cmap is None: cmap = rcParams['image.cmap']\n        cmap = matplotlib.cm.get_cmap(cmap)\n        colors = [cmap(c) for c in colors]\n        colorbar = True\n    if len(colors) < len(node_labels):\n        print(node_labels, colors)\n        raise ValueError(\n            '`color` list need to be at least as long as `goups`/`node_labels` list.')\n    # count number of connected components\n    n_components, labels = scipy.sparse.csgraph.connected_components(adjacency_solid)\n    if n_components > 1 and not single_component:\n        logg.info(\n            'Graph has more than a single connected component. '\n            'To restrict to this component, pass `single_component=True`.')\n    if n_components > 1 and single_component:\n        component_sizes = np.bincount(labels)\n        largest_component = np.where(\n            component_sizes == component_sizes.max())[0][0]\n        adjacency_solid = adjacency_solid.tocsr()[labels == largest_component, :]\n        adjacency_solid = adjacency_solid.tocsc()[:, labels == largest_component]\n        colors = np.array(colors)[labels == largest_component]\n        node_labels = np.array(node_labels)[labels == largest_component]\n        logg.info(\n            'Restricting graph to largest connected component by dropping categories\\n'\n            '{}'.format(\n                adata.obs[groups_key].cat.categories[labels != largest_component].tolist()))\n        nx_g_solid = nx.Graph(adjacency_solid)\n        if dashed_edges is not None:\n            raise ValueError('`single_component` only if `dashed_edges` is `None`.')\n    # node positions from adjacency_solid\n    if pos is None:\n        if layout is None:\n            layout = 'fr'\n        if layout == 'fa':\n            try:\n                from fa2 import ForceAtlas2\n            except:\n                logg.warn('Package \\'fa2\\' is not installed, falling back to layout \\'fr\\'.'\n                          'To use the faster and better ForceAtlas2 layout, '\n                          'install package \\'fa2\\' (`pip install fa2`).')\n                layout = 'fr'\n        if layout == 'fa':\n            np.random.seed(random_state)\n            if init_pos is None:\n                init_coords = np.random.random((adjacency_solid.shape[0], 2))\n            else:\n                init_coords = init_pos.copy()\n            forceatlas2 = ForceAtlas2(\n                # Behavior alternatives\n                outboundAttractionDistribution=False,  # Dissuade hubs\n                linLogMode=False,  # NOT IMPLEMENTED\n                adjustSizes=False,  # Prevent overlap (NOT IMPLEMENTED)\n                edgeWeightInfluence=1.0,\n                # Performance\n                jitterTolerance=1.0,  # Tolerance\n                barnesHutOptimize=True,\n                barnesHutTheta=1.2,\n                multiThreaded=False,  # NOT IMPLEMENTED\n                # Tuning\n                scalingRatio=2.0,\n                strongGravityMode=False,\n                gravity=1.0,\n                # Log\n                verbose=False)\n            if 'maxiter' in layout_kwds:\n                iterations = layout_kwds['maxiter']\n            elif 'iterations' in layout_kwds:\n                iterations = layout_kwds['iterations']\n            else:\n                iterations = 500\n            pos_list = forceatlas2.forceatlas2(\n                adjacency_solid, pos=init_coords, iterations=iterations)\n            pos = {n: [p[0], -p[1]] for n, p in enumerate(pos_list)}\n        elif layout == 'eq_tree':\n            nx_g_tree = nx_g_solid\n            if solid_edges == 'connectivities':\n                adj_tree = adata.uns['paga']['confidence_tree']\n                nx_g_tree = nx.Graph(adj_tree)\n            pos = utils.hierarchy_pos(nx_g_tree, root)\n            if len(pos) < adjacency_solid.shape[0]:\n                raise ValueError('This is a forest and not a single tree. '\n                                 'Try another `layout`, e.g., {\\'fr\\'}.')\n        else:\n            # igraph layouts\n            from ... import utils as sc_utils\n            g = sc_utils.get_igraph_from_adjacency(adjacency_solid)\n            if 'rt' in layout:\n                g_tree = g\n                if solid_edges == 'connectivities':\n                    adj_tree = adata.uns['paga']['confidence_tree']\n                    g_tree = sc_utils.get_igraph_from_adjacency(adj_tree)\n                pos_list = g_tree.layout(\n                    layout, root=root if isinstance(root, list) else [root]).coords\n            elif layout == 'circle':\n                pos_list = g.layout(layout).coords\n            else:\n                # I don't know why this is necessary\n                np.random.seed(random_state)\n                if init_pos is None:\n                    init_coords = np.random.random((adjacency_solid.shape[0], 2)).tolist()\n                else:\n                    init_pos = init_pos.copy()\n                    # this is a super-weird hack that is necessary as igraphs layout function\n                    # seems to do some strange stuff, here\n                    init_pos[:, 1] *= -1\n                    init_coords = init_pos.tolist()\n                pos_list = g.layout(\n                    layout, seed=init_coords,\n                    weights='weight', **layout_kwds).coords\n            pos = {n: [p[0], -p[1]] for n, p in enumerate(pos_list)}\n        pos_array = np.array([pos[n] for count, n in enumerate(nx_g_solid)])\n    else:\n        if isinstance(pos, str):\n            if not pos.endswith('.gdf'):\n                raise ValueError('Currently only supporting reading positions from .gdf files.'\n                                 'Consider generating them using, for instance, Gephi.')\n            s = ''  # read the node definition from the file\n            with open(pos) as f:\n                f.readline()\n                for line in f:\n                    if line.startswith('edgedef>'):\n                        break\n                    s += line\n            from io import StringIO\n            df = pd.read_csv(StringIO(s), header=-1)\n            pos = df[[4, 5]].values\n        pos_array = pos\n        # convert to dictionary\n        pos = {n: [p[0], p[1]] for n, p in enumerate(pos)}\n    if len(pos) == 1: pos[0] = (0.5, 0.5)\n    # edge widths\n    base_edge_width = edge_width_scale * 5 * rcParams['lines.linewidth']\n    # draw dashed edges\n    if dashed_edges is not None:\n        widths = [x[-1]['weight'] for x in nx_g_dashed.edges(data=True)]\n        widths = base_edge_width * np.array(widths)\n        if max_edge_width is not None:\n            widths = np.clip(widths, None, max_edge_width)\n        nx.draw_networkx_edges(nx_g_dashed, pos, ax=ax, width=widths, edge_color='grey',\n                               style='dashed', alpha=0.5)\n    # draw solid edges\n    if transitions is None:\n        widths = [x[-1]['weight'] for x in nx_g_solid.edges(data=True)]\n        widths = base_edge_width * np.array(widths)\n        if min_edge_width is not None or max_edge_width is not None:\n            widths = np.clip(widths, min_edge_width, max_edge_width)\n        nx.draw_networkx_edges(nx_g_solid, pos, ax=ax, width=widths, edge_color='black')\n    # draw directed edges\n    else:\n        adjacency_transitions = adata.uns['paga'][transitions].copy()\n        if threshold_arrows is None:\n            threshold_arrows = 0.005\n        adjacency_transitions[adjacency_transitions < threshold_arrows] = 0\n        adjacency_transitions.eliminate_zeros()\n        g_dir = nx.DiGraph(adjacency_transitions.T)\n        widths = [x[-1]['weight'] for x in g_dir.edges(data=True)]\n        widths = 100 * base_edge_width * np.array(widths)\n        if min_edge_width is not None or max_edge_width is not None:\n            widths = np.clip(widths, min_edge_width, max_edge_width)\n        nx.draw_networkx_edges(g_dir, pos, ax=ax, width=widths, edge_color='black')\n    if export_to_gexf:\n        if isinstance(colors[0], tuple):\n            from matplotlib.colors import rgb2hex\n            colors = [rgb2hex(c) for c in colors]\n        for count, n in enumerate(nx_g_solid.nodes()):\n            nx_g_solid.node[count]['label'] = node_labels[count]\n            nx_g_solid.node[count]['color'] = colors[count]\n            nx_g_solid.node[count]['viz'] = {\n                'position': {'x': 1000*pos[count][0],\n                             'y': 1000*pos[count][1],\n                             'z': 0}}\n        filename = settings.writedir + 'paga_graph.gexf'\n        logg.msg('exporting to {}'.format(filename), v=1)\n        if settings.writedir != '' and not os.path.exists(settings.writedir):\n            os.makedirs(settings.writedir)\n        nx.write_gexf(nx_g_solid, settings.writedir + 'paga_graph.gexf')\n    # deal with empty graph\n    # ax.plot(pos_array[:, 0], pos_array[:, 1], '.', c='white')\n    # draw the nodes (pie charts)\n    trans = ax.transData.transform\n    bbox = ax.get_position().get_points()\n    ax_x_min = bbox[0, 0]\n    ax_x_max = bbox[1, 0]\n    ax_y_min = bbox[0, 1]\n    ax_y_max = bbox[1, 1]\n    ax_len_x = ax_x_max - ax_x_min\n    ax_len_y = ax_y_max - ax_y_min\n    # print([ax_x_min, ax_x_max, ax_y_min, ax_y_max])\n    # print([ax_len_x, ax_len_y])\n    trans2 = ax.transAxes.inverted().transform\n    ax.set_frame_on(frameon)\n    ax.set_xticks([])\n    ax.set_yticks([])\n    if (groups_key is not None and groups_key + '_sizes' in adata.uns):\n        groups_sizes = adata.uns[groups_key + '_sizes']\n    else:\n        groups_sizes = np.ones(len(node_labels))\n    base_scale_scatter = 2000\n    base_pie_size = (base_scale_scatter / (np.sqrt(adjacency_solid.shape[0]) + 10)\n                     * node_size_scale)\n    median_group_size = np.median(groups_sizes)\n    groups_sizes = base_pie_size * np.power(\n        groups_sizes / median_group_size, node_size_power)\n    # usual scatter plot\n    if is_color_like(colors[0]):\n        scatter = ax.scatter(pos_array[:, 0], pos_array[:, 1],\n                             c=colors, edgecolors='face', s=groups_sizes)\n        for count, group in enumerate(node_labels):\n            ax.text(pos_array[count, 0], pos_array[count, 1], group,\n                    verticalalignment='center',\n                     horizontalalignment='center', size=fontsize, **text_kwds)\n    # else pie chart plot\n    else:\n        force_labels_to_front = True  # TODO: solve this differently!\n        for count, n in enumerate(nx_g_solid.nodes()):\n            pie_size = groups_sizes[count] / base_scale_scatter\n            xx, yy = trans(pos[n])     # data coordinates\n            xa, ya = trans2((xx, yy))  # axis coordinates\n            xa = ax_x_min + (xa - pie_size/2) * ax_len_x\n            ya = ax_y_min + (ya - pie_size/2) * ax_len_y\n            # clip, the fruchterman layout sometimes places below figure\n            if ya < 0: ya = 0\n            if xa < 0: xa = 0\n            a = ax.axes([xa, ya, pie_size * ax_len_x, pie_size * ax_len_y])\n            if not isinstance(colors[count], dict):\n                raise ValueError('{} is neither a dict of valid matplotlib colors '\n                                 'nor a valid matplotlib color.'.format(colors[count]))\n            color_single = colors[count].keys()\n            fracs = [colors[count][c] for c in color_single]\n            if sum(fracs) < 1:\n                color_single = list(color_single)\n                color_single.append('grey')\n                fracs.append(1-sum(fracs))\n            a.pie(fracs, colors=color_single)\n            if not force_labels_to_front and node_labels is not None:\n                a.text(0.5, 0.5, node_labels[count],\n                       verticalalignment='center',\n                       horizontalalignment='center',\n                       transform=a.transAxes,\n                       size=fontsize)\n        # TODO: this is a terrible hack, but if we use the solution above (`not\n        # force_labels_to_front`), labels get hidden behind pies\n        if force_labels_to_front and node_labels is not None:\n            for count, n in enumerate(nx_g_solid.nodes()):\n                pie_size = groups_sizes[count] / base_scale_scatter\n                # all copy and paste from above\n                xx, yy = trans(pos[n])     # data coordinates\n                xa, ya = trans2((xx, yy))  # axis coordinates\n                # make sure a new axis is created\n                xa = ax_x_min + (xa - pie_size/2.0000001) * ax_len_x\n                ya = ax_y_min + (ya - pie_size/2.0000001) * ax_len_y\n                # clip, the fruchterman layout sometimes places below figure\n                if ya < 0: ya = 0\n                if xa < 0: xa = 0\n                a = pl.axes([xa, ya, pie_size * ax_len_x, pie_size * ax_len_y])\n                a.set_frame_on(False)\n                a.set_xticks([])\n                a.set_yticks([])\n                a.text(0.5, 0.5, node_labels[count],\n                       verticalalignment='center',\n                       horizontalalignment='center',\n                       transform=a.transAxes, size=fontsize)\n    if title is not None: ax.set_title(title)\n    cb = None\n    if colorbar:\n        cax = pl.axes([0.95, 0.1, 0.03, 0.8]) if cax is None else cax\n        cb = matplotlib.colorbar.ColorbarBase(cax, cmap=cmap,\n                                              norm=norm, **cb_kwds)\n    return pos_array, cb", "idx": 725}
{"project": "Scanpy", "commit_id": "935_scanpy_1.4.4_scanpy_plotting__tools_scatterplots.py_embedding.py", "target": 1, "func": "def embedding(\n    adata: AnnData,\n    basis: str,\n    *,\n    color: Union[str, Sequence[str], None] = None,\n    gene_symbols: Optional[str] = None,\n    use_raw: Optional[bool] = None,\n    sort_order: bool = True,\n    edges: bool = False,\n    edges_width: float = 0.1,\n    edges_color: Union[str, Sequence[float], Sequence[str]] = 'grey',\n    arrows: bool = False,\n    arrows_kwds: Optional[Mapping[str, Any]] = None,\n    groups: Optional[str] = None,\n    components: Union[str, Sequence[str]] = None,\n    layer: Optional[str] = None,\n    projection: str = '2d',\n    color_map: Union[Colormap, str, None] = None,\n    palette: Union[str, Sequence[str], Cycler, None] = None,\n    size: Optional[float] = None,\n    frameon: Optional[bool] = None,\n    legend_fontsize: Optional[int] = None,\n    legend_fontweight: str = 'bold',\n    legend_loc: str = 'right margin',\n    legend_fontoutline: Optional[int] = None,\n    vmax: Union[VMinMax, Sequence[VMinMax], None] = None,\n    vmin: Union[VMinMax, Sequence[VMinMax], None] = None,\n    add_outline: Optional[bool] = False,\n    outline_width: Tuple[float, float] = (0.3, 0.05),\n    outline_color: Tuple[str, str] = ('black', 'white'),\n    ncols: int = 4,\n    hspace: float = 0.25,\n    wspace: Optional[float] = None,\n    title: Union[str, Sequence[str], None] = None,\n    show: Optional[bool] = None,\n    save: Union[bool, str, None] = None,\n    ax: Optional[Axes] = None,\n    return_fig: Optional[bool] = None,\n    **kwargs,\n) -> Union[Figure, Axes, None]:\n    \"\"\"\\\n    Scatter plot for user specified embedding basis (e.g. umap, pca, etc)\n    Parameters\n    ----------\n    basis\n        Name of the `obsm` basis to use.\n    {adata_color_etc}\n    {edges_arrows}\n    {scatter_bulk}\n    {show_save_ax}\n    Returns\n    -------\n    If `show==False` a :class:`~matplotlib.axes.Axes` or a list of it.\n    \"\"\"\n    sanitize_anndata(adata)\n    if color_map is not None:\n        kwargs['cmap'] = color_map\n    if size is not None:\n        kwargs['s'] = size\n    if 'edgecolor' not in kwargs:\n        # by default turn off edge color. Otherwise, for\n        # very small sizes the edge will not reduce its size\n        # (https://github.com/theislab/scanpy/issues/293)\n        kwargs['edgecolor'] = 'none'\n    if projection == '3d':\n        from mpl_toolkits.mplot3d import Axes3D\n        args_3d = {'projection': '3d'}\n    else:\n        args_3d = {}\n    # Deal with Raw\n    if use_raw is None:\n        # check if adata.raw is set\n        use_raw = layer is None and adata.raw is not None\n    if use_raw and layer is not None:\n        raise ValueError(\n            \"Cannot use both a layer and the raw representation. Was passed:\"\n            f\"use_raw={use_raw}, layer={layer}.\"\n        )\n    if wspace is None:\n        #  try to set a wspace that is not too large or too small given the\n        #  current figure size\n        wspace = 0.75 / rcParams['figure.figsize'][0] + 0.02\n    if adata.raw is None and use_raw:\n        raise ValueError(\n            \"`use_raw` is set to True but AnnData object does not have raw. \"\n            \"Please check.\"\n        )\n    # turn color into a python list\n    color = [color] if isinstance(color, str) or color is None else list(color)\n    if title is not None:\n        # turn title into a python list if not None\n        title = [title] if isinstance(title, str) else list(title)\n    # get the points position and the components list\n    # (only if components is not None)\n    data_points, components_list = _get_data_points(adata, basis, projection, components)\n    # Setup layout.\n    # Most of the code is for the case when multiple plots are required\n    # 'color' is a list of names that want to be plotted.\n    # Eg. ['Gene1', 'louvain', 'Gene2'].\n    # component_list is a list of components [[0,1], [1,2]]\n    if (isinstance(color, abc.Sequence) and len(color) > 1) or len(components_list) > 1:\n        if ax is not None:\n            raise ValueError(\n                \"Cannot specify `ax` when plotting multiple panels \"\n                \"(each for a given value of 'color').\"\n            )\n        if len(components_list) == 0:\n            components_list = [None]\n        # each plot needs to be its own panel\n        num_panels = len(color) * len(components_list)\n        fig, grid = _panel_grid(hspace, wspace, ncols, num_panels)\n    else:\n        if len(components_list) == 0:\n            components_list = [None]\n        grid = None\n        if ax is None:\n            fig = pl.figure()\n            ax = fig.add_subplot(111, **args_3d)\n\n    # turn vmax and vmin into a sequence\n    if not isinstance(vmax, abc.Sequence):\n        vmax = [vmax]\n    if not isinstance(vmin, abc.Sequence):\n        vmin = [vmin]\n\n    if 's' not in kwargs:\n        kwargs['s'] = 120000 / adata.shape[0]\n    size = kwargs.pop('s')\n    ###\n    # make the plots\n    axs = []\n    import itertools\n    idx_components = range(len(components_list))\n    # use itertools.product to make a plot for each color and for each component\n    # For example if color=[gene1, gene2] and components=['1,2, '2,3'].\n    # The plots are: [\n    #     color=gene1, components=[1,2], color=gene1, components=[2,3],\n    #     color=gene2, components = [1, 2], color=gene2, components=[2,3],\n    # ]\n    for count, (value_to_plot, component_idx) in enumerate(itertools.product(color, idx_components)):\n        color_vector, categorical = _get_color_values(\n            adata, value_to_plot, layer=layer,\n            groups=groups, palette=palette,\n            use_raw=use_raw, gene_symbols=gene_symbols,\n        )\n        # check if higher value points should be plot on top\n        if sort_order is True and value_to_plot is not None and categorical is False:\n            order = np.argsort(color_vector)\n            color_vector = color_vector[order]\n            _data_points = data_points[component_idx][order, :]\n            # check if 'size' is given (stored in kwargs['s']\n            # and reorder it.\n            import pandas.core.series\n            if (\n                's' in kwargs\n                and kwargs['s'] is not None\n                and isinstance(kwargs['s'], (\n                    abc.Sequence,\n                    pandas.core.series.Series,\n                    np.ndarray,\n                ))\n                and len(kwargs['s']) == len(color_vector)\n            ):\n                kwargs['s'] = np.array(kwargs['s'])[order]\n        else:\n            _data_points = data_points[component_idx]\n        # if plotting multiple panels, get the ax from the grid spec\n        # else use the ax value (either user given or created previously)\n        if grid:\n            ax = pl.subplot(grid[count], **args_3d)\n            axs.append(ax)\n        if not (settings._frameon if frameon is None else frameon):\n            ax.axis('off')\n        if title is None:\n            if value_to_plot is not None:\n                ax.set_title(value_to_plot)\n            else:\n                ax.set_title('')\n        else:\n            try:\n                ax.set_title(title[count])\n            except IndexError:\n                logg.warning(\n                    \"The title list is shorter than the number of panels. \"\n                    \"Using 'color' value instead for some plots.\"\n                )\n                ax.set_title(value_to_plot)\n        # check vmin and vmax options\n        if categorical:\n            kwargs['vmin'] = kwargs['vmax'] = None\n        else:\n            kwargs['vmin'], kwargs['vmax'] = _get_vmin_vmax(vmin, vmax, count, color_vector)\n        # make the scatter plot\n        if projection == '3d':\n            cax = ax.scatter(\n                _data_points[:, 0], _data_points[:, 1], _data_points[:, 2],\n                marker=\".\", c=color_vector, rasterized=settings._vector_friendly,\n                **kwargs,\n            )\n        else:\n            if add_outline:\n                # the default contour is a black edge followd by a\n                # thin white edged added around connected clusters.\n                # To add a contour\n                # three overlapping scatter plots are drawn:\n                # First black dots with slightly larger size,\n                # then, white dots a bit smaller, but still larger\n                # than the final dots. Then the final dots are drawn\n                # with some transparency.\n                bg_width, gap_width = outline_width\n                point = np.sqrt(size)\n                gap_size = (point + (point * gap_width)*2)**2\n                bg_size = (np.sqrt(gap_size) + (point * bg_width)*2)**2\n                # the default black and white colors can be changes using\n                # the contour_config parameter\n                bg_color, gap_color = outline_color\n                # remove edge from kwargs if present\n                # because edge needs to be set to None\n                kwargs['edgecolor'] = 'none'\n                # remove alpha for contour\n                alpha = kwargs.pop('alpha') if 'alpha' in kwargs else None\n                ax.scatter(\n                    _data_points[:, 0], _data_points[:, 1], s=bg_size,\n                    marker=\".\", c=bg_color, rasterized=settings._vector_friendly,\n                    **kwargs)\n                ax.scatter(\n                    _data_points[:, 0], _data_points[:, 1], s=gap_size,\n                    marker=\".\", c=gap_color, rasterized=settings._vector_friendly,\n                    **kwargs)\n                # if user did not set alpha, set alpha to 0.7\n                kwargs['alpha'] = 0.7 if alpha is None else alpha\n            cax = ax.scatter(\n                _data_points[:, 0], _data_points[:, 1], s=size,\n                marker=\".\", c=color_vector, rasterized=settings._vector_friendly,\n                **kwargs,\n            )\n        # remove y and x ticks\n        ax.set_yticks([])\n        ax.set_xticks([])\n        if projection == '3d':\n            ax.set_zticks([])\n        # set default axis_labels\n        name = _basis2name(basis)\n        if components is not None:\n            axis_labels = [name + str(x + 1) for x in components_list[component_idx]]\n        elif projection == '3d':\n            axis_labels = [name + str(x + 1) for x in range(3)]\n        else:\n            axis_labels = [name + str(x + 1) for x in range(2)]\n        ax.set_xlabel(axis_labels[0])\n        ax.set_ylabel(axis_labels[1])\n        if projection == '3d':\n            # shift the label closer to the axis\n            ax.set_zlabel(axis_labels[2], labelpad=-7)\n        ax.autoscale_view()\n        if edges:\n            _utils.plot_edges(ax, adata, basis, edges_width, edges_color)\n        if arrows:\n            _utils.plot_arrows(ax, adata, basis, arrows_kwds)\n        if value_to_plot is None:\n            # if only dots were plotted without an associated value\n            # there is not need to plot a legend or a colorbar\n            continue\n        if legend_fontoutline is not None:\n            path_effect = [patheffects.withStroke(\n                linewidth=legend_fontoutline,\n                foreground='w',\n            )]\n        else:\n            path_effect = None\n        _add_legend_or_colorbar(\n            adata, ax, cax, categorical, value_to_plot, legend_loc,\n            _data_points, legend_fontweight, legend_fontsize, path_effect,\n            groups, bool(grid),\n        )\n    if return_fig is True:\n        return fig\n    axs = axs if grid else ax\n    _utils.savefig_or_show(basis, show=show, save=save)\n    if show is False:\n        return axs", "idx": 726}
{"project": "Scanpy", "commit_id": "917_scanpy_1.4.4_scanpy_plotting__tools_scatterplots.py_embedding.py", "target": 1, "func": "def embedding(\n    adata: AnnData,\n    basis: str,\n    *,\n    color: Union[str, Sequence[str], None] = None,\n    gene_symbols: Optional[str] = None,\n    use_raw: Optional[bool] = None,\n    sort_order: bool = True,\n    edges: bool = False,\n    edges_width: float = 0.1,\n    edges_color: Union[str, Sequence[float], Sequence[str]] = 'grey',\n    arrows: bool = False,\n    arrows_kwds: Optional[Mapping[str, Any]] = None,\n    groups: Optional[str] = None,\n    components: Union[str, Sequence[str]] = None,\n    layer: Optional[str] = None,\n    projection: str = '2d',\n    color_map: Union[Colormap, str, None] = None,\n    palette: Union[str, Sequence[str], Cycler, None] = None,\n    size: Optional[float] = None,\n    frameon: Optional[bool] = None,\n    legend_fontsize: Optional[int] = None,\n    legend_fontweight: str = 'bold',\n    legend_loc: str = 'right margin',\n    legend_fontoutline: Optional[int] = None,\n    vmax: Union[VMinMax, Sequence[VMinMax], None] = None,\n    vmin: Union[VMinMax, Sequence[VMinMax], None] = None,\n    add_outline: Optional[bool] = False,\n    outline_width: Tuple[float, float] = (0.3, 0.05),\n    outline_color: Tuple[str, str] = ('black', 'white'),\n    ncols: int = 4,\n    hspace: float = 0.25,\n    wspace: Optional[float] = None,\n    title: Union[str, Sequence[str], None] = None,\n    show: Optional[bool] = None,\n    save: Union[bool, str, None] = None,\n    ax: Optional[Axes] = None,\n    return_fig: Optional[bool] = None,\n    **kwargs\n) -> Union[Figure, Axes, None]:\n    \"\"\"\\\n    Scatter plot for user specified embedding basis (e.g. umap, pca, etc)\n    Parameters\n    ----------\n    basis\n        Name of the `obsm` basis to use.\n    {adata_color_etc}\n    {edges_arrows}\n    {scatter_bulk}\n    {show_save_ax}\n    Returns\n    -------\n    If `show==False` a :class:`~matplotlib.axes.Axes` or a list of it.\n    \"\"\"\n    sanitize_anndata(adata)\n    if color_map is not None:\n        kwargs['cmap'] = color_map\n    if size is not None:\n        kwargs['s'] = size\n    if 'edgecolor' not in kwargs:\n        # by default turn off edge color. Otherwise, for\n        # very small sizes the edge will not reduce its size\n        # (https://github.com/theislab/scanpy/issues/293)\n        kwargs['edgecolor'] = 'none'\n    if projection == '3d':\n        from mpl_toolkits.mplot3d import Axes3D\n        args_3d = {'projection': '3d'}\n    else:\n        args_3d = {}\n    # Deal with Raw\n    if use_raw is None:\n        # check if adata.raw is set\n        use_raw = layer is None and adata.raw is not None\n    if use_raw and layer is not None:\n        raise ValueError(\n            \"Cannot use both a layer and the raw representation. Was passed:\"\n            f\"use_raw={use_raw}, layer={layer}.\"\n        )\n    if wspace is None:\n        #  try to set a wspace that is not too large or too small given the\n        #  current figure size\n        wspace = 0.75 / rcParams['figure.figsize'][0] + 0.02\n    if adata.raw is None and use_raw:\n        raise ValueError(\n            \"`use_raw` is set to True but AnnData object does not have raw. \"\n            \"Please check.\"\n        )\n    # turn color into a python list\n    color = [color] if isinstance(color, str) or color is None else list(color)\n    if title is not None:\n        # turn title into a python list if not None\n        title = [title] if isinstance(title, str) else list(title)\n    ####\n    # get the points position and the components list (only if components is not 'None)\n    data_points, components_list = _get_data_points(adata, basis, projection, components)\n    ###\n    # setup layout. Most of the code is for the case when multiple plots are required\n    # 'color' is a list of names that want to be plotted. Eg. ['Gene1', 'louvain', 'Gene2'].\n    # component_list is a list of components [[0,1], [1,2]]\n    if (isinstance(color, abc.Sequence) and len(color) > 1) or len(components_list) > 1:\n        if ax is not None:\n            raise ValueError(\n                \"When plotting multiple panels (each for a given value of 'color') \"\n                \"a given ax can not be used\"\n            )\n        if len(components_list) == 0:\n            components_list = [None]\n        multi_panel = True\n        # each plot needs to be its own panel\n        from matplotlib import gridspec\n        # set up the figure\n        num_panels = len(color) * len(components_list)\n        n_panels_x = min(ncols, num_panels)\n        n_panels_y = np.ceil(num_panels / n_panels_x).astype(int)\n        # each panel will have the size of rcParams['figure.figsize']\n        fig = pl.figure(figsize=(n_panels_x * rcParams['figure.figsize'][0] * (1 + wspace),\n                                 n_panels_y * rcParams['figure.figsize'][1]))\n        left = 0.2 / n_panels_x\n        bottom = 0.13 / n_panels_y\n        gs = gridspec.GridSpec(\n            nrows=n_panels_y, ncols=n_panels_x,\n            left=left, right=1-(n_panels_x-1)*left-0.01/n_panels_x,\n            bottom=bottom, top=1-(n_panels_y-1)*bottom-0.1/n_panels_y,\n            hspace=hspace, wspace=wspace,\n        )\n    else:\n        if len(components_list) == 0:\n            components_list = [None]\n        multi_panel = False\n        if ax is None:\n            fig = pl.figure()\n            ax = fig.add_subplot(111, **args_3d)\n    # turn vmax and vmin into a sequence\n    if not isinstance(vmax, List):\n        vmax = [vmax]\n    if not isinstance(vmin, List):\n        vmin = [vmin]\n    if 's' not in kwargs:\n        kwargs['s'] = 120000 / adata.shape[0]\n    size = kwargs.pop('s')\n    ###\n    # make the plots\n    axs = []\n    import itertools\n    idx_components = range(len(components_list))\n    # use itertools.product to make a plot for each color and for each component\n    # For example if color=[gene1, gene2] and components=['1,2, '2,3'].\n    # The plots are: [color=gene1, components=[1,2], color=gene1, components=[2,3],\n    #                 color=gene2, components = [1, 2], color=gene2, components=[2,3]]\n    for count, (value_to_plot, component_idx) in enumerate(itertools.product(color, idx_components)):\n        color_vector, categorical = _get_color_values(\n            adata, value_to_plot, layer=layer,\n            groups=groups, palette=palette,\n            use_raw=use_raw, gene_symbols=gene_symbols,\n        )\n        # check if higher value points should be plot on top\n        if sort_order is True and value_to_plot is not None and categorical is False:\n            order = np.argsort(color_vector)\n            color_vector = color_vector[order]\n            _data_points = data_points[component_idx][order, :]\n            # check if 'size' is given (stored in kwargs['s']\n            # and reorder it.\n            import pandas.core.series\n            if 's' in kwargs and kwargs['s'] is not None \\\n                and isinstance(kwargs['s'],(list, pandas.core.series.Series, np.ndarray)) \\\n                and len(kwargs['s']) == len(color_vector):\n                kwargs['s'] = np.array(kwargs['s'])[order]\n        else:\n            _data_points = data_points[component_idx]\n        # if plotting multiple panels, get the ax from the grid spec\n        # else use the ax value (either user given or created previously)\n        if multi_panel is True:\n            ax = pl.subplot(gs[count], **args_3d)\n            axs.append(ax)\n        if not (settings._frameon if frameon is None else frameon):\n            ax.axis('off')\n        if title is None:\n            if value_to_plot is not None:\n                ax.set_title(value_to_plot)\n            else:\n                ax.set_title('')\n        else:\n            try:\n                ax.set_title(title[count])\n            except IndexError:\n                logg.warning(\n                    \"The title list is shorter than the number of panels. \"\n                    \"Using 'color' value instead for some plots.\"\n                )\n                ax.set_title(value_to_plot)\n\n        # check vmin and vmax options\n        kwargs['vmin'], kwargs['vmax'] = _get_vmin_vmax(vmin, vmax, count, color_vector)\n\n        # make the scatter plot\n        if projection == '3d':\n            cax = ax.scatter(\n                _data_points[:, 0], _data_points[:, 1], _data_points[:, 2],\n                marker=\".\", c=color_vector, rasterized=settings._vector_friendly,\n                **kwargs,\n            )\n        else:\n            if add_outline:\n                # the default contour is a black edge followd by a\n                # thin white edged added around connected clusters.\n                # To add a contour\n                # three overlapping scatter plots are drawn:\n                # First black dots with slightly larger size,\n                # then, white dots a bit smaller, but still larger\n                # than the final dots. Then the final dots are drawn\n                # with some transparency.\n                bg_width, gap_width = outline_width\n                point = np.sqrt(size)\n                gap_size = (point + (point * gap_width)*2)**2\n                bg_size = (np.sqrt(gap_size) + (point * bg_width)*2)**2\n                # the default black and white colors can be changes using\n                # the contour_config parameter\n                bg_color, gap_color = outline_color\n                # remove edge from kwargs if present\n                # because edge needs to be set to None\n                kwargs['edgecolor'] = 'none'\n                # remove alpha for contour\n                alpha = kwargs.pop('alpha') if 'alpha' in kwargs else None\n                ax.scatter(\n                    _data_points[:, 0], _data_points[:, 1], s=bg_size,\n                    marker=\".\", c=bg_color, rasterized=settings._vector_friendly,\n                    **kwargs)\n                ax.scatter(\n                    _data_points[:, 0], _data_points[:, 1], s=gap_size,\n                    marker=\".\", c=gap_color, rasterized=settings._vector_friendly,\n                    **kwargs)\n                # if user did not set alpha, set alpha to 0.7\n                kwargs['alpha'] = 0.7 if alpha is None else alpha\n            cax = ax.scatter(\n                _data_points[:, 0], _data_points[:, 1], s=size,\n                marker=\".\", c=color_vector, rasterized=settings._vector_friendly,\n                **kwargs,\n            )\n        # remove y and x ticks\n        ax.set_yticks([])\n        ax.set_xticks([])\n        if projection == '3d':\n            ax.set_zticks([])\n        # set default axis_labels\n        name = _basis2name(basis)\n        if components is not None:\n            axis_labels = [name + str(x + 1) for x in components_list[component_idx]]\n        elif projection == '3d':\n            axis_labels = [name + str(x + 1) for x in range(3)]\n        else:\n            axis_labels = [name + str(x + 1) for x in range(2)]\n        ax.set_xlabel(axis_labels[0])\n        ax.set_ylabel(axis_labels[1])\n        if projection == '3d':\n            # shift the label closer to the axis\n            ax.set_zlabel(axis_labels[2], labelpad=-7)\n        ax.autoscale_view()\n        if edges:\n            utils.plot_edges(ax, adata, basis, edges_width, edges_color)\n        if arrows:\n            utils.plot_arrows(ax, adata, basis, arrows_kwds)\n        if value_to_plot is None:\n            # if only dots were plotted without an associated value\n            # there is not need to plot a legend or a colorbar\n            continue\n        if legend_fontoutline is not None:\n            legend_fontoutline = [patheffects.withStroke(linewidth=legend_fontoutline,\n                                                         foreground='w')]\n        _add_legend_or_colorbar(\n            adata, ax, cax, categorical, value_to_plot, legend_loc,\n            _data_points, legend_fontweight, legend_fontsize, legend_fontoutline,\n            groups, multi_panel,\n        )\n    if return_fig is True:\n        return fig\n    axs = axs if multi_panel else ax\n    utils.savefig_or_show(basis, show=show, save=save)\n    if show is False:\n        return axs", "idx": 727}
{"project": "Scanpy", "commit_id": "844_scanpy_1.4_scanpy_readwrite.py__read_softgz.py", "target": 1, "func": "def _read_softgz(filename):\n    \"\"\"Read a SOFT format data file.\n    The SOFT format is documented here\n    http://www.ncbi.nlm.nih.gov/geo/info/soft2.html.\n    Returns\n    -------\n    adata\n    Notes\n    -----\n    The function is based on a script by Kerby Shedden.\n    http://dept.stat.lsa.umich.edu/~kshedden/Python-Workshop/gene_expression_comparison.html\n    \"\"\"\n    filename = str(filename)  # allow passing pathlib.Path objects\n    import gzip\n    with gzip.open(filename, mode='rt') as file:\n        # The header part of the file contains information about the\n        # samples. Read that information first.\n        samples_info = {}\n        for line in file:\n            if line.startswith(\"!dataset_table_begin\"):\n                break\n            elif line.startswith(\"!subset_description\"):\n                subset_description = line.split(\"=\")[1].strip()\n            elif line.startswith(\"!subset_sample_id\"):\n                subset_ids = line.split(\"=\")[1].split(\",\")\n                subset_ids = [x.strip() for x in subset_ids]\n                for k in subset_ids:\n                    samples_info[k] = subset_description\n        # Next line is the column headers (sample id's)\n        sample_names = file.readline().strip().split(\"\\t\")\n        # The column indices that contain gene expression data\n        I = [i for i, x in enumerate(sample_names) if x.startswith(\"GSM\")]\n        # Restrict the column headers to those that we keep\n        sample_names = [sample_names[i] for i in I]\n        # Get a list of sample labels\n        groups = [samples_info[k] for k in sample_names]\n        # Read the gene expression data as a list of lists, also get the gene\n        # identifiers\n        gene_names, X = [], []\n        for line in file:\n            # This is what signals the end of the gene expression data\n            # section in the file\n            if line.startswith(\"!dataset_table_end\"):\n                break\n            V = line.split(\"\\t\")\n            # Extract the values that correspond to gene expression measures\n            # and convert the strings to numbers\n            x = [float(V[i]) for i in I]\n            X.append(x)\n            gene_names.append(V[1])\n    # Convert the Python list of lists to a Numpy array and transpose to match\n    # the Scanpy convention of storing samples in rows and variables in colums.\n    X = np.array(X).T\n    row_names = sample_names\n    col_names = gene_names\n    obs = np.zeros((len(row_names),), dtype=[('obs_names', 'S21'), ('groups', 'S21')])\n    obs['obs_names'] = sample_names\n    obs['groups'] = groups\n    var = np.zeros((len(gene_names),), dtype=[('var_names', 'S21')])\n    var['var_names'] = gene_names\n    ddata = {'X': X, 'obs': obs, 'var': var}\n    return AnnData(ddata)", "idx": 728}
{"project": "Scanpy", "commit_id": "795_scanpy_1.9.0__dpt.py_kendall_tau_split.py", "target": 0, "func": "def kendall_tau_split(self, a, b) -> int:\n        \"\"\"Return splitting index that maximizes correlation in the sequences.\n\n        Compute difference in Kendall tau for all splitted sequences.\n\n        For each splitting index i, compute the difference of the two\n        correlation measures kendalltau(a[:i], b[:i]) and\n        kendalltau(a[i:], b[i:]).\n\n        Returns the splitting index that maximizes\n            kendalltau(a[:i], b[:i]) - kendalltau(a[i:], b[i:])\n\n        Parameters\n        ----------\n        a, b : np.ndarray\n            One dimensional sequences.\n\n        Returns\n        -------\n        Splitting index according to above description.\n        \"\"\"\n        if a.size != b.size:\n            raise ValueError('a and b need to have the same size')\n        if a.ndim != b.ndim != 1:\n            raise ValueError('a and b need to be one-dimensional arrays')\n        import scipy as sp\n\n        min_length = 5\n        n = a.size\n        idx_range = np.arange(min_length, a.size - min_length - 1, dtype=int)\n        corr_coeff = np.zeros(idx_range.size)\n        pos_old = sp.stats.kendalltau(a[:min_length], b[:min_length])[0]\n        neg_old = sp.stats.kendalltau(a[min_length:], b[min_length:])[0]\n        for ii, i in enumerate(idx_range):\n            if True:\n                # compute differences in concordance when adding a[i] and b[i]\n                # to the first subsequence, and removing these elements from\n                # the second subsequence\n                diff_pos, diff_neg = self._kendall_tau_diff(a, b, i)\n                pos = pos_old + self._kendall_tau_add(i, diff_pos, pos_old)\n                neg = neg_old + self._kendall_tau_subtract(n - i, diff_neg, neg_old)\n                pos_old = pos\n                neg_old = neg\n            if False:\n                # computation using sp.stats.kendalltau, takes much longer!\n                # just for debugging purposes\n                pos = sp.stats.kendalltau(a[: i + 1], b[: i + 1])[0]\n                neg = sp.stats.kendalltau(a[i + 1 :], b[i + 1 :])[0]\n            if False:\n                # the following is much slower than using sp.stats.kendalltau,\n                # it is only good for debugging because it allows to compute the\n                # tau-a version, which does not account for ties, whereas\n                # sp.stats.kendalltau computes tau-b version, which accounts for\n                # ties\n                pos = sp.stats.mstats.kendalltau(a[:i], b[:i], use_ties=False)[0]\n                neg = sp.stats.mstats.kendalltau(a[i:], b[i:], use_ties=False)[0]\n            corr_coeff[ii] = pos - neg\n        iimax = np.argmax(corr_coeff)\n        imax = min_length + iimax\n        corr_coeff_max = corr_coeff[iimax]\n        if corr_coeff_max < 0.3:\n            logg.debug('    is root itself, never obtain significant correlation')\n        return imax", "idx": 740}
{"project": "Scanpy", "commit_id": "96_scanpy_1.9.0__settings.py_max_memory.py", "target": 0, "func": "def max_memory(self, max_memory: Union[int, float]):\n        _type_check(max_memory, \"max_memory\", (int, float))\n        self._max_memory = max_memory", "idx": 748}
{"project": "Scanpy", "commit_id": "781_scanpy_1.9.0__dpt.py___init__.py", "target": 0, "func": "def __init__(\n        self,\n        adata,\n        n_dcs=None,\n        min_group_size=0.01,\n        n_branchings=0,\n        allow_kendall_tau_shift=False,\n        neighbors_key=None,\n    ):\n        super().__init__(adata, n_dcs=n_dcs, neighbors_key=neighbors_key)\n        self.flavor = 'haghverdi16'\n        self.n_branchings = n_branchings\n        self.min_group_size = (\n            min_group_size\n            if min_group_size >= 1\n            else int(min_group_size * self._adata.shape[0])\n        )\n        self.passed_adata = adata  # just for debugging purposes\n        self.choose_largest_segment = False\n        self.allow_kendall_tau_shift = allow_kendall_tau_shift", "idx": 750}
{"project": "Scanpy", "commit_id": "380_scanpy_1.9.0___init__.py_rank_genes_groups_dotplot.py", "target": 0, "func": "def rank_genes_groups_dotplot(\n    adata: AnnData,\n    groups: Union[str, Sequence[str]] = None,\n    n_genes: Optional[int] = None,\n    groupby: Optional[str] = None,\n    values_to_plot: Optional[\n        Literal[\n            'scores',\n            'logfoldchanges',\n            'pvals',\n            'pvals_adj',\n            'log10_pvals',\n            'log10_pvals_adj',\n        ]\n    ] = None,\n    var_names: Optional[Union[Sequence[str], Mapping[str, Sequence[str]]]] = None,\n    gene_symbols: Optional[str] = None,\n    min_logfoldchange: Optional[float] = None,\n    key: Optional[str] = None,\n    show: Optional[bool] = None,\n    save: Optional[bool] = None,\n    return_fig: Optional[bool] = False,\n    **kwds,\n):\n    \"\"\"\\\n    Plot ranking of genes using dotplot plot (see :func:`~scanpy.pl.dotplot`)\n\n    Parameters\n    ----------\n    {params}\n    {vals_to_plot}\n    {show_save_ax}\n    return_fig\n        Returns :class:`DotPlot` object. Useful for fine-tuning\n        the plot. Takes precedence over `show=False`.\n    **kwds\n        Are passed to :func:`~scanpy.pl.dotplot`.\n\n    Returns\n    -------\n    If `return_fig` is `True`, returns a :class:`DotPlot` object,\n    else if `show` is false, return axes dict\n\n    Examples\n    --------\n\n    .. plot::\n        :context: close-figs\n\n        import scanpy as sc\n        adata = sc.datasets.pbmc68k_reduced()\n        sc.tl.rank_genes_groups(adata, 'bulk_labels', n_genes=adata.raw.shape[1])\n\n    Plot top 2 genes per group.\n\n    .. plot::\n        :context: close-figs\n\n        sc.pl.rank_genes_groups_dotplot(adata,n_genes=2)\n\n    Plot with scaled expressions for easier identification of differences.\n\n    .. plot::\n        :context: close-figs\n\n        sc.pl.rank_genes_groups_dotplot(adata, n_genes=2, standard_scale='var')\n\n    Plot `logfoldchanges` instead of gene expression. In this case a diverging colormap\n    like `bwr` or `seismic` works better. To center the colormap in zero, the minimum\n    and maximum values to plot are set to -4 and 4 respectively.\n    Also, only genes with a log fold change of 3 or more are shown.\n\n    .. plot::\n        :context: close-figs\n\n        sc.pl.rank_genes_groups_dotplot(\n            adata,\n            n_genes=4,\n            values_to_plot=\"logfoldchanges\", cmap='bwr',\n            vmin=-4,\n            vmax=4,\n            min_logfoldchange=3,\n            colorbar_title='log fold change'\n        )\n\n    Also, the last genes can be plotted. This can be useful to identify genes\n    that are lowly expressed in a group. For this `n_genes=-4` is used\n\n    .. plot::\n        :context: close-figs\n\n        sc.pl.rank_genes_groups_dotplot(\n            adata,\n            n_genes=-4,\n            values_to_plot=\"logfoldchanges\",\n            cmap='bwr',\n            vmin=-4,\n            vmax=4,\n            min_logfoldchange=3,\n            colorbar_title='log fold change',\n        )\n\n    A list specific genes can be given to check their log fold change. If a\n    dictionary, the dictionary keys will be added as labels in the plot.\n\n    .. plot::\n        :context: close-figs\n\n        var_names = {{'T-cell': ['CD3D', 'CD3E', 'IL32'],\n                      'B-cell': ['CD79A', 'CD79B', 'MS4A1'],\n                      'myeloid': ['CST3', 'LYZ'] }}\n        sc.pl.rank_genes_groups_dotplot(\n            adata,\n            var_names=var_names,\n            values_to_plot=\"logfoldchanges\",\n            cmap='bwr',\n            vmin=-4,\n            vmax=4,\n            min_logfoldchange=3,\n            colorbar_title='log fold change',\n        )\n\n    .. currentmodule:: scanpy\n\n    See also\n    --------\n    tl.rank_genes_groups\n    \"\"\"\n    return _rank_genes_groups_plot(\n        adata,\n        plot_type='dotplot',\n        groups=groups,\n        n_genes=n_genes,\n        groupby=groupby,\n        values_to_plot=values_to_plot,\n        var_names=var_names,\n        gene_symbols=gene_symbols,\n        key=key,\n        min_logfoldchange=min_logfoldchange,\n        show=show,\n        save=save,\n        return_fig=return_fig,\n        **kwds,", "idx": 753}
{"project": "Scanpy", "commit_id": "289_scanpy_0.1_scanpy_data_structs_data_graph.py___init__.py", "target": 1, "func": "def __init__(self, adata_or_X, k=30, knn=True,\n             n_jobs=None, n_pcs=30, n_pcs_post=30,\n             recompute_pca=None,\n             recompute_diffmap=None,\n             flavor='haghverdi16'):\n    logg.m('initializing data graph')\n    self.k = k\n    self.knn = knn\n    self.n_jobs = sett.n_jobs if n_jobs is None else n_jobs\n    self.n_pcs = n_pcs\n    self.n_pcs_post = 30\n    self.flavor = flavor  # this is to experiment around\n    self.sym = True  # we do not allow asymetric cases\n    self.iroot = None\n    isadata = isinstance(adata_or_X, AnnData)\n    if isadata:\n        adata = adata_or_X\n        X = adata_or_X.X\n    else:\n        X = adata_or_X\n    # retrieve xroot\n    xroot = None\n    if 'xroot' in adata.add:\n        xroot = adata.add['xroot']\n    elif 'xroot' in adata.var:\n        xroot = adata.var['xroot']\n    if (self.n_pcs == 0  # use the full X as n_pcs == 0\n            or X.shape[1] < self.n_pcs):\n        self.X = X\n        logg.m('... using X for building graph')\n        if xroot is not None: self.set_root(xroot)\n    # use the precomupted X_pca\n    elif (isadata\n          and not recompute_pca\n          and 'X_pca' in adata.smp\n          and adata.smp['X_pca'].shape[1] >= self.n_pcs):\n        logg.m('... using X_pca for building graph')\n        if xroot is not None and xroot.size == adata.X.shape[1]:\n            self.X = adata.X\n            self.set_root(xroot)\n        self.X = adata.smp['X_pca'][:, :n_pcs]\n        if xroot is not None and xroot.size == adata.smp['X_pca'].shape[1]:\n            self.set_root(xroot[:n_pcs])\n    # compute X_pca\n    else:\n        self.X = X\n        if (isadata\n                and xroot is not None\n                and xroot.size == adata.X.shape[1]):\n            self.set_root(xroot)\n        logg.m('... compute X_pca for building graph')\n        from ..preprocessing import pca\n        pca(adata, n_comps=self.n_pcs)\n        self.X = adata.smp['X_pca']\n        if xroot is not None and xroot.size == adata.smp['X_pca'].shape[1]:\n            self.set_root(xroot)\n    self.Dchosen = None\n    # use diffmap from previous calculation\n    if isadata and 'X_diffmap' in adata.smp and not recompute_diffmap:\n        logg.m('... using `X_diffmap` for distance computations')\n        self.X_diffmap = adata.smp['X_diffmap']\n        self.evals = np.r_[1, adata.add['diffmap_evals']]\n        self.rbasis = np.c_[adata.smp['X_diffmap0'][:, None], adata.smp['X_diffmap']]\n        self.lbasis = self.rbasis\n        if knn: self.Dsq = adata.add['distance']\n        if knn: self.Ktilde = adata.add['Ktilde']\n        self.Dchosen = OnFlySymMatrix(self.get_Ddiff_row,\n                                      shape=(self.X.shape[0], self.X.shape[0]))\n    else:\n        self.evals = None\n        self.rbasis = None\n        self.lbasis = None\n        self.Dsq = None\n    # further attributes that might be written during the computation\n    self.M = None", "idx": 765}
{"project": "Scanpy", "commit_id": "273_scanpy_1.9.0__baseplot_class.py_savefig.py", "target": 0, "func": "def savefig(self, filename: str, bbox_inches: Optional[str] = 'tight', **kwargs):\n        \"\"\"\n        Save the current figure\n\n        Parameters\n        ----------\n        filename\n            Figure filename. Figure *format* is taken from the file ending unless\n            the parameter `format` is given.\n        bbox_inches\n            By default is set to 'tight' to avoid cropping of the legends.\n        kwargs\n            Passed to :func:`matplotlib.pyplot.savefig`\n\n        See also\n        --------\n        `render()`: Renders the plot but does not call :func:`matplotlib.pyplot.show`\n        `show()`: Renders and shows the plot\n\n        Examples\n        -------\n        >>> adata = sc.datasets.pbmc68k_reduced()\n        >>> markers = ['C1QA', 'PSAP', 'CD79A', 'CD79B', 'CST3', 'LYZ']\n        >>> sc.pl.BasePlot(adata, markers, groupby='bulk_labels').savefig('plot.pdf')\n\n        \"\"\"\n        self.make_figure()\n        pl.savefig(filename, bbox_inches=bbox_inches, **kwargs)", "idx": 772}
{"project": "Scanpy", "commit_id": "110_scanpy_1.9.0__datasets.py_pbmc68k_reduced.py", "target": 0, "func": "def pbmc68k_reduced() -> ad.AnnData:\n    \"\"\"\\\n    Subsampled and processed 68k PBMCs.\n\n    10x PBMC 68k dataset from\n    https://support.10xgenomics.com/single-cell-gene-expression/datasets\n\n    The original PBMC 68k dataset was preprocessed using scanpy and was saved\n    keeping only 724 cells and 221 highly variable genes.\n\n    The saved file contains the annotation of cell types (key: `'bulk_labels'`),\n    UMAP coordinates, louvain clustering and gene rankings based on the\n    `bulk_labels`.\n\n    Returns\n    -------\n    Annotated data matrix.\n    \"\"\"\n\n    filename = HERE / '10x_pbmc68k_reduced.h5ad'\n    with warnings.catch_warnings():\n        warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"anndata\")\n        return read(filename)", "idx": 779}
{"project": "Scanpy", "commit_id": "57_scanpy_0.0_scanpy_exs_builtin.py_burczynski06.py", "target": 1, "func": "def burczynski06():\n    \"\"\"\n    Bulk data with conditions ulcerative colitis (UC) and Crohn's disease (CD).\n    The study assesses transcriptional profiles in peripheral blood mononuclear\n    cells from 42 healthy individuals, 59 CD patients, and 26 UC patients by\n    hybridization to microarrays interrogating more than 22,000 sequences.\n    Available from https://www.ncbi.nlm.nih.gov/sites/GDSbrowser?acc=GDS1615.\n    Reference\n    ---------\n    Burczynski ME, Peterson RL, Twine NC, Zuberek KA et al.\n    \"Molecular classification of Crohn's disease and ulcerative colitis patients\n    using transcriptional profiles in peripheral blood mononuclear cells\"\n    J Mol Diagn 8, 51 (2006). PMID:16436634.\n    \"\"\"\n    filename = 'data/burczynski06/GDS1615_full.soft.gz'\n    url = 'ftp://ftp.ncbi.nlm.nih.gov/geo/datasets/GDS1nnn/GDS1615/soft/GDS1615_full.soft.gz'\n    ddata = sc.read(filename, backup_url=url)\n#     print(ddata)\n#     adata = AnnData.from_ddata(**ddata)\n#     print(adata.X)\n#     print(adata.smp_names)\n    return ddata", "idx": 785}
{"project": "Scanpy", "commit_id": "730_scanpy_1.3.2_scanpy_plotting_anndata.py__compute_dendrogram.py", "target": 1, "func": "def _compute_dendrogram(adata, groupby, categories=None, var_names=None, var_group_labels=None,\n                        var_group_positions=None, use_raw=True, log=False, num_categories=7,\n                        cor_method='pearson', linkage_method='complete'):\n    # compute a correlation matrix based on all the data in adata (or adata.raw if use_raw is true)\n    # this is to avoid doing the computation only on the few genes that are being plotted\n    # which could bias the results.\n    if 'dendrogram' not in adata.uns or 'linkage' not in adata.uns['dendrogram']:\n        has_var_groups = True if var_group_positions is not None and len(var_group_positions) > 0 else False\n\n        gene_names = adata.var_names if use_raw is False else adata.raw.var_names\n        cat, df = _prepare_dataframe(adata, gene_names, groupby, use_raw, log, num_categories)\n\n        mean_df = df.groupby(level=0).mean()\n\n        import scipy.cluster.hierarchy as sch\n\n        corr_matrix = mean_df.T.corr(method=cor_method)\n        z_var = sch.linkage(corr_matrix, method=linkage_method)\n        dendro_info = sch.dendrogram(z_var, labels=categories, no_plot=True)\n\n        # order of groupby categories\n        categories_idx_ordered = dendro_info['leaves']\n        # reorder var_groups (if any)\n        if var_names is not None:\n            var_names_idx_ordered = range(len(var_names))\n\n        if has_var_groups:\n            if list(var_group_labels) == list(categories):\n                positions_ordered = []\n                labels_ordered = []\n                position_start = 0\n                var_names_idx_ordered = []\n                for idx in categories_idx_ordered:\n                    position = var_group_positions[idx]\n                    _var_names = var_names[position[0]:position[1] + 1]\n                    var_names_idx_ordered.extend(range(position[0], position[1] + 1))\n                    positions_ordered.append((position_start, position_start + len(_var_names) -1))\n                    position_start += len(_var_names)\n                    labels_ordered.append(var_group_labels[idx])\n                var_group_labels = labels_ordered\n                var_group_positions = positions_ordered\n            else:\n                logg.warn(\"Gene order with respect to groups are not reordered because \"\n                          \"the groupby categories and  the `var_group_labels` are different. \")\n        else:\n            var_names_idx_ordered = None\n\n        adata.uns['dendrogram'] = {'linkage': z_var,\n                                   'cor_method': cor_method,\n                                   'linkage_method': 'linkage_method',\n                                   'use_raw': use_raw,\n                                   'categories_idx_ordered': categories_idx_ordered,\n                                   'var_names_idx_ordered': var_names_idx_ordered,\n                                   'var_group_labels': var_group_labels,\n                                   'var_group_positions': var_group_positions,\n                                   'dendrogram_info': dendro_info}\n\n    return adata.uns['dendrogram']", "idx": 788}
{"project": "Scanpy", "commit_id": "360_scanpy_0.2.4_scanpy_data_structs_ann_data.py___new__.py", "target": 1, "func": "def __new__(cls, source, index_key, is_attr_of, n_row=None,\n            keys_multicol=None, new_index_key=None):\n    \"\"\"Dimensionally structured dict, lowlevel alternative to pandas dataframe.\n    Behaves like a dict except that\n    - data has to match shape constraints\n    - slicing in the row-dimension is possible\n    Behaves like a numpy array except that:\n    - single and multiple columns can be accessed with keys (strings)\n    - the `index` column is hidden from the user and enables sclicing in AnnData\n    - you can add new columns via [] (`__setitem__`)\n    - it is bound to AnnData\n    Can be exported to a pandas dataframe via `.to_df()`.\n    Parameters\n    ----------\n    index_key : str\n        The key or field name that shall be used as the index. If not found\n        generate a dummy index np.array(['0', '1', '2' ...]).\n    is_attr_of : (object, x)\n        Tuple `(o, x)` containing the object `o` to which the instance is\n        bound and the name `x` of the attribute.\n    new_index_key : str or None\n        Only needed if the internal index_key of the object should differ from\n        `index_key`.\n    Attributes\n    ----------\n    index : np.ndarray\n        Access to the index column. Can also be accessed via ['index'].\n    \"\"\"\n    old_index_key = index_key\n    new_index_key = index_key if new_index_key is None else new_index_key\n    # create from existing array\n    if isinstance(source, np.ndarray):\n        # create from existing BoundStructArray\n        if isinstance(source, BoundStructArray):\n            keys_multicol = source._keys_multicol if keys_multicol is None else keys_multicol\n        # we need to explicitly make a deep copy of the dtype\n        arr = np.array(source, dtype=[t for t in source.dtype.descr])\n        # rename the index\n        arr.dtype.names = tuple(new_index_key if n == old_index_key else n\n                                for n in source.dtype.names)\n    # create from None or Dict\n    else:\n        if source is None:  # empty array\n            cols = [np.arange(n_row).astype(STRING_TYPE)]\n            dtype = [(new_index_key, cols[0].dtype)]\n        else:\n            if not isinstance(source, Mapping):\n                raise ValueError('Expected np.ndarray or dictlike type, not {}.'\n                                 .format(type(source)))\n            # meta is dict-like\n            names = list(source.keys())\n            try:  # transform to byte-strings\n                cols = [np.asarray(col) if np.array(col[0]).dtype.char not in {'U', 'S'}\n                        else np.asarray(col).astype(STRING_TYPE) for col in source.values()]\n            except UnicodeEncodeError:\n                raise ValueError('Currently only support ascii strings. Don\\'t use \"\u00f6\" etc. for sample annotation.')\n            if old_index_key not in source:\n                names.append(new_index_key)\n                cols.append(np.arange(len(cols[0]) if cols else n_row).astype(STRING_TYPE))\n            else:\n                names[names.index(old_index_key)] = new_index_key\n                cols[names.index(old_index_key)] = cols[names.index(old_index_key)].astype(STRING_TYPE)\n            dtype = list(zip(names, [str(c.dtype) for c in cols]))\n        try:\n            dtype = np.dtype(dtype)\n        except TypeError:\n            # TODO: fix compat with Python 2\n            # print(dtype, file=sys.stderr)\n            raise\n        arr = np.zeros((len(cols[0]),), dtype)\n        # here, we do not want to call BoundStructArray.__getitem__\n        # but np.ndarray.__getitem__, therefore we avoid the following line\n        # arr = np.ndarray.__new__(cls, (len(cols[0]),), dtype)\n        for i, name in enumerate(dtype.names):\n            arr[name] = np.array(cols[i], dtype=dtype[name])\n    # generate an instance of BoundStructArray\n    arr = np.asarray(arr).view(cls)\n    # the index_key used to look up the index column\n    arr.index_key = new_index_key\n    #\n    arr._is_attr_of = is_attr_of\n    # only the multicol keys\n    arr._keys_multicol = ([] if keys_multicol is None\n                          else list(keys_multicol))\n    # fast lookup of single-column keys\n    arr._keys_multicol_lookup = ({} if keys_multicol is None\n                                 else dict.fromkeys(keys_multicol))\n    # all keys (multi- and single-col keys that do not belong to multicol)\n    arr._keys = []  # this excludes self.index_key\n    # initialize arr._keys\n    loop_over_keys = (key for key in arr.dtype.names if key != index_key)\n    for key in loop_over_keys:\n        imk = _key_belongs_to_which_key_multicol(key, arr._keys_multicol)\n        if imk < 0:\n            arr._keys += [key]\n        else:\n            if arr._keys_multicol[imk] not in arr._keys:\n                arr._keys += [arr._keys_multicol[imk]]\n                arr._keys_multicol_lookup[arr._keys_multicol[imk]] = [key]\n            else:\n                arr._keys_multicol_lookup[arr._keys_multicol[imk]].append(\n                    key)\n\n    return arr", "idx": 791}
{"project": "Scanpy", "commit_id": "440_scanpy_1.9.0__utils.py_sparse_mean_var_minor_axis.py", "target": 0, "func": "def sparse_mean_var_minor_axis(data, indices, major_len, minor_len, dtype):\n    \"\"\"\n    Computes mean and variance for a sparse matrix for the minor axis.\n\n    Given arrays for a csr matrix, returns the means and variances for each\n    column back.\n    \"\"\"\n    non_zero = indices.shape[0]\n\n    means = np.zeros(minor_len, dtype=dtype)\n    variances = np.zeros_like(means, dtype=dtype)\n\n    counts = np.zeros(minor_len, dtype=np.int64)\n\n    for i in range(non_zero):\n        col_ind = indices[i]\n        means[col_ind] += data[i]\n\n    for i in range(minor_len):\n        means[i] /= major_len\n\n    for i in range(non_zero):\n        col_ind = indices[i]\n        diff = data[i] - means[col_ind]\n        variances[col_ind] += diff * diff\n        counts[col_ind] += 1\n\n    for i in range(minor_len):\n        variances[i] += (major_len - counts[i]) * means[i] ** 2\n        variances[i] /= major_len\n\n    return means, variances", "idx": 797}
{"project": "Scanpy", "commit_id": "218_scanpy_0.0_scanpy_data_structs_ann_data.py___new__.py", "target": 1, "func": "def __new__(cls, source, index_key, is_attr_of, n_row=None, keys_multicol=None, new_index_key=None):\n    \"\"\"Dimensionally structured dict, lowlevel alternative to pandas dataframe.\n    Behaves like a dict except that\n    - data has to match shape constraints\n    - slicing in the row-dimension is possible\n    Behaves like a numpy array except that:\n    - single and multiple columns can be accessed with keys (strings)\n    - the `index` column is hidden from the user and enables sclicing in AnnData\n    - you can add new columns via [] (`__setitem__`)\n    - it is bound to AnnData\n    Can be exported to a pandas dataframe via `.to_df()`.\n    Parameters\n    ----------\n    index_key : str\n        The key or field name that shall be used as the index. If not found\n        generate a dummy index np.array(['0', '1', '2' ...]).\n    is_attr_of : (object, x)\n        Tuple `(o, x)` containing the object `o` to which the instance is\n        bound and the name `x` of the attribute.\n    new_index_key : str or None\n        Only needed if the internal index_key of the object should differ from\n        `index_key`.\n    Attributes\n    ----------\n    index : np.ndarray\n        Access to the index column. Can also be accessed via ['index'].\n    \"\"\"\n    old_index_key = index_key\n    new_index_key = index_key if new_index_key is None else new_index_key\n    # create from existing array\n    if isinstance(source, np.ndarray):\n        # create from existing BoundStructArray\n        if isinstance(source, BoundStructArray):\n            keys_multicol = source._keys_multicol if keys_multicol is None else keys_multicol\n        # we need to explicitly make a deep copy of the dtype\n        arr = np.array(source, dtype=[t for t in source.dtype.descr])\n        # rename the index\n        arr.dtype.names = tuple(new_index_key if n == old_index_key else n\n                                for n in source.dtype.names)\n    # create from None or Dict\n    else:\n        if source is None:  # empty array\n            cols = [np.arange(n_row).astype(np.string_)]\n            dtype = [(new_index_key, cols[0].dtype)]\n        else:\n            if not isinstance(source, Mapping):\n                raise ValueError('Expected np.ndarray or dictlike type, not {}.'\n                                 .format(type(source)))\n            # meta is dict-like\n            names = list(source.keys())\n            try:  # transform to byte-strings\n                cols = [np.asarray(col) if np.array(col[0]).dtype.char != 'U'\n                        else np.asarray(col).astype('S') for col in source.values()]\n            except UnicodeEncodeError:\n                raise ValueError('Currently only support ascii strings. Don\\'t use \"\u00f6\" etc. for sample annotation.')\n\n            if old_index_key not in source:\n                names.append(new_index_key)\n                cols.append(np.arange(len(cols[0]) if cols else n_row).astype(np.string_))\n            else:\n                names[names.index(old_index_key)] = new_index_key\n                if isinstance(source[old_index_key][0], np.string_):\n                    raise ValueError('Value for key \"{}\" for initializing index of BoundStructArray '\n                                     'needs to be of type str, not {}.'\n                                     .format(index_key, type(source[index_key][0])))\n            dtype = list(zip(names, [str(c.dtype) for c in cols]))\n        try:\n            dtype = np.dtype(dtype)\n        except TypeError:\n            # TODO: fix compat with Python 2\n            # print(dtype, file=sys.stderr)\n            raise\n        arr = np.zeros((len(cols[0]),), dtype)\n        # here, we do not want to call BoundStructArray.__getitem__\n        # but np.ndarray.__getitem__, therefore we avoid the following line\n        # arr = np.ndarray.__new__(cls, (len(cols[0]),), dtype)\n        for i, name in enumerate(dtype.names):\n            arr[name] = np.array(cols[i], dtype=dtype[name])\n    # generate an instance of BoundStructArray\n    arr = np.asarray(arr).view(cls)\n    # the index_key used to look up the index column\n    arr.index_key = new_index_key\n    #\n    arr._is_attr_of = is_attr_of\n    # only the multicol keys\n    arr._keys_multicol = ([] if keys_multicol is None\n                          else list(keys_multicol))\n    # fast lookup of single-column keys\n    arr._keys_multicol_lookup = ({} if keys_multicol is None\n                                 else dict.fromkeys(keys_multicol))\n    # all keys (multi- and single-col keys that do not belong to multicol)\n    arr._keys = []  # this excludes self.index_key\n    # initialize arr._keys\n    loop_over_keys = (key for key in arr.dtype.names if key != index_key)\n    for key in loop_over_keys:\n        imk = _key_belongs_to_which_key_multicol(key, arr._keys_multicol)\n        if imk < 0:\n            arr._keys += [key]\n        else:\n            if arr._keys_multicol[imk] not in arr._keys:\n                arr._keys += [arr._keys_multicol[imk]]\n                arr._keys_multicol_lookup[arr._keys_multicol[imk]] = [key]\n            else:\n                arr._keys_multicol_lookup[arr._keys_multicol[imk]].append(\n                    key)\n    return arr", "idx": 798}
{"project": "Scanpy", "commit_id": "75_scanpy_0.0_scanpy_plotting.py_plot_tool.py", "target": 1, "func": "def plot_tool(dplot, adata,\n              smp=None,\n              comps='1,2',\n              cont=None,\n              layout='2d',\n              legendloc='lower right',\n              cmap=None,\n              adjust_right=0.75,\n              subtitles=('one title',),\n              component_name='comp'):\n    \"\"\"\n    Scatter plots.\n    Parameters\n    ----------\n    dplot : dict\n        Dict returned by plotting tool.\n    adata : AnnData\n        Annotated data matrix.\n    smp : str, optional (default: first annotation)\n        Sample annotation to choose for coloring. String annotation is plotted\n        assuming categorical annotation, float and integer annotation is plotted\n        assuming continuous annoation. Option 'cont' allows to switch between\n        these default choices.\n    comps : str, optional (default: '1,2')\n         String in the form '1,2,3'.\n    cont : bool, None (default: None)\n        Switch on continuous layout, switch off categorical layout.\n    layout : {'2d', '3d', 'unfolded 3d'}, optional (default: '2d')\n         Layout of plot.\n    legendloc : see matplotlib.legend, optional (default: 'lower right')\n         Options for keyword argument 'loc'.\n    cmap : str (default: continuous: inferno/ categorical: finite palette)\n         String denoting matplotlib color map.\n    adjust_right : float (default: 0.75)\n         Adjust how far the plotting panel extends to the right.\n    \"\"\"\n    params = locals(); del params['adata']; del params['dplot']\n    # compute components\n    from numpy import array\n    comps = array(params['comps'].split(',')).astype(int) - 1\n    # highlights\n    highlights = []\n    if False:\n        if 'highlights' in adata:\n            highlights = adata['highlights']\n    # base figure\n    try:\n        Y = dplot['Y'][:, comps]\n    except IndexError:\n        sett.mi('IndexError: Only computed', dplot['Y'].shape[1], ' components')\n        sett.mi('--> recompute using scanpy exkey diffmap -p nr_comps YOUR_NR')\n        from sys import exit\n        exit(0)\n\n    c = 'grey'\n    categorical = False\n    continuous = False\n    if len(adata.smp_keys()) > 0:\n        if smp is None:\n            smp = adata.smp_keys()[0]\n            sett.m(0, '... coloring according to', smp)\n        # test whether we have categorial or continuous annotation\n        if smp in adata.smp_keys():\n            if adata.smp[smp].dtype.char in ['S', 'U']:\n                categorical = True\n                if cont is True:\n                    c = adata.smp[smp]\n            else:\n                continuous = True\n                c = adata.smp[smp]\n        # coloring according to gene expression\n        elif smp in adata.var_names:\n            c = adata.X[:, np.where(smp==adata.var_names)[0][0]]\n            continuous = True\n            sett.m(0, '... coloring according to expression of gene', smp)\n        else:\n            raise ValueError('specify valid sample annotation, one of '\n                             + str(adata.smp_keys()) + ' or a gene name '\n                             + str(adata.var_names))\n    if cont is not None:\n        categorical = not cont\n        continuous = cont\n    if continuous:\n        cmap = 'inferno'\n    adjust_right = params['adjust_right']\n    axs = scatter(Y,\n                  subtitles=[smp],\n                  component_name=component_name,\n                  component_indexnames=comps + 1,\n                  layout=params['layout'],\n                  c=c,\n                  highlights=highlights,\n                  colorbar=continuous,\n                  cmap=cmap)\n\n    if categorical:\n        if not smp + '_colors' in adata:\n            adata[smp + '_colors'] = pl.cm.get_cmap('jet' if cmap is None else cmap)(\n                                            pl.Normalize()(adata[smp + '_ids']))\n        for icat in adata[smp + '_ids']:\n            group(axs[0], smp, icat, adata, dplot['Y'][:, comps], params['layout'])\n        if params['legendloc'] != 'none':\n            axs[0].legend(frameon=False, loc='center left', bbox_to_anchor=(1, 0.5))\n    else:\n        adjust_right *= 1.2\n\n    pl.subplots_adjust(right=adjust_right)\n\n    savefig(dplot['writekey'] + '_' + smp)\n    if not sett.savefigs and sett.autoshow:\n        pl.show()", "idx": 814}
{"project": "Scanpy", "commit_id": "509_scanpy_0.4.4_scanpy_neighbors___init__.py_get_distance_matrix_and_neighbors.py", "target": 1, "func": "def get_distance_matrix_and_neighbors(X, k, sparse=True, n_jobs=1):\n    \"\"\"Compute distance matrix in squared Euclidian norm.\n    \"\"\"\n    if not sparse:\n        if False: Dsq = utils.comp_distance(X, metric='sqeuclidean')\n        else: Dsq = utils.comp_sqeuclidean_distance_using_matrix_mult(X, X)\n        sample_range = np.arange(Dsq.shape[0])[:, None]\n        indices = np.argpartition(Dsq, k-1, axis=1)[:, :k]\n        indices = indices[sample_range, np.argsort(Dsq[sample_range, indices])]\n        indices = indices[:, 1:]  # exclude first data point (point itself)\n        distances = Dsq[sample_range, indices]\n    elif X.shape[0] > 1e5:\n        # sklearn is slower, but for large sample numbers more stable\n        from sklearn.neighbors import NearestNeighbors\n        sklearn_neighbors = NearestNeighbors(n_neighbors=k-1, n_jobs=n_jobs)\n        sklearn_neighbors.fit(X)\n        distances, indices = sklearn_neighbors.kneighbors()\n        distances = distances.astype('float32')**2\n    else:\n        # assume we can fit at max 20000 data points into memory\n        len_chunk = np.ceil(min(20000, X.shape[0]) / n_jobs).astype(int)\n        n_chunks = np.ceil(X.shape[0] / len_chunk).astype(int)\n        chunks = [np.arange(start, min(start + len_chunk, X.shape[0]))\n                 for start in range(0, n_chunks * len_chunk, len_chunk)]\n        indices = np.zeros((X.shape[0], k-1), dtype=int)\n        distances = np.zeros((X.shape[0], k-1), dtype=np.float32)\n        if n_jobs > 1:\n            # set backend threading, said to be meaningful for computations\n            # with compiled code. more important: avoids hangs\n            # when using Parallel below, threading is much slower than\n            # multiprocessing\n            result_lst = Parallel(n_jobs=n_jobs, backend='threading')(\n                delayed(get_neighbors)(X[chunk], X, k) for chunk in chunks)\n        else:\n            logg.info('--> can be sped up by setting `n_jobs` > 1')\n        for i_chunk, chunk in enumerate(chunks):\n            if n_jobs > 1:\n                indices_chunk, distances_chunk = result_lst[i_chunk]\n            else:\n                indices_chunk, distances_chunk = get_neighbors(X[chunk], X, k)\n            indices[chunk] = indices_chunk\n            distances[chunk] = distances_chunk\n    if sparse:\n        Dsq = get_sparse_distance_matrix(indices, distances, X.shape[0], k)\n    return Dsq, indices, distances", "idx": 816}
{"project": "Scanpy", "commit_id": "217_scanpy_1.9.0___init__.py__make_forest_dict.py", "target": 0, "func": "def _make_forest_dict(forest):\n    d = {}\n    props = ('hyperplanes', 'offsets', 'children', 'indices')\n    for prop in props:\n        d[prop] = {}\n        sizes = np.fromiter(\n            (getattr(tree, prop).shape[0] for tree in forest), dtype=int\n        )\n        d[prop]['start'] = np.zeros_like(sizes)\n        if prop == 'offsets':\n            dims = sizes.sum()\n        else:\n            dims = (sizes.sum(), getattr(forest[0], prop).shape[1])\n        dtype = getattr(forest[0], prop).dtype\n        dat = np.empty(dims, dtype=dtype)\n        start = 0\n        for i, size in enumerate(sizes):\n            d[prop]['start'][i] = start\n            end = start + size\n            dat[start:end] = getattr(forest[i], prop)\n            start = end\n        d[prop]['data'] = dat\n    return d", "idx": 818}
{"project": "Scanpy", "commit_id": "349_scanpy_1.9.0_scatterplots.py__wraps_plot_scatter.py", "target": 0, "func": "def _wraps_plot_scatter(wrapper):\n    import inspect\n\n    params = inspect.signature(embedding).parameters.copy()\n    wrapper_sig = inspect.signature(wrapper)\n    wrapper_params = wrapper_sig.parameters.copy()\n\n    params.pop(\"basis\")\n    params.pop(\"kwargs\")\n    wrapper_params.pop(\"adata\")\n\n    params.update(wrapper_params)\n    annotations = {\n        k: v.annotation\n        for k, v in params.items()\n        if v.annotation != inspect.Parameter.empty\n    }\n    if wrapper_sig.return_annotation is not inspect.Signature.empty:\n        annotations[\"return\"] = wrapper_sig.return_annotation\n\n    wrapper.__signature__ = inspect.Signature(\n        list(params.values()), return_annotation=wrapper_sig.return_annotation\n    )\n    wrapper.__annotations__ = annotations\n\n    return wrapper", "idx": 819}
{"project": "Scanpy", "commit_id": "127_scanpy_0.0_scanpy_classes_ann_data.py_to_ddata.py", "target": 1, "func": "def to_ddata(self):\n    smp = {k: self.smp[k] for k in self.smp_keys() if k != 'smp_names'}\n    var = {k: self.var[k] for k in self.var_keys() if k != 'var_names'}\n    d = {'X': self.X, 'smp': smp, 'var': var,\n         'smp_names': self.smp_names, 'var_names': self.var_names}\n    for k, v in self._meta.items():\n        d[k] = v\n    return d", "idx": 830}
{"project": "Scanpy", "commit_id": "467_scanpy_0.3.1_scanpy_data_structs_data_graph.py_no_recompute_of_graph_necessary.py", "target": 1, "func": "def no_recompute_of_graph_necessary(\n        adata,\n        recompute_pca=False,\n        recompute_distances=False,\n        recompute_graph=False,\n        n_neighbors=None,\n        knn=None,\n        n_dcs=None):\n    conditions = [\n        not recompute_pca,\n        not recompute_distances,\n        not recompute_graph,\n        # make sure X_diffmap is there\n        'X_diffmap' in adata.smpm_keys(),\n        # make sure enough DCs are there\n        (adata.smpm['X_diffmap'].shape[1] >= n_dcs-1\n             if n_dcs is not None else True),\n        # make sure that it's sparse\n        (issparse(adata.uns['data_graph_norm_weights']) == knn\n             if knn is not None else True),\n        # make sure n_neighbors matches\n        (n_neighbors == adata.uns[\n            'data_graph_distance_local'][0].nonzero()[0].size + 1\n            if n_neighbors is not None else True)]\n    return all(conditions)", "idx": 833}
{"project": "Scanpy", "commit_id": "67_scanpy_1.9.0_readwrite.py_is_float.py", "target": 0, "func": "def is_float(string: str) -> float:\n    \"\"\"Check whether string is float.\n\n    See also\n    --------\n    http://stackoverflow.com/questions/736043/checking-if-a-string-can-be-converted-to-float-in-python\n    \"\"\"\n    try:\n        float(string)\n        return True\n    except ValueError:\n        return False", "idx": 834}
{"project": "Scanpy", "commit_id": "416_scanpy_1.9.0__recipes.py_recipe_seurat.py", "target": 0, "func": "def recipe_seurat(\n    adata: AnnData, log: bool = True, plot: bool = False, copy: bool = False\n) -> Optional[AnnData]:\n    \"\"\"\\\n    Normalization and filtering as of Seurat [Satija15]_.\n\n    This uses a particular preprocessing.\n\n    Expects non-logarithmized data.\n    If using logarithmized data, pass `log=False`.\n    \"\"\"\n    if copy:\n        adata = adata.copy()\n    pp.filter_cells(adata, min_genes=200)\n    pp.filter_genes(adata, min_cells=3)\n    normalize_total(adata, target_sum=1e4)\n    filter_result = filter_genes_dispersion(\n        adata.X, min_mean=0.0125, max_mean=3, min_disp=0.5, log=not log\n    )\n    if plot:\n        from ..plotting import (\n            _preprocessing as ppp,\n        )  # should not import at the top of the file\n\n        ppp.filter_genes_dispersion(filter_result, log=not log)\n    adata._inplace_subset_var(filter_result.gene_subset)  # filter genes\n    if log:\n        pp.log1p(adata)\n    pp.scale(adata, max_value=10)\n    return adata if copy else None", "idx": 835}
{"project": "Scanpy", "commit_id": "882_scanpy_1.9.0__sim.py_coupl_model_krumsiek11.py", "target": 0, "func": "def coupl_model_krumsiek11(self):\n        \"\"\"Variant of toggle switch.\"\"\"\n        self.Coupl = self.Adj_signed", "idx": 839}
{"project": "Scanpy", "commit_id": "352_scanpy_0.1_scanpy_preprocessing_simple.py_subsample.py", "target": 1, "func": "def subsample(data, fraction, seed=0, copy=False):\n    \"\"\"Subsample to `fraction` of the data.\n    Parameters\n    ----------\n    data : AnnData or array-like\n        Annotated data matrix.\n    fraction : float in [0, 1]\n        Subsample to a fraction the number of samples.\n    seed : int\n        Random seed to change subsampling.\n    copy : bool (default: False)\n        If an AnnData is passed, determines whether a copy is returned.\n    Returns\n    -------\n    Updates or returns the subsampled data object, depending on `copy`.\n    Notes\n    -----\n    Returns X, smp_indices if data is array-like, otherwise subsamples the passed\n    AnnData (copy == False) or a copy of it (copy == True).\n    \"\"\"\n    if fraction > 1 or fraction < 0:\n        raise ValueError('`fraction` needs to be within [0, 1], not {}'\n                         .format(fraction))\n    np.random.seed(seed)\n    if isinstance(data, AnnData):\n        adata = data.copy() if copy else data\n        new_n_smps = int(fraction * adata.n_smps)\n        logg.m('... subsampled to {} data points'.format(new_n_smps), v=4)\n        smp_indices = np.random.choice(adata.n_smps, size=new_n_smps, replace=False)\n        adata.inplace_subset_smp(smp_indices)\n        return adata if copy else None\n    else:\n        X = data\n        new_n_smps = int(fraction * X.shape[0])\n        logg.m('... subsampled to {} data points'.format(new_n_smps), v=4)\n        smp_indices = np.random.choice(X.shape[0], size=new_n_smps, replace=False)\n        return X[smp_indices]", "idx": 840}
{"project": "Scanpy", "commit_id": "493_scanpy_0.4.1_scanpy_plotting_tools.py_aga_path.py", "target": 1, "func": "def aga_path(\n        adata,\n        nodes,\n        keys,\n        annotations=['aga_pseudotime'],\n        color_map=None,\n        color_maps_annotations={'aga_pseudotime': 'Greys'},\n        palette_groups=None,\n        n_avg=1,\n        groups_key=None,\n        xlim=[None, None],\n        title=None,\n        left_margin=None,\n        ytick_fontsize=None,\n        title_fontsize=None,\n        show_node_names=True,\n        show_yticks=True,\n        show_colorbar=True,\n        legend_fontsize=None,\n        legend_fontweight=None,\n        normalize_to_zero_one=False,\n        as_heatmap=True,\n        return_data=False,\n        show=None,\n        save=None,\n        ax=None):\n    \"\"\"Gene expression and annotation changes along paths in the abstracted graph.\n    Parameters\n    ----------\n    adata : :class:`~scanpy.api.AnnData`\n        An annotated data matrix.\n    nodes : list of group names or their category indices\n        A path through nodes of the abstracted graph, that is, names or indices\n        (within `.categories`) of groups that have been used to run AGA.\n    keys : list of str\n        Either variables in `adata.var_names` or annotations in\n        `adata.obs`. They are plotted using `color_map`.\n    annotations : list of annotations, optional (default: ['aga_pseudotime'])\n        Plot these keys with `color_maps_annotations`. Need to be keys for\n        `adata.obs`.\n    color_map : color map for plotting keys or `None`, optional (default: `None`)\n        Matplotlib colormap.\n    color_maps_annotations : dict storing color maps or `None`, optional (default: {'aga_pseudotime': 'Greys'})\n        Color maps for plotting the annotations. Keys of the dictionary must\n        appear in `annotations`.\n    palette_groups : list of colors or `None`, optional (default: `None`)\n        Ususally, use the same `sc.pl.palettes...` as used for coloring the\n        abstracted graph.\n    n_avg : `int`, optional (default: 1)\n        Number of data points to include in computation of running average.\n    groups_key : `str`, optional (default: `None`)\n        Key of the grouping used to run AGA. If `None`, defaults to\n        `adata.uns['aga_groups_key']`.\n    as_heatmap : `bool`, optional (default: `True`)\n        Plot the timeseries as heatmap. If not plotting as heatmap,\n        `annotations` have no effect.\n    show_node_names : `bool`, optional (default: `True`)\n        Plot the node names on the nodes bar.\n    show_colorbar : `bool`, optional (default: `True`)\n        Show the colorbar.\n    show_yticks : `bool`, optional (default: `True`)\n        Show the y ticks.\n    normalize_to_zero_one : `bool`, optional (default: `True`)\n        Shift and scale the running average to [0, 1] per gene.\n    return_data : `bool`, optional (default: `False`)\n        Return the timeseries data in addition to the axes if `True`.\n    show : `bool`, optional (default: `None`)\n         Show the plot, do not return axis.\n    save : `bool` or `str`, optional (default: `None`)\n        If `True` or a `str`, save the figure. A string is appended to the\n        default filename. Infer the filetype if ending on \\{'.pdf', '.png', '.svg'\\}.\n    ax : `matplotlib.Axes`\n         A matplotlib axes object.\n    Returns\n    -------\n    A `matplotlib.Axes`, if `ax` is `None`, else `None`. If `return_data`,\n    return the timeseries data in addition to an axes.\n    \"\"\"\n    ax_was_none = ax is None\n    if groups_key is None:\n        if 'aga_groups_key' not in adata.uns:\n            raise KeyError(\n                'Pass the key of the grouping with which you ran AGA, '\n                'using the parameter `groups_key`.')\n        groups_key = adata.uns['aga_groups_key']\n    groups_names = adata.obs[groups_key].cat.categories\n\n    if palette_groups is None:\n        palette_groups = palettes.default_20\n        palette_groups = utils.adjust_palette(\n            palette_groups, len(adata.obs[groups_key].cat.categories))", "idx": 842}
{"project": "Scanpy", "commit_id": "571_scanpy_1.9.0_test_logging.py_test_formats.py", "target": 0, "func": "def test_formats(capsys, logging_state):\n    s.logfile = sys.stderr\n    s.verbosity = Verbosity.debug\n    log.error('0')\n    assert capsys.readouterr().err == 'ERROR: 0\\n'\n    log.warning('1')\n    assert capsys.readouterr().err == 'WARNING: 1\\n'\n    log.info('2')\n    assert capsys.readouterr().err == '2\\n'\n    log.hint('3')\n    assert capsys.readouterr().err == '--> 3\\n'\n    log.debug('4')\n    assert capsys.readouterr().err == '    4\\n'", "idx": 843}
{"project": "Scanpy", "commit_id": "512_scanpy_1.9.0_test_embedding_plots.py_na_color.py", "target": 0, "func": "def na_color(request):\n    return request.param", "idx": 844}
{"project": "Scanpy", "commit_id": "109_scanpy_0.0_scanpy_plotting.py_timeseries_subplot.py", "target": 1, "func": "def timeseries_subplot(X,\n                       varnames=[],\n                       highlightsX=[],\n                       c = None,\n                       xlabel='segments / pseudotime order',\n                       ylabel='gene expression',\n                       yticks=None,\n                       xlim=None,\n                       legend=True,\n                       cmap='jet'): # consider changing to 'viridis'\n    \"\"\"\n    Plot X.\n    \"\"\"\n    for i in range(X.shape[1]):\n        pl.scatter(\n            np.arange(X.shape[0]), X[:, i],\n            marker='.',\n            edgecolor='face',\n            s=rcParams['lines.markersize'],\n            c=cl[i] if c is None else c,\n            label = (varnames[i] if len(varnames) > 0 else ''),\n            cmap=cmap,\n        )\n    ylim = pl.ylim()\n    for ih,h in enumerate(highlightsX):\n        pl.plot([h,h],[ylim[0],ylim[1]],\n                '--',color='black')\n    pl.ylim(ylim)\n    if xlim is not None:\n        pl.xlim(xlim)\n    pl.xlabel(xlabel)\n    pl.ylabel(ylabel)\n    if yticks is not None:\n        pl.yticks(yticks)\n    if len(varnames) > 0 and legend==True:\n        pl.legend(frameon=False)", "idx": 845}
{"project": "Scanpy", "commit_id": "1008_scanpy_1.4.6_scanpy_preprocessing__simple.py__regress_out_chunk.py", "target": 1, "func": "def _regress_out_chunk(data):\n    # data is a tuple containing the selected columns from adata.X\n    # and the regressors dataFrame\n    data_chunk = data[0]\n    regressors = data[1]\n    variable_is_categorical = data[2]\n    responses_chunk_list = []\n    import statsmodels.api as sm\n    from statsmodels.tools.sm_exceptions import PerfectSeparationError\n\n    for col_index in range(data_chunk.shape[1]):\n        if variable_is_categorical:\n            regres = np.c_[np.ones(regressors.shape[0]), regressors[:, col_index]]\n        else:\n            regres = regressors\n        try:\n            result = sm.GLM(data_chunk[:, col_index], regres, family=sm.families.Gaussian()).fit()\n            new_column = result.resid_response\n        except PerfectSeparationError:  # this emulates R's behavior\n            logg.warning('Encountered PerfectSeparationError, setting to 0 as in R.')\n            new_column = np.zeros(data_chunk.shape[0])\n        responses_chunk_list.append(new_column)\n    return np.vstack(responses_chunk_list)", "idx": 858}
{"project": "Scanpy", "commit_id": "371_scanpy_0.2.6_scanpy_utils.py_identify_groups.py", "target": 1, "func": "def identify_groups(ref_labels, pred_labels, return_overlaps=False):\n    \"\"\"Which predicted label explains which reference label?\n    A predicted label explains the reference label which maximizes the minimum\n    of ``relative_overlaps_pred`` and ``relative_overlaps_ref``.\n    Returns\n    -------\n    A dictionary of length ``len(np.unique(ref_labels))`` that stores for each\n    reference label the predicted label that best explains it.\n    If ``return_overlaps`` is ``True``, this will in addition return the overlap\n    of the reference group with the predicted group; normalized with respect to\n    the reference group size and the predicted group size, respectively.\n    \"\"\"\n    ref_unique, ref_counts = np.unique(ref_labels, return_counts=True)\n    ref_dict = dict(zip(ref_unique, ref_counts))\n    pred_unique, pred_counts = np.unique(pred_labels, return_counts=True)\n    pred_dict = dict(zip(pred_unique, pred_counts))\n    associated_predictions = {}\n    associated_overlaps = {}\n    for ref_label in ref_unique:\n        sub_pred_unique, sub_pred_counts = np.unique(pred_labels[ref_label == ref_labels], return_counts=True)\n        relative_overlaps_pred = [sub_pred_counts[i] / pred_dict[n] for i, n in enumerate(sub_pred_unique)]\n        relative_overlaps_ref = [sub_pred_counts[i] / ref_dict[ref_label] for i, n in enumerate(sub_pred_unique)]\n        relative_overlaps = np.c_[relative_overlaps_pred, relative_overlaps_ref]\n        relative_overlaps_min = np.min(relative_overlaps, axis=1)\n        pred_best_index = np.argmax(relative_overlaps_min)\n        associated_predictions[ref_label] = sub_pred_unique[pred_best_index]\n        associated_overlaps[ref_label] = relative_overlaps[pred_best_index]\n    if return_overlaps: return associated_predictions, associated_overlaps\n    else: return associated_predictions", "idx": 859}
{"project": "Scanpy", "commit_id": "480_scanpy_0.3.2_scanpy_plotting_utils.py_scatter_base.py", "target": 1, "func": "def scatter_base(Y,\n                 colors='blue',\n                 sort_order=True,\n                 alpha=None,\n                 highlights=[],\n                 right_margin=None,\n                 left_margin=None,\n                 projection='2d',\n                 title=None,\n                 component_name='DC',\n                 component_indexnames=[1, 2, 3],\n                 axis_labels=None,\n                 colorbars=[False],\n                 sizes=[1],\n                 color_map='viridis',\n                 show_ticks=True,\n                 ax=None):\n    \"\"\"Plot scatter plot of data.\n    Parameters\n    ----------\n    Y : np.ndarray\n        Data array.\n    projection : {'2d', '3d'}\n    Returns\n    -------\n    axs : matplotlib.axis or list of matplotlib.axis\n        Depending on whether supplying a single array or a list of arrays,\n        return a single axis or a list of axes.\n    \"\"\"\n    if isinstance(highlights, dict):\n        highlights_indices = sorted(highlights)\n        highlights_labels = [highlights[i] for i in highlights_indices]\n    else:\n        highlights_indices = highlights\n        highlights_labels = []\n    # if we have a single array, transform it into a list with a single array\n    if type(colors) == str: colors = [colors]\n    if len(sizes) != len(colors) and len(sizes) == 1:\n        sizes = [sizes[0] for i in range(len(colors))]\n    axs, panel_pos, draw_region_width, figure_width = setup_axes(\n        ax=ax, colors=colors, colorbars=colorbars, projection=projection,\n        right_margin=right_margin, left_margin=left_margin,\n        show_ticks=show_ticks)\n    for icolor, color in enumerate(colors):\n        ax = axs[icolor]\n        left = panel_pos[2][2*icolor]\n        bottom = panel_pos[0][0]\n        width = draw_region_width / figure_width\n        height = panel_pos[1][0] - bottom\n        Y_sort = Y\n        if not is_color_like(color) and sort_order:\n            sort = np.argsort(color)\n            color = color[sort]\n            Y_sort = Y[sort]\n        if projection == '2d': data = Y_sort[:, 0], Y_sort[:, 1]\n        elif projection == '3d': data = Y_sort[:, 0], Y_sort[:, 1], Y_sort[:, 2]\n        if not isinstance(color, str) or color != 'white':\n            sct = ax.scatter(*data,\n                             marker='.',\n                             c=color,\n                             alpha=alpha,\n                             edgecolors='none',  # 'face',\n                             s=sizes[icolor],\n                             cmap=color_map)\n        if colorbars[icolor]:\n            width = 0.006 * draw_region_width\n            left = panel_pos[2][2*icolor+1] + (1.2 if projection == '3d' else 0.2) * width\n            rectangle = [left, bottom, width, height]\n            fig = pl.gcf()\n            ax_cb = fig.add_axes(rectangle)\n            cb = pl.colorbar(sct, format=ticker.FuncFormatter(ticks_formatter),\n                             cax=ax_cb)\n        # set the title\n        if title is not None: ax.set_title(title[icolor])\n        # output highlighted data points\n        for iihighlight, ihighlight in enumerate(highlights_indices):\n            data = [Y[ihighlight, 0]], [Y[ihighlight, 1]]\n            if '3d' in projection:\n                data = [Y[ihighlight, 0]], [Y[ihighlight, 1]], [Y[ihighlight, 2]]\n            ax.scatter(*data, c='black',\n                       facecolors='black', edgecolors='black',\n                       marker='x', s=10, zorder=20)\n            highlight_text = (highlights_labels[iihighlight] if\n                              len(highlights_labels) > 0\n                              else str(ihighlight))\n            # the following is a Python 2 compatibility hack\n            ax.text(*([d[0] for d in data] + [highlight_text]),\n                    zorder=20,\n                    fontsize=10,\n                    color='black')\n        if not show_ticks:\n            ax.set_xticks([])\n            ax.set_yticks([])\n            if '3d' in projection: ax.set_zticks([])\n        axs.append(ax)\n    # set default axis_labels\n    if axis_labels is None:\n        axis_labels = [[component_name + str(i) for i in idcs]\n                       for idcs in\n                       [component_indexnames for iax in range(len(axs))]]\n    else:\n        axis_labels = [[axis_labels[0], axis_labels[1]] for i in range(len(axs))]\n    for iax, ax in enumerate(axs):\n        ax.set_xlabel(axis_labels[iax][0])\n        ax.set_ylabel(axis_labels[iax][1])\n        if '3d' in projection:\n            # shift the label closer to the axis\n            ax.set_zlabel(axis_labels[iax][2], labelpad=-7)\n    for ax in axs:\n        # scale limits to match data\n        ax.autoscale_view()\n    return axs", "idx": 865}
{"project": "Scanpy", "commit_id": "1074_scanpy_1.7.1_scanpy_plotting__tools___init__.py__fig_show_save_or_axes.py", "target": 1, "func": "def _fig_show_save_or_axes(plot_obj, return_fig, show, save):\n    \"\"\"\n    Decides what to return\n    \"\"\"\n    if return_fig:\n        return plot_obj\n    else:\n        plot_obj.make_figure()\n        savefig_or_show(plot_obj.DEFAULT_SAVE_PREFIX, show=show, save=save)\n        show = settings.autoshow if show is None else show\n        if not show:\n            return plot_obj.get_axes()", "idx": 866}
{"project": "Scanpy", "commit_id": "11_scanpy_0.0_scanpy___init__.py_run_args.py", "target": 1, "func": "def run_args(toolkey, args):\n    \"\"\"\n    Run specified tool, do preprocessing and read/write outfiles.\n    Output files store the dictionary returned by the tool. File type is\n    determined by variable sett.extd allowed are 'h5' (hdf5), 'xlsx' (Excel) or\n    'csv' (comma separated value file).\n    If called twice with the same settings the existing output file is returned.\n    Parameters\n    ----------\n    toolkey : str\n        Name of the tool.\n    args : dict containing\n        exkey : str\n            String that identifies the example use key.\n    Returns\n    -------\n    dfunc : dict of type toolkey\n    dadd : dict\n         Additional dict used for plotting in a later step.\n    \"\"\"\n    if args['plotparams']:\n        if args['plotparams'][0] == 'help':\n            from sys import exit\n            exit(get_tool(toolkey).plot.__doc__)\n    writekey = sett.basekey + '_' + toolkey + sett.fsig\n    resultfile = sett.writedir + writekey + '.' + sett.extd\n    paramsfile = sett.writedir + writekey + '_params.txt'\n    if args['logfile']:\n        logfile = sett.writedir + writekey + '_log.txt'\n        sett.logfile(logfile)\n    if toolkey == 'sim':\n        if args['paramsfile'] != '':\n            params = read_params(args['paramsfile'])\n        else:\n            paramsfile_sim = 'sim/' + args['exkey'] + '_params.txt'\n            params = read_params(paramsfile_sim)\n            sett.m(0, '--> you can specify your custom params file using the option\\n'\n                      '    \"--paramsfile\" or provide parameters directly via \"--params\"')\n        if 'writedir' not in params:\n            params['writedir'] = sett.writedir + sett.basekey + '_' + toolkey\n    else:\n        ddata, exmodule = example(args['exkey'], return_module=True)\n        params = {}\n        if args['params']:\n            params = utils.get_params_from_list(args['params'])\n        # if a parameter file has been specified, load the parameter file\n        elif args['paramsfile'] != '':\n            params = read_params(args['paramsfile'])\n        # otherwise, load tool parameters from dexamples\n        else:\n            try:\n                dexample = exmodule.dexamples[args['exkey']]\n                params = {}\n                for key in dexample.keys():\n                    if toolkey in key:\n                        params = dexample[key]\n            except:\n                sett.m(0, 'did not find any example parameters')\n                pass\n    # subsampling\n    if args['subsample'] != 1:\n        ddata = subsample(ddata, args['subsample'])\n    # previous tool\n    if 'prev' in args:\n        prevkey = sett.basekey + '_' + args['prev'] + sett.fsig\n        dprev = read(prevkey)\n\n    # simply load resultfile\n    if os.path.exists(resultfile) and not sett.recompute:\n        dtool = read(writekey)\n    # call the tool resultfile\n    else:\n        # TODO: solve this in a nicer way, also get an ordered dict for params\n        from inspect import getcallargs\n        tool = get_tool(toolkey, func=True)\n        if toolkey == 'sim':\n            dtool = tool(**params)\n            params = getcallargs(tool, **params)\n        elif 'prev' in args:\n            dtool = tool(dprev, ddata, **params)\n            params = getcallargs(tool, dprev, ddata, **params)\n            # TODO: Would be good to name the first argument dprev_or_ddata\n            #       in difftest, but this doesn't work\n            del params['dprev']\n            del params['ddata']\n        else:\n            dtool = tool(ddata, **params)\n            params = getcallargs(tool, ddata, **params)\n            del params['ddata']\n        dtool['writekey'] = writekey\n        write(writekey, dtool)\n        # save a copy of the parameters to a file\n        utils.write_params(paramsfile, params)\n    # plotting and postprocessing\n    plotparams = {}\n    if args['plotparams']:\n        plotparams = utils.get_params_from_list(args['plotparams'])\n    if toolkey == 'sim':\n        plot(dtool, plotparams)\n    else:\n        # post-processing specific to example and tool\n        postprocess = args['exkey'] + '_' + toolkey\n        if postprocess in dir(exmodule) and args['subsample'] == 1:\n            dtool = getattr(exmodule, postprocess)(dtool)\n            write(writekey, dtool)\n        # plot\n        plot(dtool, ddata, **plotparams)", "idx": 867}
{"project": "Scanpy", "commit_id": "913_scanpy_1.9.0___init__.py_get_igraph_from_adjacency.py", "target": 0, "func": "def get_igraph_from_adjacency(adjacency, directed=None):\n    \"\"\"Get igraph graph from adjacency matrix.\"\"\"\n    import igraph as ig\n\n    sources, targets = adjacency.nonzero()\n    weights = adjacency[sources, targets]\n    if isinstance(weights, np.matrix):\n        weights = weights.A1\n    g = ig.Graph(directed=directed)\n    g.add_vertices(adjacency.shape[0])  # this adds adjacency.shape[0] vertices\n    g.add_edges(list(zip(sources, targets)))\n    try:\n        g.es['weight'] = weights\n    except KeyError:\n        pass\n    if g.vcount() != adjacency.shape[0]:\n        logg.warning(\n            f'The constructed graph has only {g.vcount()} nodes. '\n            'Your adjacency matrix contained redundant nodes.'\n        )\n    return g", "idx": 870}
{"project": "Scanpy", "commit_id": "193_scanpy_1.9.0__gearys_c.py__gearys_c_mtx_csr.py", "target": 0, "func": "def _gearys_c_mtx_csr(\n    g_data, g_indices, g_indptr, x_data, x_indices, x_indptr, x_shape\n):\n    M, N = x_shape\n    W = g_data.sum()\n    out = np.zeros(M, dtype=np.float_)\n    x_data_list = np.split(x_data, x_indptr[1:-1])\n    x_indices_list = np.split(x_indices, x_indptr[1:-1])\n    for k in numba.prange(M):\n        out[k] = _gearys_c_inner_sparse_x_sparsevec(\n            g_data,\n            g_indices,\n            g_indptr,\n            x_data_list[k],\n            x_indices_list[k],\n            N,\n            W,\n        )\n    return out", "idx": 876}
{"project": "Scanpy", "commit_id": "754_scanpy_1.3.4_scanpy_plotting_anndata.py_heatmap.py", "target": 1, "func": "def heatmap(adata, var_names, groupby=None, use_raw=None, log=False, num_categories=7,\n            dendrogram=False, var_group_positions=None, var_group_labels=None,\n            var_group_rotation=None, swap_axes=False, show_gene_labels=None, show=None, save=None, figsize=None, layer=None, **kwds):\n    \"\"\"\\\n    Heatmap of the expression values of set of genes..\n    If `groupby` is given, the heatmap is ordered by the respective group. For\n    example, a list of marker genes can be plotted, ordered by clustering. If\n    the `groupby` observation annotation is not categorical the observation\n    annotation is turned into a categorical by binning the data into the number\n    specified in `num_categories`.\n    Parameters\n    ----------\n    adata : :class:`~anndata.AnnData`\n        Annotated data matrix.\n    var_names : `str` or list of `str`\n        `var_names` should be a valid subset of  `adata.var_names`.\n    groupby : `str` or `None`, optional (default: `None`)\n        The key of the observation grouping to consider. It is expected that\n        groupby is a categorical. If groupby is not a categorical observation,\n        it would be subdivided into `num_categories`.\n    log : `bool`, optional (default: `False`)\n        Use the log of the values\n    use_raw : `bool`, optional (default: `None`)\n        Use `raw` attribute of `adata` if present.\n    num_categories : `int`, optional (default: `7`)\n        Only used if groupby observation is not categorical. This value\n        determines the number of groups into which the groupby observation\n        should be subdivided.\n    figsize : (float, float), optional (default: None)\n        Figure size (width, height). If not set, the figure width is set based on the\n        number of  `var_names` and the height is set to 10.\n    dendrogram: `bool` If True, hiearchical clustering between the `groupby` categories is\n        computed and a dendrogram is plotted. `groupby` categories are reordered accoring to\n        the dendrogram order. If groups of var_names are set and those groups correspond\n        to the `groupby` categories, those groups are also reordered. The 'person' method\n        is used to compute the pairwise correlation between categories using all var_names in\n        `raw` if `use_raw` is None, otherwise all adata.var_names are used. The linkage method\n        used is `complete`.\n    var_group_positions :  list of `tuples`.\n        Use this parameter to highlight groups of `var_names`. This will draw a 'bracket'\n        on top of the plot between the given start and end positions. If the\n        parameter `var_group_labels` is set, the corresponding labels is added on\n        top of the bracket. E.g. var_group_positions = [(4,10)] will add a bracket\n        between the fourth var_name and the tenth var_name. By giving more\n        positions, more brackets are drawn.\n    var_group_labels : list of `str`\n        Labels for each of the var_group_positions that want to be highlighted.\n    var_group_rotation : `float` (default: `None`)\n        Label rotation degrees. By default, labels larger than 4 characters are rotated 90 degrees\n    swap_axes: `bool`, optional (default: `False`)\n         By default, the x axis contains `var_names` (e.g. genes) and the y axis the `groupby`\n         categories (if any). By setting `swap_axes` then x are the `groupby` categories and y the `var_names`.\n    show_gene_labels: `bool`, optional (default: `None`).\n         By default gene labels are shown when there are 50 or less genes. Otherwise the labels are removed.\n    {show_save_ax}\n    **kwds : keyword arguments\n        Are passed to `seaborn.heatmap`.\n    Returns\n    -------\n    A list of `matplotlib.Axes` where the first ax is the groupby categories\n    colorcode, the second axis is the heatmap and the third axis is the\n    colorbar.\n    \"\"\"\n    if use_raw is None and adata.raw is not None: use_raw = True\n    if isinstance(var_names, str):\n        var_names = [var_names]\n    if use_raw is False:\n        # this most likely will used a scaled version of the data\n        # and thus is better to use a diverging scale\n        param_set = False\n        if 'vmin' not in kwds:\n            kwds['vmin'] = -3\n            param_set = True\n        if 'vmax' not in kwds:\n            kwds['vmax'] = 3\n            param_set = True\n        if 'cmap' not in kwds:\n            kwds['cmap'] = 'bwr'\n            param_set = True\n        if param_set is True:\n            logg.info('Divergent color map has been automatically set to plot non-raw data. Use '\n                      '`vmin`, `vmax` and `cmap` to adjust the plot.')\n\n    categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories,\n                                              layer=layer)\n\n    if groupby is None or len(categories) <= 1:\n        categorical = False\n        # dendrogram can only be computed  between groupby categories\n        dendrogram = False\n    else:\n        categorical = True\n    colors = None\n    groupby_cmap = None\n    if dendrogram:\n        dendro_data = _compute_dendrogram(adata, groupby, var_names=var_names,\n                                          categories=categories,\n                                          var_group_labels=var_group_labels,\n                                          var_group_positions=var_group_positions,\n                                          use_raw=use_raw, log=log, num_categories=num_categories)\n        var_group_labels = dendro_data['var_group_labels']\n        var_group_positions = dendro_data['var_group_positions']\n        # reorder obs_tidy\n        if dendro_data['var_names_idx_ordered'] is not None:\n            obs_tidy = obs_tidy.iloc[:, dendro_data['var_names_idx_ordered']]\n            var_names = [var_names[x] for x in dendro_data['var_names_idx_ordered']]\n        obs_tidy.index = obs_tidy.index.reorder_categories(\n            [categories[x] for x in dendro_data['categories_idx_ordered']], ordered=True)\n        # reorder groupby colors\n        if groupby + \"_colors\" in adata.uns:\n            colors = [adata.uns[groupby + \"_colors\"][x] for x in dendro_data['categories_idx_ordered']]\n    if show_gene_labels is None:\n        show_gene_labels = True if len(var_names) <= 50 else False\n    if categorical:\n        obs_tidy = obs_tidy.sort_index()\n        from matplotlib.colors import LinearSegmentedColormap\n        if colors is None:\n            if groupby + \"_colors\" in adata.uns:\n                colors = adata.uns[groupby + \"_colors\"]\n                groupby_cmap = LinearSegmentedColormap.from_list(groupby + '_cmap', colors, N=len(colors))\n            else:\n                groupby_cmap = pl.get_cmap('tab20')\n        else:\n            groupby_cmap = LinearSegmentedColormap.from_list(groupby + '_cmap', colors, N=len(colors))\n    # determine groupby label positions such that they appear\n    # centered next to the color code rectangle asigned to the category\n    value_sum = 0\n    ticks = []  # contains the centered position of the label\n    labels = []\n    label2code = {}\n    for code, (label, value) in enumerate(obs_tidy.index.value_counts(sort=False).iteritems()):\n        ticks.append(value_sum + (value / 2))\n        labels.append(label)\n        value_sum += value\n        label2code[label] = code\n    from matplotlib import gridspec\n    if not swap_axes:\n        # define a layout of 2 rows x 4 columns\n        # first row is for 'brackets' (if no brackets needed, the height of this row is zero)\n        # second row is for main content. This second row is divided into three axes:\n        #   first ax is for the categories defined by `groupby`\n        #   second ax is for the heatmap\n        #   third ax is for the dendrogram\n        #   fourth ax is for colorbar\n        dendro_width = 1.8 if dendrogram else 0\n        groupby_width = 0.25 if categorical else 0\n        if figsize is None:\n            height = 6\n            if show_gene_labels:\n                heatmap_width = 10\n            else:\n                heatmap_width = len(var_names) * 0.3\n            width = heatmap_width + dendro_width + groupby_width\n        else:\n            width, height = figsize\n            heatmap_width = width - (dendro_width + groupby_width)\n        ax_frac2width = 0.25\n        if var_group_positions is not None and len(var_group_positions) > 0:\n            # add some space in case 'brackets' want to be plotted on top of the image\n            height_ratios = [0.15, height]\n        else:\n            height_ratios = [0, height]\n        fig = pl.figure(figsize=(width, height))\n        axs = gridspec.GridSpec(nrows=2, ncols=4, left=0.05, right=0.48, wspace=0.5 / width,\n                                hspace=0.13 / height,\n                                width_ratios=[groupby_width, heatmap_width, dendro_width, ax_frac2width],\n                                height_ratios=height_ratios)\n        heatmap_ax = fig.add_subplot(axs[1, 1])\n        heatmap_cbar_ax = fig.add_subplot(axs[1, 3])\n        heatmap_cbar_ax.tick_params(axis='y', labelsize='small')\n        sns.heatmap(obs_tidy, yticklabels=\"auto\", ax=heatmap_ax, cbar_ax=heatmap_cbar_ax, **kwds)\n        if show_gene_labels:\n            heatmap_ax.tick_params(axis='x', labelsize='small')\n            heatmap_ax.set_xticks(np.arange(len(var_names)) + 0.5)\n            heatmap_ax.set_xticklabels(var_names)\n        else:\n            heatmap_ax.tick_params(axis='x', labelbottom=False, bottom=False)\n        heatmap_ax.tick_params(axis='y', left=False, labelleft=False)\n        heatmap_ax.set_ylabel('')\n        if categorical:\n            groupby_ax = fig.add_subplot(axs[1, 0])\n            groupby_ax.imshow(np.matrix([label2code[lab] for lab in obs_tidy.index]).T, aspect='auto', cmap=groupby_cmap)\n            if len(categories) > 1:\n                groupby_ax.set_yticks(ticks)\n                groupby_ax.set_yticklabels(labels)\n            # remove y ticks\n            groupby_ax.tick_params(axis='y', left=False, labelsize='small')\n            # remove x ticks and labels\n            groupby_ax.tick_params(axis='x', bottom=False, labelbottom=False)\n            # remove surrounding lines\n            groupby_ax.spines['right'].set_visible(False)\n            groupby_ax.spines['top'].set_visible(False)\n            groupby_ax.spines['left'].set_visible(False)\n            groupby_ax.spines['bottom'].set_visible(False)\n            groupby_ax.set_ylabel(groupby)\n            groupby_ax.grid(False)\n        if dendrogram:\n            dendro_ax = fig.add_subplot(axs[1, 2], sharey=heatmap_ax)\n            _plot_dendrogram(dendro_ax, adata, ticks=ticks)\n        # plot group legends on top of heatmap_ax (if given)\n        if var_group_positions is not None and len(var_group_positions) > 0:\n            gene_groups_ax = fig.add_subplot(axs[0, 1], sharex=heatmap_ax)\n            _plot_gene_groups_brackets(gene_groups_ax, group_positions=var_group_positions,\n                                       group_labels=var_group_labels, rotation=var_group_rotation,\n                                       left_adjustment=0.2, right_adjustment=0.8)\n    # swap axes case\n    else:\n        # define a layout of 3 rows x 3 columns\n        # The first row is for the dendrogram (if not dendrogram height is zero)\n        # second row is for main content. This col is divided into three axes:\n        #   first ax is for the heatmap\n        #   second ax is for 'brackets' if any (othwerise width is zero)\n        #   third ax is for colorbar\n        dendro_height = 0.5 if dendrogram else 0\n        groupby_height = 0.13 if categorical else 0\n        if figsize is None:\n            width = 8\n            heatmap_height = len(var_names) * 0.14\n            height = heatmap_height + dendro_height + groupby_height  # +2 to account for labels\n        else:\n            width, height = figsize\n            heatmap_height = height - (dendro_height + groupby_height)\n        height_ratios = [dendro_height, heatmap_height, groupby_height]\n        if var_group_positions is not None and len(var_group_positions) > 0:\n            # add some space in case 'brackets' want to be plotted on top of the image\n            width_ratios = [width, 0.3, 0.25]\n        else:\n            width_ratios = [width, 0., 0.25]\n        fig = pl.figure(figsize=(width, height))\n        axs = gridspec.GridSpec(nrows=3, ncols=3, left=0.05, right=0.48, wspace=0.5 / width,\n                                hspace=0.3 / height,\n                                width_ratios=width_ratios,\n                                height_ratios=height_ratios)\n        heatmap_ax = fig.add_subplot(axs[1, 0])\n        heatmap_cbar_ax = fig.add_subplot(axs[1, 2])\n        heatmap_cbar_ax.tick_params(axis='y', labelsize='small')\n        if categorical:\n            groupby_ax = fig.add_subplot(axs[2, 0])\n            groupby_ax.imshow(np.matrix([label2code[lab] for lab in obs_tidy.index]), aspect='auto', cmap=groupby_cmap)\n            if len(categories) > 1:\n                groupby_ax.set_xticks(ticks)\n                groupby_ax.set_xticklabels(labels, rotation=90)\n            # remove x ticks\n            groupby_ax.tick_params(axis='x', bottom=False, labelsize='small')\n            # remove y ticks and labels\n            groupby_ax.tick_params(axis='y', left=False, labelleft=False)\n            # remove surrounding lines\n            groupby_ax.spines['right'].set_visible(False)\n            groupby_ax.spines['top'].set_visible(False)\n            groupby_ax.spines['left'].set_visible(False)\n            groupby_ax.spines['bottom'].set_visible(False)\n            groupby_ax.set_xlabel(groupby)\n            groupby_ax.grid(False)\n        sns.heatmap(obs_tidy.T, yticklabels='none', ax=heatmap_ax, cbar_ax=heatmap_cbar_ax, **kwds)\n        heatmap_ax.tick_params(axis='x', bottom=False, labelbottom=False)\n        heatmap_ax.set_xlabel('')\n        if show_gene_labels:\n            heatmap_ax.tick_params(axis='y', labelsize='x-small')\n            heatmap_ax.set_yticks(np.arange(len(var_names)) + 0.5)\n            heatmap_ax.set_yticklabels(var_names, rotation=0)\n        else:\n            heatmap_ax.tick_params(axis='y', labelleft=False, left=False)\n        if dendrogram:\n            dendro_ax = fig.add_subplot(axs[0, 0], sharex=heatmap_ax)\n            _plot_dendrogram(dendro_ax, adata, ticks=ticks, orientation='top')\n        # plot group legends on top of heatmap_ax (if given)\n        if var_group_positions is not None and len(var_group_positions) > 0:\n            gene_groups_ax = fig.add_subplot(axs[1, 1])\n            arr = []\n            for idx, pos in enumerate(var_group_positions):\n                arr += [idx] * (pos[1]+1 - pos[0])\n            gene_groups_ax.imshow(np.matrix(arr).T, aspect='auto', cmap=groupby_cmap)\n            gene_groups_ax.axis('off')\n    utils.savefig_or_show('heatmap', show=show, save=save)\n    return axs", "idx": 877}
{"project": "Scanpy", "commit_id": "824_scanpy_1.4_scanpy_preprocessing_normalization.py__normalize_data.py", "target": 1, "func": "def _normalize_data(X, after=None, counts, copy=False):\n    X = X.copy() if copy else X\n    after = np.median(counts) if after is None else after\n    counts += (counts == 0)\n    counts /= after\n    if issparse(X):\n        X = sparsefuncs.inplace_row_scale(X, 1/counts)\n    else:\n        X = X/counts\n    return X if copy else None", "idx": 878}
{"project": "Scanpy", "commit_id": "139_scanpy_1.9.0_exporting.py__get_color_stats_genes.py", "target": 0, "func": "def _get_color_stats_genes(color_stats, E, gene_list):\n    means, variances = _get_mean_var(E)\n    stdevs = np.zeros(variances.shape, dtype=float)\n    stdevs[variances > 0] = np.sqrt(variances[variances > 0])\n    mins = E.min(0).todense().A1\n    maxes = E.max(0).todense().A1\n\n    pctl = 99.6\n    pctl_n = (100 - pctl) / 100.0 * E.shape[0]\n    pctls = np.zeros(E.shape[1], dtype=float)\n    for iG in range(E.shape[1]):\n        n_nonzero = E.indptr[iG + 1] - E.indptr[iG]\n        if n_nonzero > pctl_n:\n            pctls[iG] = np.percentile(\n                E.data[E.indptr[iG] : E.indptr[iG + 1]], 100 - 100 * pctl_n / n_nonzero\n            )\n        else:\n            pctls[iG] = 0\n        color_stats[gene_list[iG]] = tuple(\n            map(float, (means[iG], stdevs[iG], mins[iG], maxes[iG], pctls[iG]))\n        )\n    return color_stats", "idx": 884}
{"project": "Scanpy", "commit_id": "189_scanpy_0.0_scanpy_preprocess_simple.py_regress_out.py", "target": 1, "func": "def regress_out(adata, smp_keys, n_jobs=2):\n    \"\"\"\n    Regress out unwanted sources of variation.\n    Yields a dense matrix.\n    \"\"\"\n    sett.mt(0, 'regress out', smp_keys)\n    if issparse(adata.X):\n        sett.m(0, '... sparse input is densified and may '\n               'lead to huge memory consumption')\n    # the code here can still be much optimized\n    # ensuring a homogeneous data type seems to be necessary for GLM\n    regressors = np.array([adata.smp[key].astype(float)\n                           for key in smp_keys]).T\n    regressors = np.c_[np.ones(adata.X.shape[0]), regressors]\n    adata_corrected = adata.copy()\n    len_junk = np.ceil(min(1000, adata.X.shape[1]) / n_jobs).astype(int)\n    n_junks = np.ceil(adata.X.shape[1] / len_junk).astype(int)\n    junks = [np.arange(start, min(start + len_junk, adata.X.shape[1]))\n             for start in range(0, n_junks * len_junk, len_junk)]\n    if issparse(adata_corrected.X):\n        adata_corrected.X = adata_corrected.X.toarray()\n    if sett._is_interactive:\n        # from tqdm import tqdm_notebook as tqdm\n        # does not work in Rodeo, should be solved sometime soon\n        # tqdm = lambda x: x\n        from tqdm import tqdm\n        # sett.m(1, 'TODO: get nice waitbars also in interactive mode')\n        sett.m(0, '... nicer progress bars as on command line come soon')\n    else:\n        from tqdm import tqdm\n    for junk in tqdm(junks):\n        result_lst = Parallel(n_jobs=n_jobs)(\n                              delayed(_regress_out)(\n                                      col_index, adata.X, regressors)\n                                      for col_index in junk)\n        for i_column, column in enumerate(junk):\n            adata_corrected.X[:, column] = result_lst[i_column]\n    sett.mt(0, 'finished')\n    return adata_corrected", "idx": 886}
{"project": "Scanpy", "commit_id": "115_scanpy_1.9.0__ebi_expression_atlas.py__filter_boring.py", "target": 0, "func": "def _filter_boring(dataframe: pd.DataFrame) -> pd.DataFrame:\n    unique_vals = dataframe.apply(lambda x: len(x.unique()))\n    is_boring = (unique_vals == 1) | (unique_vals == len(dataframe))\n    return dataframe.loc[:, ~is_boring]", "idx": 887}
{"project": "Scanpy", "commit_id": "145_scanpy_0.0_scanpy_classes_data_graph.py_compute_transition_matrix.py", "target": 1, "func": "def compute_transition_matrix(self, weighted=True,\n                              neglect_selfloops=False, alpha=1):\n    \"\"\"\n    Compute similarity matrix and transition matrix.\n    Notes\n    -----\n    In the code, the following two parameters are set.\n    alpha : float\n        The density rescaling parameter of Coifman and Lafon (2006). Should\n        in all practical applications equal 1: Then only the geometry of the\n        data matters, not the sampling density.\n    neglect_selfloops : bool\n        Discard selfloops.\n    See also\n    --------\n    Also Haghverdi et al. (2016, 2015) and Coifman and Lafon (2006) and\n    Coifman et al. (2005).\n    \"\"\"\n    import scipy.sparse\n    # compute distance matrix in squared Euclidian norm\n    if False and self.params['method'] == 'local' and self.params['knn']:\n        from sklearn.neighbors import NearestNeighbors\n        # don't use metric = sqeuclidian, because this requires choosing algorithm 'brute'\n        sklearn_neighbors = NearestNeighbors(n_neighbors=self.params['k'] - 1)\n        sklearn_neighbors.fit(self.X)\n        Dsq = sklearn_neighbors.kneighbors_graph(mode='distance')\n        Dsq.data **= 2\n    else:\n        sklearn_neighbors = None\n        Dsq = utils.comp_distance(self.X, metric='sqeuclidean')\n    self.Dsq = Dsq\n    if self.params['method'] == 'local':\n        # choose sigma (width of a Gaussian kernel) according to the\n        # distance of the kth nearest neighbor of each point, including the\n        # point itself in the count\n        k = self.params['k']\n        if sklearn_neighbors is None:\n            # deterimine the distance of the k nearest neighbors\n            indices = np.zeros((Dsq.shape[0], k), dtype=np.int_)\n            distances_sq = np.zeros((Dsq.shape[0], k), dtype=np.float_)\n            for irow, row in enumerate(Dsq):\n                # the last item is already in its sorted position as\n                # argpartition puts the (k-1)th element - starting to count from\n                # zero - in its sorted position\n                idcs = np.argpartition(row, k - 1)[:k]\n                indices[irow] = idcs\n                argsort = np.argsort(row[idcs])\n                distances_sq[irow] = row[idcs][argsort]\n                indices[irow] = indices[irow][argsort]\n            # exclude the point itself\n            distances_sq = distances_sq[:, 1:]\n            indices = indices[:, 1:]\n        else:\n            distances_sq, indices = sklearn_neighbors.kneighbors()\n            distances_sq **= 2\n        # choose sigma, the heuristic here often makes not much\n        # of a difference, but is used to reproduce the figures\n        # of Haghverdi et al. (2016)\n        if self.params['knn']:\n            # as the distances are not sorted except for last element\n            # take median\n            sigmas_sq = np.median(distances_sq, axis=1)\n        else:\n            # the last item is already in its sorted position as\n            # argpartition puts the (k-1)th element - starting to count from\n            # zero - in its sorted position\n            sigmas_sq = distances_sq[:, -1] / 4\n        sigmas = np.sqrt(sigmas_sq)\n        sett.mt(0, 'determined k =', k, 'nearest neighbors of each point')\n    elif self.params['method'] == 'standard':\n        sigmas = self.params['sigma'] * np.ones(self.X.shape[0])\n        sigmas_sq = sigmas ** 2\n    # compute the symmetric weight matrix\n    if not sp.sparse.issparse(self.Dsq):\n        Num = 2 * np.multiply.outer(sigmas, sigmas)\n        Den = np.add.outer(sigmas_sq, sigmas_sq)\n        W = np.sqrt(Num / Den) * np.exp(-Dsq / Den)\n        # make the weight matrix sparse\n        if not self.params['knn']:\n            self.Mask = W > 1e-14\n            W[self.Mask == False] = 0\n        else:\n            # restrict number of neighbors to ~k\n            # build a symmetric mask\n            Mask = np.zeros(Dsq.shape, dtype=bool)\n            for i, row in enumerate(indices):\n                Mask[i, row] = True\n                for j in row:\n                    if i not in set(indices[j]):\n                        W[j, i] = W[i, j]\n                        Mask[j, i] = True\n            # set all entries that are not nearest neighbors to zero\n            W[Mask == False] = 0\n            self.Mask = Mask\n        if not weighted:\n            W = Mask.astype(float)\n    else:\n        W = Dsq\n        for i in range(len(Dsq.indptr[:-1])):\n            row = Dsq.indices[Dsq.indptr[i]: Dsq.indptr[i + 1]]\n            num = 2 * sigmas[i] * sigmas[row]\n            den = sigmas_sq[i] + sigmas_sq[row]\n            W.data[Dsq.indptr[i]: Dsq.indptr[i + 1]] = np.sqrt(num / den) * np.exp(\n                -Dsq.data[Dsq.indptr[i]: Dsq.indptr[i + 1]] / den)\n        W = W.tolil()\n        for i, row in enumerate(indices):\n            for j in row:\n                if i not in set(indices[j]):\n                    W[j, i] = W[i, j]\n        W = W.tocsr()\n\n    sett.mt(0, 'computed W (weight matrix) with \"knn\" =', self.params['knn'])\n    # neglect self-loops\n    # also proposed by Haghverdi et al. (2015)\n    # notice that then, the kernel does not encode a notion of similarity\n    # then anymore and is not positive semidefinite anymore in practice, it\n    # doesn't matter too much\n    if neglect_selfloops:\n        np.fill_diagonal(W, 0)\n\n    if False:\n        pl.matshow(W)\n        pl.title('$ W$')\n        pl.colorbar()\n\n    # density normalization\n    # as discussed in Coifman et al. (2005)\n    # ensure that kernel matrix is independent of sampling density\n    if alpha == 0:\n        # nothing happens here, simply use the isotropic similarity matrix\n        self.K = W\n    else:\n        # q[i] is an estimate for the sampling density at point x_i\n        # it's also the degree of the underlying graph\n        if not sp.sparse.issparse(W):\n            q = np.sum(W, axis=0)\n            # raise to power alpha\n            if alpha != 1:\n                q = q ** alpha\n            Den = np.outer(q, q)\n            self.K = W / Den\n        else:\n            q = np.array(np.sum(W, axis=0)).flatten()\n            self.K = W\n            for i in range(len(W.indptr[:-1])):\n                row = W.indices[W.indptr[i]: W.indptr[i + 1]]\n                num = q[i] * q[row]\n                W.data[W.indptr[i]: W.indptr[i + 1]] = W.data[W.indptr[i]: W.indptr[i + 1]] / num\n    sett.mt(0, 'computed K (anisotropic kernel)')\n    if False:\n        pl.matshow(self.K)\n        pl.title('$ K$')\n        pl.colorbar()\n    if not sp.sparse.issparse(self.K):\n        # now compute the row normalization to build the transition matrix T\n        # and the adjoint Ktilde: both have the same spectrum\n        self.z = np.sum(self.K, axis=0)\n        # the following is the transition matrix\n        self.T = self.K / self.z[:, np.newaxis]\n        # now we need the square root of the density\n        self.sqrtz = np.array(np.sqrt(self.z))\n        # now compute the density-normalized Kernel\n        # it's still symmetric\n        szszT = np.outer(self.sqrtz, self.sqrtz)\n        self.Ktilde = self.K / szszT\n        sett.mt(0, 'computed Ktilde (normalized anistropic kernel)')\n        if False:\n            pl.matshow(self.Ktilde)\n            pl.title('$ \\widetilde K$')\n            pl.colorbar()\n            pl.show()\n    else:\n        self.z = np.array(np.sum(self.K, axis=0)).flatten()\n        # now we need the square root of the density\n        self.sqrtz = np.array(np.sqrt(self.z))\n        # now compute the density-normalized Kernel\n        # it's still symmetric\n        self.Ktilde = self.K\n        for i in range(len(self.K.indptr[:-1])):\n            row = self.K.indices[self.K.indptr[i]: self.K.indptr[i + 1]]\n            num = self.sqrtz[i] * self.sqrtz[row]\n            self.Ktilde.data[self.K.indptr[i]: self.K.indptr[i + 1]] = self.K.data[\n                                                                       self.K.indptr[i]: self.K.indptr[i + 1]] / num", "idx": 897}
{"project": "Scanpy", "commit_id": "161_scanpy_0.0_scanpy_plotting.py_timeseries_subplot.py", "target": 1, "func": "def timeseries_subplot(X,\n                       c=None,\n                       varnames=(),\n                       highlightsX=(),\n                       xlabel='',\n                       ylabel='gene expression',\n                       yticks=None,\n                       xlim=None,\n                       legend=True,\n                       pal=None,\n                       cmap='viridis'):\n    \"\"\"\n    Plot X. Call this with:\n    X with one column, c categorical\n    X with one column, c continuous\n    X with n columns, c is of length n\n    \"\"\"\n\n    if c is not None:\n        use_cmap = isinstance(c[0], float)\n    pal = default_pal(pal)\n    x_range = np.arange(X.shape[0])\n    if X.shape[1] > 1:\n        colors = pal[:X.shape[1]].by_key()['color']\n        subsets = [(x_range, X[:, i]) for i in range(X.shape[1])]\n    elif use_cmap:\n        colors = [c]\n        subsets = [(x_range, X[:, 0])]\n    else:\n        levels, _ = np.unique(c, return_inverse=True)\n        colors = np.array(pal[:len(levels)].by_key()['color'])\n        subsets = [(x_range[c == l], X[c == l, :]) for l in levels]\n    for i, (x, y) in enumerate(subsets):\n        pl.scatter(\n            x, y,\n            marker='.',\n            edgecolor='face',\n            s=rcParams['lines.markersize'],\n            c=colors[i],\n            label=varnames[i] if len(varnames) > 0 else '',\n            cmap=cmap,\n        )\n    ylim = pl.ylim()\n    for ih,h in enumerate(highlightsX):\n        pl.plot([h,h],[ylim[0],ylim[1]],\n                '--',color='black')\n    pl.ylim(ylim)\n    if xlim is not None:\n        pl.xlim(xlim)\n    pl.xlabel(xlabel)\n    pl.ylabel(ylabel)\n    if yticks is not None:\n        pl.yticks(yticks)\n    if len(varnames) > 0 and legend==True:\n        pl.legend(frameon=False)", "idx": 900}
{"project": "Scanpy", "commit_id": "342_scanpy_0.1_scanpy_data_structs_ann_data.py___getitem__.py", "target": 1, "func": "def __getitem__(self, index):\n    # Note: this cannot be made inplace\n    # http://stackoverflow.com/questions/31916617/using-keyword-arguments-in-getitem-method-in-python\n    smp, var = self._normalize_indices(index)\n    X = self.X[smp, var]\n    smp_ann = self.smp[smp]\n    var_ann = self.var[var]\n    assert smp_ann.shape[0] == X.shape[0], (smp, smp_ann)\n    assert var_ann.shape[0] == X.shape[1], (var, var_ann)\n    add_ann = self.add\n    raised_warning = False\n    for k, v in self.add.items():  # TODO: make sure this really works as expected\n        if isinstance(v, sp.spmatrix) and v.shape == (self.n_smps, self.n_smps):\n            add_ann[k] = v.tocsc()[:, smp].tocsr()[smp, :]\n            if not raised_warning:\n                logg.warn('Slicing adjacency matrices can be dangerous. '\n                          'Consider recomputing the data graph.')\n                raised_warning = True\n    adata = AnnData(X, smp_ann, var_ann, add_ann)\n    return adata", "idx": 901}
{"project": "Scanpy", "commit_id": "640_scanpy_1.9.0_test_plotting.py_test_correlation.py", "target": 0, "func": "def test_correlation(image_comparer):\n    save_and_compare_images = image_comparer(ROOT, FIGS, tol=15)\n\n    pbmc = pbmc68k_reduced()\n    sc.pl.correlation_matrix(pbmc, 'bulk_labels')\n    save_and_compare_images('correlation')", "idx": 902}
{"project": "Scanpy", "commit_id": "187_scanpy_1.9.0__gearys_c.py_gearys_c.py", "target": 0, "func": "def gearys_c(\n    adata: AnnData,\n    *,\n    vals: Optional[Union[np.ndarray, sparse.spmatrix]] = None,\n    use_graph: Optional[str] = None,\n    layer: Optional[str] = None,\n    obsm: Optional[str] = None,\n    obsp: Optional[str] = None,\n    use_raw: bool = False,\n) -> Union[np.ndarray, float]:\n    r\"\"\"\n    Calculate `Geary's C <https://en.wikipedia.org/wiki/Geary's_C>`_, as used\n    by `VISION <https://doi.org/10.1038/s41467-019-12235-0>`_.\n\n    Geary's C is a measure of autocorrelation for some measure on a graph. This\n    can be to whether measures are correlated between neighboring cells. Lower\n    values indicate greater correlation.\n\n    .. math::\n\n        C =\n        \\frac{\n            (N - 1)\\sum_{i,j} w_{i,j} (x_i - x_j)^2\n        }{\n            2W \\sum_i (x_i - \\bar{x})^2\n        }\n\n    Params\n    ------\n    adata\n    vals\n        Values to calculate Geary's C for. If this is two dimensional, should\n        be of shape `(n_features, n_cells)`. Otherwise should be of shape\n        `(n_cells,)`. This matrix can be selected from elements of the anndata\n        object by using key word arguments: `layer`, `obsm`, `obsp`, or\n        `use_raw`.\n    use_graph\n        Key to use for graph in anndata object. If not provided, default\n        neighbors connectivities will be used instead.\n    layer\n        Key for `adata.layers` to choose `vals`.\n    obsm\n        Key for `adata.obsm` to choose `vals`.\n    obsp\n        Key for `adata.obsp` to choose `vals`.\n    use_raw\n        Whether to use `adata.raw.X` for `vals`.\n\n\n    This function can also be called on the graph and values directly. In this case\n    the signature looks like:\n\n    Params\n    ------\n    g\n        The graph\n    vals\n        The values\n\n\n    See the examples for more info.\n\n    Returns\n    -------\n    If vals is two dimensional, returns a 1 dimensional ndarray array. Returns\n    a scalar if `vals` is 1d.\n\n\n    Examples\n    --------\n\n    Calculate Gearys C for each components of a dimensionality reduction:\n\n    .. code:: python\n\n        import scanpy as sc, numpy as np\n\n        pbmc = sc.datasets.pbmc68k_processed()\n        pc_c = sc.metrics.gearys_c(pbmc, obsm=\"X_pca\")\n\n\n    It's equivalent to call the function directly on the underlying arrays:\n\n    .. code:: python\n\n        alt = sc.metrics.gearys_c(pbmc.obsp[\"connectivities\"], pbmc.obsm[\"X_pca\"].T)\n        np.testing.assert_array_equal(pc_c, alt)\n    \"\"\"\n    if use_graph is None:\n        # Fix for anndata<0.7\n        if hasattr(adata, \"obsp\") and \"connectivities\" in adata.obsp:\n            g = adata.obsp[\"connectivities\"]\n        elif \"neighbors\" in adata.uns:\n            g = adata.uns[\"neighbors\"][\"connectivities\"]\n        else:\n            raise ValueError(\"Must run neighbors first.\")\n    else:\n        raise NotImplementedError()\n    if vals is None:\n        vals = _get_obs_rep(adata, use_raw=use_raw, layer=layer, obsm=obsm, obsp=obsp).T\n    return gearys_c(g, vals)", "idx": 908}
{"project": "Scanpy", "commit_id": "29_scanpy_0.0_scanpy_tools_dpt.py_plot_groups.py", "target": 1, "func": "def plot_groups(ddpt, ddata, params, colors,\n                highlights=[], highlights_labels=[]):\n    \"\"\"\n    Plot groups in diffusion map visualization.\n    \"\"\"\n    from numpy import array\n    comps = array(params['comps'].split(',')).astype(int) - 1\n    # base figure\n    try:\n        Y = ddpt['Y'][:, comps]\n    except IndexError:\n        sett.mi('IndexError: Only computed', ddpt['Y'].shape[1], ' components')\n        sett.mi('--> recompute using scanpy exkey dpt -p n_components YOUR_NR')\n        from sys import exit\n        exit(0)\n    axs = plott.scatter(ddpt['Y'][:, comps],\n                        subtitles=['pseudotime','segments',\n                                   'experimental groups'],\n                        component_name=params['component_name'],\n                        component_indexnames=comps + 1,\n                        layout=params['layout'],\n                        c=colors,\n                        highlights=highlights,\n                        highlights_labels=highlights_labels,\n                        cmap=params['cmap'])\n\n    # dpt groups (segments)\n    for igroup, group in enumerate(ddpt['groupmasks']):\n        plott.group(axs[1], igroup, ddpt, ddpt['Y'][:, comps], params['layout'])\n    axs[1].legend(frameon=False, loc=params['legendloc'])\n\n    # annotated groups in data dict\n    if 'groupmasks' in ddata:\n        for igroup, group in enumerate(ddata['groupmasks']):\n            plott.group(axs[2], igroup, ddata, ddpt['Y'][:, comps], params['layout'])\n        axs[2].legend(frameon=False, loc='center left', bbox_to_anchor=(1, 0.5))\n        pl.subplots_adjust(right=0.88)\n\n    if sett.savefigs:\n        pl.savefig(sett.figdir + ddpt['groups_writekey']\n                   + sett.plotsuffix + '.'+sett.extf)", "idx": 911}
{"project": "Scanpy", "commit_id": "441_scanpy_0.2.9.1_scanpy_preprocessing_simple.py_pca.py", "target": 1, "func": "def pca(data, n_comps=50, zero_center=True, svd_solver='auto', random_state=0,\n        recompute=True, mute=False, return_info=None, copy=False,\n        dtype='float32'):\n    \"\"\"Principal component analysis [Pedregosa11]_.\n    Computes PCA coordinates, loadings and variance decomposition. Uses the\n    implementation of *scikit-learn* [Pedregosa11]_.\n    Parameters\n    ----------\n    data : AnnData, array-like\n        Data matrix of shape n_smps \u00d7 n_vars.\n    n_comps : int, optional (default: 10)\n        Number of principal components to compute.\n    zero_center : bool or None, optional (default: None)\n        If True, compute standard PCA from Covariance matrix. If False, omit\n        zero-centering variables, which allows to handle sparse input\n        efficiently. If None, defaults to True for dense and to False for sparse\n        input.\n    svd_solver : str, optional (default: 'auto')\n        SVD solver to use. Either 'arpack' for the ARPACK wrapper in SciPy\n        (scipy.sparse.linalg.svds), or 'randomized' for the randomized algorithm\n        due to Halko (2009). \"auto\" chooses automatically depending on the size\n        of the problem.\n    random_state : int, optional (default: 0)\n        Change to use different intial states for the optimization.\n    recompute : bool, optional (default: True)\n        Use the result of previous calculation, if possible.\n    return_info : bool or None, optional (default: None)\n        If providing an array, this defaults to False, if providing an AnnData,\n        defaults to true.\n    copy : bool (default: False)\n        If an AnnData is passed, determines whether a copy is returned.\n    dtype : str (default: 'float32')\n        Numpy data type string to which to convert the result.\n    Returns\n    -------\n    If X is array-like and ``return_info == True``, only returns ``X_pca``, otherwise adds to ``adata``:\n    X_pca : np.ndarray (adata.smp)\n         PCA representation of the data with shape n_variables \u00d7 n_comps.\n    components / PC1, PC2, PC3, ... : np.ndarray (adata.var)\n         The PCs containing the loadings as shape n_comps \u00d7 n_vars.\n    variance_ratio : np.ndarray (adata.add)\n         Ratio of explained variance.\n    \"\"\"\n    if isinstance(data, AnnData):\n        adata = data.copy() if copy else data\n        from .. import settings as sett  # why is this necessary?\n        if ('X_pca' in adata.smp\n            and adata.get_multicol_field_smp('X_pca').shape[1] >= n_comps\n            and not recompute\n            and (sett.recompute == 'none' or sett.recompute == 'pp')):\n            logg.m('    not recomputing PCA, using \"X_pca\" contained '\n                   'in `adata.smp` (set `recompute=True` to avoid this)', v=4)\n            return adata\n        else:\n            logg.m('compute PCA with n_comps =', n_comps, r=True, v=4)\n            result = pca(adata.X, n_comps=n_comps, zero_center=zero_center,\n                         svd_solver=svd_solver, random_state=random_state,\n                         recompute=recompute, mute=mute, return_info=True)\n            X_pca, components, pca_variance_ratio = result\n            adata.set_multicol_field_smp('X_pca', X_pca)\n            for icomp, comp in enumerate(components):\n                adata.var['PC' + str(icomp+1)] = comp\n            adata.add['pca_variance_ratio'] = pca_variance_ratio\n            logg.m('    finished', t=True, end=' ', v=4)\n            logg.m('and added\\n'\n                      '    \"X_pca\", the PCA coordinates (adata.smp)\\n'\n                      '    \"PC1\", \"PC2\", ..., the loadings (adata.var)\\n'\n                      '    \"pca_variance_ratio\", the variance ratio (adata.add)', v=4)\n        return adata if copy else None\n    X = data  # proceed with data matrix\n    from .. import settings as sett\n    if X.shape[1] < n_comps:\n        n_comps = X.shape[1] - 1\n        logg.m('reducing number of computed PCs to',\n               n_comps, 'as dim of data is only', X.shape[1], v=4)\n    zero_center = zero_center if zero_center is not None else False if issparse(X) else True\n    from sklearn.decomposition import PCA, TruncatedSVD\n    verbosity_level = np.inf if mute else 0\n    if zero_center:\n        if issparse(X):\n            logg.m('    as `zero_center=True`, '\n                   'sparse input is densified and may '\n                   'lead to huge memory consumption', v=4)\n            X = X.toarray()\n        pca_ = PCA(n_components=n_comps, svd_solver=svd_solver, random_state=random_state)\n    else:\n        logg.m('    without zero-centering: \\n'\n               '    the explained variance does not correspond to the exact statistical defintion\\n'\n               '    the first component, e.g., might be heavily influenced by different means\\n'\n               '    the following components often resemble the exact PCA very closely', v=4)\n        pca_ = TruncatedSVD(n_components=n_comps, random_state=random_state)\n    X_pca = pca_.fit_transform(X)\n    if X_pca.dtype.descr != np.dtype(dtype).descr: X_pca = X_pca.astype(dtype)\n    if False if return_info is None else return_info:\n        return X_pca, pca_.components_, pca_.explained_variance_ratio_\n    else:\n        return X_pca", "idx": 927}
{"project": "Scanpy", "commit_id": "615_scanpy_1.0.4_scanpy_preprocessing_simple.py_pca.py", "target": 1, "func": "def pca(data, n_comps=None, zero_center=True, svd_solver='auto', random_state=0,\n        return_info=None, dtype='float32', copy=False, chunked=False, chunk_size=None):\n    \"\"\"Principal component analysis [Pedregosa11]_.\n    Computes PCA coordinates, loadings and variance decomposition. Uses the\n    implementation of *scikit-learn* [Pedregosa11]_.\n    Parameters\n    ----------\n    data : :class:`~scanpy.api.AnnData`, `np.ndarray`, `sp.sparse`\n        The (annotated) data matrix of shape `n_obs` \u00d7 `n_vars`. Rows correspond\n        to cells and columns to genes.\n    n_comps : `int`, optional (default: 50)\n        Number of principal components to compute.\n    zero_center : `bool` or `None`, optional (default: `True`)\n        If `True`, compute standard PCA from covariance matrix. If `False`, omit\n        zero-centering variables (uses *TruncatedSVD* from scikit-learn), which\n        allows to handle sparse input efficiently.\n    svd_solver : `str`, optional (default: 'auto')\n        SVD solver to use. Either 'arpack' for the ARPACK wrapper in SciPy\n        (scipy.sparse.linalg.svds), or 'randomized' for the randomized algorithm\n        due to Halko (2009). \"auto\" chooses automatically depending on the size\n        of the problem.\n    random_state : `int`, optional (default: 0)\n        Change to use different intial states for the optimization.\n    return_info : `bool` or `None`, optional (default: `None`)\n        Only relevant when not passing an :class:`~scanpy.api.AnnData`: see\n        \"Returns\".\n    dtype : `str` (default: 'float32')\n        Numpy data type string to which to convert the result.\n    copy : `bool`, optional (default: `False`)\n        If an :class:`~scanpy.api.AnnData` is passed, determines whether a copy\n        is returned.\n    Returns\n    -------\n    If `data` is array-like and `return_info == False`, only returns `X_pca`,\\\n    otherwise returns or adds to `adata`:\n    X_pca : `.obsm`\n         PCA representation of data.\n    PCs : `.varm`\n         The principal components containing the loadings.\n    variance_ratio : `.uns['pca']`\n         Ratio of explained variance.\n    variance : `.uns['pca']`\n         Explained variance, equivalent to the eigenvalues of the covariance matrix.\n    \"\"\"\n    if n_comps is None: n_comps = N_PCS\n    if isinstance(data, AnnData):\n        adata = data.copy() if copy else data\n        logg.msg('computing PCA with n_comps =', n_comps, r=True, v=4)\n\n        if chunked:\n            from sklearn.decomposition import IncrementalPCA\n            n_comps = adata.X.shape[1] - 1 if adata.X.shape[1] < n_comps else n_comps\n            X_pca = np.zeros((adata.X.shape[0], n_comps), adata.X.dtype)\n\n            ipca = IncrementalPCA(n_components=n_comps)\n\n            for chunk, _, _ in adata.chunks(chunk_size):\n                chunk = chunk.toarray() if issparse(chunk) else chunk\n                ipca.partial_fit(chunk)\n            for chunk, start, end in adata.chunks(chunk_size):\n                chunk = chunk.toarray() if issparse(chunk) else chunk\n                X_pca[start:end] = ipca.transform(chunk)\n\n            if X_pca.dtype.descr != np.dtype(dtype).descr: X_pca = X_pca.astype(dtype)\n\n            components = ipca.components_\n            pca_variance_ratio = ipca.explained_variance_ratio_\n            pca_variance = ipca.explained_variance_\n\n        else:\n            result = pca(adata.X, n_comps=n_comps, zero_center=zero_center,\n                         svd_solver=svd_solver, random_state=random_state,\n                         return_info=True)\n            X_pca, components, pca_variance_ratio, pca_variance = result\n        adata.obsm['X_pca'] = X_pca\n        adata.varm['PCs'] = components.T\n        adata.uns['pca'] = {}\n        adata.uns['pca']['variance'] = pca_variance\n        adata.uns['pca']['variance_ratio'] = pca_variance_ratio\n        logg.msg('    finished', t=True, end=' ', v=4)\n        logg.msg('and added\\n'\n                 '    \\'X_pca\\', the PCA coordinates (adata.obs)\\n'\n                 '    \\'PC1\\', \\'PC2\\', ..., the loadings (adata.var)\\n'\n                 '    \\'pca_variance\\', the variance / eigenvalues (adata.uns)\\n'\n                 '    \\'pca_variance_ratio\\', the variance ratio (adata.uns)', v=4)\n        return adata if copy else None\n    X = data  # proceed with data matrix\n    if X.shape[1] < n_comps:\n        n_comps = X.shape[1] - 1\n        logg.msg('reducing number of computed PCs to',\n               n_comps, 'as dim of data is only', X.shape[1], v=4)\n    zero_center = zero_center if zero_center is not None else False if issparse(X) else True\n    from sklearn.decomposition import PCA, TruncatedSVD\n    if zero_center:\n        if issparse(X):\n            logg.msg('    as `zero_center=True`, '\n                   'sparse input is densified and may '\n                   'lead to huge memory consumption', v=4)\n            X = X.toarray()\n        pca_ = PCA(n_components=n_comps, svd_solver=svd_solver, random_state=random_state)\n    else:\n        logg.msg('    without zero-centering: \\n'\n               '    the explained variance does not correspond to the exact statistical defintion\\n'\n               '    the first component, e.g., might be heavily influenced by different means\\n'\n               '    the following components often resemble the exact PCA very closely', v=4)\n        pca_ = TruncatedSVD(n_components=n_comps, random_state=random_state)\n\n    X_pca = pca_.fit_transform(X)\n    if X_pca.dtype.descr != np.dtype(dtype).descr: X_pca = X_pca.astype(dtype)\n    if False if return_info is None else return_info:\n        return X_pca, pca_.components_, pca_.explained_variance_ratio_, pca_.explained_variance_\n    else:\n        return X_pca", "idx": 938}
{"project": "Scanpy", "commit_id": "300_scanpy_1.9.0__stacked_violin.py__setup_violin_axes_ticks.py", "target": 0, "func": "def _setup_violin_axes_ticks(self, row_ax, num_cols):\n        \"\"\"\n        Configures each of the violin plot axes ticks like remove or add labels etc.\n\n        \"\"\"\n        # remove the default seaborn grids because in such a compact\n        # plot are unnecessary\n\n        row_ax.grid(False)\n        if self.ylim is not None:\n            row_ax.set_ylim(self.ylim)\n        if self.log:\n            row_ax.set_yscale('log')\n\n        if self.plot_yticklabels:\n            for spine in ['top', 'bottom', 'left']:\n                row_ax.spines[spine].set_visible(False)\n\n            # make line a bit ticker to see the extend of the yaxis in the\n            # final plot\n            row_ax.spines['right'].set_linewidth(1.5)\n            row_ax.spines['right'].set_position(('data', num_cols))\n\n            row_ax.tick_params(\n                axis='y',\n                left=False,\n                right=True,\n                labelright=True,\n                labelleft=False,\n                labelsize='x-small',\n            )\n            # use only the smallest and the largest y ticks\n            # and align the firts label on top of the tick and\n            # the second below the tick. This avoid overlapping\n            # of nearby ticks\n            import matplotlib.ticker as ticker\n\n            # use MaxNLocator to set 2 ticks\n            row_ax.yaxis.set_major_locator(\n                ticker.MaxNLocator(nbins=2, steps=[1, 1.2, 10])\n            )\n            yticks = row_ax.get_yticks()\n            row_ax.set_yticks([yticks[0], yticks[-1]])\n            ticklabels = row_ax.get_yticklabels()\n            ticklabels[0].set_va(\"bottom\")\n            ticklabels[-1].set_va(\"top\")\n        else:\n            row_ax.axis('off')\n            # remove labels\n            row_ax.set_yticklabels([])\n            row_ax.tick_params(axis='y', left=False, right=False)\n\n        row_ax.set_ylabel('')\n\n        row_ax.set_xlabel('')\n\n        row_ax.set_xticklabels([])\n        row_ax.tick_params(\n            axis='x', bottom=False, top=False, labeltop=False, labelbottom=False", "idx": 939}
{"project": "Scanpy", "commit_id": "730_scanpy_1.9.0_test_read_10x.py_test_error_10x_h5_legacy.py", "target": 0, "func": "def test_error_10x_h5_legacy(tmp_path):\n    onepth = ROOT / '1.2.0' / 'filtered_gene_bc_matrices_h5.h5'\n    twopth = tmp_path / \"two_genomes.h5\"\n    with h5py.File(onepth, \"r\") as one, h5py.File(twopth, \"w\") as two:\n        one.copy(\"hg19_chr21\", two)\n        one.copy(\"hg19_chr21\", two, name=\"hg19_chr21_copy\")\n    with pytest.raises(ValueError):\n        sc.read_10x_h5(twopth)\n    sc.read_10x_h5(twopth, genome=\"hg19_chr21_copy\")", "idx": 959}
{"project": "Scanpy", "commit_id": "463_scanpy_0.3.1_scanpy_data_structs_data_graph.py___init__.py", "target": 1, "func": "def __init__(self,\n             adata,\n             k=None,\n             knn=True,\n             n_jobs=None,\n             n_pcs=50,\n             n_dcs=None,\n             recompute_pca=False,\n             recompute_distances=False,\n             recompute_graph=False,\n             flavor='haghverdi16'):\n    self.sym = True  # we do not allow asymetric cases\n    self.flavor = flavor  # this is to experiment around\n    self.n_pcs = n_pcs\n    self.n_dcs = n_dcs if n_dcs is not None else N_DCS\n    self.init_iroot_and_X(adata, recompute_pca, n_pcs)\n    # use the graph in adata\n    if no_recompute_of_graph_necessary(\n            adata,\n            recompute_pca=recompute_pca,\n            recompute_distances=recompute_distances,\n            recompute_graph=recompute_graph,\n            n_neighbors=k,\n            knn=knn,\n            n_dcs=n_dcs):\n        self.fresh_compute = False\n        self.knn = issparse(adata.uns['data_graph_norm_weights'])\n        self.Ktilde = adata.uns['data_graph_norm_weights']\n        self.Dsq = adata.uns['data_graph_distance_local']\n        if self.knn:\n            self.k = adata.uns['data_graph_distance_local'][0].nonzero()[0].size + 1\n        else:\n            self.k = None  # currently do not store this, is unknown\n        # for output of spectrum\n        if n_dcs is None: n_dcs = adata.smpm['X_diffmap'].shape[1] + 1\n        self.X_diffmap = adata.smpm['X_diffmap'][:, :n_dcs - 1]\n        self.evals = np.r_[1, adata.uns['diffmap_evals'][:n_dcs - 1]]\n        self.rbasis = np.c_[adata.smp['X_diffmap0'].values[:, None],\n        adata.smpm['X_diffmap'][:, :n_dcs - 1]]\n        self.lbasis = self.rbasis\n        self.Dchosen = OnFlySymMatrix(self.get_Ddiff_row,\n                                      shape=(self.X.shape[0], self.X.shape[0]))\n        np.set_printoptions(precision=10)\n        logg.info('    using stored data graph with n_neighbors = {} and '\n                  'spectrum\\n    {}'\n                  .format(self.k,\n                          str(self.evals).replace('\\n', '\\n    ')))\n    # recompute the graph\n    else:\n        self.fresh_compute = True\n        self.k = k if k is not None else 30\n        logg.info('    computing data graph with n_neighbors = {} '\n                  .format(self.k))\n        self.evals = None\n        self.rbasis = None\n        self.lbasis = None\n        self.X_diffmap = None\n        self.Dsq = None\n        self.knn = knn\n        self.n_jobs = sett.n_jobs if n_jobs is None else n_jobs\n        self.Dchosen = None\n        if False:  # TODO\n            # in case we already computed distance relations\n            if not recompute_distances and 'data_graph_distance_local' in adata.uns:\n                n_neighbors = adata.uns['data_graph_distance_local'][0].nonzero()[0].size + 1\n                if (knn and issparse(adata.uns['data_graph_distance_local'])\n                        and n_neighbors == self.k):\n                    logg.info('    using stored distances with `n_neighbors={}`'\n                              .format(self.k))\n                    self.Dsq = adata.uns['data_graph_distance_local']", "idx": 961}
{"project": "Scanpy", "commit_id": "601_scanpy_1.0.4_scanpy_tools_umap.py_umap.py", "target": 1, "func": "def umap(\n        adata,\n        min_dist=0.5,\n        spread=1.0,\n        n_components=2,\n        n_epochs=0,\n        alpha=1.0,\n        gamma=1.0,\n        negative_sample_rate=5,\n        init='spectral',\n        random_state=0,\n        a=None,\n        b=None,\n        copy=False):\n    \"\"\"\\\n    Embed the neighborhood graph using UMAP [McInnes18]_.\n    UMAP (Uniform Manifold Approximation and Projection) is a manifold learning\n    technique suitable for visualizing high-dimensional data. Besides tending to\n    be faster than tSNE, it optimizes the embedding such that it best reflects\n    the topology of the data, which we represent throughout Scanpy using a\n    neighborhood graph. tSNE, by contrast, optimizes the distribution of\n    nearest-neighbor distances in the embedding such that these best match the\n    distribution of distances in the high-dimensional space.  We use the\n    implementation of `umap-learn <https://github.com/lmcinnes/umap>`_\n    [McInnes18]_. For a few comparisons of UMAP with tSNE, see this `preprint\n    <https://doi.org/10.1101/298430>`_.\n    Parameters\n    ----------\n    adata : :class:`~scanpy.api.AnnData`\n        Annotated data matrix.\n    min_dist : `float`, optional (default: 0.5)\n        The effective minimum distance between embedded points. Smaller values\n        will result in a more clustered/clumped embedding where nearby points\n        on the manifold are drawn closer together, while larger values will\n        result on a more even dispersal of points. The value should be set\n        relative to the ``spread`` value, which determines the scale at which\n        embedded points will be spread out.\n    spread : `float` (optional, default 1.0)\n        The effective scale of embedded points. In combination with `min_dist`\n        this determines how clustered/clumped the embedded points are.\n    n_components : `int`, optional (default: 2)\n        The number of dimensions of the embedding.\n    n_epochs : `int`, optional (default: 0)\n        The number of epochs of the optimization.\n    alpha : `float`, optional (default: 1.0)\n        The initial learning rate for the embedding optimization.\n    gamma : `float` (optional, default 1.0)\n        Weighting applied to negative samples in low dimensional embedding\n        optimization. Values higher than one will result in greater weight\n        being given to negative samples.\n    negative_sample_rate : `int` (optional, default 5)\n        The number of negative edge/1-simplex samples to use per positive\n        edge/1-simplex sample in optimizing the low dimensional embedding.\n    init : `string` or `np.array`, optional (default: 'spectral')\n        How to initialize the low dimensional embedding.\n        Options are:\n            * 'spectral': use a spectral embedding of the fuzzy 1-skeleton\n            * 'random': assign initial embedding positions at random.\n            * A numpy array of initial embedding positions.\n    random_state : `int`, `RandomState` or `None`, optional (default: `None`)\n        If `int`, `random_state` is the seed used by the random number generator;\n        If `RandomState`, `random_state` is the random number generator;\n        If `None`, the random number generator is the `RandomState` instance used\n        by `np.random`.\n    a : `float` (optional, default `None`)\n        More specific parameters controlling the embedding. If `None` these\n        values are set automatically as determined by `min_dist` and\n        `spread`.\n    b : `float` (optional, default `None`)\n        More specific parameters controlling the embedding. If `None` these\n        values are set automatically as determined by `min_dist` and\n        `spread`.\n    copy : `bool` (default: `False`)\n        Return a copy instead of writing to adata.\n    Returns\n    -------\n    Depending on `copy`, returns or updates `adata` with the following fields.\n    X_umap : `adata.obsm`\n        UMAP coordinates of data.\n    \"\"\"\n    adata = adata.copy() if copy else adata\n    if 'neighbors' not in adata.uns:\n        raise ValueError(\n            'Did not find \\'neighbors/connectivities\\'. Run `sc.pp.neighbors` first.')\n    logg.info('computing UMAP', r=True)\n    if not adata.uns['neighbors']['params']['method'] == 'umap':\n        logg.warn('neighbors/connectivities have not been computed using umap')\n    from sklearn.utils import check_random_state\n    random_state = check_random_state(random_state)\n    from ..neighbors.umap.umap_ import find_ab_params, simplicial_set_embedding\n    if a is None or b is None:\n        a, b = find_ab_params(spread, min_dist)\n    else:\n        a = a\n        b = b\n    X_umap = simplicial_set_embedding(\n        adata.uns['neighbors']['connectivities'].tocoo(),\n        n_components,\n        alpha,\n        a,\n        b,\n        gamma,\n        negative_sample_rate,\n        n_epochs,\n        init,\n        random_state,\n        max(0, settings.verbosity-3))\n    adata.obsm['X_umap'] = X_umap  # annotate samples with UMAP coordinates\n    logg.info('    finished', time=True, end=' ' if settings.verbosity > 2 else '\\n')\n    logg.hint('added\\n'\n              '    \\'X_umap\\', UMAP coordinates (adata.obsm)')\n    return adata if copy else None", "idx": 962}
{"project": "Scanpy", "commit_id": "758_scanpy_1.3.4_scanpy_plotting_anndata.py_heatmap.py", "target": 1, "func": "def heatmap(adata, var_names, groupby=None, use_raw=None, log=False, num_categories=7,\n            dendrogram=False, var_group_positions=None, var_group_labels=None,\n            var_group_rotation=None, swap_axes=False, show_gene_labels=None, layer=None, show=None, save=None, figsize=None, **kwds):\n    \"\"\"\\\n    Heatmap of the expression values of set of genes..\n    If `groupby` is given, the heatmap is ordered by the respective group. For\n    example, a list of marker genes can be plotted, ordered by clustering. If\n    the `groupby` observation annotation is not categorical the observation\n    annotation is turned into a categorical by binning the data into the number\n    specified in `num_categories`.\n    Parameters\n    ----------\n    adata : :class:`~anndata.AnnData`\n        Annotated data matrix.\n    var_names : `str` or list of `str`\n        `var_names` should be a valid subset of  `adata.var_names`.\n    groupby : `str` or `None`, optional (default: `None`)\n        The key of the observation grouping to consider. It is expected that\n        groupby is a categorical. If groupby is not a categorical observation,\n        it would be subdivided into `num_categories`.\n    log : `bool`, optional (default: `False`)\n        Use the log of the values\n    use_raw : `bool`, optional (default: `None`)\n        Use `raw` attribute of `adata` if present.\n    num_categories : `int`, optional (default: `7`)\n        Only used if groupby observation is not categorical. This value\n        determines the number of groups into which the groupby observation\n        should be subdivided.\n    figsize : (float, float), optional (default: None)\n        Figure size (width, height). If not set, the figure width is set based on the\n        number of  `var_names` and the height is set to 10.\n    dendrogram: `bool` If True, hiearchical clustering between the `groupby` categories is\n        computed and a dendrogram is plotted. `groupby` categories are reordered accoring to\n        the dendrogram order. If groups of var_names are set and those groups correspond\n        to the `groupby` categories, those groups are also reordered. The 'person' method\n        is used to compute the pairwise correlation between categories using all var_names in\n        `raw` if `use_raw` is None, otherwise all adata.var_names are used. The linkage method\n        used is `complete`.\n    var_group_positions :  list of `tuples`.\n        Use this parameter to highlight groups of `var_names`. This will draw a 'bracket'\n        on top of the plot between the given start and end positions. If the\n        parameter `var_group_labels` is set, the corresponding labels is added on\n        top of the bracket. E.g. var_group_positions = [(4,10)] will add a bracket\n        between the fourth var_name and the tenth var_name. By giving more\n        positions, more brackets are drawn.\n    var_group_labels : list of `str`\n        Labels for each of the var_group_positions that want to be highlighted.\n    var_group_rotation : `float` (default: `None`)\n        Label rotation degrees. By default, labels larger than 4 characters are rotated 90 degrees\n    swap_axes: `bool`, optional (default: `False`)\n         By default, the x axis contains `var_names` (e.g. genes) and the y axis the `groupby`\n         categories (if any). By setting `swap_axes` then x are the `groupby` categories and y the `var_names`.\n    show_gene_labels: `bool`, optional (default: `None`).\n         By default gene labels are shown when there are 50 or less genes. Otherwise the labels are removed.\n    layer: `str`, (default `None`)\n         Name of the AnnData object layer that wants to be plotted. By default adata.raw.X is plotted.\n         If `use_raw=False` is set, then `adata.X` is plotted. If `layer` is set to a valid layer name,\n         then the layer is plotted.\n    {show_save_ax}\n    **kwds : keyword arguments\n        Are passed to `seaborn.heatmap`.\n    Returns\n    -------\n    A list of `matplotlib.Axes` where the first ax is the groupby categories\n    colorcode, the second axis is the heatmap and the third axis is the\n    colorbar.\n    \"\"\"\n    if use_raw is None and adata.raw is not None: use_raw = True\n    if isinstance(var_names, str):\n        var_names = [var_names]\n    if not use_raw and layer is None:\n        # this most likely will used a scaled version of the data\n        # and thus is better to use a diverging scale\n        param_set = False\n        if 'vmin' not in kwds:\n            kwds['vmin'] = -3\n            param_set = True\n        if 'vmax' not in kwds:\n            kwds['vmax'] = 3\n            param_set = True\n        if 'cmap' not in kwds:\n            kwds['cmap'] = 'bwr'\n            param_set = True\n        if param_set:\n            logg.info('Divergent color map has been automatically set to plot non-raw data. Use '\n                      '`vmin`, `vmax` and `cmap` to adjust the plot.')\n    categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer)\n    if groupby is None or len(categories) <= 1:\n        categorical = False\n        # dendrogram can only be computed  between groupby categories\n        dendrogram = False\n    else:\n        categorical = True\n    colors = None\n    groupby_cmap = None\n    if dendrogram:\n        dendro_data = _compute_dendrogram(adata, groupby, var_names=var_names,\n                                          categories=categories,\n                                          var_group_labels=var_group_labels,\n                                          var_group_positions=var_group_positions,\n                                          use_raw=use_raw, log=log, num_categories=num_categories)\n        var_group_labels = dendro_data['var_group_labels']\n        var_group_positions = dendro_data['var_group_positions']\n        # reorder obs_tidy\n        if dendro_data['var_names_idx_ordered'] is not None:\n            obs_tidy = obs_tidy.iloc[:, dendro_data['var_names_idx_ordered']]\n            var_names = [var_names[x] for x in dendro_data['var_names_idx_ordered']]\n        obs_tidy.index = obs_tidy.index.reorder_categories(\n            [categories[x] for x in dendro_data['categories_idx_ordered']], ordered=True)\n        # reorder groupby colors\n        if groupby + \"_colors\" in adata.uns:\n            colors = [adata.uns[groupby + \"_colors\"][x] for x in dendro_data['categories_idx_ordered']]\n\n    if show_gene_labels is None:\n        show_gene_labels = True if len(var_names) <= 50 else False\n\n    if categorical:\n        obs_tidy = obs_tidy.sort_index()\n        from matplotlib.colors import LinearSegmentedColormap\n        if colors is None:\n            if groupby + \"_colors\" in adata.uns:\n                colors = adata.uns[groupby + \"_colors\"]\n                groupby_cmap = LinearSegmentedColormap.from_list(groupby + '_cmap', colors, N=len(colors))\n            else:\n                groupby_cmap = pl.get_cmap('tab20')\n        else:\n            groupby_cmap = LinearSegmentedColormap.from_list(groupby + '_cmap', colors, N=len(colors))\n    # determine groupby label positions such that they appear\n    # centered next to the color code rectangle asigned to the category\n    value_sum = 0\n    ticks = []  # contains the centered position of the label\n    labels = []\n    label2code = {}\n    for code, (label, value) in enumerate(obs_tidy.index.value_counts(sort=False).iteritems()):\n        ticks.append(value_sum + (value / 2))\n        labels.append(label)\n        value_sum += value\n        label2code[label] = code\n    from matplotlib import gridspec\n    if not swap_axes:\n        # define a layout of 2 rows x 4 columns\n        # first row is for 'brackets' (if no brackets needed, the height of this row is zero)\n        # second row is for main content. This second row is divided into three axes:\n        #   first ax is for the categories defined by `groupby`\n        #   second ax is for the heatmap\n        #   third ax is for the dendrogram\n        #   fourth ax is for colorbar\n        dendro_width = 1.8 if dendrogram else 0\n        groupby_width = 0.25 if categorical else 0\n        if figsize is None:\n            height = 6\n            if show_gene_labels:\n                heatmap_width = 10\n            else:\n                heatmap_width = len(var_names) * 0.3\n            width = heatmap_width + dendro_width + groupby_width\n        else:\n            width, height = figsize\n            heatmap_width = width - (dendro_width + groupby_width)\n        ax_frac2width = 0.25\n        if var_group_positions is not None and len(var_group_positions) > 0:\n            # add some space in case 'brackets' want to be plotted on top of the image\n            height_ratios = [0.15, height]\n        else:\n            height_ratios = [0, height]\n        fig = pl.figure(figsize=(width, height))\n        axs = gridspec.GridSpec(nrows=2, ncols=4, left=0.05, right=0.48, wspace=0.5 / width,\n                                hspace=0.13 / height,\n                                width_ratios=[groupby_width, heatmap_width, dendro_width, ax_frac2width],\n                                height_ratios=height_ratios)\n\n        heatmap_ax = fig.add_subplot(axs[1, 1])\n        heatmap_cbar_ax = fig.add_subplot(axs[1, 3])\n        heatmap_cbar_ax.tick_params(axis='y', labelsize='small')\n\n        sns.heatmap(obs_tidy, yticklabels=\"auto\", ax=heatmap_ax, cbar_ax=heatmap_cbar_ax, **kwds)\n        if show_gene_labels:\n            heatmap_ax.tick_params(axis='x', labelsize='small')\n            heatmap_ax.set_xticks(np.arange(len(var_names)) + 0.5)\n            heatmap_ax.set_xticklabels(var_names)\n        else:\n            heatmap_ax.tick_params(axis='x', labelbottom=False, bottom=False)\n        heatmap_ax.tick_params(axis='y', left=False, labelleft=False)\n        heatmap_ax.set_ylabel('')\n\n        if categorical:\n            groupby_ax = fig.add_subplot(axs[1, 0])\n            groupby_ax.imshow(np.matrix([label2code[lab] for lab in obs_tidy.index]).T, aspect='auto', cmap=groupby_cmap)\n            if len(categories) > 1:\n                groupby_ax.set_yticks(ticks)\n                groupby_ax.set_yticklabels(labels)\n            # remove y ticks\n            groupby_ax.tick_params(axis='y', left=False, labelsize='small')\n            # remove x ticks and labels\n            groupby_ax.tick_params(axis='x', bottom=False, labelbottom=False)\n            # remove surrounding lines\n            groupby_ax.spines['right'].set_visible(False)\n            groupby_ax.spines['top'].set_visible(False)\n            groupby_ax.spines['left'].set_visible(False)\n            groupby_ax.spines['bottom'].set_visible(False)\n\n            groupby_ax.set_ylabel(groupby)\n            groupby_ax.grid(False)\n        if dendrogram:\n            dendro_ax = fig.add_subplot(axs[1, 2], sharey=heatmap_ax)\n            _plot_dendrogram(dendro_ax, adata, ticks=ticks)\n        # plot group legends on top of heatmap_ax (if given)\n        if var_group_positions is not None and len(var_group_positions) > 0:\n            gene_groups_ax = fig.add_subplot(axs[0, 1], sharex=heatmap_ax)\n            _plot_gene_groups_brackets(gene_groups_ax, group_positions=var_group_positions,\n                                       group_labels=var_group_labels, rotation=var_group_rotation,\n                                       left_adjustment=0.2, right_adjustment=0.8)\n    # swap axes case\n    else:\n        # define a layout of 3 rows x 3 columns\n        # The first row is for the dendrogram (if not dendrogram height is zero)\n        # second row is for main content. This col is divided into three axes:\n        #   first ax is for the heatmap\n        #   second ax is for 'brackets' if any (othwerise width is zero)\n        #   third ax is for colorbar\n        dendro_height = 0.5 if dendrogram else 0\n        groupby_height = 0.13 if categorical else 0\n        if figsize is None:\n            width = 8\n            heatmap_height = len(var_names) * 0.14\n            height = heatmap_height + dendro_height + groupby_height  # +2 to account for labels\n        else:\n            width, height = figsize\n            heatmap_height = height - (dendro_height + groupby_height)\n        height_ratios = [dendro_height, heatmap_height, groupby_height]\n        if var_group_positions is not None and len(var_group_positions) > 0:\n            # add some space in case 'brackets' want to be plotted on top of the image\n            width_ratios = [width, 0.3, 0.25]\n        else:\n            width_ratios = [width, 0., 0.25]\n        fig = pl.figure(figsize=(width, height))\n        axs = gridspec.GridSpec(nrows=3, ncols=3, left=0.05, right=0.48, wspace=0.5 / width,\n                                hspace=0.3 / height,\n                                width_ratios=width_ratios,\n                                height_ratios=height_ratios)\n\n        heatmap_ax = fig.add_subplot(axs[1, 0])\n        heatmap_cbar_ax = fig.add_subplot(axs[1, 2])\n        heatmap_cbar_ax.tick_params(axis='y', labelsize='small')\n        if categorical:\n            groupby_ax = fig.add_subplot(axs[2, 0])\n            groupby_ax.imshow(np.matrix([label2code[lab] for lab in obs_tidy.index]), aspect='auto', cmap=groupby_cmap)\n            if len(categories) > 1:\n                groupby_ax.set_xticks(ticks)\n                groupby_ax.set_xticklabels(labels, rotation=90)\n            # remove x ticks\n            groupby_ax.tick_params(axis='x', bottom=False, labelsize='small')\n            # remove y ticks and labels\n            groupby_ax.tick_params(axis='y', left=False, labelleft=False)\n            # remove surrounding lines\n            groupby_ax.spines['right'].set_visible(False)\n            groupby_ax.spines['top'].set_visible(False)\n            groupby_ax.spines['left'].set_visible(False)\n            groupby_ax.spines['bottom'].set_visible(False)\n\n            groupby_ax.set_xlabel(groupby)\n            groupby_ax.grid(False)\n\n        sns.heatmap(obs_tidy.T, yticklabels='none', ax=heatmap_ax, cbar_ax=heatmap_cbar_ax, **kwds)\n        heatmap_ax.tick_params(axis='x', bottom=False, labelbottom=False)\n        heatmap_ax.set_xlabel('')\n        if show_gene_labels:\n            heatmap_ax.tick_params(axis='y', labelsize='x-small')\n            heatmap_ax.set_yticks(np.arange(len(var_names)) + 0.5)\n            heatmap_ax.set_yticklabels(var_names, rotation=0)\n        else:\n            heatmap_ax.tick_params(axis='y', labelleft=False, left=False)\n\n        if dendrogram:\n            dendro_ax = fig.add_subplot(axs[0, 0], sharex=heatmap_ax)\n            _plot_dendrogram(dendro_ax, adata, ticks=ticks, orientation='top')\n        # plot group legends on top of heatmap_ax (if given)\n        if var_group_positions is not None and len(var_group_positions) > 0:\n            gene_groups_ax = fig.add_subplot(axs[1, 1])\n            arr = []\n            for idx, pos in enumerate(var_group_positions):\n                arr += [idx] * (pos[1]+1 - pos[0])\n            gene_groups_ax.imshow(np.matrix(arr).T, aspect='auto', cmap=groupby_cmap)\n            gene_groups_ax.axis('off')\n\n    utils.savefig_or_show('heatmap', show=show, save=save)\n\n    return axs", "idx": 964}
{"project": "Scanpy", "commit_id": "719_scanpy_1.9.0_test_rank_genes_groups.py_test_singlets.py", "target": 0, "func": "def test_singlets():\n    pbmc = pbmc68k_reduced()\n    pbmc.obs['louvain'] = pbmc.obs['louvain'].cat.add_categories(['11'])\n    pbmc.obs['louvain'][0] = '11'\n\n    with pytest.raises(ValueError, match=rf\"Could not calculate statistics.*{'11'}\"):\n        rank_genes_groups(pbmc, groupby='louvain')", "idx": 976}
{"project": "Scanpy", "commit_id": "672_scanpy_1.2.2_scanpy_tools_rank_genes_groups.py_rank_genes_groups.py", "target": 1, "func": "def rank_genes_groups(\n        adata,\n        groupby,\n        use_raw=True,\n        groups='all',\n        reference='rest',\n        n_genes=100,\n        only_positive=True,\n        key_added=None,\n        copy=False,\n        method='t-test_overestim_var',\n        **kwds):\n    \"\"\"Rank genes for characterizing groups.\n    Parameters\n    ----------\n    adata : :class:`~anndata.AnnData`\n        Annotated data matrix.\n    groupby : `str`\n        The key of the observations grouping to consider.\n    use_raw : `bool`, optional (default: `True`)\n        Use `raw` attribute of `adata` if present.\n    groups : `str`, `list`, optional (default: `'all'`)\n        Subset of groups, e.g. `['g1', 'g2', 'g3']`, to which comparison shall\n        be restricted. If not passed, a ranking will be generated for all\n        groups.\n    reference : `str`, optional (default: `'rest'`)\n        If `'rest'`, compare each group to the union of the rest of the group.  If\n        a group identifier, compare with respect to this group.\n    n_genes : `int`, optional (default: 100)\n        The number of genes that appear in the returned tables.\n    method : {'logreg', 't-test', 'wilcoxon', 't-test_overestim_var'}, optional (default: 't-test_overestim_var')\n        If 't-test', uses t-test, if 'wilcoxon', uses Wilcoxon-Rank-Sum. If\n        't-test_overestim_var', overestimates variance of each group. If\n        'logreg' uses logistic regression, see [Ntranos18]_, `here\n        <https://github.com/theislab/scanpy/issues/95>`__ and `here\n        <http://www.nxn.se/valent/2018/3/5/actionable-scrna-seq-clusters>`__, for\n        why this is meaningful.\n    only_positive : bool, optional (default: `True`)\n        Only consider positive differences.\n    **kwds : keyword parameters\n        Are passed to test methods. Currently this affects only parameters that\n        are passed to `sklearn.linear_model.LogisticRegression\n        <http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html>`__.\n        For instance, you can pass `penalty='l1'` to try to come up with a\n        minimal set of genes that are good predictors (sparse solution meaning\n        few non-zero fitted coefficients).\n    Returns\n    -------\n    Updates `adata` with the following fields.\n    names : structured `np.ndarray` (`.uns['rank_genes_groups']`)\n        Structured array to be indexed by group id storing the gene\n        names. Ordered according to scores.\n    scores : structured `np.ndarray` (`.uns['rank_genes_groups']`)\n        Structured array to be indexed by group id storing the score for each\n        gene for each group. Ordered according to scores.\n    \"\"\"\n    logg.info('ranking genes', r=True)\n    adata = adata.copy() if copy else adata\n    utils.sanitize_anndata(adata)\n    # for clarity, rename variable\n    groups_order = groups\n    if isinstance(groups_order, list) and isinstance(groups_order[0], int):\n        groups_order = [str(n) for n in groups_order]\n    if reference != 'rest' and reference not in set(groups_order):\n        groups_order += [reference]\n    if (reference != 'rest'\n        and reference not in set(adata.obs[groupby].cat.categories)):\n        raise ValueError('reference = {} needs to be one of groupby = {}.'\n                         .format(reference,\n                                 adata.obs[groupby].cat.categories.tolist()))\n    groups_order, groups_masks = utils.select_groups(\n        adata, groups_order, groupby)\n    if key_added is None:\n        key_added = 'rank_genes_groups'\n    adata.uns[key_added] = {}\n    adata.uns[key_added]['params'] = {\n        'groupby': groupby,\n        'reference': reference,\n        'method': method,\n        'use_raw': use_raw,\n    }\n    # adata_comp mocks an AnnData object if use_raw is True\n    # otherwise it's just the AnnData object\n    adata_comp = adata\n    if adata.raw is not None and use_raw:\n        adata_comp = adata.raw\n    X = adata_comp.X\n    # for clarity, rename variable\n    n_genes_user = n_genes\n    # make sure indices are not OoB in case there are less genes than n_genes\n    if n_genes_user > X.shape[1]:\n        n_genes_user = X.shape[1]\n    # in the following, n_genes is simply another name for the total number of genes\n    n_genes = X.shape[1]\n    rankings_gene_scores = []\n    rankings_gene_names = []\n    n_groups = groups_masks.shape[0]\n    ns = np.zeros(n_groups, dtype=int)\n    for imask, mask in enumerate(groups_masks):\n        ns[imask] = np.where(mask)[0].size\n    logg.msg('consider \\'{}\\' groups:'.format(groupby), groups_order, v=4)\n    logg.msg('with sizes:', ns, v=4)\n    if reference != 'rest':\n        ireference = np.where(groups_order == reference)[0][0]\n    reference_indices = np.arange(adata_comp.n_vars, dtype=int)\n    avail_methods = {'t-test', 't-test_overestim_var', 'wilcoxon', 'logreg'}\n    if method not in avail_methods:\n        raise ValueError('Method must be one of {}.'.format(avail_methods))\n    # TODO: all of this is probably going to be removed\n    if method is 't-test_correction_factors':\n        if correction_factors is None:\n            raise ValueError('For this test type, you need to enter correction factors manually.')\n        if len(correction_factors) != 2:\n            raise ValueError('We need exactly 2 correction factors, accessible via correction_factors[i], i=0,1')\n        if correction_factors[0] < 0 or correction_factors[1] < 0:\n            raise ValueError('Correction factors need to be positive numbers!')\n    if method in {'t-test', 't-test_overestim_var',\n                  't-test_double_overestim_var', 't-test_correction_factors'}:\n        # loop over all masks and compute means, variances and sample numbers\n        means = np.zeros((n_groups, n_genes))\n        vars = np.zeros((n_groups, n_genes))\n        for imask, mask in enumerate(groups_masks):\n            means[imask], vars[imask] = simple._get_mean_var(X[mask])\n        # test each either against the union of all other groups or against a\n        # specific group\n        for igroup in range(n_groups):\n            if reference == 'rest':\n                mask_rest = ~groups_masks[igroup]\n            else:\n                if igroup == ireference: continue\n                else: mask_rest = groups_masks[ireference]\n            mean_rest, var_rest = simple._get_mean_var(X[mask_rest])\n            if method == 't-test':\n                ns_rest = np.where(mask_rest)[0].size\n            elif method in {'t-test_overestim_var', 't-test_double_overestim_var'}:\n                ns_rest = ns[igroup]  # hack for overestimating the variance\n            elif method == 't-test_correction_factors':\n                # The tendency is as follows: For the comparison group (rest), overesimate variance --> smaller ns_rest\n                ns_rest = np.where(mask_rest)[0].size/correction_factors[1]\n            if method in {'t-test', 't-test_overestim_var'}:\n                ns_group = ns[igroup]\n            elif method == 't-test_correction_factors':\n                # We underestimate group variance by increasing denominator, i.e. ns_group\n                ns_group = ns[igroup] * correction_factors[0]\n            else:\n                # We do the opposite of t-test_overestim_var\n                ns_group = np.where(mask_rest)[0].size\n            denominator = np.sqrt(vars[igroup]/ns_group + var_rest/ns_rest)\n            denominator[np.flatnonzero(denominator == 0)] = np.nan\n            scores = (means[igroup] - mean_rest) / denominator\n            scores[np.isnan(scores)] = 0\n            scores = scores if only_positive else np.abs(scores)\n            partition = np.argpartition(scores, -n_genes_user)[-n_genes_user:]\n            partial_indices = np.argsort(scores[partition])[::-1]\n            global_indices = reference_indices[partition][partial_indices]\n            rankings_gene_scores.append(scores[global_indices])\n            rankings_gene_names.append(adata_comp.var_names[global_indices])\n    elif method == 'logreg':\n        from sklearn.linear_model import LogisticRegression\n        if reference != 'rest':\n            raise ValueError('\\'logreg\\' is only implemented for `reference==\\'rest\\'`.')\n        clf = LogisticRegression(**kwds)\n        clf.fit(X, adata.obs[groupby])\n        scores_all = clf.coef_\n        for igroup, group in enumerate(groups_order):\n            scores = scores_all[igroup]\n            partition = np.argpartition(scores, -n_genes_user)[-n_genes_user:]\n            partial_indices = np.argsort(scores[partition])[::-1]\n            global_indices = reference_indices[partition][partial_indices]\n            rankings_gene_scores.append(scores[global_indices])\n            rankings_gene_names.append(adata_comp.var_names[global_indices])\n    elif method == 'wilcoxon':\n        # Wilcoxon-rank-sum test is usually more powerful in detecting marker genes\n        # Limit maximal RAM that is required by the calculation. Currently set fixed to roughly 100 MByte\n        CONST_MAX_SIZE = 10000000\n        ns_rest = np.zeros(n_groups, dtype=int)\n        # initialize space for z-scores\n        scores = np.zeros(n_genes)\n        # First loop: Loop over all genes\n        if reference != 'rest':\n            for imask, mask in enumerate(groups_masks):\n                if imask == ireference: continue\n                else: mask_rest = groups_masks[ireference]\n                ns_rest[imask] = np.where(mask_rest)[0].size\n                if ns_rest[imask] <= 25 or ns[imask] <= 25:\n                    logg.hint('Few observations in a group for '\n                              'normal approximation (<=25). Lower test accuracy.')\n                n_active = ns[imask]\n                m_active = ns_rest[imask]\n                # Now calculate gene expression ranking in chunkes:\n                chunk = []\n                # Calculate chunk frames\n                n_genes_max_chunk = floor(CONST_MAX_SIZE / (n_active + m_active))\n                if n_genes_max_chunk < n_genes - 1:\n                    chunk_index = n_genes_max_chunk\n                    while chunk_index < n_genes - 1:\n                        chunk.append(chunk_index)\n                        chunk_index = chunk_index + n_genes_max_chunk\n                    chunk.append(n_genes - 1)\n                else:\n                    chunk.append(n_genes - 1)\n                left = 0\n                # Calculate rank sums for each chunk for the current mask\n                for chunk_index, right in enumerate(chunk):\n                    # Check if issparse is true: AnnData objects are currently sparse.csr or ndarray.\n                    if issparse(X):\n                        df1 = pd.DataFrame(data=X[mask, left:right].todense())\n                        df2 = pd.DataFrame(data=X[mask_rest, left:right].todense(),\n                                           index=np.arange(start=n_active, stop=n_active + m_active))\n                    else:\n                        df1 = pd.DataFrame(data=X[mask, left:right])\n                        df2 = pd.DataFrame(data=X[mask_rest, left:right],\n                                           index=np.arange(start=n_active, stop=n_active + m_active))\n                    df1 = df1.append(df2)\n                    ranks = df1.rank()\n                    # sum up adjusted_ranks to calculate W_m,n\n                    scores[left:right] = np.sum(ranks.loc[0:n_active, :])\n                    left = right + 1\n                scores = (scores - (n_active * (n_active + m_active + 1) / 2)) / sqrt(\n                    (n_active * m_active * (n_active + m_active + 1) / 12))\n                scores = scores if only_positive else np.abs(scores)\n                scores[np.isnan(scores)] = 0\n                partition = np.argpartition(scores, -n_genes_user)[-n_genes_user:]\n                partial_indices = np.argsort(scores[partition])[::-1]\n                global_indices = reference_indices[partition][partial_indices]\n                rankings_gene_scores.append(scores[global_indices])\n                rankings_gene_names.append(adata_comp.var_names[global_indices])\n        # If no reference group exists, ranking needs only to be done once (full mask)\n        else:\n            scores = np.zeros((n_groups, n_genes))\n            chunk = []\n            n_cells = X.shape[0]\n            n_genes_max_chunk = floor(CONST_MAX_SIZE / n_cells)\n            if n_genes_max_chunk < n_genes - 1:\n                chunk_index = n_genes_max_chunk\n                while chunk_index < n_genes - 1:\n                    chunk.append(chunk_index)\n                    chunk_index = chunk_index + n_genes_max_chunk\n                chunk.append(n_genes - 1)\n            else:\n                chunk.append(n_genes - 1)\n            left = 0\n            for chunk_index, right in enumerate(chunk):\n                # Check if issparse is true\n                if issparse(X):\n                    df1 = pd.DataFrame(data=X[:, left:right].todense())\n                else:\n                    df1 = pd.DataFrame(data=X[:, left:right])\n                ranks = df1.rank()\n                # sum up adjusted_ranks to calculate W_m,n\n                for imask, mask in enumerate(groups_masks):\n                    scores[imask, left:right] = np.sum(ranks.loc[mask, :])\n                left = right + 1\n            for imask, mask in enumerate(groups_masks):\n                scores[imask, :] = (scores[imask, :] - (ns[imask] * (n_cells + 1) / 2)) / sqrt(\n                    (ns[imask] * (n_cells - ns[imask]) * (n_cells + 1) / 12))\n                scores = scores if only_positive else np.abs(scores)\n                scores[np.isnan(scores)] = 0\n                partition = np.argpartition(scores[imask, :], -n_genes_user)[-n_genes_user:]\n                partial_indices = np.argsort(scores[imask, partition])[::-1]\n                global_indices = reference_indices[partition][partial_indices]\n                rankings_gene_scores.append(scores[imask, global_indices])\n                rankings_gene_names.append(adata_comp.var_names[global_indices])\n    groups_order_save = [str(g) for g in groups_order]\n    if reference != 'rest':\n        groups_order_save = [g for g in groups_order if g != reference]\n    adata.uns[key_added]['scores'] = np.rec.fromarrays(\n        [n for n in rankings_gene_scores],\n        dtype=[(rn, 'float32') for rn in groups_order_save])\n    adata.uns[key_added]['names'] = np.rec.fromarrays(\n        [n for n in rankings_gene_names],\n        dtype=[(rn, 'U50') for rn in groups_order_save])\n    logg.info('    finished', time=True, end=' ' if settings.verbosity > 2 else '\\n')\n    logg.hint(\n        'added to `.uns[\\'{}\\']`\\n'\n        '    \\'names\\', sorted np.recarray to be indexed by group ids\\n'\n        '    \\'scores\\', sorted np.recarray to be indexed by group ids'\n        .format(key_added))\n    return adata if copy else None", "idx": 979}
{"project": "Scanpy", "commit_id": "933_scanpy_1.9.0___init__.py_dec.py", "target": 0, "func": "def dec(obj):\n        obj.__orig_doc__ = obj.__doc__\n        obj.__doc__ = dedent(obj.__doc__).format_map(kwds)\n        return obj", "idx": 987}
{"project": "Scanpy", "commit_id": "209_scanpy_0.0_scanpy_preprocess_simple.py_log1p.py", "target": 1, "func": "def log1p(data):\n    \"\"\"\n    Apply logarithm to count data \"plus 1\".\n    \"\"\"\n    if isinstance(data, AnnData):\n        return AnnData(log1p(data.X), data.smp, data.var, data.add)\n    X = data  # proceed with data matrix\n    if not issparse(X):\n        X = np.log1p(X)\n    else:\n        X = X.log1p()\n    return X", "idx": 991}
{"project": "Scanpy", "commit_id": "633_scanpy_1.1_scanpy_plotting_tools___init__.py_rank_genes_groups_violin.py", "target": 1, "func": "def rank_genes_groups_violin(adata, groups=None, n_genes=20,\n                             gene_names=None, gene_symbols=None,\n                             use_raw=None,\n                             key=None,\n                             split=True,\n                             scale='width',\n                             strip=True, jitter=True, size=1,\n                             ax=None, show=None, save=None):\n    \"\"\"Plot ranking of genes for all tested comparisons.\n    Parameters\n    ----------\n    adata : :class:`~scanpy.api.AnnData`\n        Annotated data matrix.\n    groups : list of `str`, optional (default: `None`)\n        List of group names.\n    n_genes : `int`, optional (default: 20)\n        Number of genes to show. Not used if gene_names is not None.\n    gene_names : list of `str`\n        List of genes to plot.\n    gene_symbols : `str`\n        Key for field in `.var` that stores gene symbols if you do not want to\n        use `.var_names`.\n    use_raw : `bool`, optional (default: `None`)\n        Use `raw` attribute of `adata` if present. Defaults to the value that\n        was used in :func:`~scanpy.api.tl.rank_genes_groups`.\n    split : `bool`, optional (default: `True`)\n        Whether to split the violins or not.\n    scale : `str` (default: 'width')\n        See `seaborn.violinplot`.\n    strip : `bool` (default: `True`)\n        Show a strip plot on top of the violin plot.\n    jitter : `int`, `float`, `bool`, optional (default: `True`)\n        If set to 0, no points are drawn. See `seaborn.stripplot`.\n    size : `int`, optional (default: 1)\n        Size of the jitter points.\n    show : `bool`, optional (default: `None`)\n        Show the plot, do not return axis.\n    save : `bool` or `str`, optional (default: `None`)\n        If `True` or a `str`, save the figure. A string is appended to the\n        default filename. Infer the filetype if ending on {{'.pdf', '.png', '.svg'}}.\n    ax : `matplotlib.Axes`, optional (default: `None`)\n        A `matplotlib.Axes` object.\n    \"\"\"\n    if key is None:\n        key = 'rank_genes_groups'\n    from ..tools import rank_genes_groups\n    groups_key = str(adata.uns[key]['params']['groupby'])\n    if use_raw is None:\n        use_raw = bool(adata.uns[key]['params']['use_raw'])\n    reference = str(adata.uns[key]['params']['reference'])\n    groups_names = (adata.uns[key]['names'].dtype.names\n                    if groups is None else groups)\n    if isinstance(groups_names, str): groups_names = [groups_names]\n    for group_name in groups_names:\n        if gene_names is None:\n            gene_names = adata.uns[\n                key]['names'][group_name][:n_genes]\n        keys = gene_names\n        # make a \"hue\" option!\n        df = pd.DataFrame()\n        new_keys = []\n        for key in keys:\n            if adata.raw is not None and use_raw:\n                X_col = adata.raw[:, key].X\n            else:\n                X_col = adata[:, key].X\n            if issparse(X_col): X_col = X_col.toarray().flatten()\n            key = key if gene_symbols is None else adata.var[gene_symbols][key]\n            new_keys.append(key)\n            df[key] = X_col\n        df['hue'] = adata.obs[groups_key].astype(str).values\n        if reference == 'rest':\n            df.loc[df['hue'] != group_name, 'hue'] = 'rest'\n        else:\n            df.loc[~df['hue'].isin([group_name, reference]), 'hue'] = np.nan\n        df['hue'] = df['hue'].astype('category')\n        df_tidy = pd.melt(df, id_vars='hue', value_vars=new_keys)\n        x = 'variable'\n        y = 'value'\n        hue_order = [group_name, reference]\n        import seaborn as sns\n        ax = sns.violinplot(x=x, y=y, data=df_tidy, inner=None,\n                            hue_order=hue_order, hue='hue', split=split,\n                            scale=scale, orient='vertical', ax=ax)\n        if strip:\n            ax = sns.stripplot(x=x, y=y, data=df_tidy,\n                               hue='hue', dodge=True, hue_order=hue_order,\n                               jitter=jitter, color='black', size=size, ax=ax)\n        ax.set_xlabel('genes')\n        ax.set_title('{} vs. {}'.format(group_name, reference))\n        ax.legend_.remove()\n        if computed_distribution: ax.set_ylabel('z-score w.r.t. to bulk mean')\n        else: ax.set_ylabel('expression')\n        ax.set_xticklabels(gene_names, rotation='vertical')\n        writekey = ('rank_genes_groups_'\n                    + str(adata.uns[key]['params']['groupby'])\n                    + '_' + group_name)\n        utils.savefig_or_show(writekey, show=show, save=save)", "idx": 993}
{"project": "Scanpy", "commit_id": "697_scanpy_1.9.0_test_preprocessing_distributed.py_test_normalize_total.py", "target": 0, "func": "def test_normalize_total(self, adata, adata_dist):\n        normalize_total(adata_dist)\n        result = materialize_as_ndarray(adata_dist.X)\n        normalize_total(adata)\n        assert result.shape == adata.shape\n        assert result.shape == (adata.n_obs, adata.n_vars)\n        npt.assert_allclose(result, adata.X)", "idx": 995}
{"project": "Scanpy", "commit_id": "879_scanpy_1.4.3_scanpy_readwrite.py__read.py", "target": 1, "func": "def _read(filename, backed=False, sheet=None, ext=None, delimiter=None,\n          first_column_names=None, backup_url=None, cache=False,\n          suppress_cache_warning=False, **kwargs):\n    if ext is not None and ext not in avail_exts:\n        raise ValueError('Please provide one of the available extensions.\\n'\n                         + avail_exts)\n    else:\n        ext = is_valid_filename(filename, return_ext=True)\n    is_present = check_datafile_present_and_download(filename,\n                                                     backup_url=backup_url)\n    if not is_present: logg.msg('... did not find original file', filename)\n    # read hdf5 files\n    if ext in {'h5', 'h5ad'}:\n        if sheet is None:\n            return read_h5ad(filename, backed=backed)\n        else:\n            logg.msg('reading sheet', sheet, 'from file', filename, v=4)\n            return read_hdf(filename, sheet)\n    # read other file types\n    path_cache = Path(settings.cachedir) / _slugify(filename).replace('.' + ext, '.h5ad')  # type: Path\n    if path_cache.suffix in {'.gz', '.bz2'}:\n        path_cache = path_cache.with_suffix('')\n    if cache and path_cache.is_file():\n        logg.info('... reading from cache file', path_cache)\n        adata = read_h5ad(path_cache, backed=False)\n    else:\n        if not is_present:\n            raise FileNotFoundError('Did not find file {}.'.format(filename))\n        logg.msg('reading', filename, v=4)\n        if not cache and not suppress_cache_warning:\n            logg.hint('This might be very slow. Consider passing `cache=True`, '\n                      'which enables much faster reading from a cache file.')\n        # do the actual reading\n        if ext == 'xlsx' or ext == 'xls':\n            if sheet is None:\n                raise ValueError(\n                    'Provide `sheet` parameter when reading \\'.xlsx\\' files.')\n            else:\n                adata = read_excel(filename, sheet)\n        elif ext in {'mtx', 'mtx.gz'}:\n            adata = read_mtx(filename)\n        elif ext == 'csv':\n            adata = read_csv(filename, first_column_names=first_column_names)\n        elif ext in {'txt', 'tab', 'data', 'tsv'}:\n            if ext == 'data':\n                logg.msg('... assuming \\'.data\\' means tab or white-space '\n                         'separated text file', v=3)\n                logg.hint('change this by passing `ext` to sc.read')\n            adata = read_text(filename, delimiter, first_column_names)\n        elif ext == 'soft.gz':\n            adata = _read_softgz(filename)\n        elif ext == 'loom':\n            adata = read_loom(filename=filename, **kwargs)\n        else:\n            raise ValueError('Unkown extension {}.'.format(ext))\n        if cache:\n            logg.info('... writing an', settings.file_format_data,\n                      'cache file to speedup reading next time')\n            if not path_cache.parent.is_dir():\n                path_cache.parent.mkdir(parents=True)\n            # write for faster reading when calling the next time\n            adata.write(path_cache)\n    return adata", "idx": 999}
{"project": "Scanpy", "commit_id": "428_scanpy_1.9.0__simple.py_scale.py", "target": 0, "func": "def scale(\n    X: Union[AnnData, spmatrix, np.ndarray],\n    zero_center: bool = True,\n    max_value: Optional[float] = None,\n    copy: bool = False,\n    layer: Optional[str] = None,\n    obsm: Optional[str] = None,\n):\n    \"\"\"\\\n    Scale data to unit variance and zero mean.\n\n    .. note::\n        Variables (genes) that do not display any variation (are constant across\n        all observations) are retained and (for zero_center==True) set to 0\n        during this operation. In the future, they might be set to NaNs.\n\n    Parameters\n    ----------\n    X\n        The (annotated) data matrix of shape `n_obs` \u00d7 `n_vars`.\n        Rows correspond to cells and columns to genes.\n    zero_center\n        If `False`, omit zero-centering variables, which allows to handle sparse\n        input efficiently.\n    max_value\n        Clip (truncate) to this value after scaling. If `None`, do not clip.\n    copy\n        Whether this function should be performed inplace. If an AnnData object\n        is passed, this also determines if a copy is returned.\n    layer\n        If provided, which element of layers to scale.\n    obsm\n        If provided, which element of obsm to scale.\n\n    Returns\n    -------\n    Depending on `copy` returns or updates `adata` with a scaled `adata.X`,\n    annotated with `'mean'` and `'std'` in `adata.var`.\n    \"\"\"\n    _check_array_function_arguments(layer=layer, obsm=obsm)\n    if layer is not None:\n        raise ValueError(f\"`layer` argument inappropriate for value of type {type(X)}\")\n    if obsm is not None:\n        raise ValueError(f\"`obsm` argument inappropriate for value of type {type(X)}\")\n    return scale_array(X, zero_center=zero_center, max_value=max_value, copy=copy)", "idx": 1003}
{"project": "Scanpy", "commit_id": "681_scanpy_1.2.2_scanpy_preprocessing_simple.py_subsample.py", "target": 1, "func": "def subsample(data, fraction, random_state=0, copy=False):\n    \"\"\"Subsample to a fraction of the number of observations.\n    Parameters\n    ----------\n    data : :class:`~anndata.AnnData`, `np.ndarray`, `sp.sparse`\n        The (annotated) data matrix of shape `n_obs` \u00d7 `n_vars`. Rows correspond\n        to cells and columns to genes.\n    fraction : float in [0, 1]\n        Subsample to this `fraction` of the number of observations.\n    random_state : `int` or `None`, optional (default: 0)\n        Random seed to change subsampling.\n    copy : `bool`, optional (default: `False`)\n        If an :class:`~anndata.AnnData` is passed, determines whether a copy\n        is returned.\n    Returns\n    -------\n    Returns `X[obs_indices], obs_indices` if data is array-like, otherwise\n    subsamples the passed :class:`~anndata.AnnData` (`copy == False`) or\n    returns a subsampled copy of it (`copy == True`).\n    \"\"\"\n    if fraction > 1 or fraction < 0:\n        raise ValueError('`fraction` needs to be within [0, 1], not {}'\n                         .format(fraction))\n    np.random.seed(random_state)\n    n_obs = data.n_obs if isinstance(data, AnnData) else data.shape[0]\n    new_n_obs = int(fraction * n_obs)\n    obs_indices = np.random.choice(n_obs, size=new_n_obs, replace=False)\n    logg.msg('... subsampled to {} data points'.format(new_n_obs))\n    if isinstance(data, AnnData):\n        adata = data.copy() if copy else data\n        adata._inplace_subset_obs(obs_indices)\n        return adata if copy else None\n    else:\n        X = data\n        return X[obs_indices], obs_indices", "idx": 1009}
{"project": "Scanpy", "commit_id": "12_scanpy_1.9.0_github_links.py_register_links.py", "target": 0, "func": "def register_links(app: Sphinx, config: Config):\n    gh_url = 'https://github.com/{github_user}/{github_repo}'.format_map(\n        config.html_context\n    )\n    app.add_role('pr', AutoLink('pr', f'{gh_url}/pull/{{}}', 'PR {}'))\n    app.add_role('issue', AutoLink('issue', f'{gh_url}/issues/{{}}', 'issue {}'))\n    app.add_role('noteversion', AutoLink('noteversion', f'{gh_url}/releases/tag/{{}}'))\n    # tutorial links\n    scanpy_tutorials_url = 'https://scanpy-tutorials.readthedocs.io/en/latest/'\n    app.add_role(\n        'tutorial',\n        AutoLink('tutorial', f'{scanpy_tutorials_url}{{}}.html', '\u2192 tutorial: {}'),", "idx": 1010}
{"project": "Scanpy", "commit_id": "889_scanpy_1.9.0__sim.py_sim_givenAdj.py", "target": 0, "func": "def sim_givenAdj(self, Adj: np.ndarray, model='line'):\n        \"\"\"\\\n        Simulate data given only an adjacancy matrix and a model.\n\n        The model is a bivariate funtional dependence. The adjacancy matrix\n        needs to be acyclic.\n\n        Parameters\n        ----------\n        Adj\n            adjacancy matrix of shape (dim,dim).\n\n        Returns\n        -------\n        Data array of shape (n_samples,dim).\n        \"\"\"\n        # nice examples\n        examples = [  # noqa: F841 TODO We are really unsure whether this is needed.\n            dict(\n                func='sawtooth',\n                gdist='uniform',\n                sigma_glob=1.8,\n                sigma_noise=0.1,\n            )\n        ]\n\n        # nr of samples\n        n_samples = 100\n\n        # noise\n        sigma_glob = 1.8\n        sigma_noise = 0.4\n\n        # coupling function / model\n        func = self.funcs[model]\n\n        # glob distribution\n        sourcedist = 'uniform'\n\n        # loop over source nodes\n        dim = Adj.shape[0]\n        X = np.zeros((n_samples, dim))\n        # source nodes have no parents themselves\n        nrpar = 0\n        children = list(range(dim))\n        parents = []\n        for gp in range(dim):\n            if Adj[gp, :].sum() == nrpar:\n                if sourcedist == 'gaussian':\n                    X[:, gp] = np.random.normal(0, sigma_glob, n_samples)\n                if sourcedist == 'uniform':\n                    X[:, gp] = np.random.uniform(-sigma_glob, sigma_glob, n_samples)\n                parents.append(gp)\n                children.remove(gp)\n\n        # all of the following guarantees for 3 dim, that we generate the data\n        # in the correct sequence\n        # then compute all nodes that have 1 parent, then those with 2 parents\n        children_sorted = []\n        nrchildren_par = np.zeros(dim)\n        nrchildren_par[0] = len(parents)\n        for nrpar in range(1, dim):\n            # loop over child nodes\n            for gp in children:\n                if Adj[gp, :].sum() == nrpar:\n                    children_sorted.append(gp)\n                    nrchildren_par[nrpar] += 1\n        # if there is more than a child with a single parent\n        # order these children (there are two in three dim)\n        # by distance to the source/parent\n        if nrchildren_par[1] > 1:\n            if Adj[children_sorted[0], parents[0]] == 0:\n                help = children_sorted[0]\n                children_sorted[0] = children_sorted[1]\n                children_sorted[1] = help\n\n        for gp in children_sorted:\n            for g in range(dim):\n                if Adj[gp, g] > 0:\n                    X[:, gp] += 1.0 / Adj[gp, :].sum() * func(X[:, g])\n            X[:, gp] += np.random.normal(0, sigma_noise, n_samples)\n\n        #         fig = pl.figure()\n        #         fig.add_subplot(311)\n        #         pl.plot(X[:,0],X[:,1],'.',mec='white')\n        #         fig.add_subplot(312)\n        #         pl.plot(X[:,1],X[:,2],'.',mec='white')\n        #         fig.add_subplot(313)\n        #         pl.plot(X[:,2],X[:,0],'.',mec='white')\n        #         pl.show()\n\n        return X", "idx": 1012}
{"project": "Scanpy", "commit_id": "978_scanpy_1.4.5.1_scanpy_plotting__tools_scatterplots.py_embedding.py", "target": 1, "func": "def embedding(\n    adata: AnnData,\n    basis: str,\n    *,\n    color: Union[str, Sequence[str], None] = None,\n    gene_symbols: Optional[str] = None,\n    use_raw: Optional[bool] = None,\n    sort_order: bool = True,\n    edges: bool = False,\n    edges_width: float = 0.1,\n    edges_color: Union[str, Sequence[float], Sequence[str]] = 'grey',\n    arrows: bool = False,\n    arrows_kwds: Optional[Mapping[str, Any]] = None,\n    groups: Optional[str] = None,\n    components: Union[str, Sequence[str]] = None,\n    layer: Optional[str] = None,\n    projection: Literal['2d', '3d'] = '2d',\n    # image parameters\n    img_key: Optional[str] = None,\n    crop_coord: Tuple[int, int, int, int] = None,\n    alpha_img: float = 1.0,\n    bw: bool = False,\n    #\n    color_map: Union[Colormap, str, None] = None,\n    palette: Union[str, Sequence[str], Cycler, None] = None,\n    size: Union[float, Sequence[float], None] = None,\n    frameon: Optional[bool] = None,\n    legend_fontsize: Union[int, float, _FontSize, None] = None,\n    legend_fontweight: Union[int, _FontWeight] = 'bold',\n    legend_loc: str = 'right margin',\n    legend_fontoutline: Optional[int] = None,\n    vmax: Union[VMinMax, Sequence[VMinMax], None] = None,\n    vmin: Union[VMinMax, Sequence[VMinMax], None] = None,\n    add_outline: Optional[bool] = False,\n    outline_width: Tuple[float, float] = (0.3, 0.05),\n    outline_color: Tuple[str, str] = ('black', 'white'),\n    ncols: int = 4,\n    hspace: float = 0.25,\n    wspace: Optional[float] = None,\n    title: Union[str, Sequence[str], None] = None,\n    show: Optional[bool] = None,\n    save: Union[bool, str, None] = None,\n    ax: Optional[Axes] = None,\n    return_fig: Optional[bool] = None,\n    **kwargs,\n) -> Union[Figure, Axes, None]:\n    \"\"\"\\\n    Scatter plot for user specified embedding basis (e.g. umap, pca, etc)\n    Parameters\n    ----------\n    basis\n        Name of the `obsm` basis to use.\n    {adata_color_etc}\n    {edges_arrows}\n    {scatter_bulk}\n    {show_save_ax}\n    Returns\n    -------\n    If `show==False` a :class:`~matplotlib.axes.Axes` or a list of it.\n    \"\"\"\n    sanitize_anndata(adata)\n    if color_map is not None:\n        kwargs['cmap'] = color_map\n    if size is not None:\n        kwargs['s'] = size\n    if 'edgecolor' not in kwargs:\n        # by default turn off edge color. Otherwise, for\n        # very small sizes the edge will not reduce its size\n        # (https://github.com/theislab/scanpy/issues/293)\n        kwargs['edgecolor'] = 'none'\n    if groups:\n        if isinstance(groups, str):\n            groups = [groups]\n\n    if projection == '3d':\n        check_mpl_3d_bug()\n        from mpl_toolkits.mplot3d import Axes3D\n\n        args_3d = {'projection': '3d'}\n    else:\n        args_3d = {}\n\n    # Deal with Raw\n    if use_raw is None:\n        # check if adata.raw is set\n        use_raw = layer is None and adata.raw is not None\n    if use_raw and layer is not None:\n        raise ValueError(\n            \"Cannot use both a layer and the raw representation. Was passed:\"\n            f\"use_raw={use_raw}, layer={layer}.\"\n        )\n    if wspace is None:\n        #  try to set a wspace that is not too large or too small given the\n        #  current figure size\n        wspace = 0.75 / rcParams['figure.figsize'][0] + 0.02\n    if adata.raw is None and use_raw:\n        raise ValueError(\n            \"`use_raw` is set to True but AnnData object does not have raw. \"\n            \"Please check.\"\n        )\n    # turn color into a python list\n    color = [color] if isinstance(color, str) or color is None else list(color)\n    if title is not None:\n        # turn title into a python list if not None\n        title = [title] if isinstance(title, str) else list(title)\n    # get the points position and the components list\n    # (only if components is not None)\n    data_points, components_list = _get_data_points(\n        adata, basis, projection, components, img_key\n    )\n    # Setup layout.\n    # Most of the code is for the case when multiple plots are required\n    # 'color' is a list of names that want to be plotted.\n    # Eg. ['Gene1', 'louvain', 'Gene2'].\n    # component_list is a list of components [[0,1], [1,2]]\n    if (\n        not isinstance(color, str)\n        and isinstance(color, cabc.Sequence)\n        and len(color) > 1\n    ) or len(components_list) > 1:\n        if ax is not None:\n            raise ValueError(\n                \"Cannot specify `ax` when plotting multiple panels \"\n                \"(each for a given value of 'color').\"\n            )\n        if len(components_list) == 0:\n            components_list = [None]\n        # each plot needs to be its own panel\n        num_panels = len(color) * len(components_list)\n        fig, grid = _panel_grid(hspace, wspace, ncols, num_panels)\n    else:\n        if len(components_list) == 0:\n            components_list = [None]\n        grid = None\n        if ax is None:\n            fig = pl.figure()\n            ax = fig.add_subplot(111, **args_3d)\n    # turn vmax and vmin into a sequence\n    if isinstance(vmax, str) or not isinstance(vmax, cabc.Sequence):\n        vmax = [vmax]\n    if isinstance(vmin, str) or not isinstance(vmin, cabc.Sequence):\n        vmin = [vmin]\n    if 's' in kwargs:\n        size = kwargs.pop('s')\n    if size is not None:\n        # check if size is any type of sequence, and if so\n        # set as ndarray\n        import pandas.core.series\n        if (\n            size is not None\n            and isinstance(\n                size, (cabc.Sequence, pandas.core.series.Series, np.ndarray,)\n            )\n            and len(size) == adata.shape[0]\n        ):\n            size = np.array(size, dtype=float)\n    elif img_key is not None:\n        size = 1.0\n    else:\n        size = 120000 / adata.shape[0]\n    ###\n    # make the plots\n    axs = []\n    import itertools\n    idx_components = range(len(components_list))\n    # use itertools.product to make a plot for each color and for each component\n    # For example if color=[gene1, gene2] and components=['1,2, '2,3'].\n    # The plots are: [\n    #     color=gene1, components=[1,2], color=gene1, components=[2,3],\n    #     color=gene2, components = [1, 2], color=gene2, components=[2,3],\n    # ]\n    for count, (value_to_plot, component_idx) in enumerate(\n        itertools.product(color, idx_components)\n    ):\n        color_vector, categorical = _get_color_values(\n            adata,\n            value_to_plot,\n            layer=layer,\n            groups=groups,\n            palette=palette,\n            use_raw=use_raw,\n            gene_symbols=gene_symbols,\n        )\n        # check if higher value points should be plot on top\n        if sort_order is True and value_to_plot is not None and categorical is False:\n            order = np.argsort(color_vector)\n            color_vector = color_vector[order]\n            _data_points = data_points[component_idx][order, :]\n            # check if 'size' is given (stored in kwargs['s']\n            # and reorder it.\n            if isinstance(size, np.ndarray):\n                size = np.array(size)[order]\n        else:\n            _data_points = data_points[component_idx]\n        # if plotting multiple panels, get the ax from the grid spec\n        # else use the ax value (either user given or created previously)\n        if grid:\n            ax = pl.subplot(grid[count], **args_3d)\n            axs.append(ax)\n        if not (settings._frameon if frameon is None else frameon):\n            ax.axis('off')\n        if title is None:\n            if value_to_plot is not None:\n                ax.set_title(value_to_plot)\n            else:\n                ax.set_title('')\n        else:\n            try:\n                ax.set_title(title[count])\n            except IndexError:\n                logg.warning(\n                    \"The title list is shorter than the number of panels. \"\n                    \"Using 'color' value instead for some plots.\"\n                )\n                ax.set_title(value_to_plot)\n        # check vmin and vmax options\n        if categorical:\n            kwargs['vmin'] = kwargs['vmax'] = None\n        else:\n            kwargs['vmin'], kwargs['vmax'] = _get_vmin_vmax(\n                vmin, vmax, count, color_vector\n            )\n        # make the scatter plot\n        if projection == '3d':\n            cax = ax.scatter(\n                _data_points[:, 0],\n                _data_points[:, 1],\n                _data_points[:, 2],\n                marker=\".\",\n                c=color_vector,\n                rasterized=settings._vector_friendly,\n                **kwargs,\n            )\n        else:\n            if img_key is not None:\n                # had to return size_spot cause spot size is set according\n                # to the image to be plotted\n                img_processed, img_coord, size_spot, cmap_img = _process_image(\n                    adata, data_points, img_key, crop_coord, size, bw\n                )\n                ax.imshow(img_processed, cmap=cmap_img, alpha=alpha_img)\n                ax.set_xlim(img_coord[0], img_coord[1])\n                ax.set_ylim(img_coord[3], img_coord[2])\n            scatter = (\n                partial(ax.scatter, s=size)\n                if img_key is None\n                else partial(circles, s=size_spot)\n            )\n            if add_outline:\n                # the default outline is a black edge followed by a\n                # thin white edged added around connected clusters.\n                # To add an outline\n                # three overlapping scatter plots are drawn:\n                # First black dots with slightly larger size,\n                # then, white dots a bit smaller, but still larger\n                # than the final dots. Then the final dots are drawn\n                # with some transparency.\n                bg_width, gap_width = outline_width\n                point = np.sqrt(size)\n                gap_size = (point + (point * gap_width) * 2) ** 2\n                bg_size = (np.sqrt(gap_size) + (point * bg_width) * 2) ** 2\n                # the default black and white colors can be changes using\n                # the contour_config parameter\n                bg_color, gap_color = outline_color\n                # remove edge from kwargs if present\n                # because edge needs to be set to None\n                kwargs['edgecolor'] = 'none'\n                # remove alpha for outline\n                alpha = kwargs.pop('alpha') if 'alpha' in kwargs else None\n                ax.scatter(\n                    _data_points[:, 0],\n                    _data_points[:, 1],\n                    s=bg_size,\n                    marker=\".\",\n                    c=bg_color,\n                    rasterized=settings._vector_friendly,\n                    **kwargs,\n                )\n                ax.scatter(\n                    _data_points[:, 0],\n                    _data_points[:, 1],\n                    s=gap_size,\n                    marker=\".\",\n                    c=gap_color,\n                    rasterized=settings._vector_friendly,\n                    **kwargs,\n                )\n                # if user did not set alpha, set alpha to 0.7\n                kwargs['alpha'] = 0.7 if alpha is None else alpha\n            if groups:\n                # first plot non-groups and then plot the\n                # required groups on top\n                in_groups = np.array(adata.obs[value_to_plot].isin(groups))\n                if isinstance(size, np.ndarray):\n                    in_groups_size = size[in_groups]\n                    not_in_groups_size = size[~in_groups]\n                elif img_key is not None:\n                    in_groups_size = not_in_groups_size = size_spot\n                else:\n                    in_groups_size = not_in_groups_size = size\n                # only show grey points if no image is below\n                if img_key is None:\n                    ax.scatter(\n                        _data_points[~in_groups, 0],\n                        _data_points[~in_groups, 1],\n                        s=not_in_groups_size,\n                        marker=\".\",\n                        c=color_vector[~in_groups],\n                        rasterized=settings._vector_friendly,\n                        **kwargs,\n                    )\n                cax = scatter(\n                    _data_points[in_groups, 0],\n                    _data_points[in_groups, 1],\n                    s=in_groups_size,\n                    marker=\".\",\n                    c=color_vector[in_groups],\n                    rasterized=settings._vector_friendly,\n                    **kwargs,\n                )\n            else:\n                cax = scatter(\n                    _data_points[:, 0],\n                    _data_points[:, 1],\n                    marker=\".\",\n                    c=color_vector,\n                    rasterized=settings._vector_friendly,\n                    **kwargs,\n                )\n        # remove y and x ticks\n        ax.set_yticks([])\n        ax.set_xticks([])\n        if projection == '3d':\n            ax.set_zticks([])\n        # set default axis_labels\n        name = _basis2name(basis)\n        if components is not None:\n            axis_labels = [name + str(x + 1) for x in components_list[component_idx]]\n        elif projection == '3d':\n            axis_labels = [name + str(x + 1) for x in range(3)]\n        else:\n            axis_labels = [name + str(x + 1) for x in range(2)]\n        ax.set_xlabel(axis_labels[0])\n        ax.set_ylabel(axis_labels[1])\n        if projection == '3d':\n            # shift the label closer to the axis\n            ax.set_zlabel(axis_labels[2], labelpad=-7)\n        ax.autoscale_view()\n        if edges:\n            _utils.plot_edges(ax, adata, basis, edges_width, edges_color)\n        if arrows:\n            _utils.plot_arrows(ax, adata, basis, arrows_kwds)\n        if value_to_plot is None:\n            # if only dots were plotted without an associated value\n            # there is not need to plot a legend or a colorbar\n            continue\n        if legend_fontoutline is not None:\n            path_effect = [\n                patheffects.withStroke(linewidth=legend_fontoutline, foreground='w',)\n            ]\n        else:\n            path_effect = None\n        _add_legend_or_colorbar(\n            adata,\n            ax,\n            cax,\n            categorical,\n            value_to_plot,\n            legend_loc,\n            _data_points,\n            legend_fontweight,\n            legend_fontsize,\n            path_effect,\n            groups,\n            bool(grid),\n        )\n    if return_fig is True:\n        return fig\n    axs = axs if grid else ax\n    _utils.savefig_or_show(basis, show=show, save=save)\n    if show is False:\n        return axs", "idx": 1013}
{"project": "Scanpy", "commit_id": "244_scanpy_1.9.0__anndata.py_clustermap.py", "target": 0, "func": "def clustermap(\n    adata: AnnData,\n    obs_keys: str = None,\n    use_raw: Optional[bool] = None,\n    show: Optional[bool] = None,\n    save: Union[bool, str, None] = None,\n    **kwds,\n):\n    \"\"\"\\\n    Hierarchically-clustered heatmap.\n\n    Wraps :func:`seaborn.clustermap` for :class:`~anndata.AnnData`.\n\n    Parameters\n    ----------\n    adata\n        Annotated data matrix.\n    obs_keys\n        Categorical annotation to plot with a different color map.\n        Currently, only a single key is supported.\n    use_raw\n        Whether to use `raw` attribute of `adata`. Defaults to `True` if `.raw` is present.\n    {show_save_ax}\n    **kwds\n        Keyword arguments passed to :func:`~seaborn.clustermap`.\n\n    Returns\n    -------\n    If `show` is `False`, a :class:`~seaborn.ClusterGrid` object\n    (see :func:`~seaborn.clustermap`).\n\n    Examples\n    --------\n    Soon to come with figures. In the meanwile, see :func:`~seaborn.clustermap`.\n\n    >>> import scanpy as sc\n    >>> adata = sc.datasets.krumsiek11()\n    >>> sc.pl.clustermap(adata, obs_keys='cell_type')\n    \"\"\"\n    import seaborn as sns  # Slow import, only import if called\n\n    if not isinstance(obs_keys, (str, type(None))):\n        raise ValueError('Currently, only a single key is supported.')\n    sanitize_anndata(adata)\n    use_raw = _check_use_raw(adata, use_raw)\n    X = adata.raw.X if use_raw else adata.X\n    if issparse(X):\n        X = X.toarray()\n    df = pd.DataFrame(X, index=adata.obs_names, columns=adata.var_names)\n    if obs_keys is not None:\n        row_colors = adata.obs[obs_keys]\n        _utils.add_colors_for_categorical_sample_annotation(adata, obs_keys)\n        # do this more efficiently... just a quick solution\n        lut = dict(zip(row_colors.cat.categories, adata.uns[obs_keys + '_colors']))\n        row_colors = adata.obs[obs_keys].map(lut)\n        g = sns.clustermap(df, row_colors=row_colors.values, **kwds)\n    else:\n        g = sns.clustermap(df, **kwds)\n    show = settings.autoshow if show is None else show\n    _utils.savefig_or_show('clustermap', show=show, save=save)\n    if show:\n        pl.show()\n    else:\n        return g", "idx": 1028}
{"project": "Scanpy", "commit_id": "123_scanpy_0.0_scanpy_readwrite.py_write_dict_to_file.py", "target": 1, "func": "def write_dict_to_file(filename, d, ext='h5'):\n    \"\"\"\n    Write content of dictionary to file.\n    Parameters\n    ----------\n    filename : str\n        Filename of data file.\n    d : dict\n        Dictionary storing keys with np.ndarray-like data or scalars.\n    ext : string\n        Determines file type, allowed are 'h5' (hdf5),\n        'xlsx' (Excel) [or 'csv' (comma separated value file)].\n    \"\"\"\n    directory = os.path.dirname(filename)\n    if not os.path.exists(directory):\n        sett.m(0, 'creating directory', directory + '/', 'for saving output files')\n        os.makedirs(directory)\n    if 'X' in d:\n        import scipy.sparse\n        if isinstance(d['X'], scipy.sparse.spmatrix):\n            save_sparse_csr(filename.replace('.' + ext, '') + '_X.npz', d['X'])\n    if ext == 'h5':\n        with h5py.File(filename, 'w') as f:\n            for key, value in d.items():\n                key, value = prepare_writing(key, value, ext)\n                try:\n                    f.create_dataset(key, data=value)\n                except Exception as e:\n                    sett.m(0, 'Error creating dataset for key =', key)\n                    raise e\n    elif ext == 'npz':\n        d_write = {}\n        import scipy.sparse\n        for key, value in d.items():\n            if key == 'X' and isinstance(value, scipy.sparse.spmatrix):\n                continue\n            key, value = prepare_writing(key, value, ext)\n            d_write[key] = value\n        if not d_write:\n            d_write['dummy'] = np.array([1])\n        np.savez(filename, **d_write)\n    elif ext == 'csv' or ext == 'txt':\n        # here this is actually a directory that corresponds to the\n        # single hdf5 file\n        dirname = filename.replace('.' + ext, '')\n        if not os.path.exists(dirname):\n            os.makedirs(dirname)\n        for key, value in d.items():\n            key, value = prepare_writing(key, value, ext)\n            if value.dtype.kind == 'S':\n                value = value.astype('U')\n            if len(value.shape) > 0:\n                np.savetxt(dirname + '/' + key + '.' + ext, value,\n                           fmt = ('%.14e' if value.dtype.char == 'f'\n                                  else '%s' if value.dtype.char == 'U'\n                                  else '%d' if (value.dtype.char == 'b' or value.dtype.char == 'i')\n                                  else '%f'),\n                           delimiter=' ' if ext == 'txt' else ',')\n    elif ext == 'xlsx':\n        raise ValueError('TODO: this is broke.')\n        import pandas as pd\n        with pd.ExcelWriter(filename, engine='openpyxl') as writer:\n            for key, value in d.items():\n                pd.DataFrame(value).to_excel(writer,key)", "idx": 1032}
{"project": "Scanpy", "commit_id": "639_scanpy_1.1_scanpy_tools_dpt.py_dpt.py", "target": 1, "func": "def dpt(adata, n_branchings=0, min_group_size=0.01,\n        allow_kendall_tau_shift=True, n_dcs=None, copy=False):\n    \"\"\"Infer progression of cells through geodesic distance along the graph [Haghverdi16]_ [Wolf17i]_.\n    Reconstruct the progression of a biological process from snapshot\n    data. `Diffusion Pseudotime` has been introduced by [Haghverdi16]_ and\n    implemented within Scanpy [Wolf17]_. Here, we use a further developed\n    version, which is able to deal with disconnected graphs [Wolf17i]_ and can\n    be run in a `hierarchical` mode by setting the parameter\n    `n_branchings>1`. We recommend, however, to only use\n    :func:`~scanpy.api.tl.dpt` for computing pseudotime (`n_branchings=0`) and\n    to detect branchings via :func:`~scanpy.api.paga`. For pseudotime, you need\n    to annotate your data with a root cell. For instance::\n        adata.uns['iroot'] = np.flatnonzero(adata.obs['cell_types'] == 'Stem')[0]\n    This requires to run :func:`~scanpy.api.pp.neighbors`, first. In order to\n    reproduce the original implementation of DPT, use `method=='gauss'` in\n    this. Using the default `method=='umap'` only leads to minor quantitative\n    differences, though.\n    .. versionadded:: 1.1\n    :func:`~scanpy.api.tl.dpt` also requires to run\n    :func:`~scanpy.api.tl.diffmap` first. As previously,\n    :func:`~scanpy.api.tl.dpt` came with a default parameter of ``n_dcs=10`` but\n    :func:`~scanpy.api.tl.diffmap` has a default parameter of ``n_comps=15``,\n    you need to pass ``n_comps=10`` in :func:`~scanpy.api.tl.diffmap` in order\n    to exactly reproduce previous :func:`~scanpy.api.tl.dpt` results.\n    Parameters\n    ----------\n    adata : :class:`~scanpy.api.AnnData`\n        Annotated data matrix.\n    n_branchings : `int`, optional (default: 0)\n        Number of branchings to detect.\n    min_group_size : [0, 1] or `float`, optional (default: 0.01)\n        During recursive splitting of branches ('dpt groups') for `n_branchings`\n        > 1, do not consider groups that contain less than `min_group_size` data\n        points. If a float, `min_group_size` refers to a fraction of the total\n        number of data points.\n    allow_kendall_tau_shift : `bool`, optional (default: `True`)\n        If a very small branch is detected upon splitting, shift away from\n        maximum correlation in Kendall tau criterion of [Haghverdi16]_ to\n        stabilize the splitting.\n    copy : `bool`, optional (default: `False`)\n        Copy instance before computation and return a copy. Otherwise, perform\n        computation inplace and return None.\n    Returns\n    -------\n    Depending on `copy`, returns or updates `adata` with the following fields.\n    If `n_branchings==0`, no field `dpt_groups` will be written.\n    dpt_pseudotime : `pd.Series` (`adata.obs`, dtype `float`)\n        Array of dim (number of samples) that stores the pseudotime of each\n        cell, that is, the DPT distance with respect to the root cell.\n    dpt_groups : `pd.Series` (`adata.obs`, dtype `category`)\n        Array of dim (number of samples) that stores the subgroup id ('0',\n        '1', ...) for each cell. The groups  typically correspond to\n        'progenitor cells', 'undecided cells' or 'branches' of a process.\n    Notes\n    -----\n    The tool is similar to the R package `destiny` of [Angerer16]_.\n    \"\"\"\n    # backwards compat error\n    if n_dcs is not None:\n        raise ValueError(\n            'Since version 1.1, `tl.dpt` no longer accepts a parameter `n_dcs`. '\n            'You can reproduce the previous behavior by setting the parameter '\n            '`n_comps` in a previous computation of `tl.diffmap`. \\n'\n            'However, note that `n_dcs` had the default parameter 10, whereas '\n            '`n_comps` has the default parameter 15.')\n    # backwards compat warning: someone uses the default of diffmap, it might be unkowingly\n    if 'X_diffmap' in adata.obsm.keys() and adata.obsm['X_diffmap'].shape[1] == 15:\n        logg.warn(\n            'Note that previously, `tl.dpt` had a default setting of `n_dcs=10`. '\n            'Right now, you\\'re running `tl.dpt` using 15 DCs. This is '\n            'the default of `tl.diffmap`, which is the better default for complex datasets. '\n            'You can influence this behavior using the parameter `n_comps` in `tl.diffmap`, '\n            'and obtain the previous default by setting `n_comps` to 10.')\n    # standard errors, warnings etc.\n    adata = adata.copy() if copy else adata\n    if 'neighbors' not in adata.uns:\n        raise ValueError(\n            'You need to run `pp.neighbors` and `tl.diffmap` first.')\n    if 'X_diffmap' not in adata.obsm.keys():\n        raise ValueError(\n            'You need to run `pp.diffmap` first.')\n    if 'iroot' not in adata.uns and 'xroot' not in adata.var:\n        logg.warn(\n            'No root cell found. To compute pseudotime, pass the index or '\n            'expression vector of a root cell, one of:\\n'\n            '    adata.uns[\\'iroot\\'] = root_cell_index\\n'\n            '    adata.var[\\'xroot\\'] = adata[root_cell_name, :].X')\n    # start with the actual computation\n    logg.info('computing Diffusion Pseudotime', r=True)\n    if n_branchings > 1: logg.info('    this uses a hierarchical implementation')\n    dpt = DPT(adata, min_group_size=min_group_size,\n              n_branchings=n_branchings,\n              allow_kendall_tau_shift=allow_kendall_tau_shift)\n    if dpt.iroot is not None:\n        dpt._set_pseudotime()  # pseudotimes are distances from root point\n        adata.uns['iroot'] = dpt.iroot  # update iroot, might have changed when subsampling, for example\n        adata.obs['dpt_pseudotime'] = dpt.pseudotime\n    # detect branchings and partition the data into segments\n    if n_branchings > 0:\n        dpt.branchings_segments()\n        adata.obs['dpt_groups'] = pd.Categorical(\n            values=dpt.segs_names.astype('U'),\n            categories=natsorted(np.array(dpt.segs_names_unique).astype('U')))\n        # the \"change points\" separate segments in the ordering above\n        adata.uns['dpt_changepoints'] = dpt.changepoints\n        # the tip points of segments\n        adata.uns['dpt_grouptips'] = dpt.segs_tips\n        # the ordering according to segments and pseudotime\n        ordering_id = np.zeros(adata.n_obs, dtype=int)\n        for count, idx in enumerate(dpt.indices): ordering_id[idx] = count\n        adata.obs['dpt_order'] = ordering_id\n        adata.obs['dpt_order_indices'] = dpt.indices\n    logg.info('    finished', time=True, end=' ' if settings.verbosity > 2 else '\\n')\n    logg.hint('added\\n'\n           + ('    \\'dpt_pseudotime\\', the pseudotime (adata.obs)'\n              if dpt.iroot is not None else '')\n           + ('\\n    \\'dpt_groups\\', the branching subgroups of dpt (adata.obs)\\n'\n              + '    \\'dpt_order\\', cell order (adata.obs)'\n              if n_branchings > 0 else ''))\n    return adata if copy else None", "idx": 1037}
{"project": "Scanpy", "commit_id": "939_scanpy_1.4.4_scanpy_plotting__anndata.py_ranking.py", "target": 1, "func": "def ranking(\n    adata: AnnData,\n    attr: str,\n    keys: Union[str, Sequence[str]],\n    dictionary=None,\n    indices=None,\n    labels=None,\n    color='black',\n    n_points=30,\n    log=False,\n    include_lowest=False,\n    show=None,\n):\n    \"\"\"\\\n    Plot rankings.\n    See, for example, how this is used in pl.pca_ranking.\n    Parameters\n    ----------\n    adata\n        The data.\n    attr: {`'var'`, `'obs'`, `'uns'`, `'varm'`, `'obsm'`}\n        The attribute of AnnData that contains the score.\n    keys\n        The scores to look up an array from the attribute of adata.\n    Returns\n    -------\n    Returns matplotlib gridspec with access to the axes.\n    \"\"\"\n    if isinstance(keys, str) and indices is not None:\n        scores = getattr(adata, attr)[keys][:, indices]\n        keys = ['{}{}'.format(keys[:-1], i+1) for i in indices]\n    else:\n        if dictionary is None:\n            scores = getattr(adata, attr)[keys]\n        else:\n            scores = getattr(adata, attr)[dictionary][keys]\n    n_panels = len(keys) if isinstance(keys, list) else 1\n    if n_panels == 1: scores, keys = scores[:, None], [keys]\n    if log: scores = np.log(scores)\n    if labels is None:\n        labels = adata.var_names if attr in {'var', 'varm'} else np.arange(scores.shape[0]).astype(str)\n    if isinstance(labels, str):\n        labels = [labels + str(i+1) for i in range(scores.shape[0])]\n    if n_panels <= 5: n_rows, n_cols = 1, n_panels\n    else: n_rows, n_cols = 2, int(n_panels/2 + 0.5)\n    fig = pl.figure(figsize=(\n        n_cols * rcParams['figure.figsize'][0],\n        n_rows * rcParams['figure.figsize'][1],\n    ))\n    left, bottom = 0.2/n_cols, 0.13/n_rows\n    gs = gridspec.GridSpec(\n        wspace=0.2,\n        nrows=n_rows, ncols=n_cols,\n        left=left, bottom=bottom,\n        right=1 - (n_cols-1)*left - 0.01/n_cols,\n        top=1 - (n_rows-1)*bottom - 0.1/n_rows,\n    )\n    for iscore, score in enumerate(scores.T):\n        pl.subplot(gs[iscore])\n        order_scores = np.argsort(score)[::-1]\n        if not include_lowest:\n            indices = order_scores[:n_points+1]\n        else:\n            indices = order_scores[:n_points//2]\n            neg_indices = order_scores[-(n_points-(n_points//2)):]\n        txt_args = dict(\n            color=color,\n            rotation='vertical',\n            verticalalignment='bottom',\n            horizontalalignment='center',\n            fontsize=8,\n        )\n        for ig, g in enumerate(indices):\n            pl.text(ig, score[g], labels[g], **txt_args)\n        if include_lowest:\n            score_mid = (score[g] + score[neg_indices[0]]) / 2\n            pl.text(len(indices), score_mid, '\u22ee', **txt_args)\n            for ig, g in enumerate(neg_indices):\n                pl.text(ig+len(indices)+2, score[g], labels[g], **txt_args)\n            pl.xticks([])\n        pl.title(keys[iscore].replace('_', ' '))\n        if n_panels <= 5 or iscore > n_cols: pl.xlabel('ranking')\n        pl.xlim(-0.9, n_points + 0.9 + (1 if include_lowest else 0))\n        score_min, score_max = np.min(score[neg_indices if include_lowest else indices]), np.max(score[indices])\n        pl.ylim(\n            (0.95 if score_min > 0 else 1.05) * score_min,\n            (1.05 if score_max > 0 else 0.95) * score_max,\n        )\n    if show == False: return gs", "idx": 1049}
{"project": "Scanpy", "commit_id": "148_scanpy_1.9.0_pl.py_harmony_timeseries.py", "target": 0, "func": "def harmony_timeseries(\n    adata, *, show: bool = True, return_fig: bool = False, **kwargs\n) -> Union[Axes, List[Axes], None]:\n    \"\"\"\\\n    Scatter plot in Harmony force-directed layout basis.\n\n    Parameters\n    ----------\n    {adata_color_etc}\n    {edges_arrows}\n    {scatter_bulk}\n    {show_save_ax}\n\n    Returns\n    -------\n    If `return_fig` is True, a :class:`~matplotlib.figure.Figure`.\n    If `show==False` a :class:`~matplotlib.axes.Axes` or a list of it.\n    \"\"\"\n\n    tp_name = adata.uns[\"harmony_timepoint_var\"]\n    tps = adata.obs[tp_name].unique()\n\n    fig, axes = plt.subplots(1, len(tps))\n    for i, tp in enumerate(tps):\n        p = embedding(\n            adata,\n            'harmony',\n            color=tp_name,\n            groups=tp,\n            title=tp,\n            show=False,\n            ax=axes[i],\n            legend_loc='none',\n        )\n        p.set_axis_off()\n    if return_fig:\n        return fig\n    elif not show:\n        return axes", "idx": 1058}
{"project": "Scanpy", "commit_id": "310_scanpy_1.9.0__utils.py__set_default_colors_for_categorical_obs.py", "target": 0, "func": "def _set_default_colors_for_categorical_obs(adata, value_to_plot):\n    \"\"\"\n    Sets the adata.uns[value_to_plot + '_colors'] using default color palettes\n\n    Parameters\n    ----------\n    adata\n        AnnData object\n    value_to_plot\n        Name of a valid categorical observation\n\n    Returns\n    -------\n    None\n    \"\"\"\n    categories = adata.obs[value_to_plot].cat.categories\n    length = len(categories)\n\n    # check if default matplotlib palette has enough colors\n    if len(rcParams['axes.prop_cycle'].by_key()['color']) >= length:\n        cc = rcParams['axes.prop_cycle']()\n        palette = [next(cc)['color'] for _ in range(length)]\n\n    else:\n        if length <= 20:\n            palette = palettes.default_20\n        elif length <= 28:\n            palette = palettes.default_28\n        elif length <= len(palettes.default_102):  # 103 colors\n            palette = palettes.default_102\n        else:\n            palette = ['grey' for _ in range(length)]\n            logg.info(\n                f'the obs value {value_to_plot!r} has more than 103 categories. Uniform '\n                \"'grey' color will be used for all categories.\"\n            )\n\n    _set_colors_for_categorical_obs(adata, value_to_plot, palette[:length])", "idx": 1065}
{"project": "Scanpy", "commit_id": "348_scanpy_1.9.0_scatterplots.py__get_vboundnorm.py", "target": 0, "func": "def _get_vboundnorm(\n    vmin: Sequence[VBound],\n    vmax: Sequence[VBound],\n    vcenter: Sequence[VBound],\n    norm: Sequence[Normalize],\n    index: int,\n    color_vector: Sequence[float],\n) -> Tuple[Union[float, None], Union[float, None]]:\n    \"\"\"\n    Evaluates the value of vmin, vmax and vcenter, which could be a\n    str in which case is interpreted as a percentile and should\n    be specified in the form 'pN' where N is the percentile.\n    Eg. for a percentile of 85 the format would be 'p85'.\n    Floats are accepted as p99.9\n\n    Alternatively, vmin/vmax could be a function that is applied to\n    the list of color values (`color_vector`).  E.g.\n\n    def my_vmax(color_vector): np.percentile(color_vector, p=80)\n\n\n    Parameters\n    ----------\n    index\n        This index of the plot\n    color_vector\n        List or values for the plot\n\n    Returns\n    -------\n\n    (vmin, vmax, vcenter, norm) containing None or float values for\n    vmin, vmax, vcenter and matplotlib.colors.Normalize  or None for norm.\n\n    \"\"\"\n    out = []\n    for v_name, v in [('vmin', vmin), ('vmax', vmax), ('vcenter', vcenter)]:\n        if len(v) == 1:\n            # this case usually happens when the user sets eg vmax=0.9, which\n            # is internally converted into list of len=1, but is expected that this\n            # value applies to all plots.\n            v_value = v[0]\n        else:\n            try:\n                v_value = v[index]\n            except IndexError:\n                logg.error(\n                    f\"The parameter {v_name} is not valid. If setting multiple {v_name} values,\"\n                    f\"check that the length of the {v_name} list is equal to the number \"\n                    \"of plots. \"\n                )\n                v_value = None\n\n        if v_value is not None:\n            if isinstance(v_value, str) and v_value.startswith('p'):\n                try:\n                    float(v_value[1:])\n                except ValueError:\n                    logg.error(\n                        f\"The parameter {v_name}={v_value} for plot number {index + 1} is not valid. \"\n                        f\"Please check the correct format for percentiles.\"\n                    )\n                # interpret value of vmin/vmax as quantile with the following syntax 'p99.9'\n                v_value = np.nanpercentile(color_vector, q=float(v_value[1:]))\n            elif callable(v_value):\n                # interpret vmin/vmax as function\n                v_value = v_value(color_vector)\n                if not isinstance(v_value, float):\n                    logg.error(\n                        f\"The return of the function given for {v_name} is not valid. \"\n                        \"Please check that the function returns a number.\"\n                    )\n                    v_value = None\n            else:\n                try:\n                    float(v_value)\n                except ValueError:\n                    logg.error(\n                        f\"The given {v_name}={v_value} for plot number {index + 1} is not valid. \"\n                        f\"Please check that the value given is a valid number, a string \"\n                        f\"starting with 'p' for percentiles or a valid function.\"\n                    )\n                    v_value = None\n        out.append(v_value)\n    out.append(norm[0] if len(norm) == 1 else norm[index])\n    return tuple(out)", "idx": 1081}
{"project": "Scanpy", "commit_id": "597_scanpy_1.9.0_test_neighbors.py_test_restore_n_neighbors.py", "target": 0, "func": "def test_restore_n_neighbors(neigh, conv):\n    neigh.compute_neighbors(method='gauss', n_neighbors=n_neighbors)\n\n    ad = AnnData(np.array(X))\n    # Allow deprecated usage for now\n    with warnings.catch_warnings():\n        warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"anndata\")\n        ad.uns['neighbors'] = dict(connectivities=conv(neigh.connectivities))\n    neigh_restored = Neighbors(ad)\n    assert neigh_restored.n_neighbors == 1", "idx": 1083}
{"project": "Scanpy", "commit_id": "280_scanpy_0.1_scanpy_tools_ega.py_detect_splits.py", "target": 1, "func": "def detect_splits(self):\n    \"\"\"Detect all splits up to `n_splits`.\n    Writes Attributes\n    -----------------\n    segs : np.ndarray\n        List of integer index arrays.\n    segs_tips : np.ndarray\n        List of indices of the tips of segments.\n    \"\"\"\n    logg.info('... detect', self.n_splits,\n              'branching' + ('' if self.n_splits == 1 else 's'))\n    indices_all = np.arange(self.X.shape[0], dtype=int)\n    segs = [indices_all]\n    if False:  # this is safe, but not compatible with on-the-fly computation\n        tips_all = np.array(np.unravel_index(np.argmax(self.Dchosen), self.Dchosen.shape))\n    else:\n        if self.iroot is not None:\n            tip_0 = np.argmax(self.Dchosen[self.iroot])\n        else:\n            tip_0 = np.argmax(self.Dchosen[0])\n        tips_all = np.array([tip_0, np.argmax(self.Dchosen[tip_0])])\n    # we keep a list of the tips of each segment\n    segs_tips = [tips_all]\n    segs_connects = [[]]\n    segs_undecided = [True]\n    segs_adjacency = [[]]\n    logg.info('... do not consider groups with less than {} points for splitting'\n              .format(self.min_group_size))\n    for ibranch in range(self.n_splits):\n        iseg, tips3 = self.select_segment(segs, segs_tips, segs_undecided)\n        if iseg == -1:\n            logg.info('... partitioning converged')\n            break\n        logg.info('... branching {}:'.format(ibranch + 1),\n                  'split group', iseg)  # [third start end]\n        # detect branching and update segs and segs_tips\n        self.detect_branching(segs, segs_tips,\n                              segs_connects,\n                              segs_undecided,\n                              segs_adjacency, iseg, tips3)\n    # store as class members\n    self.segs = segs\n    self.segs_tips = segs_tips\n    self.segs_undecided = segs_undecided\n    # the following is a bit too much, but this allows easy storage\n    self.segs_adjacency = sp.sparse.lil_matrix((len(segs), len(segs)), dtype=float)\n    self.segs_connects = sp.sparse.lil_matrix((len(segs), len(segs)), dtype=int)\n    for i, seg_adjacency in enumerate(segs_adjacency):\n        self.segs_connects[i, seg_adjacency] = segs_connects[i]\n    for i in range(len(segs)):\n        for j in range(len(segs)):\n            self.segs_adjacency[i, j] = self.Dchosen[self.segs_connects[i, j],\n            self.segs_connects[j, i]]\n    self.segs_adjacency = self.segs_adjacency.tocsr()\n    self.segs_connects = self.segs_connects.tocsr()\n    # print(self.segs_adjacency)\n    # print([len(s) for s in self.segs])\n    if 'uncontract' not in self.flavor:\n        self.contract_segments()\n    # print(self.segs_adjacency)\n    # print([len(s) for s in self.segs])\n    # self.check_adjacency()\n    # print('new')\n    # print(self.segs_adjacency)", "idx": 1094}
{"project": "Scanpy", "commit_id": "325_scanpy_1.9.0__utils.py_get_ax_size.py", "target": 0, "func": "def get_ax_size(ax: Axes, fig: Figure):\n    \"\"\"Get axis size\n\n    Parameters\n    ----------\n    ax\n        Axis object from matplotlib.\n    fig\n        Figure.\n    \"\"\"\n    bbox = ax.get_window_extent().transformed(fig.dpi_scale_trans.inverted())\n    width, height = bbox.width, bbox.height\n    width *= fig.dpi\n    height *= fig.dpi", "idx": 1096}
{"project": "Scanpy", "commit_id": "533_scanpy_1.9.0_test_embedding_plots.py_test_manual_equivalency_no_img.py", "target": 0, "func": "def test_manual_equivalency_no_img(\n    equivalent_spatial_plotters_no_img, tmpdir, spatial_kwargs\n):\n    if \"bw\" in spatial_kwargs:\n        # Has no meaning when there is no image\n        pytest.skip()\n    orig, removed = equivalent_spatial_plotters_no_img\n\n    TESTDIR = Path(tmpdir)\n    orig_pth = TESTDIR / \"orig.png\"\n    removed_pth = TESTDIR / \"removed.png\"\n\n    orig(**spatial_kwargs)\n    plt.savefig(orig_pth, dpi=40)\n    plt.close()\n    removed(**spatial_kwargs)\n    plt.savefig(removed_pth, dpi=40)\n    plt.close()\n\n    check_images(orig_pth, removed_pth, tol=1)", "idx": 1097}
{"project": "Scanpy", "commit_id": "107_scanpy_1.9.0__datasets.py_moignard15.py", "target": 0, "func": "def moignard15() -> ad.AnnData:\n    \"\"\"\\\n    Hematopoiesis in early mouse embryos [Moignard15]_.\n\n    Returns\n    -------\n    Annotated data matrix.\n    \"\"\"\n    filename = settings.datasetdir / 'moignard15/nbt.3154-S3.xlsx'\n    backup_url = 'https://static-content.springer.com/esm/art%3A10.1038%2Fnbt.3154/MediaObjects/41587_2015_BFnbt3154_MOESM4_ESM.xlsx'\n    adata = read(filename, sheet='dCt_values.txt', backup_url=backup_url)\n    # filter out 4 genes as in Haghverdi et al. (2016)\n    gene_subset = ~np.in1d(adata.var_names, ['Eif2b1', 'Mrpl19', 'Polr2a', 'Ubc'])\n    adata = adata[:, gene_subset].copy()  # retain non-removed genes\n    # choose root cell for DPT analysis as in Haghverdi et al. (2016)\n    adata.uns[\"iroot\"] = 532  # note that in Matlab/R, counting starts at 1\n    # annotate with Moignard et al. (2015) experimental cell groups\n    groups = {\n        'HF': '#D7A83E',\n        'NP': '#7AAE5D',\n        'PS': '#497ABC',\n        '4SG': '#AF353A',\n        '4SFG': '#765099',\n    }\n    # annotate each observation/cell\n    adata.obs['exp_groups'] = [\n        next(gname for gname in groups.keys() if sname.startswith(gname))\n        for sname in adata.obs_names\n    ]\n    # fix the order and colors of names in \"groups\"\n    adata.obs['exp_groups'] = pd.Categorical(\n        adata.obs['exp_groups'], categories=list(groups.keys())\n    )\n    adata.uns['exp_groups_colors'] = list(groups.values())\n    return adata", "idx": 1108}
{"project": "Scanpy", "commit_id": "675_scanpy_1.2.2_scanpy_plotting_tools___init__.py_rank_genes_groups.py", "target": 1, "func": "def rank_genes_groups(adata, groups=None, n_genes=20, gene_symbols=None, key=None, fontsize=8,\n                      n_panels_per_row=4, show=None, save=None, ax=None):\n    \"\"\"\\\n    Plot ranking of genes.\n    Parameters\n    ----------\n    adata : :class:`~anndata.AnnData`\n        Annotated data matrix.\n    groups : `str` or `list` of `str`\n        The groups for which to show the gene ranking.\n    gene_symbols : `str`\n        Key for field in `.var` that stores gene symbols if you do not want to\n        use `.var_names`.\n    n_genes : `int`, optional (default: 20)\n        Number of genes to show.\n    fontsize : `int`, optional (default: 8)\n        Fontsize for gene names.\n    n_panels_per_row: `int`, optional (default: 4)\n        Number of panels shown per row.\n    {show_save_ax}\n    \"\"\"\n    if key is None:\n        key = 'rank_genes_groups'\n    groups_key = str(adata.uns[key]['params']['groupby'])\n    reference = str(adata.uns[key]['params']['reference'])\n    group_names = (adata.uns[key]['names'].dtype.names\n                   if groups is None else groups)\n    # one panel for each group\n    n_panels = len(group_names)\n    # set up the figure\n    n_panels_x = n_panels_per_row\n    n_panels_y = np.ceil(len(group_names) / n_panels_x).astype(int)\n    from matplotlib import gridspec\n    fig = pl.figure(figsize=(n_panels_x * rcParams['figure.figsize'][0],\n                             n_panels_y * rcParams['figure.figsize'][1]))\n    left = 0.2/n_panels_x\n    bottom = 0.13/n_panels_y\n    gs = gridspec.GridSpec(nrows=n_panels_y,\n                           ncols=n_panels_x,\n                           left=left,\n                           right=1-(n_panels_x-1)*left-0.01/n_panels_x,\n                           bottom=bottom,\n                           top=1-(n_panels_y-1)*bottom-0.1/n_panels_y,\n                           wspace=0.18)\n    for count, group_name in enumerate(group_names):\n        pl.subplot(gs[count])\n        gene_names = adata.uns[key]['names'][group_name]\n        scores = adata.uns[key]['scores'][group_name]\n        for ig, g in enumerate(gene_names[:n_genes]):\n            gene_name = gene_names[ig]\n            if adata.raw is not None and adata.uns[key]['params']['use_raw']:\n                pl.text(\n                    ig, scores[ig],\n                    gene_name if gene_symbols is None else adata.raw.var[gene_symbols][gene_name],\n                    rotation='vertical', verticalalignment='bottom',\n                    horizontalalignment='center', fontsize=fontsize)\n            else:\n                pl.text(\n                    ig, scores[ig],\n                    gene_name if gene_symbols is None else adata.var[gene_symbols][gene_name],\n                    rotation='vertical', verticalalignment='bottom',\n                    horizontalalignment='center', fontsize=fontsize)\n        pl.title('{} vs. {}'.format(group_name, reference))\n        if n_panels <= 5 or count >= n_panels_x: pl.xlabel('ranking')\n        if count == 0 or count == n_panels_x: pl.ylabel('score')\n        ymin = np.min(scores)\n        ymax = np.max(scores)\n        ymax += 0.3*(ymax-ymin)\n        pl.ylim([ymin, ymax])\n        pl.xlim(-0.9, ig+1-0.1)\n    writekey = ('rank_genes_groups_'\n                + str(adata.uns[key]['params']['groupby']))\n    utils.savefig_or_show(writekey, show=show, save=save)", "idx": 1115}
{"project": "Scanpy", "commit_id": "112_scanpy_1.9.0__datasets.py_pbmc3k_processed.py", "target": 0, "func": "def pbmc3k_processed() -> ad.AnnData:\n    \"\"\"Processed 3k PBMCs from 10x Genomics.\n\n    Processed using the `basic tutorial <https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html>`__.\n\n    Returns\n    -------\n    Annotated data matrix.\n    \"\"\"\n    with warnings.catch_warnings():\n        warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"anndata\")\n        return read(\n            settings.datasetdir / 'pbmc3k_processed.h5ad',\n            backup_url='https://raw.githubusercontent.com/chanzuckerberg/cellxgene/main/example-dataset/pbmc3k.h5ad',", "idx": 1117}
{"project": "Scanpy", "commit_id": "196_scanpy_0.0_scanpy_classes_ann_data.py___getitem__.py", "target": 1, "func": "def __getitem__(self, k):\n    \"\"\"Either a single one- or multi-column or mulitiple one-colum items.\"\"\"\n    import warnings  # ignore FutureWarning about multi-column access of structured arrays\n    with warnings.catch_warnings():\n        warnings.simplefilter('ignore')\n        try:\n            view = super(BoundStructArray, self).__getitem__(k).view(np.ndarray)\n        except ValueError:  # access multiple columns with key k\n            keys = self._keys_multicol_lookup[k]\n            view = super(BoundStructArray, self).__getitem__(keys).view(\n                self.dtype[keys[0]]).reshape(\n                self.shape + (len(self._keys_multicol_lookup[k]),))\n        if view.dtype.char == 'S':\n            return view.astype('U')\n        return view", "idx": 1123}
{"project": "Scanpy", "commit_id": "30_scanpy_1.9.0_cli.py___iter__.py", "target": 0, "func": "def __iter__(self) -> Generator[str, None, None]:\n        yield from self.parser_map\n        yield from self.commands", "idx": 1126}
{"project": "Scanpy", "commit_id": "341_scanpy_1.9.0_paga.py__paga_graph.py", "target": 0, "func": "def _paga_graph(\n    adata,\n    ax,\n    solid_edges=None,\n    dashed_edges=None,\n    adjacency_solid=None,\n    adjacency_dashed=None,\n    transitions=None,\n    threshold=None,\n    root=0,\n    colors=None,\n    labels=None,\n    fontsize=None,\n    fontweight=None,\n    fontoutline=None,\n    text_kwds: Mapping[str, Any] = MappingProxyType({}),\n    node_size_scale=1.0,\n    node_size_power=0.5,\n    edge_width_scale=1.0,\n    normalize_to_color='reference',\n    title=None,\n    pos=None,\n    cmap=None,\n    frameon=True,\n    min_edge_width=None,\n    max_edge_width=None,\n    export_to_gexf=False,\n    colorbar=None,\n    use_raw=True,\n    cb_kwds: Mapping[str, Any] = MappingProxyType({}),\n    single_component=False,\n    arrowsize=30,\n):\n    import networkx as nx\n\n    node_labels = labels  # rename for clarity\n    if (\n        node_labels is not None\n        and isinstance(node_labels, str)\n        and node_labels != adata.uns['paga']['groups']\n    ):\n        raise ValueError(\n            'Provide a list of group labels for the PAGA groups {}, not {}.'.format(\n                adata.uns['paga']['groups'], node_labels\n            )\n        )\n    groups_key = adata.uns['paga']['groups']\n    if node_labels is None:\n        node_labels = adata.obs[groups_key].cat.categories\n\n    if (colors is None or colors == groups_key) and groups_key is not None:\n        if groups_key + '_colors' not in adata.uns or len(\n            adata.obs[groups_key].cat.categories\n        ) != len(adata.uns[groups_key + '_colors']):\n            _utils.add_colors_for_categorical_sample_annotation(adata, groups_key)\n        colors = adata.uns[groups_key + '_colors']\n        for iname, name in enumerate(adata.obs[groups_key].cat.categories):\n            if name in settings.categories_to_ignore:\n                colors[iname] = 'grey'\n\n    nx_g_solid = nx.Graph(adjacency_solid)\n    if dashed_edges is not None:\n        nx_g_dashed = nx.Graph(adjacency_dashed)\n\n    # convert pos to array and dict\n    if not isinstance(pos, (Path, str)):\n        pos_array = pos\n    else:\n        pos = Path(pos)\n        if pos.suffix != '.gdf':\n            raise ValueError(\n                'Currently only supporting reading positions from .gdf files. '\n                'Consider generating them using, for instance, Gephi.'\n            )\n        s = ''  # read the node definition from the file\n        with pos.open() as f:\n            f.readline()\n            for line in f:\n                if line.startswith('edgedef>'):\n                    break\n                s += line\n        from io import StringIO\n\n        df = pd.read_csv(StringIO(s), header=-1)\n        pos_array = df[[4, 5]].values\n\n    # convert to dictionary\n    pos = {n: [p[0], p[1]] for n, p in enumerate(pos_array)}\n\n    # uniform color\n    if isinstance(colors, str) and is_color_like(colors):\n        colors = [colors for c in range(len(node_labels))]\n\n    # color degree of the graph\n    if isinstance(colors, str) and colors.startswith('degree'):\n        # see also tools.paga.paga_degrees\n        if colors == 'degree_dashed':\n            colors = [d for _, d in nx_g_dashed.degree(weight='weight')]\n        elif colors == 'degree_solid':\n            colors = [d for _, d in nx_g_solid.degree(weight='weight')]\n        else:\n            raise ValueError('`degree` either \"degree_dashed\" or \"degree_solid\".')\n        colors = (np.array(colors) - np.min(colors)) / (np.max(colors) - np.min(colors))\n\n    # plot gene expression\n    var_names = adata.var_names if adata.raw is None else adata.raw.var_names\n    if isinstance(colors, str) and colors in var_names:\n        x_color = []\n        cats = adata.obs[groups_key].cat.categories\n        for icat, cat in enumerate(cats):\n            subset = (cat == adata.obs[groups_key]).values\n            if adata.raw is not None and use_raw:\n                adata_gene = adata.raw[:, colors]\n            else:\n                adata_gene = adata[:, colors]\n            x_color.append(np.mean(adata_gene.X[subset]))\n        colors = x_color\n\n    # plot continuous annotation\n    if (\n        isinstance(colors, str)\n        and colors in adata.obs\n        and not is_categorical_dtype(adata.obs[colors])\n    ):\n        x_color = []\n        cats = adata.obs[groups_key].cat.categories\n        for icat, cat in enumerate(cats):\n            subset = (cat == adata.obs[groups_key]).values\n            x_color.append(adata.obs.loc[subset, colors].mean())\n        colors = x_color\n\n    # plot categorical annotation\n    if (\n        isinstance(colors, str)\n        and colors in adata.obs\n        and is_categorical_dtype(adata.obs[colors])\n    ):\n        asso_names, asso_matrix = _sc_utils.compute_association_matrix_of_groups(\n            adata,\n            prediction=groups_key,\n            reference=colors,\n            normalization='reference' if normalize_to_color else 'prediction',\n        )\n        _utils.add_colors_for_categorical_sample_annotation(adata, colors)\n        asso_colors = _sc_utils.get_associated_colors_of_groups(\n            adata.uns[colors + '_colors'], asso_matrix\n        )\n        colors = asso_colors\n\n    if len(colors) != len(node_labels):\n        raise ValueError(\n            f'Expected `colors` to be of length `{len(node_labels)}`, '\n            f'found `{len(colors)}`.'\n        )\n\n    # count number of connected components\n    n_components, labels = scipy.sparse.csgraph.connected_components(adjacency_solid)\n    if n_components > 1 and not single_component:\n        logg.debug(\n            'Graph has more than a single connected component. '\n            'To restrict to this component, pass `single_component=True`.'\n        )\n    if n_components > 1 and single_component:\n        component_sizes = np.bincount(labels)\n        largest_component = np.where(component_sizes == component_sizes.max())[0][0]\n        adjacency_solid = adjacency_solid.tocsr()[labels == largest_component, :]\n        adjacency_solid = adjacency_solid.tocsc()[:, labels == largest_component]\n        colors = np.array(colors)[labels == largest_component]\n        node_labels = np.array(node_labels)[labels == largest_component]\n        cats_dropped = (\n            adata.obs[groups_key].cat.categories[labels != largest_component].tolist()\n        )\n        logg.info(\n            'Restricting graph to largest connected component by dropping categories\\n'\n            f'{cats_dropped}'\n        )\n        nx_g_solid = nx.Graph(adjacency_solid)\n        if dashed_edges is not None:\n            raise ValueError('`single_component` only if `dashed_edges` is `None`.')\n\n    # edge widths\n    base_edge_width = edge_width_scale * 5 * rcParams['lines.linewidth']\n\n    # draw dashed edges\n    if dashed_edges is not None:\n        widths = [x[-1]['weight'] for x in nx_g_dashed.edges(data=True)]\n        widths = base_edge_width * np.array(widths)\n        if max_edge_width is not None:\n            widths = np.clip(widths, None, max_edge_width)\n        nx.draw_networkx_edges(\n            nx_g_dashed,\n            pos,\n            ax=ax,\n            width=widths,\n            edge_color='grey',\n            style='dashed',\n            alpha=0.5,\n        )\n\n    # draw solid edges\n    if transitions is None:\n        widths = [x[-1]['weight'] for x in nx_g_solid.edges(data=True)]\n        widths = base_edge_width * np.array(widths)\n        if min_edge_width is not None or max_edge_width is not None:\n            widths = np.clip(widths, min_edge_width, max_edge_width)\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\")\n            nx.draw_networkx_edges(\n                nx_g_solid, pos, ax=ax, width=widths, edge_color='black'\n            )\n    # draw directed edges\n    else:\n        adjacency_transitions = adata.uns['paga'][transitions].copy()\n        if threshold is None:\n            threshold = 0.01\n        adjacency_transitions.data[adjacency_transitions.data < threshold] = 0\n        adjacency_transitions.eliminate_zeros()\n        g_dir = nx.DiGraph(adjacency_transitions.T)\n        widths = [x[-1]['weight'] for x in g_dir.edges(data=True)]\n        widths = base_edge_width * np.array(widths)\n        if min_edge_width is not None or max_edge_width is not None:\n            widths = np.clip(widths, min_edge_width, max_edge_width)\n        nx.draw_networkx_edges(\n            g_dir, pos, ax=ax, width=widths, edge_color='black', arrowsize=arrowsize\n        )\n\n    if export_to_gexf:\n        if isinstance(colors[0], tuple):\n            from matplotlib.colors import rgb2hex\n\n            colors = [rgb2hex(c) for c in colors]\n        for count, n in enumerate(nx_g_solid.nodes()):\n            nx_g_solid.node[count]['label'] = str(node_labels[count])\n            nx_g_solid.node[count]['color'] = str(colors[count])\n            nx_g_solid.node[count]['viz'] = dict(\n                position=dict(\n                    x=1000 * pos[count][0],\n                    y=1000 * pos[count][1],\n                    z=0,\n                )\n            )\n        filename = settings.writedir / 'paga_graph.gexf'\n        logg.warning(f'exporting to {filename}')\n        settings.writedir.mkdir(parents=True, exist_ok=True)\n        nx.write_gexf(nx_g_solid, settings.writedir / 'paga_graph.gexf')\n\n    ax.set_frame_on(frameon)\n    ax.set_xticks([])\n    ax.set_yticks([])\n\n    # groups sizes\n    if groups_key is not None and groups_key + '_sizes' in adata.uns:\n        groups_sizes = adata.uns[groups_key + '_sizes']\n    else:\n        groups_sizes = np.ones(len(node_labels))\n    base_scale_scatter = 2000\n    base_pie_size = (\n        base_scale_scatter / (np.sqrt(adjacency_solid.shape[0]) + 10) * node_size_scale\n    )\n    median_group_size = np.median(groups_sizes)\n    groups_sizes = base_pie_size * np.power(\n        groups_sizes / median_group_size, node_size_power\n    )\n\n    if fontsize is None:\n        fontsize = rcParams['legend.fontsize']\n    if fontoutline is not None:\n        text_kwds = dict(text_kwds)\n        text_kwds['path_effects'] = [\n            patheffects.withStroke(linewidth=fontoutline, foreground='w')\n        ]\n    # usual scatter plot\n    if not isinstance(colors[0], cabc.Mapping):\n        n_groups = len(pos_array)\n        sct = ax.scatter(\n            pos_array[:, 0],\n            pos_array[:, 1],\n            c=colors[:n_groups],\n            edgecolors='face',\n            s=groups_sizes,\n            cmap=cmap,\n        )\n        for count, group in enumerate(node_labels):\n            ax.text(\n                pos_array[count, 0],\n                pos_array[count, 1],\n                group,\n                verticalalignment='center',\n                horizontalalignment='center',\n                size=fontsize,\n                fontweight=fontweight,\n                **text_kwds,\n            )\n    # else pie chart plot\n    else:\n        for ix, (xx, yy) in enumerate(zip(pos_array[:, 0], pos_array[:, 1])):\n            if not isinstance(colors[ix], cabc.Mapping):\n                raise ValueError(\n                    f'{colors[ix]} is neither a dict of valid '\n                    'matplotlib colors nor a valid matplotlib color.'\n                )\n            color_single = colors[ix].keys()\n            fracs = [colors[ix][c] for c in color_single]\n            total = sum(fracs)\n\n            if total < 1:\n                color_single = list(color_single)\n                color_single.append('grey')\n                fracs.append(1 - sum(fracs))\n            elif not np.isclose(total, 1):\n                raise ValueError(\n                    f'Expected fractions for node `{ix}` to be '\n                    f'close to 1, found `{total}`.'\n                )\n\n            cumsum = np.cumsum(fracs)\n            cumsum = cumsum / cumsum[-1]\n            cumsum = [0] + cumsum.tolist()\n\n            for r1, r2, color in zip(cumsum[:-1], cumsum[1:], color_single):\n                angles = np.linspace(2 * np.pi * r1, 2 * np.pi * r2, 20)\n                x = [0] + np.cos(angles).tolist()\n                y = [0] + np.sin(angles).tolist()\n\n                xy = np.column_stack([x, y])\n                s = np.abs(xy).max()\n\n                sct = ax.scatter(\n                    [xx], [yy], marker=xy, s=s**2 * groups_sizes[ix], color=color\n                )\n\n            if node_labels is not None:\n                ax.text(\n                    xx,\n                    yy,\n                    node_labels[ix],\n                    verticalalignment='center',\n                    horizontalalignment='center',\n                    size=fontsize,\n                    fontweight=fontweight,\n                    **text_kwds,\n                )\n\n    return sct", "idx": 1133}
{"project": "Scanpy", "commit_id": "51_scanpy_1.9.0_logging.py_format.py", "target": 0, "func": "def format(self, record: logging.LogRecord):\n        format_orig = self._style._fmt\n        if record.levelno == INFO:\n            self._style._fmt = '{message}'\n        elif record.levelno == HINT:\n            self._style._fmt = '--> {message}'\n        elif record.levelno == DEBUG:\n            self._style._fmt = '    {message}'\n        if record.time_passed:\n            # strip microseconds\n            if record.time_passed.microseconds:\n                record.time_passed = timedelta(\n                    seconds=int(record.time_passed.total_seconds())\n                )\n            if '{time_passed}' in record.msg:\n                record.msg = record.msg.replace(\n                    '{time_passed}', str(record.time_passed)\n                )\n            else:\n                self._style._fmt += ' ({time_passed})'\n        if record.deep:\n            record.msg = f'{record.msg}: {record.deep}'\n        result = logging.Formatter.format(self, record)\n        self._style._fmt = format_orig\n        return result", "idx": 1154}
{"project": "Scanpy", "commit_id": "146_scanpy_0.0_scanpy_plotting.py__scatter_base.py", "target": 1, "func": "def _scatter_base(Ys,\n            c='blue',\n            highlights=[],\n            highlights_labels=[],\n            title='',\n            right_margin=None,\n            layout='2d',\n            titles=None,\n            component_name='DC',\n            component_indexnames=[1, 2, 3],\n            axlabels=None,\n            colorbars=[False],\n            **kwargs):\n    \"\"\"\n    Plot scatter plot of data.\n    Parameters\n    ----------\n    Ys : np.ndarray or list of np.ndarray\n        Single data array or list of data arrays. Rows store observations,\n        columns store variables. For example X, or phi or [phi,psi]. Arrays must\n        be of dimension ndim=2.\n    layout : str\n        Choose from '2d', '3d' and 'unfolded 3d', default '2d'.\n    comps : iterable\n        Iterable that stores the component indices.\n    Returns\n    -------\n    axs : matplotlib.axis or list of matplotlib.axis\n        Depending on whether supplying a single array or a list of arrays,\n        return a single axis or a list of axes.\n    \"\"\"\n    # if we have a single array, transform it into a list with a single array\n    avail_layouts = ['2d', '3d', 'unfolded 3d']\n    if layout not in avail_layouts:\n        raise ValueError('choose layout from',avail_layouts)\n    colors = c\n    if type(Ys) == np.ndarray:\n        Ys = [Ys]\n    if len(colors) == len(Ys[0]) or type(colors) == str:\n        colors = [colors]\n    # make a figure with panels len(colors) x len(Ys)\n    figsize = (4*len(colors), 4*len(Ys))\n    # checks\n    if layout == 'unfolded 3d':\n        if len(Ys) != 1:\n            raise ValueError('use single 3d array')\n        if len(colors) > 1:\n            raise ValueError('choose a single color')\n        figsize = (4*2, 4*2)\n        Y = Ys[0]\n        Ys = [Y[:,[1,2]], Y[:,[0,1]], Y, Y[:,[0,2]]]\n    # try importing Axes3D\n    if '3d' in layout:\n        from mpl_toolkits.mplot3d import Axes3D\n    fig = pl.figure(figsize=figsize,\n                    subplotpars=sppars(left=0, right=1, bottom=0.08))\n    from matplotlib import gridspec\n    # grid of axes for plotting and legends/colorbars\n    if np.any(colorbars) and right_margin is None:\n        right_margin = 0.25\n    elif right_margin is None:\n        right_margin = 0.01\n    gs = gridspec.GridSpec(nrows=len(Ys),\n                           ncols=2*len(colors),\n                           width_ratios=[r for i in range(len(colors))\n                                         for r in [1-right_margin, right_margin]],\n                           left=0.08/len(colors),\n                           right=1-(len(colors)-1)*0.08/len(colors),\n                           wspace=0)\n    fig.suptitle(title)\n    count = 1\n    bool3d = True if layout == '3d' else False\n    axs = []\n    for Y in Ys:\n        for icolor, color in enumerate(colors):\n            # set up panel\n            if layout == 'unfolded 3d' and count != 3:\n                ax = fig.add_subplot(2, 2, count)\n                bool3d = False\n            elif layout == 'unfolded 3d' and count == 3:\n                ax = fig.add_subplot(2, 2, count,\n                                     projection='3d')\n                bool3d = True\n            elif layout == '2d':\n                ax = pl.subplot(gs[2*(count-1)])\n            elif layout == '3d':\n                ax = pl.subplot(gs[2*(count-1)], projection='3d')\n            if not bool3d:\n                data = Y[:,0], Y[:,1]\n            else:\n                data = Y[:,0], Y[:,1], Y[:,2]\n            # do the plotting\n            if type(color) != str or color != 'white':\n                sct = ax.scatter(*data,\n                                 c=color,\n                                 edgecolors='face',\n                                 **kwargs)\n            if colorbars[icolor]:\n                pos = gs.get_grid_positions(fig)\n                left = pos[2][2*(count-1)+1]\n                bottom = pos[0][0]\n                width = 0.2*(pos[3][2*(count-1)+1] - left)\n                height = pos[1][0] - bottom\n                # again shift to left\n                left = pos[3][2*(count-1)] + 0.1*width\n                rectangle = [left, bottom, width, height]\n                ax_cb = fig.add_axes(rectangle)\n                cb = pl.colorbar(sct, format=ticker.FuncFormatter(ticks_formatter),\n                                 cax=ax_cb)\n            # set the titles\n            if titles is not None:\n                ax.set_title(titles[icolor])\n            # output highlighted data points\n            for iihighlight,ihighlight in enumerate(highlights):\n                data = [Y[ihighlight,0]], [Y[ihighlight,1]]\n                if bool3d:\n                    data = [Y[ihighlight,0]], [Y[ihighlight,1]], [Y[ihighlight,2]]\n                ax.scatter(*data, c='black',\n                           facecolors='black', edgecolors='black',\n                           marker='x', s=40, zorder=20)\n                highlight = (highlights_labels[iihighlight] if\n                             len(highlights_labels) > 0\n                             else str(ihighlight))\n                # the following is a Python 2 compatibility hack\n                ax.text(*([d[0] for d in data]+[highlight]),zorder=20)\n            ax.set_xticks([]); ax.set_yticks([])\n            if bool3d:\n                ax.set_zticks([])\n            axs.append(ax)\n            count += 1\n    # set default axlabels\n    if axlabels is None:\n        if layout == '2d':\n            axlabels = [[component_name + str(i) for i in idcs]\n                         for idcs in\n                         [component_indexnames for iax in range(len(axs))]]\n        elif layout == '3d':\n            axlabels = [[component_name + str(i) for i in idcs]\n                         for idcs in\n                         [component_indexnames for iax in range(len(axs))]]\n        elif layout == 'unfolded 3d':\n            axlabels = [[component_name\n                         + str(component_indexnames[i-1]) for i in idcs]\n                         for idcs in [[2, 3], [1, 2], [1, 2, 3], [1, 3]]]\n    # set axlabels\n    bool3d = True if layout == '3d' else False\n    for iax,ax in enumerate(axs):\n        if layout == 'unfolded 3d' and iax != 2:\n            bool3d = False\n        elif layout == 'unfolded 3d' and iax == 2:\n            bool3d = True\n        if axlabels is not None:\n            ax.set_xlabel(axlabels[iax][0])\n            ax.set_ylabel(axlabels[iax][1])\n            if bool3d:\n                # shift the label closer to the axis\n                ax.set_zlabel(axlabels[iax][2],labelpad=-7)\n    return axs", "idx": 1161}
{"project": "Scanpy", "commit_id": "901_scanpy_1.9.0__utils.py_get_init_pos_from_paga.py", "target": 0, "func": "def get_init_pos_from_paga(\n    adata, adjacency=None, random_state=0, neighbors_key=None, obsp=None\n):\n    np.random.seed(random_state)\n    if adjacency is None:\n        adjacency = _choose_graph(adata, obsp, neighbors_key)\n    if 'paga' in adata.uns and 'pos' in adata.uns['paga']:\n        groups = adata.obs[adata.uns['paga']['groups']]\n        pos = adata.uns['paga']['pos']\n        connectivities_coarse = adata.uns['paga']['connectivities']\n        init_pos = np.ones((adjacency.shape[0], 2))\n        for i, group_pos in enumerate(pos):\n            subset = (groups == groups.cat.categories[i]).values\n            neighbors = connectivities_coarse[i].nonzero()\n            if len(neighbors[1]) > 0:\n                connectivities = connectivities_coarse[i][neighbors]\n                nearest_neighbor = neighbors[1][np.argmax(connectivities)]\n                noise = np.random.random((len(subset[subset]), 2))\n                dist = pos[i] - pos[nearest_neighbor]\n                noise = noise * dist\n                init_pos[subset] = group_pos - 0.5 * dist + noise\n            else:\n                init_pos[subset] = group_pos\n    else:\n        raise ValueError(\n            'Plot PAGA first, so that adata.uns[\\'paga\\']' 'with key \\'pos\\'.'\n        )\n    return init_pos", "idx": 1162}
{"project": "Scanpy", "commit_id": "771_scanpy_1.9.0_test_scrublet.py_create_sim_from_parents.py", "target": 0, "func": "def create_sim_from_parents(adata, parents):\n\n        # Now simulate doublets based on the randomly selected parents used\n        # previously\n\n        N_sim = parents.shape[0]\n        I = sparse.coo_matrix(\n            (\n                np.ones(2 * N_sim),\n                (np.repeat(np.arange(N_sim), 2), parents.flat),\n            ),\n            (N_sim, adata_obs.n_obs),\n        )\n        X = I @ adata_obs.layers['raw']\n        return ad.AnnData(\n            X,\n            var=pd.DataFrame(index=adata_obs.var_names),\n            obs=pd.DataFrame({\"total_counts\": np.ravel(X.sum(axis=1))}),\n            obsm={\"doublet_parents\": parents.copy()},", "idx": 1169}
{"project": "Scanpy", "commit_id": "230_scanpy_1.9.0___init__.py_compute_neighbors.py", "target": 0, "func": "def compute_neighbors(\n        self,\n        n_neighbors: int = 30,\n        knn: bool = True,\n        n_pcs: Optional[int] = None,\n        use_rep: Optional[str] = None,\n        method: _Method = 'umap',\n        random_state: AnyRandom = 0,\n        write_knn_indices: bool = False,\n        metric: _Metric = 'euclidean',\n        metric_kwds: Mapping[str, Any] = MappingProxyType({}),\n    ) -> None:\n        \"\"\"\\\n        Compute distances and connectivities of neighbors.\n\n        Parameters\n        ----------\n        n_neighbors\n             Use this number of nearest neighbors.\n        knn\n             Restrict result to `n_neighbors` nearest neighbors.\n        {n_pcs}\n        {use_rep}\n\n        Returns\n        -------\n        Writes sparse graph attributes `.distances` and `.connectivities`.\n        Also writes `.knn_indices` and `.knn_distances` if\n        `write_knn_indices==True`.\n        \"\"\"\n        from sklearn.metrics import pairwise_distances\n\n        start_neighbors = logg.debug('computing neighbors')\n        if n_neighbors > self._adata.shape[0]:  # very small datasets\n            n_neighbors = 1 + int(0.5 * self._adata.shape[0])\n            logg.warning(f'n_obs too small: adjusting to `n_neighbors = {n_neighbors}`')\n        if method == 'umap' and not knn:\n            raise ValueError('`method = \\'umap\\' only with `knn = True`.')\n        if method not in {'umap', 'gauss', 'rapids'}:\n            raise ValueError(\"`method` needs to be 'umap', 'gauss', or 'rapids'.\")\n        if self._adata.shape[0] >= 10000 and not knn:\n            logg.warning('Using high n_obs without `knn=True` takes a lot of memory...')\n        # do not use the cached rp_forest\n        self._rp_forest = None\n        self.n_neighbors = n_neighbors\n        self.knn = knn\n        X = _choose_representation(self._adata, use_rep=use_rep, n_pcs=n_pcs)\n        # neighbor search\n        use_dense_distances = (metric == 'euclidean' and X.shape[0] < 8192) or not knn\n        if use_dense_distances:\n            _distances = pairwise_distances(X, metric=metric, **metric_kwds)\n            knn_indices, knn_distances = _get_indices_distances_from_dense_matrix(\n                _distances, n_neighbors\n            )\n            if knn:\n                self._distances = _get_sparse_matrix_from_indices_distances_numpy(\n                    knn_indices, knn_distances, X.shape[0], n_neighbors\n                )\n            else:\n                self._distances = _distances\n        elif method == 'rapids':\n            knn_indices, knn_distances = compute_neighbors_rapids(\n                X, n_neighbors, metric=metric\n            )\n        else:\n            # non-euclidean case and approx nearest neighbors\n            if X.shape[0] < 4096:\n                X = pairwise_distances(X, metric=metric, **metric_kwds)\n                metric = 'precomputed'\n            knn_indices, knn_distances, forest = compute_neighbors_umap(\n                X, n_neighbors, random_state, metric=metric, metric_kwds=metric_kwds\n            )\n            # very cautious here\n            try:\n                if forest:\n                    self._rp_forest = _make_forest_dict(forest)\n            except Exception:  # TODO catch the correct exception\n                pass\n        # write indices as attributes\n        if write_knn_indices:\n            self.knn_indices = knn_indices\n            self.knn_distances = knn_distances\n        start_connect = logg.debug('computed neighbors', time=start_neighbors)\n        if not use_dense_distances or method in {'umap', 'rapids'}:\n            # we need self._distances also for method == 'gauss' if we didn't\n            # use dense distances\n            self._distances, self._connectivities = _compute_connectivities_umap(\n                knn_indices,\n                knn_distances,\n                self._adata.shape[0],\n                self.n_neighbors,\n            )\n        # overwrite the umap connectivities if method is 'gauss'\n        # self._distances is unaffected by this\n        if method == 'gauss':\n            self._compute_connectivities_diffmap()\n        logg.debug('computed connectivities', time=start_connect)\n        self._number_connected_components = 1\n        if issparse(self._connectivities):\n            from scipy.sparse.csgraph import connected_components\n\n            self._connected_components = connected_components(self._connectivities)\n            self._number_connected_components = self._connected_components[0]", "idx": 1171}
{"project": "Scanpy", "commit_id": "1106_scanpy_1.9.0_scanpy_tools__utils.py__choose_representation.py", "target": 1, "func": "def _choose_representation(adata, use_rep=None, n_pcs=None, silent=False):\n    verbosity = settings.verbosity\n    if silent and settings.verbosity > 1:\n        settings.verbosity = 1\n    if use_rep is None and n_pcs == 0:  # backwards compat for specifying `.X`\n        use_rep = 'X'\n    if use_rep is None:\n        if adata.n_vars > settings.N_PCS:\n            if 'X_pca' in adata.obsm.keys():\n                if n_pcs is not None and n_pcs > adata.obsm['X_pca'].shape[1]:\n                    raise ValueError(\n                        '`X_pca` does not have enough PCs. Rerun `sc.pp.pca` with adjusted `n_comps`.'\n                    )\n                X = adata.obsm['X_pca'][:, :n_pcs]\n                logg.info(f'    using \\'X_pca\\' with n_pcs = {X.shape[1]}')\n            else:\n                logg.warning(\n                    f'You\u2019re trying to run this on {adata.n_vars} dimensions of `.X`, '\n                    'if you really want this, set `use_rep=\\'X\\'`.\\n         '\n                    'Falling back to preprocessing with `sc.pp.pca` and default params.'\n                )\n                X = pca(adata.X)\n                adata.obsm['X_pca'] = X[:, :n_pcs]\n        else:\n            logg.info('    using data matrix X directly')\n            X = adata.X\n    else:\n        if use_rep in adata.obsm.keys():\n            X = adata.obsm[use_rep]\n            if use_rep == 'X_pca' and n_pcs is not None:\n                X = adata.obsm[use_rep][:, :n_pcs]\n        elif use_rep == 'X':\n            X = adata.X\n        else:\n            raise ValueError(\n                'Did not find {} in `.obsm.keys()`. '\n                'You need to compute it first.'.format(use_rep)\n            )\n    settings.verbosity = verbosity  # resetting verbosity\n    return X", "idx": 1175}
{"project": "Scanpy", "commit_id": "875_scanpy_1.9.0__sim.py_read_model.py", "target": 0, "func": "def read_model(self):\n        \"\"\"Read the model and the couplings from the model file.\"\"\"\n        if self.verbosity > 0:\n            settings.m(0, 'reading model', self.model)\n        # read model\n        boolRules = []\n        for line in self.model.open():\n            if line.startswith('#') and 'modelType =' in line:\n                keyval = line\n                if '|' in line:\n                    keyval, type = line.split('|')[:2]\n                self.modelType = keyval.split('=')[1].strip()\n            if line.startswith('#') and 'invTimeStep =' in line:\n                keyval = line\n                if '|' in line:\n                    keyval, type = line.split('|')[:2]\n                self.invTimeStep = float(keyval.split('=')[1].strip())\n            if not line.startswith('#'):\n                boolRules.append([s.strip() for s in line.split('=')])\n            if line.startswith('# coupling list:'):\n                break\n        self.dim = len(boolRules)\n        self.boolRules = dict(boolRules)\n        self.varNames = {s: i for i, s in enumerate(self.boolRules.keys())}\n        names = self.varNames\n        # read couplings via names\n        self.Coupl = np.zeros((self.dim, self.dim))\n        boolContinue = True\n        for (\n            line\n        ) in self.model.open():  # open(self.model.replace('/model','/couplList')):\n            if line.startswith('# coupling list:'):\n                boolContinue = False\n            if boolContinue:\n                continue\n            if not line.startswith('#'):\n                gps, gs, val = line.strip().split()\n                self.Coupl[int(names[gps]), int(names[gs])] = float(val)\n        # adjancecy matrices\n        self.Adj_signed = np.sign(self.Coupl)\n        self.Adj = np.abs(np.array(self.Adj_signed))\n        # build bool coefficients (necessary for odefy type\n        # version of the discrete model)\n        self.build_boolCoeff()", "idx": 1189}
{"project": "Scanpy", "commit_id": "454_scanpy_0.2.9.1_scanpy_readwrite.py_write_anndata_to_file.py", "target": 1, "func": "def write_anndata_to_file(filename, adata, ext='h5'):\n    \"\"\"Write dictionary to file.\n    Values need to be np.arrays or transformable to numpy arrays.\n    Parameters\n    ----------\n    filename : str, Path\n        Filename of data file.\n    d : dict\n        Dictionary storing keys with np.ndarray-like data or scalars.\n    ext : string\n        Determines file type, allowed are 'h5' (hdf5),\n        'xlsx' (Excel) [or 'csv' (comma separated value file)].\n    \"\"\"\n    filename = str(filename)  # allow passing pathlib.Path objects\n    directory = os.path.dirname(filename)\n    if not os.path.exists(directory):\n        logg.info('creating directory', directory + '/', 'for saving output files')\n        os.makedirs(directory)\n    # output the following at warning level, it's very important for the users\n    if ext in {'h5', 'npz'}:\n        logg.msg('writing', filename, v=4)\n        d = adata._to_dict_fixed_width_arrays()\n    else:\n        d = adata._to_dict_dataframes()\n    from scipy.sparse import issparse\n    d_write = {}\n    for key, value in d.items():\n        if issparse(value):\n            for k, v in save_sparse_csr(value, key=key).items():\n                d_write[k] = v\n        elif not isinstance(value, pd.DataFrame):\n            key, value = preprocess_writing(key, value)\n            d_write[key] = value\n        else:\n            d_write[key] = value\n    # now open the file\n    if ext == 'h5':\n        if issparse(value):\n            for k, v in save_sparse_csr(value, key=key).items():\n                d_write[k] = v\n        with h5py.File(filename, 'w') as f:\n            for key, value in d_write.items():\n                try:\n                    # ignore arrays with empty dtypes\n                    if value.dtype.descr:\n                        f.create_dataset(key, data=value)\n                except TypeError:\n                    # try writing it as byte strings\n                    try:\n                        if value.dtype.names is None:\n                            f.create_dataset(key, data=value.astype('S'))\n                        else:\n                            new_dtype = [(dt[0], 'S{}'.format(int(dt[1][2:])*4))\n                                         for dt in value.dtype.descr]\n                            f.create_dataset(key, data=value.astype(new_dtype))\n                    except Exception as e:\n                        logg.info(str(e))\n                        logg.warn('Could not save field with key = \"{}\" to h5 file.'\n                                  .format(key))\n    elif ext == 'npz':\n        np.savez(filename, **d_write)\n    elif ext == 'csv' or ext == 'txt':\n        # here this is actually a directory that corresponds to the\n        # single hdf5 file\n        dirname = filename.replace('.' + ext, '/')\n        # write the following at warning level, it's very important for the users\n        logg.info('writing', ext, 'files to', dirname)\n        if not os.path.exists(dirname): os.makedirs(dirname)\n        if not os.path.exists(dirname + 'uns'): os.makedirs(dirname + 'uns')\n        not_yet_raised_data_graph_warning = True\n        for key, value in d_write.items():\n            if key.startswith('data_graph'):\n                if not_yet_raised_data_graph_warning:\n                    logg.warn('Omitting to write neighborhood graph (`adata.uns[\\'data_graph...\\']`).')\n                    not_yet_raised_data_graph_warning = False\n                continue\n            filename = dirname\n            if key not in {'data', 'var', 'smp', 'smpm', 'varm'}:\n                filename += 'uns/'\n            filename += key + '.' + ext\n            df = value\n            if not isinstance(value, pd.DataFrame):\n                try:\n                    df = pd.DataFrame(value)\n                except:\n                    logg.warn('Omitting to write \\'{}\\'.'.format(key))\n                    continue\n            df.to_csv(filename, sep=(' ' if ext == 'txt' else ','),\n                      header=True if key != 'data' else False,\n                      index=True if key in {'smp', 'var'} else False)", "idx": 1200}
{"project": "Scanpy", "commit_id": "535_scanpy_0.4.4_scanpy_tools_louvain.py_louvain.py", "target": 1, "func": "def louvain(\n        adata,\n        resolution=None,\n        random_state=0,\n        restrict_to=None,\n        key_added=None,\n        key='neighbors_similarities',\n        flavor='vtraag',\n        directed=True,\n        n_jobs=None,\n        copy=False):\n    \"\"\"Cluster cells into subgroups [Blondel08]_ [Levine15]_ [Traag17]_.\n    Cluster cells using the Louvain algorithm [Blondel08]_ in the implementation\n    of [Traag17]_. The Louvain algorithm has been proposed for single-cell\n    analysis by [Levine15]_.\n    Parameters\n    ----------\n    adata : :class:`~scanpy.api.AnnData`\n        The annotated data matrix.\n    n_neighbors : `int`, optional (default: 30)\n        Number of neighbors to use for construction of knn graph.\n    resolution : `float` or `None`, optional (default: 1)\n        For the default flavor ('vtraag'), you can provide a resolution (higher\n        resolution means finding more and smaller clusters), which defaults to\n        1.0.\n    random_state : `int`, optional (default: 0)\n        Change the initialization of the optimization.\n    restrict_to : `tuple`, optional (default: None)\n        Restrict the clustering to the categories within the key for sample\n        annotation, tuple needs to contain (obs key, list of categories).\n    key_added : `str`, optional (default: `None`)\n        Key under which to add the cluster labels.\n    key : `str`, optional (default: 'neighbors_similarities')\n        Key for accessing the sparse adjacency matrix of the graph in\n        `adata.uns`.\n    flavor : {'vtraag', 'igraph'}\n        Choose between to packages for computing the clustering. 'vtraag' is\n        much more powerful.\n    copy : `bool` (default: False)\n        Copy adata or modify it inplace.\n    Returns\n    -------\n    Depending on `copy`, returns or updates `adata` with the following fields.\n    louvain_groups : `pd.Series` (``adata.obs``, dtype `category`)\n        Array of dim (number of samples) that stores the subgroup id ('0',\n        '1', ...) for each cell.\n    \"\"\"\n    logg.info('running Louvain clustering', r=True)\n    adata = adata.copy() if copy else adata\n    if key not in adata.uns:\n        raise ValueError(\n            '\\'{}\\' is not present in `adata.uns`. '\n            'You need to run `pp.neighbors` first to compute a neighborhood graph.'\n            .format(key))\n    adjacency = adata.uns[key]\n    if restrict_to is not None:\n        restrict_key, restrict_categories = restrict_to\n        if not isinstance(restrict_categories[0], str):\n            raise ValueError('You need to use strings to label categories, '\n                             'e.g. \\'1\\' instead of 1.')\n        restrict_indices = adata.obs[restrict_key].isin(restrict_categories).values\n        adjacency = adjacency[restrict_indices, :]\n        adjacency = adjacency[:, restrict_indices]\n    if flavor in {'vtraag', 'igraph'}:\n        if flavor == 'igraph' and resolution is not None:\n            logg.warn('`resolution` parameter has no effect for flavor \"igraph\"')\n        if directed and flavor == 'igraph':\n            directed = False\n        if not directed: logg.m('    using the undirected graph', v=4)\n        g = utils.get_igraph_from_adjacency(adjacency, directed=directed)\n        if flavor == 'vtraag':\n            import louvain\n            if resolution is None: resolution = 1\n            try:\n                logg.info('    using the \"louvain\" package of Traag (2017)')\n                louvain.set_rng_seed(random_state)\n                part = louvain.find_partition(g, louvain.RBConfigurationVertexPartition,\n                                              resolution_parameter=resolution)\n                # adata.uns['louvain_quality'] = part.quality()\n            except AttributeError:\n                logg.warn('Did not find package louvain>=0.6, '\n                          'the clustering result will therefore not '\n                          'be 100% reproducible, '\n                          'but still meaningful. '\n                          'If you want 100% reproducible results, '\n                          'update via \"pip install louvain --upgrade\".')\n                part = louvain.find_partition(g, method='RBConfiguration',\n                                              resolution_parameter=resolution)\n        elif flavor == 'igraph':\n            part = g.community_multilevel()\n        groups = np.array(part.membership)\n    elif flavor == 'taynaud':\n        # this is deprecated\n        import networkx as nx\n        import community\n        g = nx.Graph(adata.uns['data_graph_distance_local'])\n        partition = community.best_partition(g)\n        groups = np.zeros(len(partition), dtype=int)\n        for k, v in partition.items(): groups[k] = v\n    else:\n        raise ValueError('`flavor` needs to be \"vtraag\" or \"igraph\" or \"taynaud\".')\n    unique_groups = np.unique(groups)\n    n_clusters = len(unique_groups)\n    if restrict_to is None:\n        groups = groups.astype('U')\n        key_added = 'louvain_groups' if key_added is None else key_added\n        adata.obs[key_added] = pd.Categorical(\n            values=groups,\n            categories=natsorted(unique_groups.astype('U')))\n    else:\n        key_added = restrict_key + '_R' if key_added is None else key_added\n        groups += 1\n        adata.obs[key_added] = adata.obs[restrict_key].astype('U')\n        adata.obs[key_added] += ','\n        adata.obs[key_added].iloc[restrict_indices] += groups.astype('U')\n        adata.obs[key_added].iloc[~restrict_indices] += '0'\n        adata.obs[key_added] = adata.obs[key_added].astype(\n            'category', categories=natsorted(adata.obs[key_added].unique()))\n    adata.uns['louvain_params'] = np.array((resolution, random_state,),\n                                           dtype=[('resolution', float), ('random_state', int)])\n    logg.info('    finished', time=True, end=' ' if settings.verbosity > 2 else '\\n')\n    logg.hint('found {} clusters and added\\n'\n              '    \\'{}\\', the cluster labels (adata.obs, dtype=category)'\n              .format(n_clusters, key_added))\n    return adata if copy else None", "idx": 1204}
{"project": "Scanpy", "commit_id": "1062_scanpy_1.6.1_scanpy_plotting__tools_scatterplots.py_spatial.py", "target": 1, "func": "def spatial(\n    adata,\n    *,\n    img_key: Union[str, None, Empty] = _empty,\n    library_id: Union[str, Empty] = _empty,\n    crop_coord: Tuple[int, int, int, int] = None,\n    alpha_img: float = 1.0,\n    bw: bool = False,\n    size: float = None,\n    na_color: ColorLike = (0.0, 0.0, 0.0, 0.0),\n    **kwargs,\n) -> Union[Axes, List[Axes], None]:\n    \"\"\"\\\n    Scatter plot in spatial coordinates.\n    Use the parameter `img_key` to see the image in the background\n    And the parameter `library_id` to select the image.\n    By default, `'hires'` and `'lowres'` are attempted.\n    Also by default the first entry of `library_id` is attempted.\n    Use `crop_coord`, `alpha_img`, and `bw` to control how it is displayed.\n    Use `size` to scale the size of the Visium spots plotted on top.\n    Parameters\n    ----------\n    {adata_color_etc}\n    {scatter_bulk}\n    {show_save_ax}\n    Returns\n    -------\n    If `show==False` a :class:`~matplotlib.axes.Axes` or a list of it.\n    \"\"\"\n    if library_id is _empty:\n        library_id = next((i for i in adata.uns['spatial'].keys()))\n    else:\n        if library_id not in adata.uns['spatial'].keys():\n            raise KeyError(\n                f\"Could not find '{library_id}' in adata.uns['spatial'].keys().\\n\"\n                f\"Available keys are: {list(adata.uns['spatial'].keys())}.\"\n            )\n\n    spatial_data = adata.uns['spatial'][library_id]\n    if img_key is _empty:\n        img_key = next(\n            (k for k in ['hires', 'lowres'] if k in spatial_data['images']),\n            None,\n        )\n\n    if size is None:\n        size = 1.0\n\n    return embedding(\n        adata,\n        'spatial',\n        img_key=img_key,\n        crop_coord=crop_coord,\n        alpha_img=alpha_img,\n        bw=bw,\n        library_id=library_id,\n        size=size,\n        na_color=na_color,\n        **kwargs,\n    )", "idx": 1217}
{"project": "Scanpy", "commit_id": "417_scanpy_1.9.0__recipes.py_recipe_zheng17.py", "target": 0, "func": "def recipe_zheng17(\n    adata: AnnData,\n    n_top_genes: int = 1000,\n    log: bool = True,\n    plot: bool = False,\n    copy: bool = False,\n) -> Optional[AnnData]:\n    \"\"\"\\\n    Normalization and filtering as of [Zheng17]_.\n\n    Reproduces the preprocessing of [Zheng17]_ \u2013 the Cell Ranger R Kit of 10x\n    Genomics.\n\n    Expects non-logarithmized data.\n    If using logarithmized data, pass `log=False`.\n\n    The recipe runs the following steps\n\n    .. code:: python\n\n        sc.pp.filter_genes(adata, min_counts=1)         # only consider genes with more than 1 count\n        sc.pp.normalize_per_cell(                       # normalize with total UMI count per cell\n             adata, key_n_counts='n_counts_all'\n        )\n        filter_result = sc.pp.filter_genes_dispersion(  # select highly-variable genes\n            adata.X, flavor='cell_ranger', n_top_genes=n_top_genes, log=False\n        )\n        adata = adata[:, filter_result.gene_subset]     # subset the genes\n        sc.pp.normalize_per_cell(adata)                 # renormalize after filtering\n        if log: sc.pp.log1p(adata)                      # log transform: adata.X = log(adata.X + 1)\n        sc.pp.scale(adata)                              # scale to unit variance and shift to zero mean\n\n\n    Parameters\n    ----------\n    adata\n        Annotated data matrix.\n    n_top_genes\n        Number of genes to keep.\n    log\n        Take logarithm.\n    plot\n        Show a plot of the gene dispersion vs. mean relation.\n    copy\n        Return a copy of `adata` instead of updating it.\n\n    Returns\n    -------\n    Returns or updates `adata` depending on `copy`.\n    \"\"\"\n    start = logg.info('running recipe zheng17')\n    if copy:\n        adata = adata.copy()\n    # only consider genes with more than 1 count\n    pp.filter_genes(adata, min_counts=1)\n    # normalize with total UMI count per cell\n    normalize_total(adata, key_added='n_counts_all')\n    filter_result = filter_genes_dispersion(\n        adata.X, flavor='cell_ranger', n_top_genes=n_top_genes, log=False\n    )\n    if plot:  # should not import at the top of the file\n        from ..plotting import _preprocessing as ppp\n\n        ppp.filter_genes_dispersion(filter_result, log=True)\n    # actually filter the genes, the following is the inplace version of\n    #     adata = adata[:, filter_result.gene_subset]\n    adata._inplace_subset_var(filter_result.gene_subset)  # filter genes\n    normalize_total(adata)  # renormalize after filtering\n    if log:\n        pp.log1p(adata)  # log transform: X = log(X + 1)\n    pp.scale(adata)\n    logg.info('    finished', time=start)\n    return adata if copy else None", "idx": 1228}
{"project": "Scanpy", "commit_id": "329_scanpy_1.9.0__utils.py_check_projection.py", "target": 0, "func": "def check_projection(projection):\n    \"\"\"Validation for projection argument.\"\"\"\n    if projection not in {\"2d\", \"3d\"}:\n        raise ValueError(f\"Projection must be '2d' or '3d', was '{projection}'.\")\n    if projection == \"3d\":\n        from packaging.version import parse\n\n        mpl_version = parse(mpl.__version__)\n        if mpl_version < parse(\"3.3.3\"):\n            raise ImportError(\n                f\"3d plotting requires matplotlib > 3.3.3. Found {mpl.__version__}\"", "idx": 1232}
{"project": "Scanpy", "commit_id": "16_scanpy_1.9.0_has_attr_test.py_setup.py", "target": 0, "func": "def setup(app: Sphinx):\n    DEFAULT_NAMESPACE[\"has_attr\"] = has_attr", "idx": 1236}
{"project": "Scanpy", "commit_id": "910_scanpy_1.9.0___init__.py__doc_params.py", "target": 0, "func": "def _doc_params(**kwds):\n    \"\"\"\\\n    Docstrings should start with \"\\\" in the first line for proper formatting.\n    \"\"\"\n\n    def dec(obj):\n        obj.__orig_doc__ = obj.__doc__\n        obj.__doc__ = dedent(obj.__doc__).format_map(kwds)\n        return obj\n\n    return dec", "idx": 1240}
{"project": "Scanpy", "commit_id": "888_scanpy_1.4.3_scanpy_logging.py_format.py", "target": 1, "func": "def format(self, record: logging.LogRecord):\n    format_orig = self._style._fmt\n    if record.levelno == INFO:\n        self._style._fmt = '{message}'\n    elif record.levelno == HINT:\n        self._style._fmt = '--> {message}'\n    elif record.levelno == DEBUG:\n        self._style._fmt = '    {message}'\n    if record.time_passed:\n        # strip microseconds\n        if record.time_passed.microseconds:\n            record.time_passed = timedelta(seconds=int(record.time_passed.total_seconds()))\n        if '{time_passed}' not in record.msg:\n            self._style._fmt += ' ({time_passed})'\n    result = logging.Formatter.format(self, record)\n    self._style._fmt = format_orig\n    return result", "idx": 1241}
{"project": "Scanpy", "commit_id": "249_scanpy_0.1_scanpy_plotting___init__.py_dpt.py", "target": 1, "func": "def dpt(adata,\n        basis='diffmap',\n        color=None,\n        names=None,\n        comps=None,\n        cont=None,\n        layout='2d',\n        legendloc='right margin',\n        cmap=None,\n        pal=None,\n        right_margin=None,\n        size=None,\n        titles=None,\n        show=None):\n    \"\"\"Plot results of DPT analysis.\n    Parameters\n    ----------\n    adata : AnnData\n        Annotated data matrix.\n    basis : {'diffmap', 'pca', 'tsne', 'spring'}\n        Choose the basis in which to plot.\n    color : str, optional (default: first annotation)\n        Sample/ cell annotation for coloring in the form \"ann1,ann2,...\". String\n        annotation is plotted assuming categorical annotation, float and integer\n        annotation is plotted assuming continuous annoation. Option 'cont'\n        allows to switch between these default choices.\n    comps : str, optional (default: '1,2')\n         String of the form '1,2' or 'all'.\n    cont : bool, None (default: None)\n        Switch on continuous layout, switch off categorical layout.\n    layout : {'2d', '3d', 'unfolded 3d'}, optional (default: '2d')\n         Layout of plot.\n    legendloc : {'right margin', see matplotlib.legend}, optional (default: 'right margin')\n         Options for keyword argument 'loc'.\n    cmap : str (default: 'viridis')\n         String denoting matplotlib color map.\n    pal : list of str (default: matplotlib.rcParams['axes.prop_cycle'].by_key()['color'])\n         Colors cycle to use for categorical groups.\n    right_margin : float (default: None)\n         Adjust how far the plotting panel extends to the right.\n    \"\"\"\n    dpt_scatter(adata,\n                basis=basis,\n                color=color,\n                names=names,\n                comps=comps,\n                cont=cont,\n                layout=layout,\n                legendloc=legendloc,\n                cmap=cmap,\n                pal=pal,\n                right_margin=right_margin,\n                size=size,\n                titles=titles,\n                show=False)\n    colors = ['dpt_pseudotime']\n    if len(np.unique(adata.smp['dpt_groups'])) > 1: colors += ['dpt_groups']\n    if color is not None:\n        if not isinstance(color, list): colors = color.split(',')\n        else: colors = color\n    if 'dpt_groups' in colors:\n        dpt_graph(adata, colors)\n    dpt_timeseries(adata, cmap=cmap, show=show)", "idx": 1242}
{"project": "Scanpy", "commit_id": "665_scanpy_1.9.0_test_plotting.py_test_repeated_colors_w_missing_value.py", "target": 0, "func": "def test_repeated_colors_w_missing_value():\n    # https://github.com/theislab/scanpy/issues/2133\n    v = pd.Series(np.arange(10).astype(str))\n    v[0] = np.nan\n    v = v.astype(\"category\")\n\n    ad = sc.AnnData(obs=pd.DataFrame(v, columns=[\"value\"]))\n    ad.obsm[\"X_umap\"] = np.random.normal(size=(ad.n_obs, 2))\n\n    sc.pl.umap(ad, color=\"value\")\n\n    ad.uns['value_colors'][1] = ad.uns['value_colors'][0]\n\n    sc.pl.umap(ad, color=\"value\")", "idx": 1243}
{"project": "Scanpy", "commit_id": "619_scanpy_1.9.0_test_paga.py_test_paga_positions_reproducible.py", "target": 0, "func": "def test_paga_positions_reproducible():\n    \"\"\"Check exact reproducibility and effect of random_state on paga positions\"\"\"\n    # https://github.com/theislab/scanpy/issues/1859\n    pbmc = pbmc68k_reduced()\n    sc.tl.paga(pbmc, \"bulk_labels\")\n\n    a = pbmc.copy()\n    b = pbmc.copy()\n    c = pbmc.copy()\n\n    sc.pl.paga(a, show=False, random_state=42)\n    sc.pl.paga(b, show=False, random_state=42)\n    sc.pl.paga(c, show=False, random_state=13)\n\n    np.testing.assert_array_equal(a.uns[\"paga\"][\"pos\"], b.uns[\"paga\"][\"pos\"])\n    assert a.uns[\"paga\"][\"pos\"].tolist() != c.uns[\"paga\"][\"pos\"].tolist()", "idx": 1244}
{"project": "Scanpy", "commit_id": "905_scanpy_1.4.4_scanpy_plotting__anndata.py_heatmap.py", "target": 1, "func": "def heatmap(adata, var_names, groupby=None, use_raw=None, log=False, num_categories=7,\n            dendrogram=False, gene_symbols=None, var_group_positions=None, var_group_labels=None,\n            var_group_rotation=None, layer=None, standard_scale=None, swap_axes=False,\n            show_gene_labels=None, show=None, save=None, figsize=None, **kwds):\n    \"\"\"\\\n    Heatmap of the expression values of genes.\n    If `groupby` is given, the heatmap is ordered by the respective group. For\n    example, a list of marker genes can be plotted, ordered by clustering. If\n    the `groupby` observation annotation is not categorical the observation\n    annotation is turned into a categorical by binning the data into the number\n    specified in `num_categories`.\n    Parameters\n    ----------\n    {common_plot_args}\n    standard_scale : {{'var', 'obs'}}, optional (default: None)\n        Whether or not to standardize that dimension between 0 and 1, meaning for each variable or observation,\n        subtract the minimum and divide each by its maximum.\n    swap_axes: `bool`, optional (default: `False`)\n         By default, the x axis contains `var_names` (e.g. genes) and the y axis the `groupby`\n         categories (if any). By setting `swap_axes` then x are the `groupby` categories and y the `var_names`.\n    show_gene_labels: `bool`, optional (default: `None`).\n         By default gene labels are shown when there are 50 or less genes. Otherwise the labels are removed.\n    {show_save_ax}\n    **kwds : keyword arguments\n        Are passed to `matplotlib.imshow`.\n    Returns\n    -------\n    List of :class:`~matplotlib.axes.Axes`\n    Examples\n    -------\n    >>> adata = sc.datasets.pbmc68k_reduced()\n    >>> sc.pl.heatmap(adata, ['C1QA', 'PSAP', 'CD79A', 'CD79B', 'CST3', 'LYZ'],\n    ...               groupby='bulk_labels', dendrogram=True, swap_axes=True)\n    Using var_names as dict:\n    >>> markers = {{'T-cell': 'CD3D', 'B-cell': 'CD79A', 'myeloid': 'CST3'}}\n    >>> sc.pl.heatmap(adata, markers, groupby='bulk_labels', dendrogram=True)\n    See also\n    --------\n    rank_genes_groups_heatmap: to plot marker genes identified using the :func:`~scanpy.tl.rank_genes_groups` function.\n    \"\"\"\n    if use_raw is None and adata.raw is not None: use_raw = True\n    var_names, var_group_labels, var_group_positions = _check_var_names_type(var_names,\n                                                                             var_group_labels, var_group_positions)\n    categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories,\n                                              gene_symbols=gene_symbols, layer=layer)\n    if standard_scale == 'obs':\n        obs_tidy = obs_tidy.sub(obs_tidy.min(1), axis=0)\n        obs_tidy = obs_tidy.div(obs_tidy.max(1), axis=0).fillna(0)\n    elif standard_scale == 'var':\n        obs_tidy -= obs_tidy.min(0)\n        obs_tidy = (obs_tidy / obs_tidy.max(0)).fillna(0)\n    elif standard_scale is None:\n        pass\n    else:\n        logg.warning('Unknown type for standard_scale, ignored')\n    if groupby is None or len(categories) <= 1:\n        categorical = False\n        # dendrogram can only be computed  between groupby categories\n        dendrogram = False\n    else:\n        categorical = True\n        # get categories colors:\n        if groupby + \"_colors\" in adata.uns:\n            groupby_colors = adata.uns[groupby + \"_colors\"]\n        else:\n            groupby_colors = None\n    if dendrogram:\n        dendro_data = _reorder_categories_after_dendrogram(adata, groupby, dendrogram,\n                                                           var_names=var_names,\n                                                           var_group_labels=var_group_labels,\n                                                           var_group_positions=var_group_positions)\n        var_group_labels = dendro_data['var_group_labels']\n        var_group_positions = dendro_data['var_group_positions']\n        # reorder obs_tidy\n        if dendro_data['var_names_idx_ordered'] is not None:\n            obs_tidy = obs_tidy.iloc[:, dendro_data['var_names_idx_ordered']]\n            var_names = [var_names[x] for x in dendro_data['var_names_idx_ordered']]\n        obs_tidy.index = obs_tidy.index.reorder_categories(\n            [categories[x] for x in dendro_data['categories_idx_ordered']], ordered=True)\n        # reorder groupby colors\n        if groupby_colors is not None:\n            groupby_colors = [groupby_colors[x] for x in dendro_data['categories_idx_ordered']]\n    if show_gene_labels is None:\n        if len(var_names) <= 50:\n            show_gene_labels = True\n        else:\n            show_gene_labels = False\n            logg.warning(\n                'Gene labels are not shown when more than 50 genes are visualized. '\n                'To show gene labels set `show_gene_labels=True`'\n            )\n    if categorical:\n        obs_tidy = obs_tidy.sort_index()\n    colorbar_width = 0.2\n    if not swap_axes:\n        # define a layout of 2 rows x 4 columns\n        # first row is for 'brackets' (if no brackets needed, the height of this row is zero)\n        # second row is for main content. This second row is divided into three axes:\n        #   first ax is for the categories defined by `groupby`\n        #   second ax is for the heatmap\n        #   third ax is for the dendrogram\n        #   fourth ax is for colorbar\n        dendro_width = 1 if dendrogram else 0\n        groupby_width = 0.2 if categorical else 0\n        if figsize is None:\n            height = 6\n            if show_gene_labels:\n                heatmap_width = len(var_names) * 0.3\n            else:\n                heatmap_width = 8\n            width = heatmap_width + dendro_width + groupby_width\n        else:\n            width, height = figsize\n            heatmap_width = width - (dendro_width + groupby_width)\n        if var_group_positions is not None and len(var_group_positions) > 0:\n            # add some space in case 'brackets' want to be plotted on top of the image\n            height_ratios = [0.15, height]\n        else:\n            height_ratios = [0, height]\n        width_ratios = [groupby_width, heatmap_width, dendro_width, colorbar_width]\n        fig = pl.figure(figsize=(width, height))\n        axs = gridspec.GridSpec(nrows=2, ncols=4, width_ratios=width_ratios, wspace=0.15 / width,\n                                hspace=0.13 / height, height_ratios=height_ratios)\n        heatmap_ax = fig.add_subplot(axs[1, 1])\n        im = heatmap_ax.imshow(obs_tidy.values, aspect='auto', **kwds)\n        heatmap_ax.set_ylim(obs_tidy.shape[0] - 0.5, -0.5)\n        heatmap_ax.set_xlim(-0.5, obs_tidy.shape[1] - 0.5)\n        heatmap_ax.tick_params(axis='y', left=False, labelleft=False)\n        heatmap_ax.set_ylabel('')\n        heatmap_ax.grid(False)\n        # sns.heatmap(obs_tidy, yticklabels=\"auto\", ax=heatmap_ax, cbar_ax=heatmap_cbar_ax, **kwds)\n        if show_gene_labels:\n            heatmap_ax.tick_params(axis='x', labelsize='small')\n            heatmap_ax.set_xticks(np.arange(len(var_names)))\n            heatmap_ax.set_xticklabels(var_names, rotation=90)\n        else:\n            heatmap_ax.tick_params(axis='x', labelbottom=False, bottom=False)\n        # plot colorbar\n        _plot_colorbar(im, fig, axs[1, 3])\n        if categorical:\n            groupby_ax = fig.add_subplot(axs[1, 0])\n            ticks, labels, groupby_cmap, norm = _plot_categories_as_colorblocks(groupby_ax, obs_tidy,\n                                                                          colors=groupby_colors, orientation='left')\n\n            # add lines to main heatmap\n            line_positions = np.cumsum(obs_tidy.index.value_counts(sort=False))[:-1]\n            heatmap_ax.hlines(line_positions, -1, len(var_names) + 1, lw=0.5)\n\n        if dendrogram:\n            dendro_ax = fig.add_subplot(axs[1, 2], sharey=heatmap_ax)\n            _plot_dendrogram(dendro_ax, adata, groupby, ticks=ticks, dendrogram_key=dendrogram)\n        # plot group legends on top of heatmap_ax (if given)\n        if var_group_positions is not None and len(var_group_positions) > 0:\n            gene_groups_ax = fig.add_subplot(axs[0, 1], sharex=heatmap_ax)\n            _plot_gene_groups_brackets(gene_groups_ax, group_positions=var_group_positions,\n                                       group_labels=var_group_labels, rotation=var_group_rotation,\n                                       left_adjustment=-0.3, right_adjustment=0.3)\n    # swap axes case\n    else:\n        # define a layout of 3 rows x 3 columns\n        # The first row is for the dendrogram (if not dendrogram height is zero)\n        # second row is for main content. This col is divided into three axes:\n        #   first ax is for the heatmap\n        #   second ax is for 'brackets' if any (othwerise width is zero)\n        #   third ax is for colorbar\n        dendro_height = 0.8 if dendrogram else 0\n        groupby_height = 0.13 if categorical else 0\n        if figsize is None:\n            if show_gene_labels:\n                heatmap_height = len(var_names) * 0.18\n            else:\n                heatmap_height = 4\n            width = 10\n            height = heatmap_height + dendro_height + groupby_height  # +2 to account for labels\n        else:\n            width, height = figsize\n            heatmap_height = height - (dendro_height + groupby_height)\n        height_ratios = [dendro_height, heatmap_height, groupby_height]\n        if var_group_positions is not None and len(var_group_positions) > 0:\n            # add some space in case 'brackets' want to be plotted on top of the image\n            width_ratios = [width, 0.14, colorbar_width]\n        else:\n            width_ratios = [width, 0, colorbar_width]\n        fig = pl.figure(figsize=(width, height))\n        axs = gridspec.GridSpec(nrows=3, ncols=3, wspace=0.25 / width,\n                                hspace=0.3 / height,\n                                width_ratios=width_ratios,\n                                height_ratios=height_ratios)\n        # plot heatmap\n        heatmap_ax = fig.add_subplot(axs[1, 0])\n        im = heatmap_ax.imshow(obs_tidy.T.values, aspect='auto', **kwds)\n        heatmap_ax.set_xlim(0, obs_tidy.shape[0])\n        heatmap_ax.set_ylim(obs_tidy.shape[1] - 0.5, -0.5)\n        heatmap_ax.tick_params(axis='x', bottom=False, labelbottom=False)\n        heatmap_ax.set_xlabel('')\n        heatmap_ax.grid(False)\n        if show_gene_labels:\n            heatmap_ax.tick_params(axis='y', labelsize='small', length=1)\n            heatmap_ax.set_yticks(np.arange(len(var_names)))\n            heatmap_ax.set_yticklabels(var_names, rotation=0)\n        else:\n            heatmap_ax.tick_params(axis='y', labelleft=False, left=False)\n        if categorical:\n            groupby_ax = fig.add_subplot(axs[2, 0])\n            ticks, labels, groupby_cmap, norm = _plot_categories_as_colorblocks(groupby_ax, obs_tidy, colors=groupby_colors,\n                                                                          orientation='bottom')\n            # add lines to main heatmap\n            line_positions = np.cumsum(obs_tidy.index.value_counts(sort=False))[:-1]\n            heatmap_ax.vlines(line_positions, -1, len(var_names) + 1, lw=0.5)\n\n        if dendrogram:\n            dendro_ax = fig.add_subplot(axs[0, 0], sharex=heatmap_ax)\n            _plot_dendrogram(dendro_ax, adata, groupby, dendrogram_key=dendrogram, ticks=ticks, orientation='top')\n        # plot group legends next to the heatmap_ax (if given)\n        if var_group_positions is not None and len(var_group_positions) > 0:\n            gene_groups_ax = fig.add_subplot(axs[1, 1])\n            arr = []\n            for idx, pos in enumerate(var_group_positions):\n                arr += [idx] * (pos[1]+1 - pos[0])\n            gene_groups_ax.imshow(np.matrix(arr).T, aspect='auto', cmap=groupby_cmap, norm=norm)\n            gene_groups_ax.axis('off')\n        # plot colorbar\n        _plot_colorbar(im, fig, axs[1, 2])\n    utils.savefig_or_show('heatmap', show=show, save=save)\n    return axs", "idx": 1248}
{"project": "Scanpy", "commit_id": "412_scanpy_0.2.8_scanpy_tools_rank_genes_groups.py_rank_genes_groups.py", "target": 1, "func": "def rank_genes_groups(\n        adata,\n        grouping,\n        groups='all',\n        n_genes=100,\n        compute_distribution=False,\n        only_positive=True,\n        copy=False):\n    \"\"\"Rank genes according to differential expression [Wolf17]_.\n    Rank genes by differential expression. By default, a t-test-like ranking is\n    used, in which means are normalized with variances. Soon, a Wilcoxon-rank\n    test and other alternatives will be provided.\n    Parameters\n    ----------\n    adata : `AnnData`\n        Annotated data matrix.\n    grouping : `str`\n        The key of the sample grouping to consider.\n    groups : `str`, `list`, optional (default: `'all'`)\n        Subset of groups, e.g. `['g1', 'g2', 'g3']`, to which comparison shall\n        be restricted. If not passed, all categories will be compared to all\n        other categories.\n    n_genes : `int` (default: 100)\n        How many genes to rank by default.\n    compute_distribution : `bool`\n        If `True`, also computes the distribution for top-ranked genes, which\n        can be visualized using `sc.pl.rank_genes_groups_violin(adata)`.\n    Returns\n    -------\n    rank_genes_groups_zscores : np.ndarray of dtype float (adata.add)\n        Array of shape (number of comparisons) \u00d7 (number of genes) storing the\n        zscore of the each gene for each test.\n    rank_genes_groups_rankings_names : np.ndarray of dtype str (adata.add)\n        Array of shape (number of comparisons). Stores the labels for each comparison,\n        for example \"C1 vs. C2\" when comparing category 'C1' with 'C2'.\n    rank_genes_groups_rankings_geneidcs : np.ndarray of dtype int (adata.add)\n        Array of shape (number of comparisons) \u00d7 (number of genes) storing gene\n        indices that sort them according to decreasing absolute value of the\n        zscore.\n    \"\"\"\n    logg.info('find differentially expressed genes', r=True)\n    adata = adata.copy() if copy else adata\n    n_genes_user = n_genes\n    utils.check_adata(adata)\n    # for clarity, rename variable\n    group_key = grouping\n    groups_order = groups\n    if isinstance(groups_order, list) and isinstance(groups_order[0], int):\n        groups_order = [str(n) for n in groups_order]\n    groups_order, groups_masks = utils.select_groups(adata, groups_order, group_key)\n    adata.add['rank_genes_groups'] = group_key\n    adata.add['rank_genes_groups_order'] = groups_order\n    X = adata.X\n    # loop over all masks and compute means, variances and sample numbers\n    n_groups = groups_masks.shape[0]\n    n_genes = X.shape[1]\n    means = np.zeros((n_groups, n_genes))\n    vars = np.zeros((n_groups, n_genes))\n    ns = np.zeros(n_groups, dtype=int)\n    for imask, mask in enumerate(groups_masks):\n        means[imask], vars[imask] = simple._get_mean_var(X[mask])\n        ns[imask] = np.where(mask)[0].size\n    logg.info('... consider \"{}\":'.format(group_key), groups_order,\n              'with sample numbers', ns)\n    # test each group against the rest of the data\n    rankings_gene_zscores = []\n    rankings_gene_names = []\n    reference_indices = np.arange(adata.n_vars, dtype=int)\n    for igroup in range(n_groups):\n        mask_rest = ~groups_masks[igroup]\n        mean_rest, var_rest = simple._get_mean_var(X[mask_rest])\n        # Make a more conservative assumption on the variance reduction\n        # in the reference. Instead of this\n        # ns_rest = np.where(mask_rest)[0].size\n        # use this\n        ns_rest = ns[igroup]\n        denominator = np.sqrt(vars[igroup]/ns[igroup] + var_rest/ns_rest)\n        denominator[np.flatnonzero(denominator == 0)] = np.nan\n        zscores = (means[igroup] - mean_rest) / denominator\n        zscores[np.isnan(zscores)] = 0\n        zscores = zscores if only_positive else np.abs(zscores)\n        partition = np.argpartition(zscores, -n_genes_user)[-n_genes_user:]\n        partial_indices = np.argsort(zscores[partition])[::-1]\n        global_indices = reference_indices[partition][partial_indices]\n        rankings_gene_zscores.append(zscores[global_indices])\n        rankings_gene_names.append(adata.var_names[global_indices])\n        if compute_distribution:\n            mask = groups_masks[igroup]\n            for gene_counter in range(n_genes_user):\n                gene_idx = global_indices[gene_counter]\n                X_col = X[mask, gene_idx]\n                if issparse(X): X_col = X_col.toarray()[:, 0]\n                identifier = _build_identifier(group_key, groups_order[igroup],\n                                               gene_counter, adata.var_names[gene_idx])\n                full_col = np.empty(adata.n_smps)\n                full_col[:] = np.nan\n                full_col[mask] = (X_col - mean_rest[gene_idx])/denominator[gene_idx]\n                adata.smp[identifier] = full_col\n    adata.add['rank_genes_groups_gene_scores'] = np.rec.fromarrays(\n        [n for n in rankings_gene_zscores],\n        dtype=[(rn, 'float32') for rn in groups_order])\n    adata.add['rank_genes_groups_gene_names'] = np.rec.fromarrays(\n        [n for n in rankings_gene_names],\n        dtype=[(rn, 'U50') for rn in groups_order])\n    logg.m('    finished', t=True, end=' ')\n    logg.m('and added\\n'\n           '    \"rank_genes_groups_gene_names\", np.recarray to be indexed by the `groups` (adata.add)\\n'\n           '    \"rank_genes_groups_gene_zscores\", the scores (adata.add)\\n'\n           '    \"rank_genes_...\", distributions of top-ranked genes (adata.smp)')\n    return adata if copy else None", "idx": 1250}
{"project": "Scanpy", "commit_id": "587_scanpy_1.0.4_scanpy_plotting_tools_paga.py__paga_graph.py", "target": 1, "func": "def _paga_graph(\n        adata,\n        ax,\n        solid_edges=None,\n        dashed_edges=None,\n        threshold_solid=None,\n        threshold_dashed=1e-6,\n        root=0,\n        rootlevel=None,\n        color=None,\n        groups=None,\n        fontsize=None,\n        node_size_scale=1,\n        node_size_power=0.5,\n        edge_width_scale=1,\n        title=None,\n        layout=None,\n        pos=None,\n        cmap=None,\n        frameon=True,\n        min_edge_width=None,\n        max_edge_width=None,\n        export_to_gexf=False,\n        random_state=0):\n    node_labels = groups\n    if (node_labels is not None\n        and isinstance(node_labels, str)\n        and node_labels != adata.uns['paga']['groups']):\n        raise ValueError('Provide a list of group labels for the PAGA groups {}, not {}.'\n                         .format(adata.uns['paga']['groups'], node_labels))\n    groups_key = adata.uns['paga']['groups']\n    if node_labels is None:\n        node_labels = adata.obs[groups_key].cat.categories\n    if color is None and groups_key is not None:\n        if (groups_key + '_colors' not in adata.uns\n            or len(adata.obs[groups_key].cat.categories)\n               != len(adata.uns[groups_key + '_colors'])):\n            utils.add_colors_for_categorical_sample_annotation(adata, groups_key)\n        color = adata.uns[groups_key + '_colors']\n        for iname, name in enumerate(adata.obs[groups_key].cat.categories):\n            if name in settings.categories_to_ignore: color[iname] = 'grey'\n    if isinstance(root, str):\n        if root in node_labels:\n            root = list(node_labels).index(root)\n        else:\n            raise ValueError(\n                'If `root` is a string, it needs to be one of {} not \\'{}\\'.'\n                .format(node_labels.tolist(), root))\n    if isinstance(root, list) and root[0] in node_labels:\n        root = [list(node_labels).index(r) for r in root]\n\n    # define the objects\n    adjacency_solid = adata.uns['paga'][solid_edges].copy()\n    if threshold_solid is not None:\n        adjacency_solid[adjacency_solid < threshold_solid] = 0\n    nx_g_solid = nx.Graph(adjacency_solid)\n    if dashed_edges is not None:\n        adjacency_dashed = adata.uns['paga'][dashed_edges].copy()\n        if threshold_dashed is not None:\n            adjacency_dashed[adjacency_dashed < threshold_dashed] = 0\n        nx_g_dashed = nx.Graph(adjacency_dashed)\n    # degree of the graph for coloring\n    if isinstance(color, str) and color.startswith('degree'):\n        # see also tools.paga.paga_degrees\n        if color == 'degree_dashed':\n            color = [d for _, d in nx_g_dashed.degree(weight='weight')]\n        elif color == 'degree_solid':\n            color = [d for _, d in nx_g_solid.degree(weight='weight')]\n        else:\n            raise ValueError('`degree` either \"degree_dashed\" or \"degree_solid\".')\n        color = (np.array(color) - np.min(color)) / (np.max(color) - np.min(color))\n    # plot numeric colors\n    colorbar = False\n    if isinstance(color, (list, np.ndarray)) and not isinstance(color[0], (str, dict)):\n        import matplotlib\n        norm = matplotlib.colors.Normalize()\n        color = norm(color)\n        if cmap is None: cmap = rcParams['image.cmap']\n        cmap = matplotlib.cm.get_cmap(cmap)\n        color = [cmap(c) for c in color]\n        colorbar = True\n    if len(color) < len(node_labels):\n        print(node_labels, color)\n        raise ValueError('`color` list need to be at least as long as `node_labels` list.')\n    # node positions from adjacency_solid\n    if pos is None:\n        if layout is None:\n            layout = 'fr'\n        # igraph layouts\n        if layout != 'eq_tree':\n            from ... import utils as sc_utils\n            adj_solid_weights = adjacency_solid\n            g = sc_utils.get_igraph_from_adjacency(adj_solid_weights)\n            if 'rt' in layout:\n                g_tree = g\n                if solid_edges != 'confidence_tree':\n                    adj_tree = adata.uns['paga']['confidence_tree']\n                    g_tree = sc_utils.get_igraph_from_adjacency(adj_tree)\n                pos_list = g_tree.layout(\n                    layout, root=root if isinstance(root, list) else [root],\n                    rootlevel=rootlevel).coords\n            elif layout == 'circle':\n                pos_list = g.layout(layout).coords\n            else:\n                np.random.seed(random_state)\n                init_coords = np.random.random((adjacency_solid.shape[0], 2)).tolist()\n                pos_list = g.layout(layout, seed=init_coords, weights='weight').coords\n            pos = {n: [p[0], -p[1]] for n, p in enumerate(pos_list)}\n        # equally-spaced tree\n        else:\n            nx_g_tree = nx_g_solid\n            if solid_edges != 'confidence_tree':\n                adj_tree = adata.uns['paga']['confidence_tree']\n                nx_g_tree = nx.Graph(adj_tree)\n            pos = utils.hierarchy_pos(nx_g_tree, root)\n            if len(pos) < adjacency_solid.shape[0]:\n                raise ValueError('This is a forest and not a single tree. '\n                                 'Try another `layout`, e.g., {\\'fr\\'}.')\n        pos_array = np.array([pos[n] for count, n in enumerate(nx_g_solid)])\n    else:\n        if isinstance(pos, str):\n            if not pos.endswith('.gdf'):\n                raise ValueError('Currently only supporting reading positions from .gdf files.'\n                                 'Consider generating them using, for instance, Gephi.')\n            s = ''  # read the node definition from the file\n            with open(pos) as f:\n                f.readline()\n                for line in f:\n                    if line.startswith('edgedef>'):\n                        break\n                    s += line\n            from io import StringIO\n            df = pd.read_csv(StringIO(s), header=-1)\n            pos = df[[4, 5]].values\n        pos_array = pos\n        # convert to dictionary\n        pos = {n: [p[0], p[1]] for n, p in enumerate(pos)}\n    if len(pos) == 1: pos[0] = (0.5, 0.5)\n    # edge widths\n    base_edge_width = edge_width_scale * 5 * rcParams['lines.linewidth']\n    # draw dashed edges\n    if dashed_edges is not None:\n        widths = [x[-1]['weight'] for x in nx_g_dashed.edges(data=True)]\n        widths = base_edge_width * np.array(widths)\n        if max_edge_width is not None:\n            widths = np.clip(widths, None, max_edge_width)\n        nx.draw_networkx_edges(nx_g_dashed, pos, ax=ax, width=widths, edge_color='grey',\n                               style='dashed', alpha=0.5)\n    # draw solid edges\n    widths = [x[-1]['weight'] for x in nx_g_solid.edges(data=True)]\n    widths = base_edge_width * np.array(widths)\n    if min_edge_width is not None or max_edge_width is not None:\n        widths = np.clip(widths, min_edge_width, max_edge_width)\n    nx.draw_networkx_edges(nx_g_solid, pos, ax=ax, width=widths, edge_color='black')\n    if export_to_gexf:\n        for count, n in enumerate(nx_g_solid.nodes()):\n            nx_g_solid.node[count]['label'] = node_labels[count]\n            nx_g_solid.node[count]['color'] = color[count]\n            nx_g_solid.node[count]['viz'] = {\n                'position': {'x': 1000*pos[count][0],\n                             'y': 1000*pos[count][1],\n                             'z': 0}}\n        logg.msg('exporting to {}'.format(settings.writedir + 'paga_graph.gexf'), v=1)\n        nx.write_gexf(nx_g_solid, settings.writedir + 'paga_graph.gexf')\n    # deal with empty graph\n    # ax.plot(pos_array[:, 0], pos_array[:, 1], '.', c='white')\n    # draw the nodes (pie charts)\n    trans = ax.transData.transform\n    bbox = ax.get_position().get_points()\n    ax_x_min = bbox[0, 0]\n    ax_x_max = bbox[1, 0]\n    ax_y_min = bbox[0, 1]\n    ax_y_max = bbox[1, 1]\n    ax_len_x = ax_x_max - ax_x_min\n    ax_len_y = ax_y_max - ax_y_min\n    # print([ax_x_min, ax_x_max, ax_y_min, ax_y_max])\n    # print([ax_len_x, ax_len_y])\n    trans2 = ax.transAxes.inverted().transform\n    ax.set_frame_on(frameon)\n    ax.set_xticks([])\n    ax.set_yticks([])\n    if (groups_key is not None and groups_key + '_sizes' in adata.uns):\n        groups_sizes = adata.uns[groups_key + '_sizes']\n    else:\n        groups_sizes = np.ones(len(node_labels))\n    base_scale_scatter = 2000\n    base_pie_size = (base_scale_scatter / (np.sqrt(adjacency_solid.shape[0]) + 10)\n                     * node_size_scale)\n    median_group_size = np.median(groups_sizes)\n    groups_sizes = base_pie_size * np.power(\n        groups_sizes / median_group_size, node_size_power)\n    # usual scatter plot\n    if is_color_like(color[0]):\n        ax.scatter(pos_array[:, 0], pos_array[:, 1],\n                   c=color, edgecolors='face', s=groups_sizes)\n        for count, group in enumerate(node_labels):\n            ax.text(pos_array[count, 0], pos_array[count, 1], group,\n                verticalalignment='center',\n                horizontalalignment='center', size=fontsize)\n    # else pie chart plot\n    else:\n        force_labels_to_front = True  # TODO: solve this differently!\n        for count, n in enumerate(nx_g_solid.nodes()):\n            pie_size = groups_sizes[count] / base_scale_scatter\n            xx, yy = trans(pos[n])     # data coordinates\n            xa, ya = trans2((xx, yy))  # axis coordinates\n            xa = ax_x_min + (xa - pie_size/2) * ax_len_x\n            ya = ax_y_min + (ya - pie_size/2) * ax_len_y\n            # clip, the fruchterman layout sometimes places below figure\n            if ya < 0: ya = 0\n            if xa < 0: xa = 0\n            a = pl.axes([xa, ya, pie_size * ax_len_x, pie_size * ax_len_y])\n            if not isinstance(color[count], dict):\n                raise ValueError('{} is neither a dict of valid matplotlib colors '\n                                 'nor a valid matplotlib color.'.format(color[count]))\n            color_single = color[count].keys()\n            fracs = [color[count][c] for c in color_single]\n            if sum(fracs) < 1:\n                color_single = list(color_single)\n                color_single.append('grey')\n                fracs.append(1-sum(fracs))\n            a.pie(fracs, colors=color_single)\n            if not force_labels_to_front and node_labels is not None:\n                a.text(0.5, 0.5, node_labels[count],\n                       verticalalignment='center',\n                       horizontalalignment='center',\n                       transform=a.transAxes,\n                       size=fontsize)\n        # TODO: this is a terrible hack, but if we use the solution above (`not\n        # force_labels_to_front`), labels get hidden behind pies\n        if force_labels_to_front and node_labels is not None:\n            for count, n in enumerate(nx_g_solid.nodes()):\n                pie_size = groups_sizes[count] / base_scale_scatter\n                # all copy and paste from above\n                xx, yy = trans(pos[n])     # data coordinates\n                xa, ya = trans2((xx, yy))  # axis coordinates\n                # make sure a new axis is created\n                xa = ax_x_min + (xa - pie_size/2.0000001) * ax_len_x\n                ya = ax_y_min + (ya - pie_size/2.0000001) * ax_len_y\n                # clip, the fruchterman layout sometimes places below figure\n                if ya < 0: ya = 0\n                if xa < 0: xa = 0\n                a = pl.axes([xa, ya, pie_size * ax_len_x, pie_size * ax_len_y])\n                a.set_frame_on(False)\n                a.set_xticks([])\n                a.set_yticks([])\n                a.text(0.5, 0.5, node_labels[count],\n                       verticalalignment='center',\n                       horizontalalignment='center',\n                       transform=a.transAxes, size=fontsize)\n    if title is not None: ax.set_title(title)\n    if colorbar:\n        ax1 = pl.axes([0.95, 0.1, 0.03, 0.7])\n        cb = matplotlib.colorbar.ColorbarBase(ax1, cmap=cmap,\n                                              norm=norm)\n    return pos_array", "idx": 1251}
{"project": "Scanpy", "commit_id": "367_scanpy_1.9.0_scatterplots.py__check_crop_coord.py", "target": 0, "func": "def _check_crop_coord(\n    crop_coord: Optional[tuple],\n    scale_factor: float,\n) -> Tuple[float, float, float, float]:\n    \"\"\"Handle cropping with image or basis.\"\"\"\n    if crop_coord is None:\n        return None\n    if len(crop_coord) != 4:\n        raise ValueError(\"Invalid crop_coord of length {len(crop_coord)}(!=4)\")\n    crop_coord = tuple(c * scale_factor for c in crop_coord)\n    return crop_coord", "idx": 1264}
{"project": "Scanpy", "commit_id": "741_scanpy_1.9.0_test_score_genes.py_test_sparse_nanmean_on_dense_matrix.py", "target": 0, "func": "def test_sparse_nanmean_on_dense_matrix():\n    \"\"\"\n    TypeError must be thrown when calling _sparse_nanmean with a dense matrix\n    \"\"\"\n    from scanpy.tools._score_genes import _sparse_nanmean\n\n    with pytest.raises(TypeError):\n        _sparse_nanmean(np.random.rand(4, 5), 0)", "idx": 1271}
{"project": "Scanpy", "commit_id": "610_scanpy_1.0.4_scanpy_plotting_tools_paga.py_paga.py", "target": 1, "func": "def paga(\n        adata,\n        layout=None,\n        init_pos=None,\n        root=0,\n        groups=None,\n        color=None,\n        threshold=None,\n        solid_edges='confidence',\n        dashed_edges=None,\n        transitions=None,\n        threshold_arrows=None,\n        threshold_solid=None,\n        threshold_dashed=None,\n        fontsize=None,\n        text_kwds={},\n        node_size_scale=1,\n        node_size_power=0.5,\n        edge_width_scale=1,\n        min_edge_width=None,\n        max_edge_width=None,\n        title='abstracted graph',\n        left_margin=0.01,\n        random_state=0,\n        pos=None,\n        cmap=None,\n        cax=None,\n        cb_kwds={},\n        frameon=True,\n        add_pos=True,\n        export_to_gexf=False,\n        show=None,\n        save=None,\n        ax=None,\n        **kwds):\n    \"\"\"Plot the abstracted graph.\n    This uses igraph's layout algorithms for most layouts [Csardi06]_.\n    When initializing the positions, note that igraph mirrors coordinates along\n    the x axis... that is, you should increase the `maxiter` parameter by 1 if\n    the layout is flipped.\n    Parameters\n    ----------\n    adata : :class:`~scanpy.api.AnnData`\n        Annotated data matrix.\n    layout : {'fr', 'rt', 'rt_circular', 'eq_tree', ...}, optional (default: 'fr')\n        Plotting layout. 'fr' stands for Fruchterman-Reingold, 'rt' stands for\n        Reingold Tilford. 'eq_tree' stands for 'eqally spaced tree'. All but\n        'eq_tree' use the igraph layout function. All other igraph layouts are\n        also permitted. See also parameter `pos`.\n    init_pos : `np.ndarray`, optional (default: `None`)\n        Two-column array/list storing the x and y coordinates for initializing\n        the layout.\n    random_state : `int` or `None`, optional (default: 0)\n        For layouts with random initialization like 'fr', change this to use\n        different intial states for the optimization. If `None`, the initial\n        state is not reproducible.\n    root : int, str or list of int, optional (default: 0)\n        If choosing a tree layout, this is the index of the root node or root\n        nodes. If this is a non-empty vector then the supplied node IDs are used\n        as the roots of the trees (or a single tree if the graph is\n        connected. If this is `None` or an empty list, the root vertices are\n        automatically calculated based on topological sorting.\n    groups : `str`, `list`, `dict`\n        The node (groups) labels.\n    color : color string or iterable, {'degree_dashed', 'degree_solid'}, optional (default: None)\n        The node colors.  Besides cluster colors, lists and uniform colors this\n        also acceppts {'degree_dashed', 'degree_solid'} which are plotted using\n        continuous color map.\n    threshold : `float` or `None`, optional (default: 0.01)\n        Do not draw edges for weights below this threshold. Set to `None` if you\n        want all edges.\n    solid_edges : `str`, optional (default: 'paga_confidence')\n        Key for `.uns['paga']` that specifies the matrix that stores the edges\n        to be drawn solid black.\n    dashed_edges : `str` or `None`, optional (default: `None`)\n        Key for `.uns['paga']` that specifies the matrix that stores the edges\n        to be drawn dashed grey. If `None`, no dashed edges are drawn.\n    threshold_solid : `float` or `None`, optional (default: `threshold`)\n        Do not draw edges for weights below this threshold. Set to `None` if you\n        want all edges.\n    threshold_dashed : `float` or `None`, optional (default: `threshold`)\n        Do not draw edges for weights below this threshold. Set to `None` if you\n        want all edges.\n    fontsize : int (default: None)\n        Font size for node labels.\n    text_kwds : keywords for text\n        See `here\n        <https://matplotlib.org/api/_as_gen/matplotlib.axes.Axes.text.html#matplotlib.axes.Axes.text>`_.\n    node_size_scale : float (default: 1.0)\n        Increase or decrease the size of the nodes.\n    node_size_power : float (default: 0.5)\n        The power with which groups sizes influence the radius of the nodes.\n    edge_width_scale : `float`, optional (default: 5)\n        Edge with scale in units of `rcParams['lines.linewidth']`.\n    min_edge_width : `float`, optional (default: `None`)\n        Min width of solid edges.\n    max_edge_width : `float`, optional (default: `None`)\n        Max width of solid and dashed edges.\n    pos : `np.ndarray`, filename of `.gdf` file,  optional (default: `None`)\n        Two-column array/list storing the x and y coordinates for drawing.\n        Otherwise, path to a `.gdf` file that has been exported from Gephi or\n        a similar graph visualization software.\n    export_to_gexf : `bool`, optional (default: `None`)\n        Export to gexf format to be read by graph visualization programs such as\n        Gephi.\n    cmap : color map\n        The color map.\n    cax : `matplotlib.Axes`\n        A matplotlib axes object for a potential colorbar.\n    cb_kwds : colorbar keywords\n        See `here\n        <https://matplotlib.org/api/colorbar_api.html#matplotlib.colorbar.ColorbarBase>`_,\n        for instance, `ticks`.\n    add_pos : `bool`, optional (default: `True`)\n        Add the positions to `adata.uns['paga']`.\n    title : `str`, optional (default: `None`)\n         Provide title for panels either as `['title1', 'title2', ...]` or\n         `'title1,title2,...'`.\n    frameon : `bool`, optional (default: `True`)\n         Draw a frame around the abstracted graph.\n    show : `bool`, optional (default: `None`)\n         Show the plot, do not return axis.\n    save : `bool` or `str`, optional (default: `None`)\n        If `True` or a `str`, save the figure. A string is appended to the\n        default filename. Infer the filetype if ending on \\{'.pdf', '.png', '.svg'\\}.\n    ax : `matplotlib.Axes`\n         A matplotlib axes object.\n    Returns\n    -------\n    Adds `'pos'` to `adata.uns['paga']` if `add_pos` is `True`.\n    If `show==False`, one or more `matplotlib.Axis` objects. If at least one colorbar is\n    drawn, a list with colorbar instances will be returned, too.\n    \"\"\"\n    # colors is a list that contains no lists\n    if isinstance(color, Iterable) and all([not isinstance(c, Iterable) for c in color]): color = [color]\n    if color is None or isinstance(color, str): color = [color]\n\n    # groups is a list that contains no lists\n    if isinstance(groups, list) and True not in [isinstance(g, list) for g in groups]: groups = [groups]\n    if groups is None or isinstance(groups, dict) or isinstance(groups, str): groups = [groups]\n\n    if title is None or isinstance(title, str): title = [title for name in groups]\n\n    if ax is None:\n        axs, _, _, _ = utils.setup_axes(panels=color)\n    else:\n        axs = ax\n    if len(color) == 1 and not isinstance(axs, list): axs = [axs]\n\n    cbs = []\n    for icolor, c in enumerate(color):\n        pos, cb = _paga_graph(\n            adata,\n            axs[icolor],\n            layout=layout,\n            init_pos=init_pos,\n            solid_edges=solid_edges,\n            dashed_edges=dashed_edges,\n            transitions=transitions,\n            threshold=threshold,\n            threshold_arrows=threshold_arrows,\n            threshold_solid=threshold_solid,\n            threshold_dashed=threshold_dashed,\n            root=root,\n            color=c,\n            groups=groups[icolor],\n            fontsize=fontsize,\n            text_kwds=text_kwds,\n            node_size_scale=node_size_scale,\n            node_size_power=node_size_power,\n            edge_width_scale=edge_width_scale,\n            min_edge_width=min_edge_width,\n            max_edge_width=max_edge_width,\n            frameon=frameon,\n            cmap=cmap,\n            cax=cax,\n            cb_kwds=cb_kwds,\n            title=title[icolor],\n            random_state=random_state,\n            export_to_gexf=export_to_gexf,\n            pos=pos,\n            **kwds)\n        if cb is not None:\n            cbs.append(cb)\n    if add_pos:\n        adata.uns['paga']['pos'] = pos\n        logg.hint('added \\'pos\\', the PAGA positions (adata.uns[\\'paga\\'])')\n    utils.savefig_or_show('paga_graph', show=show, save=save)\n    if len(color) == 1 and isinstance(axs, list): axs = axs[0]\n    return axs if show == False else None", "idx": 1272}
{"project": "Scanpy", "commit_id": "36_scanpy_0.0_scanpy_exs_builtin_alex.py_paul15pca.py", "target": 1, "func": "def paul15pca():\n    ddata = paul15_raw()\n    ddata['X'] = sc.pp(ddata['X'], 'log')\n    # reduce to 50 components\n    ddata['Xpca'] = sc.pca(ddata['X'])\n    # adjust expression vector of root cell\n    ddata['xroot'] = ddata['Xpca'][ddata['iroot']]\n    return ddata", "idx": 1279}
{"project": "Scanpy", "commit_id": "283_scanpy_0.1_scanpy_tools_ega.py_do_split_constrained.py", "target": 1, "func": "def do_split_constrained(self, segs, segs_tips, segs_adjacency,\n                         segs_distances):\n    isegs = np.argsort([len(seg) for seg in segs])[::-1]\n    if len(segs[isegs[0]]) < self.min_group_size:\n        return True, segs_distances\n    for iseg in isegs:\n        seg = segs[iseg]\n        logg.info('... splitting group {} with size {}'.format(iseg, len(seg)))\n        jsegs = [jseg for jseg in range(len(segs)) if jseg != iseg]\n        dtip = np.zeros(len(seg))\n        for jseg in jsegs:\n            if len(segs_tips[jseg]) > 0:\n                jtip = segs_tips[jseg][0]\n                # print('    condition', jseg, ':', jtip)\n                dtip += self.Dchosen[jtip, seg]\n        if len(jsegs) > 0: dtip /= len(jsegs)\n        if len(segs_tips[iseg]) > 0:\n            itip = segs_tips[iseg][0]\n            # print('    within seg tip', itip)\n            dtip += self.Dchosen[itip, seg]\n        new_itip = np.argmax(dtip)\n        new_seg = np.ones(len(seg), dtype=bool)\n        for jseg in range(len(segs)):\n            if len(segs_tips[jseg]) > 0:\n                jtip = segs_tips[jseg][0]\n                # print('    condition', jseg, ':', jtip)\n                closer_to_jtip_than_to_new_itip = self.Dchosen[jtip, seg] < self.Dchosen[seg[new_itip], seg]\n                new_seg[closer_to_jtip_than_to_new_itip] = False\n        ssegs = [new_seg, ~new_seg]\n        ssegs_tips = [[new_itip], []]\n        l = len(np.flatnonzero(ssegs[0]))\n        if l != 0 and len(seg) - l != 0:\n            break\n    logg.info('        obtained two groups with sizes {} and {}'\n              .format(l, len(seg) - l))\n    trunk = 1\n    # map back to global indices\n    # print('        tips of split seg before map', ssegs_tips)\n    for iseg_new, seg_new in enumerate(ssegs):\n        ssegs[iseg_new] = seg[seg_new]\n        ssegs_tips[iseg_new] = seg[ssegs_tips[iseg_new]]\n    # add iseg tip\n    if len(segs_tips[iseg]) > 0: ssegs_tips[trunk] = [segs_tips[iseg][0]]\n    # print('        tips of split seg', ssegs_tips)\n    # remove previous segment\n    segs.pop(iseg)\n    segs_tips.pop(iseg)\n    # insert trunk at same position\n    segs.insert(iseg, ssegs[trunk])\n    segs_tips.insert(iseg, ssegs_tips[trunk])\n    # append other segments\n    segs += [seg for iseg, seg in enumerate(ssegs) if iseg != trunk]\n    segs_tips += [seg_tips for iseg, seg_tips in enumerate(ssegs_tips) if iseg != trunk]\n    # correct edges in adjacency matrix\n    n_add = len(ssegs) - 1\n    new_shape = (segs_distances.shape[0] + n_add, segs_distances.shape[1] + n_add)\n    # segs_distances.resize() throws an error!\n    segs_distances_help = segs_distances.copy()\n    segs_distances = np.zeros((new_shape))\n    segs_distances[np.ix_(range(segs_distances_help.shape[0]),\n                          range(segs_distances_help.shape[1]))] = segs_distances_help\n    segs_distances = self.adjust_adjacency(iseg, n_add,\n                                           segs,\n                                           segs_tips,\n                                           segs_adjacency,\n                                           segs_distances)\n    return False, segs_distances", "idx": 1281}
{"project": "Scanpy", "commit_id": "37_scanpy_0.0_scanpy_graph.py_compute_Ddiff_matrix.py", "target": 1, "func": "def compute_Ddiff_matrix(self):\n    \"\"\"\n    Returns the distance matrix in the Diffusion Pseudotime metric.\n    See Haghverdi et al. (2016).\n\n    Notes\n    -----\n    - Is based on the M matrix.\n    - self.Ddiff[self.iroot,:] stores diffusion pseudotime as a vector.\n    \"\"\"\n    if self.M.shape[0] > 1000 and self.params['nr_pcs'] == 0:\n        sett.m(0, '--> high number of dimensions for computing DPT distance matrix\\n'\n                  '    by setting nr_pcs > 0 you can speed up the computation')\n    if self.params['nr_pcs'] > 0:\n        import scanpy.preprocess as pp\n        self.M = pp(self.M, 'pca', nr_comps=self.params['nr_pcs'])\n    self.Ddiff = sp.spatial.distance.pdist(self.M)\n    self.Ddiff = sp.spatial.distance.squareform(self.Ddiff)\n    sett.mt(0, 'computed Ddiff distance matrix')\n    self.Dchosen = self.Ddiff", "idx": 1284}
{"project": "Scanpy", "commit_id": "519_scanpy_1.9.0_test_embedding_plots.py_test_missing_values_continuous.py", "target": 0, "func": "def test_missing_values_continuous(\n    fixture_request, image_comparer, adata, plotfunc, na_color, legend_loc, vbounds\n):\n    save_and_compare_images = image_comparer(\n        MISSING_VALUES_ROOT, MISSING_VALUES_FIGS, tol=15\n    )\n    base_name = fixture_request.node.name\n\n    # Passing through a dict so it's easier to use default values\n    kwargs = {}\n    kwargs.update(vbounds)\n    kwargs[\"legend_loc\"] = legend_loc\n    if na_color is not None:\n        kwargs[\"na_color\"] = na_color\n\n    plotfunc(adata, color=[\"1\", \"1_missing\"], **kwargs)\n\n    save_and_compare_images(base_name)", "idx": 1291}
{"project": "Scanpy", "commit_id": "401_scanpy_0.2.8_scanpy_plotting_tools.py__aga_graph.py", "target": 1, "func": "def _aga_graph(\n        adata,\n        ax,\n        solid_edges=None,\n        dashed_edges=None,\n        root=0,\n        rootlevel=None,\n        color=None,\n        groups=None,\n        fontsize=None,\n        node_size_scale=1,\n        node_size_power=0.5,\n        edge_width_scale=1,\n        title=None,\n        layout=None,\n        pos=None,\n        cmap=None,\n        frameon=False,\n        min_edge_width=None,\n        max_edge_width=None,\n        random_state=0):\n    if groups is not None and isinstance(groups, str) and groups not in adata.smp_keys():\n        raise ValueError('Groups {} are not in adata.smp.'.format(groups))\n    groups_name = groups if isinstance(groups, str) else None\n    if groups is None and 'aga_groups_order_original' in adata.add:\n        groups = adata.add['aga_groups_order_original']\n        groups_name = adata.add['aga_groups_original']\n    elif groups in adata.smp_keys():\n        groups = adata.add[groups + '_order']\n    elif groups is None:\n        groups = adata.add['aga_groups_order']\n        groups_name = 'aga_groups'\n\n    if color is None and groups_name is not None:\n        if groups_name == adata.add['aga_groups_original']:\n            color = adata.add['aga_groups_colors_original']\n        else:\n            if (groups_name + '_colors' not in adata.add\n                or len(adata.add[groups_name + '_order'])\n                   != len(adata.add[groups_name + '_colors'])):\n                utils.add_colors_for_categorical_sample_annotation(adata, groups_name)\n            color = adata.add[groups_name + '_colors']\n        for iname, name in enumerate(adata.add[groups_name + '_order']):\n            if name in settings._ignore_categories: color[iname] = 'grey'\n    if isinstance(root, str) and root in groups:\n        root = list(groups).index(root)\n    if isinstance(root, list) and root[0] in groups:\n        root = [list(groups).index(r) for r in root]\n    # define the objects\n    adjacency_solid = adata.add[solid_edges]\n    nx_g_solid = nx.Graph(adjacency_solid)\n    if dashed_edges is not None:\n        adjacency_dashed = adata.add[dashed_edges]\n        nx_g_dashed = nx.Graph(adjacency_dashed)\n    # degree of the graph for coloring\n    if isinstance(color, str) and color.startswith('degree'):\n        # see also tools.aga.aga_degrees\n        if color == 'degree_dashed':\n            color = [d for _, d in nx_g_dashed.degree_iter(weight='weight')]\n        elif color == 'degree_solid':\n            color = [d for _, d in nx_g_solid.degree_iter(weight='weight')]\n        else:\n            raise ValueError('`degree` either \"degree_dashed\" or \"degree_solid\".')\n        color = (np.array(color) - np.min(color)) / (np.max(color) - np.min(color))\n    # plot numeric colors\n    colorbar = False\n    if isinstance(color, (list, np.ndarray)) and not isinstance(color[0], (str, dict)):\n        import matplotlib\n        norm = matplotlib.colors.Normalize()\n        color = norm(color)\n        if cmap is None: cmap = rcParams['image.cmap']\n        cmap = matplotlib.cm.get_cmap(cmap)\n        color = [cmap(c) for c in color]\n        colorbar = True\n    if len(color) < len(groups):\n        print(groups, color)\n        raise ValueError('`color` list need to be at least as long as `groups` list.')\n    # node positions from adjacency_solid\n    if pos is None:\n        if layout is None:\n            layout = 'fr'\n        # igraph layouts\n        if layout != 'eq_tree':\n            from .. import utils as sc_utils\n            g = sc_utils.get_igraph_from_adjacency(adjacency_solid)\n            if 'rt' in layout:\n                pos_list = g.layout(layout, root=root if isinstance(root, list) else [root],\n                                    rootlevel=rootlevel).coords\n            elif layout == 'circle':\n                pos_list = g.layout(layout).coords\n            else:\n                np.random.seed(random_state)\n                init_coords = np.random.random((adjacency_solid.shape[0], 2)).tolist()\n                pos_list = g.layout(layout, seed=init_coords).coords\n            pos = {n: [p[0], -p[1]] for n, p in enumerate(pos_list)}\n        # equally-spaced tree\n        else:\n            pos = utils.hierarchy_pos(nx_g_solid, root)\n            if len(pos) < adjacency_solid.shape[0]:\n                raise ValueError('This is a forest and not a single tree. '\n                                 'Try another `layout`, e.g., {\\'fr\\'}.')\n        pos_array = np.array([pos[n] for count, n in enumerate(nx_g_solid)])\n    else:\n        pos_array = pos\n        # convert to dictionary\n        pos = {n: [p[0], p[1]] for n, p in enumerate(pos)}\n    if len(pos) == 1: pos[0] = (0.5, 0.5)\n    # edge widths\n    base_edge_width = edge_width_scale * rcParams['lines.linewidth']\n    # draw dashed edges\n    if dashed_edges is not None:\n        widths = [x[-1]['weight'] for x in nx_g_dashed.edges(data=True)]\n        widths = base_edge_width * np.array(widths)\n        if max_edge_width is not None:\n            widths = np.clip(widths, None, max_edge_width)\n        nx.draw_networkx_edges(nx_g_dashed, pos, ax=ax, width=widths, edge_color='grey',\n                               style='dashed', alpha=0.5)\n    # draw solid edges\n    widths = [x[-1]['weight'] for x in nx_g_solid.edges(data=True)]\n    widths = base_edge_width * np.array(widths)\n    if min_edge_width is not None or max_edge_width is not None:\n        widths = np.clip(widths, min_edge_width, max_edge_width)\n    nx.draw_networkx_edges(nx_g_solid, pos, ax=ax, width=widths, edge_color='black')\n    # deal with empty graph\n    ax.plot(pos_array[:, 0], pos_array[:, 1], '.', c='white')\n    # draw the nodes (pie charts)\n    trans = ax.transData.transform\n    bbox = ax.get_position().get_points()\n    ax_x_min = bbox[0, 0]\n    ax_x_max = bbox[1, 0]\n    ax_y_min = bbox[0, 1]\n    ax_y_max = bbox[1, 1]\n    ax_len_x = ax_x_max - ax_x_min\n    ax_len_y = ax_y_max - ax_y_min\n    # print([ax_x_min, ax_x_max, ax_y_min, ax_y_max])\n    # print([ax_len_x, ax_len_y])\n    trans2 = ax.transAxes.inverted().transform\n    ax.set_frame_on(frameon)\n    ax.set_xticks([])\n    ax.set_yticks([])\n    base_pie_size = 1/(np.sqrt(adjacency_solid.shape[0]) + 10) * node_size_scale\n    if (groups_name is not None and groups_name in adata.add):\n        groups_sizes = adata.add[groups_name + '_sizes']\n    elif 'aga_groups_sizes' in adata.add:\n        groups_sizes = adata.add['aga_groups_sizes']\n    else:\n        groups_sizes = np.ones(len(groups))\n    median_group_size = np.median(groups_sizes)\n    force_labels_to_front = True  # TODO: solve this differently!\n    for count, n in enumerate(nx_g_solid.nodes_iter()):\n        pie_size = base_pie_size\n        pie_size *= np.power(groups_sizes[count] / median_group_size,\n                             node_size_power)\n        xx, yy = trans(pos[n])     # data coordinates\n        xa, ya = trans2((xx, yy))  # axis coordinates\n        xa = ax_x_min + (xa - pie_size/2) * ax_len_x\n        ya = ax_y_min + (ya - pie_size/2) * ax_len_y\n        if ya < 0: ya = 0  # clip, the fruchterman layout sometimes places below figure\n        if xa < 0: xa = 0\n        a = pl.axes([xa, ya, pie_size * ax_len_x, pie_size * ax_len_y])\n        if is_color_like(color[count]):\n            fracs = [100]\n            color_single = [color[count]]\n        elif isinstance(color[count], dict):\n            color_single = color[count].keys()\n            fracs = [color[count][c] for c in color_single]\n            if sum(fracs) < 1:\n                color_single = list(color_single)\n                color_single.append('grey')\n                fracs.append(1-sum(fracs))\n        else:\n            raise ValueError('{} is neither a dict of valid matplotlib colors '\n                             'nor a valid matplotlib color.'.format(color[count]))\n        a.pie(fracs, colors=color_single)\n        if not force_labels_to_front and groups is not None:\n            a.text(0.5, 0.5, groups[count],\n                   verticalalignment='center',\n                   horizontalalignment='center',\n                   transform=a.transAxes,\n                   size=fontsize)\n    # TODO: this is a terrible hack, but if we use the solution above (``not\n    # force_labels_to_front``), labels get hidden behind pies\n    if force_labels_to_front and groups is not None:\n        for count, n in enumerate(nx_g_solid.nodes_iter()):\n            # all copy and paste from above\n            pie_size = base_pie_size\n            pie_size *= np.power(groups_sizes[count] / median_group_size,\n                                 node_size_power)\n            xx, yy = trans(pos[n])     # data coordinates\n            xa, ya = trans2((xx, yy))  # axis coordinates\n            xa = ax_x_min + (xa - pie_size/2.0000001) * ax_len_x  # make sure a new axis is created\n            ya = ax_y_min + (ya - pie_size/2.0000001) * ax_len_y\n            if ya < 0: ya = 0  # clip, the fruchterman layout sometimes places below figure\n            if xa < 0: xa = 0\n            a = pl.axes([xa, ya, pie_size * ax_len_x, pie_size * ax_len_y])\n            a.set_frame_on(False)\n            a.set_xticks([])\n            a.set_yticks([])\n            a.text(0.5, 0.5, groups[count],\n                   verticalalignment='center',\n                   horizontalalignment='center',\n                   transform=a.transAxes, size=fontsize)\n    if title is not None: ax.set_title(title)\n    if colorbar:\n        ax1 = pl.axes([0.95, 0.1, 0.03, 0.7])\n        cb = matplotlib.colorbar.ColorbarBase(ax1, cmap=cmap,\n                                              norm=norm)\n    return pos_array", "idx": 1295}
{"project": "Scanpy", "commit_id": "83_scanpy_0.0_scanpy_tools_dpt.py_plot.py", "target": 1, "func": "def plot(ddpt, adata, dplot=None,\n         smp=None,\n         comps='1,2',\n         cont=None,\n         layout='2d',\n         legendloc='lower right',\n         cmap=None,\n         adjust_right=0.75):\n    \"\"\"\n    Plot results of DPT analysis.\n    Parameters\n    ----------\n    ddpt : dict\n        Dict returned by DPT tool.\n    dplot : dict\n        Dict returned by plotting tool.\n    adata : AnnData\n        Annotated data matrix.\n    smp : str, optional (default: first annotation)\n        Sample/Cell annotation for coloring in the form \"ann1,ann2,...\". String\n        annotation is plotted assuming categorical annotation, float and integer\n        annotation is plotted assuming continuous annoation. Option 'cont'\n        allows to switch between these default choices.\n    comps : str, optional (default: '1,2')\n         String in the form '1,2,3'.\n    cont : bool, None (default: None)\n        Switch on continuous layout, switch off categorical layout.\n    layout : {'2d', '3d', 'unfolded 3d'}, optional (default: '2d')\n         Layout of plot.\n    legendloc : see matplotlib.legend, optional (default: 'lower right')\n         Options for keyword argument 'loc'.\n    cmap : str (default: continuous: inferno/ categorical: finite palette)\n         String denoting matplotlib color map.\n    adjust_right : float (default: 0.75)\n         Adjust how far the plotting panel extends to the right.\n    \"\"\"\n    adata.smp['pseudotimes'] = ddpt['pseudotimes']\n    adata.smp['segments'] = ddpt['groups']\n    adata['segments_names'] = ddpt['groups_names']\n    adata['segments_ids'] = ddpt['groups_ids']\n    adata['segments_masks'] = ddpt['groups_masks']\n    adata['highlights'] = list(ddpt['iroot'])\n\n    # plot segments and pseudotimes\n    plot_segments_pseudotimes(ddpt, 'inferno' if cmap is None else cmap)\n    # if number of genes is not too high, plot time series\n    from .. import plotting as plott\n    X = adata.X\n    if X.shape[1] <= 11:\n        # plot time series as gene expression vs time\n        plott.timeseries(X[ddpt['indices']], adata.var_names,\n                         highlightsX=ddpt['changepoints'],\n                         xlim=[0, 1.3*X.shape[0]])\n        plott.savefig(ddpt['writekey']+'_vsorder')\n    elif X.shape[1] < 50:\n        # plot time series as heatmap, as in Haghverdi et al. (2016), Fig. 1d\n        plott.timeseries_as_heatmap(X[ddpt['indices'],:40], adata.var_names,\n                                    highlightsX=ddpt['changepoints'])\n        plott.savefig(ddpt['writekey']+'_heatmap')\n    if dplot is not None:\n        ddpt['Y'] = dplot['Y']\n        ddpt['writekey'] += '_' + dplot['type']\n        component_name = ('DC' if dplot['type'] == 'diffmap'\n                          else 'FR' if dplot['type'] == 'drawg'\n                          else 'tSNE' if dplot['type'] == 'tsne'\n                          else 'PC')\n    else:\n        component_name = 'DC'\n        ddpt['writekey'] += '_diffmap'\n\n    plott.plot_tool(ddpt, adata,\n                    smp=['pseudotimes', 'segments', None],\n                    component_name=component_name,\n                    legendloc=legendloc)", "idx": 1296}
{"project": "Scanpy", "commit_id": "299_scanpy_1.9.0__stacked_violin.py__make_rows_of_violinplots.py", "target": 0, "func": "def _make_rows_of_violinplots(\n        self, ax, _matrix, colormap_array, _color_df, x_spacer_size, y_spacer_size\n    ):\n        import seaborn as sns  # Slow import, only import if called\n\n        row_palette = self.kwds.get('color', self.row_palette)\n        if 'color' in self.kwds:\n            del self.kwds['color']\n        if row_palette is not None:\n            if is_color_like(row_palette):\n                row_colors = [row_palette] * _color_df.shape[0]\n            else:\n                row_colors = sns.color_palette(row_palette, n_colors=_color_df.shape[0])\n            # when row_palette is used, there is no need for a legend\n            self.legends_width = 0.0\n        else:\n            row_colors = [None] * _color_df.shape[0]\n\n        # All columns should have a unique name, yet, frequently\n        # gene names are repeated in self.var_names,  otherwise the\n        # violin plot will not distinguish those genes\n        _matrix.columns = [f\"{x}_{idx}\" for idx, x in enumerate(_matrix.columns)]\n\n        # transform the  dataframe into a dataframe having three columns:\n        # the categories name (from groupby),\n        # the gene name\n        # the expression value\n        # This format is convenient to aggregate per gene or per category\n        # while making the violin plots.\n\n        df = (\n            pd.DataFrame(_matrix.stack(dropna=False))\n            .reset_index()\n            .rename(\n                columns={\n                    'level_1': 'genes',\n                    _matrix.index.name: 'categories',\n                    0: 'values',\n                }\n            )\n        )\n        df['genes'] = (\n            df['genes'].astype('category').cat.reorder_categories(_matrix.columns)\n        )\n        df['categories'] = (\n            df['categories']\n            .astype('category')\n            .cat.reorder_categories(_matrix.index.categories)\n        )\n\n        # the ax need to be subdivided\n        # define a layout of nrows = len(categories) rows\n        # each row is one violin plot.\n        num_rows, num_cols = _color_df.shape\n        height_ratios = [y_spacer_size] + [1] * num_rows + [y_spacer_size]\n        width_ratios = [x_spacer_size] + [1] * num_cols + [x_spacer_size]\n\n        fig, gs = make_grid_spec(\n            ax,\n            nrows=num_rows + 2,\n            ncols=num_cols + 2,\n            hspace=0.2 if self.plot_yticklabels else 0,\n            wspace=0,\n            height_ratios=height_ratios,\n            width_ratios=width_ratios,\n        )\n        axs_list = []\n        for idx, row_label in enumerate(_color_df.index):\n\n            row_ax = fig.add_subplot(gs[idx + 1, 1:-1])\n            axs_list.append(row_ax)\n\n            if row_colors[idx] is None:\n                palette_colors = colormap_array[idx, :]\n            else:\n                palette_colors = None\n\n            if not self.are_axes_swapped:\n                x = 'genes'\n                _df = df[df.categories == row_label]\n            else:\n                x = 'categories'\n                # because of the renamed matrix columns here\n                # we need to use this instead of the 'row_label'\n                # (in _color_df the values are not renamed as those\n                # values will be used to label the ticks)\n                _df = df[df.genes == _matrix.columns[idx]]\n\n            row_ax = sns.violinplot(\n                x=x,\n                y='values',\n                data=_df,\n                orient='vertical',\n                ax=row_ax,\n                palette=palette_colors,\n                color=row_colors[idx],\n                **self.kwds,\n            )\n\n            if self.stripplot:\n                row_ax = sns.stripplot(\n                    x=x,\n                    y='values',\n                    data=_df,\n                    jitter=self.jitter,\n                    color='black',\n                    size=self.jitter_size,\n                    ax=row_ax,\n                )\n\n            self._setup_violin_axes_ticks(row_ax, num_cols)", "idx": 1301}
{"project": "Scanpy", "commit_id": "265_scanpy_0.1_scanpy_tools_dpt.py_dpt.py", "target": 1, "func": "def dpt(adata, n_branchings=0, k=30, knn=True, n_pcs=50, n_pcs_post=30, n_dcs=10,\n        allow_branching_at_root=False, n_jobs=None, recompute_diffmap=False,\n        recompute_pca=False, flavor='haghverdi16', copy=False):\n    \"\"\"Hierarchical Diffusion Pseudotime\n    Infer progression of cells, identify branching subgroups.\n    Reference\n    ---------\n    - Diffusion Pseudotime: Haghverdi et al., Nature Methods 13, 3971 (2016).\n    See also\n    --------\n    - Diffusion Maps: Coifman et al., PNAS 102, 7426 (2005).\n    - Diffusion Maps applied to single-cell data: Haghverdi et al., Bioinformatics\n      31, 2989 (2015).\n    - Diffusion Maps as a flavour of spectral clustering: von Luxburg,\n      arXiv:0711.0189 (2007).\n    Parameters\n    ----------\n    adata : AnnData\n        Annotated data matrix, optionally with metadata:\n        adata.add['xroot'] : np.ndarray\n            Root of stochastic process on data points (root cell), specified\n            as expression vector of shape X.shape[1].\n        adata.smp['X_pca']: np.ndarray\n            PCA representation of the data matrix (result of preprocessing with\n            PCA). If it exists in adata, dpt will use this instead of adata.X.\n        adata.smp['X_diffmap']: np.ndarray\n            Diffmap representation of the data matrix (result of running\n            `diffmap`).  Will be used if option `recompute_diffmap` is False.\n    n_branchings : int, optional (default: 1)\n        Number of branchings to detect.\n    k : int, optional (default: 30)\n        Number of nearest neighbors on the knn graph. If knn == False, set the\n        Gaussian kernel width to the distance of the kth neighbor.\n    knn : bool, optional (default: True)\n        If True, use a hard threshold to restrict the number of neighbors to\n        k, that is, consider a knn graph. Otherwise, use a Gaussian Kernel\n        to assign low weights to neighbors more distant than the kth nearest\n        neighbor.\n    n_pcs: int, optional (default: 50)\n        Use n_pcs PCs to compute the Euclidian distance matrix, which is the\n        basis for generating the graph. Set to 0 if you don't want preprocessing\n        with PCA.\n    n_pcs_post: int, optional (default: 30)\n        Use n_pcs_post PCs to compute the DPT distance matrix. This speeds up\n        the computation at almost no loss of accuracy. Set to 0 if you don't\n        want postprocessing with PCA.\n    allow_branching_at_root : bool, optional (default: False)\n        Allow to have branching directly at root point.\n    n_jobs : int or None (default: None)\n        Number of cpus to use for parallel processing (default: sett.n_jobs).\n    recompute_diffmap : bool, (default: False)\n        Recompute diffusion maps.\n    recompute_pca : bool, (default: False)\n        Recompute PCA.\n    copy : bool, optional (default: False)\n        Copy instance before computation and return a copy. Otherwise, perform\n        computation inplace and return None.\n    Notes\n    -----\n    Writes the following arrays as sample annotation to adata.smp.\n        dpt_pseudotime : np.ndarray\n            Array of dim (number of samples) that stores the pseudotime of each\n            cell, that is, the DPT distance with respect to the root cell.\n        dpt_groups : np.ndarray of dtype string\n            Array of dim (number of samples) that stores the subgroup id ('0',\n            '1', ...) for each cell. The groups  typically correspond to\n            'progenitor cells', 'undecided cells' or 'branches' of a process.\n    Writes the following additional arrays as unstructured annotation to adata.\n        X_diffmap : np.ndarray\n            Array of shape (number of samples) x (number of eigen\n            vectors). DiffMap representation of data, which is the right eigen\n            basis of the transition matrix with eigenvectors as columns.\n        dpt_evals : np.ndarray\n            Array of size (number of eigen vectors). Eigenvalues of transition matrix.\n    \"\"\"\n    adata = adata.copy() if copy else adata\n    if 'xroot' not in adata.add and 'xroot' not in adata.var:\n        msg = \\\n   '''DPT requires specifying the expression \"xroot\" of a root cell.\n   Either\n       adata.var['xroot'] = adata.X[root_cell_index, :]\n   where \"root_cell_index\" is the integer index of the root cell, or\n       adata.var['xroot'] = adata[root_cell_name, :].X\n   where \"root_cell_name\" is the name (a string) of the root cell.'''\n        sys.exit(msg)\n    if n_branchings == 0:\n        logg.m('set parameter `n_branchings` > 0 to detect branchings', v='hint')\n    dpt = DPT(adata, k=k, knn=knn, n_pcs=n_pcs, n_pcs_post=n_pcs_post,\n              n_jobs=n_jobs, recompute_diffmap=recompute_diffmap,\n              recompute_pca=recompute_pca,\n              n_branchings=n_branchings, allow_branching_at_root=allow_branching_at_root,\n              flavor=flavor)\n    # diffusion map\n    ddmap = dpt.diffmap(n_comps=n_dcs)\n    adata.smp['X_diffmap'] = ddmap['X_diffmap']\n    # also store the 0th comp, which is skipped for plotting\n    adata.smp['X_diffmap0'] = dpt.rbasis[:, 0]\n    adata.add['diffmap_evals'] = ddmap['evals']\n    if knn: adata.add['distance'] = dpt.Dsq\n    logg.m('perform Diffusion Pseudotime analysis', r=True)\n    if False:\n        # compute M matrix of cumulative transition probabilities,\n        # see Haghverdi et al. (2016)\n        dpt.compute_M_matrix()\n    # compute DPT distance matrix, which we refer to as 'Ddiff'\n    if False:  # we do not compute the full Ddiff matrix, only the elements we need\n        dpt.compute_Ddiff_matrix()\n    dpt.set_pseudotime()  # pseudotimes are distances from root point\n    adata.add['iroot'] = dpt.iroot  # update iroot, might have changed when subsampling, for example\n    adata.smp['dpt_pseudotime'] = dpt.pseudotime\n    # detect branchings and partition the data into segments\n    dpt.branchings_segments()\n    # vector of length n_groups\n    adata.add['dpt_groups_names'] = [str(n) for n in dpt.segs_names_unique]\n    # for itips, tips in enumerate(dpt.segs_tips):\n    #     # if tips[0] == -1: adata.add['dpt_groups_names'][itips] = '?'\n    #     if dpt.segs_undecided[itips]: adata.add['dpt_groups_names'][itips] += '?'\n    # vector of length n_samples of groupnames\n    adata.smp['dpt_groups'] = dpt.segs_names.astype('U')\n    # the ordering according to segments and pseudotime\n    adata.smp['dpt_order'] = dpt.indices\n    # the changepoints - marking different segments - in the ordering above\n    adata.add['dpt_changepoints'] = dpt.changepoints\n    # the tip points of segments\n    adata.add['dpt_grouptips'] = dpt.segs_tips\n    # the connecting points\n    # adata.add['dpt_groupconnects'] = dpt.segs_connects\n    # the tree/graph adjacency matrix\n    adata.add['dpt_groups_adjacency'] = dpt.segs_adjacency\n    logg.m('finished', t=True, end=' ')\n    logg.m('and added\\n'\n           '    \"dpt_pseudotime\", stores pseudotime (adata.smp),\\n'\n           '    \"dpt_groups\", the segments of trajectories a long a tree (adata.smp),\\n'\n           '    \"dpt_groups_adjacency\", the adjacency matrix between segments that defines the tree (adata.add),\\n'\n           '    \"dpt_order\", is an index array for sorting the cells (adata.smp),\\n'\n           '    \"dpt_grouptips\", stores the indices of tip cells (adata.add)')\n    return adata if copy else None", "idx": 1304}
{"project": "Scanpy", "commit_id": "899_scanpy_1.4.3_scanpy_plotting__anndata.py_violin.py", "target": 1, "func": "def violin(adata, keys, groupby=None, log=False, use_raw=None, stripplot=True, jitter=True,\n           size=1, scale='width', order=None, multi_panel=None, show=None,\n           xlabel='', rotation=None, save=None, ax=None, **kwds):\n    \"\"\"\\\n    Violin plot.\n    Wraps `seaborn.violinplot` for :class:`~anndata.AnnData`.\n    Parameters\n    ----------\n    adata : :class:`~anndata.AnnData`\n        Annotated data matrix.\n    keys : `str` or list of `str`\n        Keys for accessing variables of `.var_names` or fields of `.obs`.\n    groupby : `str` or `None`, optional (default: `None`)\n        The key of the observation grouping to consider.\n    log : `bool`, optional (default: `False`)\n        Plot on logarithmic axis.\n    use_raw : `bool`, optional (default: `None`)\n        Use `raw` attribute of `adata` if present.\n    multi_panel : `bool`, optional (default: `False`)\n        Display keys in multiple panels also when `groupby is not None`.\n    stripplot : `bool` optional (default: `True`)\n        Add a stripplot on top of the violin plot.\n        See `seaborn.stripplot`.\n    jitter : `float` or `bool`, optional (default: `True`)\n        Add jitter to the stripplot (only when stripplot is True)\n        See `seaborn.stripplot`.\n    size : int, optional (default: 1)\n        Size of the jitter points.\n    order : list of str, optional (default: `True`)\n        Order in which to show the categories.\n    scale : {{'area', 'count', 'width'}}, optional (default: 'width')\n        The method used to scale the width of each violin. If 'area', each\n        violin will have the same area. If 'count', the width of the violins\n        will be scaled by the number of observations in that bin. If 'width',\n        each violin will have the same width.\n    xlabel : `str`, optional (default: `''`)\n        Label of the x axis. Defaults to `groupby` if `rotation` is `None`,\n        otherwise, no label is shown.\n    rotation : `float`, optional (default: `None`)\n        Rotation of xtick labels.\n    {show_save_ax}\n    **kwds : keyword arguments\n        Are passed to `seaborn.violinplot`.\n    Returns\n    -------\n    A :class:`~matplotlib.axes.Axes` object if `ax` is `None` else `None`.\n    \"\"\"\n    import seaborn as sns  # Slow import, only import if called\n    sanitize_anndata(adata)\n    if use_raw is None and adata.raw is not None: use_raw = True\n    if isinstance(keys, str): keys = [keys]\n    obs_keys = False\n    for key in keys:\n        if key in adata.obs_keys(): obs_keys = True\n        if obs_keys and key not in set(adata.obs_keys()):\n            raise ValueError(\n                'Either use observation keys or variable names, but do not mix. '\n                'Did not find {} in adata.obs_keys().'.format(key))\n    if obs_keys:\n        obs_df = adata.obs\n    else:\n        if groupby is None: obs_df = pd.DataFrame()\n        else: obs_df = pd.DataFrame(adata.obs[groupby])\n        for key in keys:\n            if adata.raw is not None and use_raw:\n                X_col = adata.raw[:, key].X\n            else:\n                X_col = adata[:, key].X\n            obs_df[key] = X_col\n    if groupby is None:\n        obs_tidy = pd.melt(obs_df, value_vars=keys)\n        x = 'variable'\n        ys = ['value']\n    else:\n        obs_tidy = obs_df\n        x = groupby\n        ys = keys\n    if multi_panel:\n        if groupby is None and len(ys) == 1:\n            # This is a quick and dirty way for adapting scales across several\n            # keys if groupby is None.\n            y = ys[0]\n            g = sns.FacetGrid(obs_tidy, col=x, col_order=keys, sharey=False)\n            # don't really know why this gives a warning without passing `order`\n            g = g.map(sns.violinplot, y, inner=None, orient='vertical',\n                      scale=scale, order=keys, **kwds)\n            if stripplot:\n                g = g.map(sns.stripplot, y, orient='vertical', jitter=jitter, size=size, order=keys,\n                          color='black')\n            if log:\n                g.set(yscale='log')\n            g.set_titles(col_template='{col_name}').set_xlabels('')\n            if rotation is not None:\n                for ax in g.axes[0]:\n                    ax.tick_params(labelrotation=rotation)\n    else:\n        if ax is None:\n            axs, _, _, _ = setup_axes(\n                ax=ax, panels=['x'] if groupby is None else keys, show_ticks=True, right_margin=0.3)\n        else:\n            axs = [ax]\n        for ax, y in zip(axs, ys):\n            ax = sns.violinplot(x, y=y, data=obs_tidy, inner=None, order=order,\n                                orient='vertical', scale=scale, ax=ax, **kwds)\n            if stripplot:\n                ax = sns.stripplot(x, y=y, data=obs_tidy, order=order,\n                                   jitter=jitter, color='black', size=size, ax=ax)\n            if xlabel == '' and groupby is not None and rotation is None:\n                xlabel = groupby.replace('_', ' ')\n            ax.set_xlabel(xlabel)\n            if log:\n                ax.set_yscale('log')\n            if rotation is not None:\n                ax.tick_params(labelrotation=rotation)\n    utils.savefig_or_show('violin', show=show, save=save)\n    if show is False:\n        if multi_panel:\n            return g\n        elif len(axs) == 1:\n            return axs[0]\n        else:\n            return axs", "idx": 1308}
{"project": "Scanpy", "commit_id": "130_scanpy_1.9.0_exporting.py_spring_project.py", "target": 0, "func": "def spring_project(\n    adata: AnnData,\n    project_dir: Union[Path, str],\n    embedding_method: str,\n    subplot_name: Optional[str] = None,\n    cell_groupings: Union[str, Iterable[str], None] = None,\n    custom_color_tracks: Union[str, Iterable[str], None] = None,\n    total_counts_key: str = 'n_counts',\n    neighbors_key: Optional[str] = None,\n    overwrite: bool = False,\n):\n    \"\"\"\\\n    Exports to a SPRING project directory [Weinreb17]_.\n\n    Visualize annotation present in `adata`. By default, export all gene expression data\n    from `adata.raw` and categorical and continuous annotations present in `adata.obs`.\n\n    See `SPRING <https://github.com/AllonKleinLab/SPRING>`__ or [Weinreb17]_ for details.\n\n    Parameters\n    ----------\n    adata\n        Annotated data matrix: `adata.uns['neighbors']` needs to\n        be present.\n    project_dir\n        Path to directory for exported SPRING files.\n    embedding_method\n        Name of a 2-D embedding in `adata.obsm`\n    subplot_name\n        Name of subplot folder to be created at `project_dir+\"/\"+subplot_name`\n    cell_groupings\n        Instead of importing all categorical annotations when `None`,\n        pass a list of keys for `adata.obs`.\n    custom_color_tracks\n        Specify specific `adata.obs` keys for continuous coloring.\n    total_counts_key\n        Name of key for total transcript counts in `adata.obs`.\n    overwrite\n        When `True`, existing counts matrices in `project_dir` are overwritten.\n\n    Examples\n    --------\n    See this `tutorial <https://github.com/theislab/scanpy_usage/tree/master/171111_SPRING_export>`__.\n    \"\"\"\n\n    # need to get nearest neighbors first\n    if neighbors_key is None:\n        neighbors_key = 'neighbors'\n\n    if neighbors_key not in adata.uns:\n        raise ValueError('Run `sc.pp.neighbors` first.')\n\n    # check that requested 2-D embedding has been generated\n    if embedding_method not in adata.obsm_keys():\n        if 'X_' + embedding_method in adata.obsm_keys():\n            embedding_method = 'X_' + embedding_method\n        else:\n            if embedding_method in adata.uns:\n                embedding_method = (\n                    'X_'\n                    + embedding_method\n                    + '_'\n                    + adata.uns[embedding_method]['params']['layout']\n                )\n            else:\n                raise ValueError(\n                    'Run the specified embedding method `%s` first.' % embedding_method\n                )\n\n    coords = adata.obsm[embedding_method]\n\n    # Make project directory and subplot directory (subplot has same name as project)\n    # For now, the subplot is just all cells in adata\n    project_dir: Path = Path(project_dir)\n    subplot_dir: Path = (\n        project_dir.parent if subplot_name is None else project_dir / subplot_name\n    )\n    subplot_dir.mkdir(parents=True, exist_ok=True)\n    print(f'Writing subplot to {subplot_dir}')\n\n    # Write counts matrices as hdf5 files and npz if they do not already exist\n    # or if user requires overwrite.\n    # To do: check if Alex's h5sparse format will allow fast loading from just\n    # one file.\n    write_counts_matrices = True\n    base_dir_filelist = [\n        'counts_norm_sparse_genes.hdf5',\n        'counts_norm_sparse_cells.hdf5',\n        'counts_norm.npz',\n        'total_counts.txt',\n        'genes.txt',\n    ]\n    if all((project_dir / f).is_file() for f in base_dir_filelist):\n        if not overwrite:\n            logg.warning(\n                f'{project_dir} is an existing SPRING folder. A new subplot will be created, but '\n                'you must set `overwrite=True` to overwrite counts matrices.'\n            )\n            write_counts_matrices = False\n        else:\n            logg.warning(f'Overwriting the files in {project_dir}.')\n\n    # Ideally, all genes will be written from adata.raw\n    if adata.raw is not None:\n        E = adata.raw.X.tocsc()\n        gene_list = list(adata.raw.var_names)\n    else:\n        E = adata.X.tocsc()\n        gene_list = list(adata.var_names)\n\n    # Keep track of total counts per cell if present\n    if total_counts_key in adata.obs:\n        total_counts = np.array(adata.obs[total_counts_key])\n    else:\n        total_counts = E.sum(1).A1\n\n    # Write the counts matrices to project directory\n    if write_counts_matrices:\n        write_hdf5_genes(E, gene_list, project_dir / 'counts_norm_sparse_genes.hdf5')\n        write_hdf5_cells(E, project_dir / 'counts_norm_sparse_cells.hdf5')\n        write_sparse_npz(E, project_dir / 'counts_norm.npz')\n        with (project_dir / 'genes.txt').open('w') as o:\n            for g in gene_list:\n                o.write(g + '\\n')\n        np.savetxt(project_dir / 'total_counts.txt', total_counts)\n\n    # Get categorical and continuous metadata\n    categorical_extras = {}\n    continuous_extras = {}\n    if cell_groupings is None:\n        for obs_name in adata.obs:\n            if is_categorical(adata.obs[obs_name]):\n                categorical_extras[obs_name] = [str(x) for x in adata.obs[obs_name]]\n    else:\n        if isinstance(cell_groupings, str):\n            cell_groupings = [cell_groupings]\n        for obs_name in cell_groupings:\n            if obs_name not in adata.obs:\n                logg.warning(f'Cell grouping {obs_name!r} is not in adata.obs')\n            elif is_categorical(adata.obs[obs_name]):\n                categorical_extras[obs_name] = [str(x) for x in adata.obs[obs_name]]\n            else:\n                logg.warning(\n                    f'Cell grouping {obs_name!r} is not a categorical variable'\n                )\n    if custom_color_tracks is None:\n        for obs_name in adata.obs:\n            if not is_categorical(adata.obs[obs_name]):\n                continuous_extras[obs_name] = np.array(adata.obs[obs_name])\n    else:\n        if isinstance(custom_color_tracks, str):\n            custom_color_tracks = [custom_color_tracks]\n        for obs_name in custom_color_tracks:\n            if obs_name not in adata.obs:\n                logg.warning(f'Custom color track {obs_name!r} is not in adata.obs')\n            elif not is_categorical(adata.obs[obs_name]):\n                continuous_extras[obs_name] = np.array(adata.obs[obs_name])\n            else:\n                logg.warning(\n                    f'Custom color track {obs_name!r} is not a continuous variable'\n                )\n\n    # Write continuous colors\n    continuous_extras['Uniform'] = np.zeros(E.shape[0])\n    _write_color_tracks(continuous_extras, subplot_dir / 'color_data_gene_sets.csv')\n\n    # Create and write a dictionary of color profiles to be used by the visualizer\n    color_stats = {}\n    color_stats = _get_color_stats_genes(color_stats, E, gene_list)\n    color_stats = _get_color_stats_custom(color_stats, continuous_extras)\n    _write_color_stats(subplot_dir / 'color_stats.json', color_stats)\n\n    # Write categorical data\n    categorical_coloring_data = {}\n    categorical_coloring_data = _build_categ_colors(\n        categorical_coloring_data, categorical_extras\n    )\n    _write_cell_groupings(\n        subplot_dir / 'categorical_coloring_data.json', categorical_coloring_data\n    )\n\n    # Write graph in two formats for backwards compatibility\n    edges = _get_edges(adata, neighbors_key)\n    _write_graph(subplot_dir / 'graph_data.json', E.shape[0], edges)\n    _write_edges(subplot_dir / 'edges.csv', edges)\n\n    # Write cell filter; for now, subplots must be generated from within SPRING,\n    # so cell filter includes all cells.\n    np.savetxt(subplot_dir / 'cell_filter.txt', np.arange(E.shape[0]), fmt='%i')\n    np.save(subplot_dir / 'cell_filter.npy', np.arange(E.shape[0]))\n\n    # Write 2-D coordinates, after adjusting to roughly match SPRING's default d3js force layout parameters\n    coords = coords - coords.min(0)[None, :]\n    coords = (\n        coords * (np.array([1000, 1000]) / coords.ptp(0))[None, :]\n        + np.array([200, -200])[None, :]\n    )\n    np.savetxt(\n        subplot_dir / 'coordinates.txt',\n        np.hstack((np.arange(E.shape[0])[:, None], coords)),\n        fmt='%i,%.6f,%.6f',\n    )\n\n    # Write some useful intermediates, if they exist\n    if 'X_pca' in adata.obsm_keys():\n        np.savez_compressed(\n            subplot_dir / 'intermediates.npz',\n            Epca=adata.obsm['X_pca'],\n            total_counts=total_counts,\n        )\n\n    # Write PAGA data, if present\n    if 'paga' in adata.uns:\n        clusts = np.array(adata.obs[adata.uns['paga']['groups']].cat.codes)\n        uniq_clusts = adata.obs[adata.uns['paga']['groups']].cat.categories\n        paga_coords = [coords[clusts == i, :].mean(0) for i in range(len(uniq_clusts))]\n        _export_PAGA_to_SPRING(adata, paga_coords, subplot_dir / 'PAGA_data.json')", "idx": 1321}
{"project": "Scanpy", "commit_id": "527_scanpy_1.9.0_test_embedding_plots.py_test_spatial_general.py", "target": 0, "func": "def test_spatial_general(image_comparer):  # general coordinates\n    save_and_compare_images = image_comparer(ROOT, FIGS, tol=15)\n    adata = sc.read_visium(HERE / '_data' / 'visium_data' / '1.0.0')\n    adata.obs = adata.obs.astype({'array_row': 'str'})\n    spatial_metadata = adata.uns.pop(\n        \"spatial\"\n    )  # spatial data don't have imgs, so remove entry from uns\n    # Required argument for now\n    spot_size = list(spatial_metadata.values())[0][\"scalefactors\"][\n        \"spot_diameter_fullres\"\n    ]\n\n    sc.pl.spatial(adata, show=False, spot_size=spot_size)\n    save_and_compare_images('master_spatial_general_nocol')\n\n    # category\n    sc.pl.spatial(adata, show=False, spot_size=spot_size, color=\"array_row\")\n    save_and_compare_images('master_spatial_general_cat')\n\n    # continuous\n    sc.pl.spatial(adata, show=False, spot_size=spot_size, color=\"array_col\")\n    save_and_compare_images('master_spatial_general_cont')", "idx": 1329}
{"project": "Scanpy", "commit_id": "260_scanpy_1.9.0__baseplot_class.py___init__.py", "target": 0, "func": "def __init__(\n        self,\n        adata: AnnData,\n        var_names: Union[_VarNames, Mapping[str, _VarNames]],\n        groupby: Union[str, Sequence[str]],\n        use_raw: Optional[bool] = None,\n        log: bool = False,\n        num_categories: int = 7,\n        categories_order: Optional[Sequence[str]] = None,\n        title: Optional['str'] = None,\n        figsize: Optional[Tuple[float, float]] = None,\n        gene_symbols: Optional[str] = None,\n        var_group_positions: Optional[Sequence[Tuple[int, int]]] = None,\n        var_group_labels: Optional[Sequence[str]] = None,\n        var_group_rotation: Optional[float] = None,\n        layer: Optional[str] = None,\n        ax: Optional[_AxesSubplot] = None,\n        vmin: Optional[float] = None,\n        vmax: Optional[float] = None,\n        vcenter: Optional[float] = None,\n        norm: Optional[Normalize] = None,\n        **kwds,\n    ):\n        self.var_names = var_names\n        self.var_group_labels = var_group_labels\n        self.var_group_positions = var_group_positions\n        self.var_group_rotation = var_group_rotation\n        self.width, self.height = figsize if figsize is not None else (None, None)\n\n        self.has_var_groups = (\n            True\n            if var_group_positions is not None and len(var_group_positions) > 0\n            else False\n        )\n\n        self._update_var_groups()\n\n        self.categories, self.obs_tidy = _prepare_dataframe(\n            adata,\n            self.var_names,\n            groupby,\n            use_raw,\n            log,\n            num_categories,\n            layer=layer,\n            gene_symbols=gene_symbols,\n        )\n        if len(self.categories) > self.MAX_NUM_CATEGORIES:\n            warn(\n                f\"Over {self.MAX_NUM_CATEGORIES} categories found. \"\n                \"Plot would be very large.\"\n            )\n\n        if categories_order is not None:\n            if set(self.obs_tidy.index.categories) != set(categories_order):\n                logg.error(\n                    \"Please check that the categories given by \"\n                    \"the `order` parameter match the categories that \"\n                    \"want to be reordered.\\n\\n\"\n                    \"Mismatch: \"\n                    f\"{set(self.obs_tidy.index.categories).difference(categories_order)}\\n\\n\"\n                    f\"Given order categories: {categories_order}\\n\\n\"\n                    f\"{groupby} categories: {list(self.obs_tidy.index.categories)}\\n\"\n                )\n                return\n\n        self.adata = adata\n        self.groupby = [groupby] if isinstance(groupby, str) else groupby\n        self.log = log\n        self.kwds = kwds\n\n        VBoundNorm = namedtuple('VBoundNorm', ['vmin', 'vmax', 'vcenter', 'norm'])\n        self.vboundnorm = VBoundNorm(vmin=vmin, vmax=vmax, vcenter=vcenter, norm=norm)\n\n        # set default values for legend\n        self.color_legend_title = self.DEFAULT_COLOR_LEGEND_TITLE\n        self.legends_width = self.DEFAULT_LEGENDS_WIDTH\n\n        # set style defaults\n        self.cmap = self.DEFAULT_COLORMAP\n\n        # style default parameters\n        self.are_axes_swapped = False\n        self.categories_order = categories_order\n        self.var_names_idx_order = None\n\n        self.wspace = self.DEFAULT_WSPACE\n\n        # minimum height required for legends to plot properly\n        self.min_figure_height = self.MIN_FIGURE_HEIGHT\n\n        self.fig_title = title\n\n        self.group_extra_size = 0\n        self.plot_group_extra = None\n        # after .render() is called the fig value is assigned and ax_dict\n        # contains a dictionary of the axes used in the plot\n        self.fig = None\n        self.ax_dict = None\n        self.ax = ax", "idx": 1333}
{"project": "Scanpy", "commit_id": "863_scanpy_1.9.0__sim.py__check_branching.py", "target": 0, "func": "def _check_branching(\n    X: np.ndarray, Xsamples: np.ndarray, restart: int, threshold: float = 0.25\n) -> Tuple[bool, List[np.ndarray]]:\n    \"\"\"\\\n    Check whether time series branches.\n\n    Parameters\n    ----------\n    X\n        current time series data.\n    Xsamples\n        list of previous branching samples.\n    restart\n        counts number of restart trials.\n    threshold\n        sets threshold for attractor identification.\n\n    Returns\n    -------\n    check\n        true if branching realization\n    Xsamples\n        updated list\n    \"\"\"\n    check = True\n    Xsamples = list(Xsamples)\n    if restart == 0:\n        Xsamples.append(X)\n    else:\n        for Xcompare in Xsamples:\n            Xtmax_diff = np.absolute(X[-1, :] - Xcompare[-1, :])\n            # If the second largest element is smaller than threshold\n            # set check to False, i.e. at least two elements\n            # need to change in order to have a branching.\n            # If we observe all parameters of the system,\n            # a new attractor state must involve changes in two\n            # variables.\n            if np.partition(Xtmax_diff, -2)[-2] < threshold:\n                check = False\n        if check:\n            Xsamples.append(X)\n    logg.debug(f'realization {restart}: {\"\" if check else \"no\"} new branch')\n    return check, Xsamples", "idx": 1337}
{"project": "Scanpy", "commit_id": "122_scanpy_0.0_scanpy_readwrite.py_read_file_to_dict.py", "target": 1, "func": "def read_file_to_dict(filename, ext='h5'):\n    \"\"\"\n    Read file and return dict with keys.\n\n    The recommended format for this is hdf5.\n\n    If reading from an Excel file, key names correspond to sheet names.\n    Parameters\n    ----------\n    filename : str\n        Filename of data file.\n    ext : {'h5', 'xlsx'}, optional\n        Choose file format. Excel is much slower.\n    Returns\n    -------\n    d : dict\n    \"\"\"\n    sett.m(0, 'reading file', filename)\n    d = {}\n    if ext == 'h5':\n        with h5py.File(filename, 'r') as f:\n            for key in f.keys():\n                # the '()' means 'read everything' (by contrast, ':' only works\n                # if not reading a scalar type)\n                value = f[key][()]\n                key, value = postprocess_reading(key, value)\n                d[key] = value\n    elif ext == 'npz':\n        d_read = np.load(filename)\n        for key, value in d_read.items():\n            key, value = postprocess_reading(key, value)\n            d[key] = value\n    elif ext == 'xlsx':\n        raise ValueError('TODO: this is broke.')\n        import pandas as pd\n        xl = pd.ExcelFile(filename)\n        for sheet in xl.sheet_names:\n            d[sheet] = xl.parse(sheet).values\n    if 'X' not in d:\n        if os.path.exists(filename.replace('.' + ext, '') + '_X.npz'):\n            X = load_sparse_csr(filename.replace('.' + ext, '') + '_X.npz')\n            d['X'] = X\n    return d", "idx": 1345}
{"project": "Scanpy", "commit_id": "420_scanpy_0.2.9_scanpy_tools_rank_genes_groups.py_rank_genes_groups.py", "target": 1, "func": "def rank_genes_groups(\n        adata,\n        groupby,\n        groups='all',\n        group_reference=None,\n        n_genes=100,\n        compute_distribution=False,\n        only_positive=True,\n        copy=False):\n    \"\"\"Rank genes according to differential expression [Wolf17]_.\n    Rank genes by differential expression. By default, a t-test-like ranking is\n    used, in which means are normalized with variances. Soon, a Wilcoxon-rank\n    test and other alternatives will be provided.\n    Parameters\n    ----------\n    adata : `AnnData`\n        Annotated data matrix.\n    groupby : `str`\n        The key of the sample grouping to consider.\n    groups : `str`, `list`, optional (default: `'all'`)\n        Subset of groups, e.g. `['g1', 'g2', 'g3']`, to which comparison shall\n        be restricted. If not passed, a ranking will be generated for all\n        groups.\n    group_reference : `str` or `None`, optional (default: `None`)\n        If `None`, compare each group to the union of the rest of the group.  If\n        a group identifier, the comparison will be with respect to this group.\n    n_genes : `int` (default: 100)\n        How many genes to rank by default.\n    compute_distribution : `bool`\n        If `True`, also computes the distribution for top-ranked genes, which\n        can be visualized using `sc.pl.rank_genes_groups_violin(adata)`.\n    Returns\n    -------\n    rank_genes_groups_gene_zscores : np.ndarray of dtype float (adata.add)\n        Array of shape (number of comparisons) \u00d7 (number of genes) storing the\n        zscore of the each gene for each test.\n    rank_genes_groups_gene_names : np.ndarray of dtype str (adata.add)\n        Array of shape (number of comparisons). Stores the labels for each comparison,\n        for example \"C1 vs. C2\" when comparing category 'C1' with 'C2'.\n    \"\"\"\n    logg.info('find differentially expressed genes', r=True)\n    adata = adata.copy() if copy else adata\n    n_genes_user = n_genes\n    utils.check_adata(adata)\n    # for clarity, rename variable\n    groups_order = groups\n    if isinstance(groups_order, list) and isinstance(groups_order[0], int):\n        groups_order = [str(n) for n in groups_order]\n    if group_reference is not None and group_reference not in set(groups_order):\n        groups_order += [group_reference]\n    if (group_reference is not None\n            and group_reference not in set(adata.add[groupby + '_order'])):\n        raise ValueError('group_reference = {} needs to be one of groupby = {}.'\n                         .format(group_reference, groupby))\n    groups_order, groups_masks = utils.select_groups(\n        adata, groups_order, groupby)\n    adata.add['rank_genes_groups'] = groupby\n    adata.add['rank_genes_groups_order'] = groups_order\n    X = adata.X\n\n    # loop over all masks and compute means, variances and sample numbers\n    n_groups = groups_masks.shape[0]\n    n_genes = X.shape[1]\n    means = np.zeros((n_groups, n_genes))\n    vars = np.zeros((n_groups, n_genes))\n    ns = np.zeros(n_groups, dtype=int)\n    for imask, mask in enumerate(groups_masks):\n        means[imask], vars[imask] = simple._get_mean_var(X[mask])\n        ns[imask] = np.where(mask)[0].size\n    logg.info('... consider \"{}\":'.format(groupby), groups_order,\n              'with sample numbers', ns)\n\n    if group_reference is not None:\n        ireference = np.where(groups_order == group_reference)[0][0]\n\n    # test each either against the union of all other groups\n    # or against a specific group\n    rankings_gene_zscores = []\n    rankings_gene_names = []\n    reference_indices = np.arange(adata.n_vars, dtype=int)\n    for igroup in range(n_groups):\n        if group_reference is None:\n            mask_rest = ~groups_masks[igroup]\n        else:\n            if igroup == ireference:\n                continue\n            else:\n                mask_rest = groups_masks[ireference]\n        mean_rest, var_rest = simple._get_mean_var(X[mask_rest])\n        # Make a more conservative assumption on the variance reduction\n        # in the reference. Instead of this\n        ns_rest = np.where(mask_rest)[0].size\n        # use this\n        # ns_rest = ns[igroup]\n        denominator = np.sqrt(vars[igroup] / ns[igroup] + var_rest / ns_rest)\n        denominator[np.flatnonzero(denominator == 0)] = np.nan\n        zscores = (means[igroup] - mean_rest) / denominator\n        zscores[np.isnan(zscores)] = 0\n        zscores = zscores if only_positive else np.abs(zscores)\n        partition = np.argpartition(zscores, -n_genes_user)[-n_genes_user:]\n        partial_indices = np.argsort(zscores[partition])[::-1]\n        global_indices = reference_indices[partition][partial_indices]\n        rankings_gene_zscores.append(zscores[global_indices])\n        rankings_gene_names.append(adata.var_names[global_indices])\n        if compute_distribution:\n            mask = groups_masks[igroup]\n            for gene_counter in range(n_genes_user):\n                gene_idx = global_indices[gene_counter]\n                X_col = X[mask, gene_idx]\n                if issparse(X): X_col = X_col.toarray()[:, 0]\n                identifier = _build_identifier(groupby, groups_order[igroup],\n                                               gene_counter, adata.var_names[gene_idx])\n                full_col = np.empty(adata.n_smps)\n                full_col[:] = np.nan\n                full_col[mask] = (X_col - mean_rest[gene_idx]) / denominator[gene_idx]\n                adata.smp[identifier] = full_col\n\n    groups_order_save = groups_order\n    if group_reference is not None:\n        groups_order_save = [g for g in groups_order if g != group_reference]\n\n    adata.add['rank_genes_groups_gene_scores'] = np.rec.fromarrays(\n        [n for n in rankings_gene_zscores],\n        dtype=[(rn, 'float32') for rn in groups_order_save])\n    adata.add['rank_genes_groups_gene_names'] = np.rec.fromarrays(\n        [n for n in rankings_gene_names],\n        dtype=[(rn, 'U50') for rn in groups_order_save])\n    logg.m('    finished', t=True, end=' ')\n    logg.m('and added\\n'\n           '    \"rank_genes_groups_gene_names\", np.recarray to be indexed by the `groups` (adata.add)\\n'\n           '    \"rank_genes_groups_gene_zscores\", the scores (adata.add)\\n'\n           '    \"rank_genes_...\", distributions of top-ranked genes (adata.smp)')\n    return adata if copy else None", "idx": 1347}
{"project": "Scanpy", "commit_id": "238_scanpy_1.9.0___init__.py_count_nonzero.py", "target": 0, "func": "def count_nonzero(a: Union[np.ndarray, csr_matrix]) -> int:\n                    return a.count_nonzero() if issparse(a) else np.count_nonzero(a)", "idx": 1348}
{"project": "Scanpy", "commit_id": "238_scanpy_0.1_scanpy_plotting___init__.py_dpt.py", "target": 1, "func": "def dpt(adata,\n        basis='diffmap',\n        color=None,\n        names=None,\n        comps=None,\n        cont=None,\n        layout='2d',\n        legendloc='right margin',\n        cmap=None,\n        pal=None,\n        right_margin=None,\n        size=None,\n        show=None):\n    \"\"\"Plot results of DPT analysis.\n    Parameters\n    ----------\n    adata : AnnData\n        Annotated data matrix.\n    basis : {'diffmap', 'pca', 'tsne', 'spring'}\n        Choose the basis in which to plot.\n    color : str, optional (default: first annotation)\n        Sample/ cell annotation for coloring in the form \"ann1,ann2,...\". String\n        annotation is plotted assuming categorical annotation, float and integer\n        annotation is plotted assuming continuous annoation. Option 'cont'\n        allows to switch between these default choices.\n    comps : str, optional (default: '1,2')\n         String of the form '1,2' or 'all'.\n    cont : bool, None (default: None)\n        Switch on continuous layout, switch off categorical layout.\n    layout : {'2d', '3d', 'unfolded 3d'}, optional (default: '2d')\n         Layout of plot.\n    legendloc : {'right margin', see matplotlib.legend}, optional (default: 'right margin')\n         Options for keyword argument 'loc'.\n    cmap : str (default: 'viridis')\n         String denoting matplotlib color map.\n    pal : list of str (default: matplotlib.rcParams['axes.prop_cycle'].by_key()['color'])\n         Colors cycle to use for categorical groups.\n    right_margin : float (default: None)\n         Adjust how far the plotting panel extends to the right.\n    \"\"\"\n    from ..examples import check_adata\n    adata = check_adata(adata)\n    # scatter plot\n    colors = ['dpt_pseudotime']\n    if len(np.unique(adata.smp['dpt_groups'])) > 1:\n        colors += ['dpt_groups']\n    adata.add['highlights'] = (list([adata.add['iroot']]))   # also plot the tip cell indices\n                               # + [adata.add['dpt_grouptips'][i][1]\n                               #    for i in range(len(adata.add['dpt_grouptips']))\n                               #    if adata.add['dpt_grouptips'][i][1] != -1])\n                               # + [adata.add['dpt_grouptips'][i][0]\n                               #    for i in range(len(adata.add['dpt_grouptips']))\n                               # if adata.add['dpt_grouptips'][i][1] != -1])\n    if color is not None:\n        if not isinstance(color, list): colors = color.split(',')\n        else: colors = color\n    if comps == 'all':\n        comps_list = ['1,2', '1,3', '1,4', '1,5', '2,3', '2,4', '2,5', '3,4', '3,5', '4,5']\n    else:\n        if comps is None:\n            comps = '1,2' if '2d' in layout else '1,2,3'\n        if not isinstance(comps, list): comps_list = [comps]\n        else: comps_list = comps\n    for comps in comps_list:\n        axs = scatter(adata,\n                      basis=basis,\n                      color=colors,\n                      names=names,\n                      comps=comps,\n                      cont=cont,\n                      layout=layout,\n                      legendloc=legendloc,\n                      cmap=cmap,\n                      pal=pal,\n                      right_margin=right_margin,\n                      size=size,\n                      show=False)\n        writekey = 'dpt_' + basis + '_comps' + comps.replace(',', '')\n        if sett.savefigs: savefig(writekey)\n    # plot the tree\n    import networkx as nx\n    pl.figure()\n    colors = adata.add['dpt_groups_colors']\n    for iname, name in enumerate(adata.add['dpt_groups_names']):\n        if name in sett._ignore_categories:\n            colors[iname] = 'grey'\n    with warnings.catch_warnings():\n        warnings.simplefilter('ignore')\n        nx.draw_spring(nx.Graph(adata.add['dpt_groups_adjacency']),\n                       with_labels=True, node_color=colors)\n    if sett.savefigs: savefig('dpt_tree')\n    # plot segments and pseudotime\n    if False:\n        dpt_segments_pseudotime(adata, 'viridis' if cmap is None else cmap)\n        # time series plot\n        # only if number of genes is not too high\n        if adata.X.shape[1] <= 11:\n            # plot time series as gene expression vs time\n            timeseries(adata.X[adata.smp['dpt_order']],\n                             varnames=adata.var_names,\n                             highlightsX=adata.add['dpt_changepoints'],\n                             xlim=[0, 1.3*X.shape[0]])\n            pl.xlabel('dpt order')\n            if sett.savefigs: savefig('dpt_vsorder')\n        elif adata.X.shape[1] < 50:\n            # plot time series as heatmap, as in Haghverdi et al. (2016), Fig. 1d\n            timeseries_as_heatmap(adata.X[adata.smp['dpt_order'], :40],\n                                        varnames=adata.var_names,\n                                        highlightsX=adata.add['dpt_changepoints'])\n            pl.xlabel('dpt order')\n            if sett.savefigs: savefig('dpt_heatmap')\n    show = sett.autoshow if show is None else show\n    if not sett.savefigs and show: pl.show()", "idx": 1360}
{"project": "Scanpy", "commit_id": "875_scanpy_1.4.2_scanpy_preprocessing__normalization.py__normalize_data.py", "target": 1, "func": "def _normalize_data(X, counts, after=None, copy=False):\n    X = X.copy() if copy else X\n    after = np.median(counts[counts>0]) if after is None else after\n    counts += (counts == 0)\n    counts /= after\n    if issparse(X):\n        X = sparsefuncs.inplace_row_scale(X, 1/counts)\n    else:\n        X /= counts[:, None]\n    return X if copy else None", "idx": 1370}
{"project": "Scanpy", "commit_id": "308_scanpy_0.1_scanpy_plotting___init__.py_aga_timeseries.py", "target": 1, "func": "def aga_timeseries(\n        adata,\n        nodes=[0],\n        keys=[0],\n        xlim=[None, None],\n        n_avg=1,\n        left_margin=0.4,\n        show_left_y_ticks=None,\n        show_nodes=True,\n        legend_fontsize=None,\n        ax=None,\n        show=None):\n    ax_was_none = ax is None\n    if show_left_y_ticks is None:\n        show_left_y_ticks = False if show_nodes else True\n\n    # from matplotlib import rcParams\n    # pl.axes([left_margin, rcParams['figure.subplot.bottom'], 1, 1])\n    for key in keys:\n        x = []\n        for group in nodes:\n            idcs = np.arange(adata.n_smps)[adata.smp['aga_groups'] == str(group)]\n            idcs_group = np.argsort(adata.smp['aga_pseudotime'][adata.smp['aga_groups'] == str(group)])\n            idcs = idcs[idcs_group]\n            if key in adata.smp_keys(): x += list(adata.smp[key][idcs])\n            else: x += list(adata[:, key].X[idcs])\n        if n_avg > 1: x = moving_average(x)\n        pl.plot(x[xlim[0]:xlim[1]], label=key)\n    pl.xlabel('order along trajectory')\n    pl.legend(frameon=False, loc='center left',\n              bbox_to_anchor=(-left_margin, 0.5),\n              fontsize=legend_fontsize)\n    if show_left_y_ticks:\n        utils.pimp_axis(pl.gca().get_yaxis())\n        pl.ylabel('as indicated on legend')\n    else:\n        pl.yticks([])\n        pl.ylabel('as indicated on legend (a.u.)')\n    if show_nodes:\n        pl.twinx()\n        x = []\n        for g in nodes:\n            x += list(adata.smp['aga_groups'][adata.smp['aga_groups'] == str(g)].astype(int))\n        if n_avg > 1: x = moving_average(x)\n        pl.plot(x[xlim[0]:xlim[1]], '--', color='black')\n        pl.ylabel('aga groups')\n    if show is None and not ax_was_none: show = False\n    else: show = sett.autoshow if show is None else show\n    savefig_or_show('aga_timeseries', show)\n    return ax if ax_was_none else None", "idx": 1371}
{"project": "Scanpy", "commit_id": "520_scanpy_0.4.4_scanpy_tools_diffmap.py_diffmap.py", "target": 1, "func": "def diffmap(adata, n_comps=15, n_neighbors=None, knn=True, n_pcs=50, sigma=0,\n            n_jobs=None, flavor='haghverdi16', copy=False):\n    \"\"\"Diffusion Maps [Coifman05]_ [Haghverdi15]_ [Wolf17]_.\n    Diffusion maps [Coifman05]_ has been proposed for visualizing single-cell\n    data by [Haghverdi15]_. The tool uses the adapted Gaussian kernel suggested\n    by [Haghverdi16]_ in the implementation of [Wolf17]_.\n    Parameters\n    ----------\n    adata : :class:`~scanpy.api.AnnData`\n        Annotated data matrix.\n    n_comps : `int`, optional (default: 15)\n        The number of dimensions of the representation.\n    n_neighbors : `int`, optional (default: 30)\n        Specify the number of nearest neighbors in the knn graph. If knn ==\n        False, set the Gaussian kernel width to the distance of the kth\n        neighbor (method 'local').\n    knn : `bool`, optional (default: `True`)\n        If `True`, use a hard threshold to restrict the number of neighbors to\n        `n_neighbors`, that is, consider a knn graph. Otherwise, use a Gaussian\n        Kernel to assign low weights to neighbors more distant than the\n        `n_neighbors` nearest neighbor.\n    n_pcs : `int`, optional (default: 50)\n        Use `n_pcs` PCs to compute the Euclidian distance matrix, which is the\n        basis for generating the graph. Set to 0 if you don't want preprocessing\n        with PCA.\n    n_jobs : `int` or `None`\n        Number of CPUs to use (default: `sc.settings.n_jobs`).\n    copy : `bool` (default: `False`)\n        Return a copy instead of writing to adata.\n    Returns\n    -------\n    Depending on `copy`, returns or updates `adata` with the following fields.\n    X_diffmap : `adata.obsm`\n        Array of shape (#samples) \u00d7 (#eigen vectors). Diffusion map\n        representation of data, which is the right eigen basis of the transition\n        matrix with eigenvectors as columns.\n    diffmap_evals : `np.ndarray` (`adata.uns`)\n        Array of size (number of eigen vectors). Eigenvalues of transition matrix.\n    \"\"\"\n    logg.info('computing Diffusion Maps', r=True)\n    if n_comps <= 2:\n        raise ValueError('Provide any value greater than 2. '\n                         'Mind that the 0th component of diffusion maps '\n                         'is not used for visualization.')\n    adata = adata.copy() if copy else adata\n    dmap = dpt.DPT(adata, n_neighbors=n_neighbors, knn=knn, n_pcs=n_pcs,\n                   n_dcs=n_comps, n_jobs=n_jobs, recompute_graph=True,\n                   flavor=flavor)\n    dmap.update_diffmap()\n    adata.uns['data_graph_distance_local'] = dmap.Dsq\n    adata.uns['data_graph_norm_weights'] = dmap.Ktilde\n    adata.obsm['X_diffmap'] = dmap.rbasis[:, 1:]\n    adata.obs['X_diffmap0'] = dmap.rbasis[:, 0]\n    adata.uns['diffmap_evals'] = dmap.evals[1:]\n    logg.info('    finished', time=True, end=' ' if settings.verbosity > 2 else '\\n')\n    logg.hint('added\\n'\n              '    \\'X_diffmap\\', tSNE coordinates (adata.obsm)\\n'\n              '    \\'diffmap_evals\\', eigenvalues of transition matrix (adata.uns)')\n    return adata if copy else None", "idx": 1373}
{"project": "Scanpy", "commit_id": "3_scanpy_1.9.0_conftest.py__pbmc3k_normalized.py", "target": 0, "func": "def _pbmc3k_normalized():\n    import scanpy as sc\n\n    pbmc = sc.datasets.pbmc3k()\n    pbmc.X = pbmc.X.astype(\"float64\")  # For better accuracy\n    sc.pp.filter_genes(pbmc, min_counts=1)\n    sc.pp.log1p(pbmc)\n    sc.pp.normalize_total(pbmc)\n    sc.pp.highly_variable_genes(pbmc)\n    return pbmc", "idx": 1378}
{"project": "Scanpy", "commit_id": "296_scanpy_1.9.0__stacked_violin.py___init__.py", "target": 0, "func": "def __init__(\n        self,\n        adata: AnnData,\n        var_names: Union[_VarNames, Mapping[str, _VarNames]],\n        groupby: Union[str, Sequence[str]],\n        use_raw: Optional[bool] = None,\n        log: bool = False,\n        num_categories: int = 7,\n        categories_order: Optional[Sequence[str]] = None,\n        title: Optional[str] = None,\n        figsize: Optional[Tuple[float, float]] = None,\n        gene_symbols: Optional[str] = None,\n        var_group_positions: Optional[Sequence[Tuple[int, int]]] = None,\n        var_group_labels: Optional[Sequence[str]] = None,\n        var_group_rotation: Optional[float] = None,\n        layer: Optional[str] = None,\n        standard_scale: Literal['var', 'group'] = None,\n        ax: Optional[_AxesSubplot] = None,\n        vmin: Optional[float] = None,\n        vmax: Optional[float] = None,\n        vcenter: Optional[float] = None,\n        norm: Optional[Normalize] = None,\n        **kwds,\n    ):\n        BasePlot.__init__(\n            self,\n            adata,\n            var_names,\n            groupby,\n            use_raw=use_raw,\n            log=log,\n            num_categories=num_categories,\n            categories_order=categories_order,\n            title=title,\n            figsize=figsize,\n            gene_symbols=gene_symbols,\n            var_group_positions=var_group_positions,\n            var_group_labels=var_group_labels,\n            var_group_rotation=var_group_rotation,\n            layer=layer,\n            ax=ax,\n            vmin=vmin,\n            vmax=vmax,\n            vcenter=vcenter,\n            norm=norm,\n            **kwds,\n        )\n\n        if standard_scale == 'obs':\n            self.obs_tidy = self.obs_tidy.sub(self.obs_tidy.min(1), axis=0)\n            self.obs_tidy = self.obs_tidy.div(self.obs_tidy.max(1), axis=0).fillna(0)\n        elif standard_scale == 'var':\n            self.obs_tidy -= self.obs_tidy.min(0)\n            self.obs_tidy = (self.obs_tidy / self.obs_tidy.max(0)).fillna(0)\n        elif standard_scale is None:\n            pass\n        else:\n            logg.warning('Unknown type for standard_scale, ignored')\n\n        # Set default style parameters\n        self.cmap = self.DEFAULT_COLORMAP\n        self.row_palette = self.DEFAULT_ROW_PALETTE\n        self.stripplot = self.DEFAULT_STRIPPLOT\n        self.jitter = self.DEFAULT_JITTER\n        self.jitter_size = self.DEFAULT_JITTER_SIZE\n        self.plot_yticklabels = self.DEFAULT_PLOT_YTICKLABELS\n        self.ylim = self.DEFAULT_YLIM\n        self.plot_x_padding = self.DEFAULT_PLOT_X_PADDING\n        self.plot_y_padding = self.DEFAULT_PLOT_Y_PADDING\n\n        self.kwds.setdefault('cut', self.DEFAULT_CUT)\n        self.kwds.setdefault('inner', self.DEFAULT_INNER)\n        self.kwds.setdefault('linewidth', self.DEFAULT_LINE_WIDTH)\n        self.kwds.setdefault('scale', self.DEFAULT_SCALE)", "idx": 1379}
{"project": "Scanpy", "commit_id": "267_scanpy_0.1_scanpy_tools_dpt.py_select_segment.py", "target": 1, "func": "def select_segment(self, segs, segs_tips, segs_undecided):\n    \"\"\"Out of a list of line segments, choose segment that has the most\n    distant second data point.\n    Assume the distance matrix Ddiff is sorted according to seg_idcs.\n    Compute all the distances.\n    Returns\n    -------\n    iseg : int\n        Index identifying the position within the list of line segments.\n    tips3 : int\n        Positions of tips within chosen segment.\n    \"\"\"\n    scores_tips = np.zeros((len(segs), 4))\n    allindices = np.arange(self.X.shape[0], dtype=int)\n    for iseg, seg in enumerate(segs):\n        # do not consider too small segments\n        if segs_tips[iseg][0] == -1: continue\n        # restrict distance matrix to points in segment\n        if not isinstance(self.Dchosen, data_graph.OnFlySymMatrix):\n            Dseg = self.Dchosen[np.ix_(seg, seg)]\n        else:\n            Dseg = self.Dchosen.restrict(seg)\n        third_maximizer = None\n        if segs_undecided[iseg]:\n            # check that none of our tips \"connects\" with a tip of the\n            # other segments\n            for jseg in range(len(segs)):\n                if jseg != iseg:\n                    # take the inner tip, the \"second tip\" of the segment\n                    for itip in range(2):\n                        if (self.Dchosen[segs_tips[jseg][1], segs_tips[iseg][itip]]\n                                < 0.5 * self.Dchosen[segs_tips[iseg][~itip], segs_tips[iseg][itip]]):\n                            # logg.m('... group', iseg, 'with tip', segs_tips[iseg][itip],\n                            #        'connects with', jseg, 'with tip', segs_tips[jseg][1], v=4)\n                            # logg.m('    do not use the tip for \"triangulation\"', v=4)\n                            third_maximizer = itip\n        # map the global position to the position within the segment\n        tips = [np.where(allindices[seg] == tip)[0][0]\n                for tip in segs_tips[iseg]]\n        # find the third point on the segment that has maximal\n        # added distance from the two tip points\n        dseg = Dseg[tips[0]] + Dseg[tips[1]]\n        # add this point to tips, it's a third tip, we store it at the first\n        # position in an array called tips3\n        third_tip = np.argmax(dseg)\n        if third_maximizer is not None:\n            # find a fourth point that has maximal distance to all three\n            dseg += Dseg[third_tip]\n            fourth_tip = np.argmax(dseg)\n            if fourth_tip != tips[0] and fourth_tip != third_tip:\n                tips[1] = fourth_tip\n                dseg -= Dseg[tips[1]]\n            else:\n                dseg -= Dseg[third_tip]\n        tips3 = np.insert(tips, 0, third_tip)\n        # compute the score as ratio of the added distance to the third tip,\n        # to what it would be if it were on the straight line between the\n        # two first tips, given by Dseg[tips[:2]]\n        # if we did not normalize, there would be a danger of simply\n        # assigning the highest score to the longest segment\n        score = dseg[tips3[0]] / Dseg[tips3[1], tips3[2]]\n        logg.m('... group', iseg, 'score', score, 'n_points', len(seg),\n               '(too small)' if len(seg) < self.min_group_size else '', v=4)\n        if len(seg) < self.min_group_size: score = 0\n        # write result\n        scores_tips[iseg, 0] = score\n        scores_tips[iseg, 1:] = tips3\n    iseg = np.argmax(scores_tips[:, 0])\n    tips3 = scores_tips[iseg, 1:].astype(int)\n    return iseg, tips3", "idx": 1385}
{"project": "Scanpy", "commit_id": "73_scanpy_1.9.0_readwrite.py__download.py", "target": 0, "func": "def _download(url: str, path: Path):\n    try:\n        import ipywidgets\n        from tqdm.auto import tqdm\n    except ImportError:\n        from tqdm import tqdm\n\n    from urllib.request import urlopen, Request\n    from urllib.error import URLError\n\n    blocksize = 1024 * 8\n    blocknum = 0\n\n    try:\n        req = Request(url, headers={\"User-agent\": \"scanpy-user\"})\n\n        try:\n            open_url = urlopen(req)\n        except URLError:\n            logg.warning(\n                'Failed to open the url with default certificates, trying with certifi.'\n            )\n\n            from certifi import where\n            from ssl import create_default_context\n\n            open_url = urlopen(req, context=create_default_context(cafile=where()))\n\n        with open_url as resp:\n            total = resp.info().get(\"content-length\", None)\n            with tqdm(\n                unit=\"B\",\n                unit_scale=True,\n                miniters=1,\n                unit_divisor=1024,\n                total=total if total is None else int(total),\n            ) as t, path.open(\"wb\") as f:\n                block = resp.read(blocksize)\n                while block:\n                    f.write(block)\n                    blocknum += 1\n                    t.update(len(block))\n                    block = resp.read(blocksize)\n\n    except (KeyboardInterrupt, Exception):\n        # Make sure file doesn\u2019t exist half-downloaded\n        if path.is_file():\n            path.unlink()\n        raise", "idx": 1389}
{"project": "Scanpy", "commit_id": "835_scanpy_1.4_scanpy_plotting__tools___init__.py_embedding_density.py", "target": 1, "func": "def embedding_density(\n        adata: AnnData,\n        basis: str,\n        key: str,\n        group: Optional[str] = None,\n        color_map: Union[Colormap, str] = 'YlOrRd',\n        bg_dotsize: Optional[int] = 80,\n        fg_dotsize: Optional[int] = 160,\n        vmax: Optional[int] = 1,\n        vmin: Optional[int] = 0,\n        save: Union[bool, str, None] = None,\n        **kwargs):\n    \"\"\"Plot the density of cells in an embedding (per condition)\n    Plots the gaussian kernel density estimates (over condition) from the\n    `sc.tl.embedding_density()` output.\n    This function was written by Sophie Tritschler and implemented into\n    Scanpy by Malte Luecken.\n\n    Parameters\n    ----------\n    adata : :class:`~anndata.AnnData`\n        The annotated data matrix.\n    basis : `str`\n        The embedding over which the density was calculated. This embedded\n        representation should be found in `adata.obsm['X_[basis]']``.\n    key : `str`\n        Name of the `.obs` covariate that contains the density estimates\n    group : `str`, optional (default: `None`)\n        The category in the categorical observation annotation to be plotted.\n        For example, 'G1' in the cell cycle 'phase' covariate.\n    color_map : Union[`Colormap`, `str`] (default: `YlOrRd`)\n        Matplolib color map to use for density plotting.\n    bg_dotsize : `int`, optional (default: `80`)\n        Dot size for background data points not in the `group`.\n    fg_dotsize : `int`, optional (default: `160`)\n        Dot size for foreground data points in the `group`.\n    vmax : `int`, optional (default: `1`)\n        Density that corresponds to color bar maximum.\n    vmin : `int`, optional (default: `0`)\n        Density that corresponds to color bar minimum.\n    {show_save_ax}\n    Examples\n    --------\n    >>> adata = sc.datasets.pbmc68k_reduced()\n    >>> sc.tl.umap(adata)\n    >>> sc.tl.embedding_density(adata, basis='umap', groupby='phase')\n    >>> sc.pl.embedding_density(adata, basis='umap', key='umap_density_phase',\n    ...                         group='G1')\n    >>> sc.pl.embedding_density(adata, basis='umap', key='umap_density_phase',\n    ...                         group='S')\n    \"\"\"\n    sanitize_anndata(adata)\n\n    # Test user inputs\n    basis = basis.lower()\n    if basis == 'fa':\n        basis = 'draw_graph_fa'\n\n    if 'X_' + basis not in adata.obsm_keys():\n        raise ValueError('Cannot find the embedded representation `adata.obsm[X_{!r}]`.'\n                         'Compute the embedding first.'.format(basis))\n\n    if key not in adata.obs:\n        raise ValueError('Please run `sc.tl.embedding_density()` first and specify the correct key.')\n    if key + '_params' not in adata.uns:\n        raise ValueError('Please run `sc.tl.embedding_density()` first and specify the correct key.')\n\n    if 'components' in kwargs:\n        logg.warn('Components were specified, but will be ignored. Only the'\n                  'components used to calculate the density can be plotted.')\n\n    components = adata.uns[key + '_params']['components']\n    groupby = adata.uns[key + '_params']['covariate']\n\n    if (group is None) and (groupby is not None):\n        raise ValueError('Densities were calculated over an `.obs` covariate.'\n                         'Please specify a group from this covariate to plot.')\n\n    if (group is not None) and (group not in adata.obs[groupby].cat.categories):\n        raise ValueError('Please specify a group from the `.obs` category over which the density was calculated.')\n\n    if (np.min(adata.obs[key]) < 0) or (np.max(adata.obs[key]) > 1):\n        raise ValueError('Densities should be scaled between 0 and 1.')\n\n    # Define plotting data\n    dens_values = -np.ones(adata.n_obs)\n    dot_sizes = np.ones(adata.n_obs) * bg_dotsize\n    if group is not None:\n        group_mask = (adata.obs[groupby] == group)\n        dens_values[group_mask] = adata.obs[key][group_mask]\n        dot_sizes[group_mask] = np.ones(sum(group_mask)) * fg_dotsize\n    else:\n        dens_values = adata.obs[key]\n        dot_sizes = adata.ones(adata.n_obs) * fg_dotsize\n    # Make the color map\n    if isinstance(color_map, str):\n        cmap = cm.get_cmap(color_map)\n    else:\n        cmap = color_map\n    # norm = colors.Normalize(vmin=-1, vmax=1)\n    adata_vis = adata.copy()\n    adata_vis.obs['Density'] = dens_values\n\n    # colour transformation is not really working! Probably need to layer the plots...\n    norm = colors.Normalize(vmin=vmin, vmax=vmax)\n    cmap.set_over('black')\n    cmap.set_under('lightgray')\n\n    # Plot the graph\n    return plot_scatter(adata_vis, basis, components=components, color='Density', color_map=cmap, norm=norm,\n                        size=dot_sizes, vmax=vmax, vmin=vmin, save=save, **kwargs)", "idx": 1399}
{"project": "Scanpy", "commit_id": "183_scanpy_1.9.0_get.py_obs_df.py", "target": 0, "func": "def obs_df(\n    adata: AnnData,\n    keys: Iterable[str] = (),\n    obsm_keys: Iterable[Tuple[str, int]] = (),\n    *,\n    layer: str = None,\n    gene_symbols: str = None,\n    use_raw: bool = False,\n) -> pd.DataFrame:\n    \"\"\"\\\n    Return values for observations in adata.\n\n    Params\n    ------\n    adata\n        AnnData object to get values from.\n    keys\n        Keys from either `.var_names`, `.var[gene_symbols]`, or `.obs.columns`.\n    obsm_keys\n        Tuple of `(key from obsm, column index of obsm[key])`.\n    layer\n        Layer of `adata` to use as expression values.\n    gene_symbols\n        Column of `adata.var` to search for `keys` in.\n    use_raw\n        Whether to get expression values from `adata.raw`.\n\n    Returns\n    -------\n    A dataframe with `adata.obs_names` as index, and values specified by `keys`\n    and `obsm_keys`.\n\n    Examples\n    --------\n    Getting value for plotting:\n\n    >>> pbmc = sc.datasets.pbmc68k_reduced()\n    >>> plotdf = sc.get.obs_df(\n            pbmc,\n            keys=[\"CD8B\", \"n_genes\"],\n            obsm_keys=[(\"X_umap\", 0), (\"X_umap\", 1)]\n        )\n    >>> plotdf.plot.scatter(\"X_umap0\", \"X_umap1\", c=\"CD8B\")\n\n    Calculating mean expression for marker genes by cluster:\n\n    >>> pbmc = sc.datasets.pbmc68k_reduced()\n    >>> marker_genes = ['CD79A', 'MS4A1', 'CD8A', 'CD8B', 'LYZ']\n    >>> genedf = sc.get.obs_df(\n            pbmc,\n            keys=[\"louvain\", *marker_genes]\n        )\n    >>> grouped = genedf.groupby(\"louvain\")\n    >>> mean, var = grouped.mean(), grouped.var()\n    \"\"\"\n    if use_raw:\n        assert (\n            layer is None\n        ), \"Cannot specify use_raw=True and a layer at the same time.\"\n        var = adata.raw.var\n    else:\n        var = adata.var\n    if gene_symbols is not None:\n        alias_index = pd.Index(var[gene_symbols])\n    else:\n        alias_index = None\n\n    obs_cols, var_idx_keys, var_symbols = _check_indices(\n        adata.obs,\n        var.index,\n        \"obs\",\n        keys,\n        alias_index=alias_index,\n        use_raw=use_raw,\n    )\n\n    # Make df\n    df = pd.DataFrame(index=adata.obs_names)\n\n    # add var values\n    if len(var_idx_keys) > 0:\n        matrix = _get_array_values(\n            _get_obs_rep(adata, layer=layer, use_raw=use_raw),\n            var.index,\n            var_idx_keys,\n            axis=1,\n            backed=adata.isbacked,\n        )\n        df = pd.concat(\n            [df, pd.DataFrame(matrix, columns=var_symbols, index=adata.obs_names)],\n            axis=1,\n        )\n\n    # add obs values\n    if len(obs_cols) > 0:\n        df = pd.concat([df, adata.obs[obs_cols]], axis=1)\n\n    # reorder columns to given order (including duplicates keys if present)\n    if keys:\n        df = df[keys]\n\n    for k, idx in obsm_keys:\n        added_k = f\"{k}-{idx}\"\n        val = adata.obsm[k]\n        if isinstance(val, np.ndarray):\n            df[added_k] = np.ravel(val[:, idx])\n        elif isinstance(val, spmatrix):\n            df[added_k] = np.ravel(val[:, idx].toarray())\n        elif isinstance(val, pd.DataFrame):\n            df[added_k] = val.loc[:, idx]\n\n    return df", "idx": 1402}
{"project": "Scanpy", "commit_id": "66_scanpy_0.0_scanpy_plotting.py_plot_tool.py", "target": 1, "func": "def plot_tool(dplot, ddata,\n              rowcat='',\n              comps='1,2',\n              layout='2d',\n              legendloc='lower right',\n              cmap='jet',\n              adjust_right=0.75,\n              subtitles=['one title'],\n              component_name='comp'):\n    \"\"\"\n    Plot the results of a DPT analysis.\n    Parameters\n    ----------\n    dplot : dict\n        Dict returned by plotting tool.\n    ddata : dict\n        Data dictionary.\n    comps : str\n         String in the form \"comp1,comp2,comp3\".\n    layout : {'2d', '3d', 'unfolded 3d'}, optional (default: '2d')\n         Layout of plot.\n    legendloc : see matplotlib.legend, optional (default: 'lower right')\n         Options for keyword argument 'loc'.\n    cmap : str (default: jet)\n         String denoting matplotlib color map.\n    \"\"\"\n    params = locals(); del params['ddata']; del params['dplot']\n    from numpy import array\n    comps = array(params['comps'].split(',')).astype(int) - 1\n    # highlights\n    highlights = []\n    if False:\n        if 'highlights' in ddata:\n            highlights = ddata['highlights']\n    # base figure\n    try:\n        Y = dplot['Y'][:, comps]\n    except IndexError:\n        sett.mi('IndexError: Only computed', dplot['Y'].shape[1], ' components')\n        sett.mi('--> recompute using scanpy exkey diffmap -p nr_comps YOUR_NR')\n        from sys import exit\n        exit(0)\n    axs = scatter(Y,\n                  subtitles=subtitles,\n                  component_name=component_name,\n                  component_indexnames=comps + 1,\n                  layout=params['layout'],\n                  c='grey',\n                  highlights=highlights,\n                  cmap=params['cmap'])\n    # rowcategories in ddata\n    if 'rowcat' in ddata:\n        if rowcat == '':\n            rowcat = list(ddata['rowcat'].keys())[0]\n            sett.m(0, 'coloring according to', rowcat)\n        elif rowcat not in ddata['rowcat']:\n            print('specify valid row category class')\n        # colors for categories\n        if not rowcat + '_colors' in ddata:\n            ddata[rowcat + '_colors'] = pl.cm.get_cmap(params['cmap'])(\n                                                  pl.Normalize()(ddata[rowcat + '_ids']))\n        for icat in ddata[rowcat + '_ids']:\n            group(axs[0], rowcat, icat, ddata, dplot['Y'][:, comps], params['layout'])\n        if params['legendloc'] != 'none':\n            axs[0].legend(frameon=False, loc='center left', bbox_to_anchor=(1, 0.5))\n        # right margin\n        pl.subplots_adjust(right=params['adjust_right'])\n\n    savefig(dplot['writekey'])\n    if not sett.savefigs and sett.autoshow:\n        pl.show()", "idx": 1403}
{"project": "Scanpy", "commit_id": "587_scanpy_1.9.0_test_metrics.py_test_confusion_matrix.py", "target": 0, "func": "def test_confusion_matrix():\n    mtx = sc.metrics.confusion_matrix([\"a\", \"b\"], [\"c\", \"d\"], normalize=False)\n    assert mtx.loc[\"a\", \"c\"] == 1\n    assert mtx.loc[\"a\", \"d\"] == 0\n    assert mtx.loc[\"b\", \"d\"] == 1\n    assert mtx.loc[\"b\", \"c\"] == 0\n\n    mtx = sc.metrics.confusion_matrix([\"a\", \"b\"], [\"c\", \"d\"], normalize=True)\n    assert mtx.loc[\"a\", \"c\"] == 1.0\n    assert mtx.loc[\"a\", \"d\"] == 0.0\n    assert mtx.loc[\"b\", \"d\"] == 1.0\n    assert mtx.loc[\"b\", \"c\"] == 0.0\n\n    mtx = sc.metrics.confusion_matrix(\n        [\"a\", \"a\", \"b\", \"b\"], [\"c\", \"d\", \"c\", \"d\"], normalize=True\n    )\n    assert np.all(mtx == 0.5)", "idx": 1407}
{"project": "Scanpy", "commit_id": "221_scanpy_0.0_scanpy_preprocessing_simple.py_filter_genes_dispersion.py", "target": 1, "func": "def filter_genes_dispersion(data, log=True,\n                            min_disp=0.5, max_disp=None,\n                            min_mean=0.0125, max_mean=3,\n                            n_top_genes=None,\n                            flavor='seurat',\n                            plot=False, copy=False):\n    \"\"\"Extract highly variable genes.\n    Similar functions are used, for example, by Cell Ranger (Zheng et al., 2017)\n    and Seurat (Macosko et al., 2015).\n    Parameters\n    ----------\n    X : AnnData or array-like\n        Data matrix storing unlogarithmized data.\n    log : bool\n        Use the logarithm of mean and variance.\n    min_mean=0.0125, max_mean=3, min_disp=0.5, max_disp=None : float\n        Cutoffs for the gene expression, used if n_top_genes is None.\n    n_top_genes : int or None (default: None)\n        Number of highly-variable genes to keep.\n    flavor : {'seurat', 'cell_ranger'}\n        Choose method for computing normalized dispersion. Note that Seurat\n        passes the cutoffs whereas Cell Ranger passes `n_top_genes`.\n    plot : bool (default: False)\n        Plot the result.\n    copy : bool (default: False)\n        If an AnnData is passed, determines whether a copy is returned.\n    Notes\n    -----\n    If an AnnData is passed and copy == True, the following is returned, otherwise, adata is updated.\n    adata : AnnData\n        Filtered AnnData object.\n    with the following fields to adata.var:\n        means : np.ndarray of shape n_genes\n            Means per gene.\n        dispersions : np.ndarray of shape n_genes\n            Dispersions per gene.\n        dispersions_norm : np.ndarray of shape n_genes\n            Dispersions per gene.\n    If a data matrix is passed, the information is returned as tuple.\n        gene_filter, means, dispersions, dispersion_norm\n    \"\"\"\n    if isinstance(data, AnnData):\n        adata = data.copy() if copy else data\n        result = filter_genes_dispersion(adata.X, log=log,\n                                         min_disp=min_disp, max_disp=max_disp,\n                                         min_mean=min_mean, max_mean=max_mean,\n                                         n_top_genes=n_top_genes,\n                                         flavor=flavor, plot=plot)\n        gene_filter, means, dispersions, dispersions_norm = result\n        adata.var['means'] = means\n        adata.var['dispersions'] = dispersions\n        adata.var['dispersions_norm'] = dispersions_norm\n        if plot:\n            plot_filter_genes_dispersion(adata, gene_filter=gene_filter, log=not log)\n        adata.filter_var(gene_filter)\n        return adata if copy else None\n    sett.m(0, '... filter highly varying genes by dispersion and mean')\n    X = data  # proceed with data matrix\n    if False:  # the following is less efficient and has no support for sparse matrices\n        mean = np.mean(X, axis=0)\n        std = np.std(X, axis=0, ddof=1)  # use R convention\n        var = np.var(X, axis=0, ddof=1)\n    else:\n        from sklearn.preprocessing import StandardScaler\n        scaler = StandardScaler(with_mean=False).partial_fit(X)\n        mean = scaler.mean_\n        var = scaler.var_ * (X.shape[0]/(X.shape[0]-1))  # user R convention (unbiased estimator)\n        dispersion = var / (mean + 1e-12)\n        if log:  # consider logarithmized mean as in Seurat\n            dispersion[dispersion == 0] = np.nan\n            dispersion = np.log(dispersion)\n            mean = np.log1p(mean)\n    # all of the following quantities are \"per-gene\" here\n    import pandas as pd\n    df = pd.DataFrame()\n    df['mean'] = mean\n    df['dispersion'] = dispersion\n    if flavor == 'seurat':\n        df['mean_bin'] = pd.cut(df['mean'], bins=20)\n        disp_grouped = df.groupby('mean_bin')['dispersion']\n        disp_mean_bin = disp_grouped.mean()\n        disp_std_bin = disp_grouped.std(ddof=1)\n        df['dispersion_norm'] = (df['dispersion'].values  # use values here as index differs\n                                 - disp_mean_bin[df['mean_bin']].values) \\\n                                 / disp_std_bin[df['mean_bin']].values\n    elif flavor == 'cell_ranger':\n        df['mean_bin'] = pd.cut(df['mean'],\n                                np.r_[-np.inf, np.percentile(df['mean'],\n                                                             np.arange(10, 105, 5)),\n                                      np.inf])\n        var_by_bin = pd.DataFrame()\n        from statsmodels import robust\n        import warnings  # this raises a warning we do not want to display\n        with warnings.catch_warnings():\n            warnings.simplefilter('ignore')\n            var_by_bin['bin_disp_median'] = df.groupby('mean_bin').apply(\n                lambda group: np.median(group['dispersion']))\n            var_by_bin['bin_disp_mad'] = df.groupby('mean_bin').apply(\n                lambda group: robust.mad(group['dispersion']))\n        df = df.merge(var_by_bin, left_on='mean_bin', right_index=True)\n        df['dispersion_norm'] = np.abs(df['dispersion']\n                                       - df['bin_disp_median']) \\\n                                       / df['bin_disp_mad']\n    else:\n        raise ValueError('`flavor` needs to be \"seurat\" or \"cell_ranger\"')\n    dispersion_norm = np.array(df['dispersion_norm'].values)\n    if n_top_genes is not None:\n        dispersion_norm[::-1].sort()  # interestingly, np.argpartition is slightly slower\n        disp_cut_off = dispersion_norm[n_top_genes-1]\n        gene_filter = df['dispersion_norm'].values >= disp_cut_off\n        sett.m(0, 'dispersion cutoff', disp_cut_off)\n    else:\n        sett.m(0, '    using `min_disp`, `max_disp`, `min_mean` and `max_mean`')\n        sett.m(0, '--> set `n_top_genes` to simply select top-scoring genes instead')\n        max_disp = np.inf if max_disp is None else max_disp\n        dispersion_norm[np.isnan(dispersion_norm)] = 0  # similar to Seurat\n        gene_filter = np.logical_and.reduce((mean > min_mean, mean < max_mean,\n                                             dispersion_norm > min_disp,\n                                             dispersion_norm < max_disp))\n    return gene_filter, df['mean'].values, df['dispersion'].values, df['dispersion_norm'].values", "idx": 1411}
{"project": "Scanpy", "commit_id": "364_scanpy_1.9.0_scatterplots.py__check_scale_factor.py", "target": 0, "func": "def _check_scale_factor(\n    spatial_data: Optional[Mapping],\n    img_key: Optional[str],\n    scale_factor: Optional[float],\n) -> float:\n    \"\"\"Resolve scale_factor, defaults to 1.\"\"\"\n    if scale_factor is not None:\n        return scale_factor\n    elif spatial_data is not None and img_key is not None:\n        return spatial_data['scalefactors'][f\"tissue_{img_key}_scalef\"]\n    else:\n        return 1.0", "idx": 1414}
{"project": "Scanpy", "commit_id": "517_scanpy_1.9.0_test_embedding_plots.py_vbounds.py", "target": 0, "func": "def vbounds(request):\n    return request.param", "idx": 1419}
{"project": "Scanpy", "commit_id": "2_scanpy_0.0_scanpy_exs___init__.py_exdata.py", "target": 1, "func": "def exdata():\n    \"\"\"\n    Show available example data.\n    \"\"\"\n    s = utils.pretty_dict_string(dexdata())\n    print(s)", "idx": 1440}
{"project": "Scanpy", "commit_id": "470_scanpy_1.9.0_helpers.py__check_check_values_warnings.py", "target": 0, "func": "def _check_check_values_warnings(function, adata, expected_warning, kwargs={}):\n    '''Runs `function` on `adata` with provided arguments `kwargs` twice: once with `check_values=True` and once with `check_values=False`. Checks that the `expected_warning` is only raised whtn `check_values=True`.'''\n\n    # expecting 0 no-int warnings\n    with warnings.catch_warnings(record=True) as record:\n        function(adata.copy(), **kwargs, check_values=False)\n    warning_msgs = [w.message.args[0] for w in record]\n    assert expected_warning not in warning_msgs\n\n    # expecting 1 no-int warning\n    with warnings.catch_warnings(record=True) as record:\n        function(adata.copy(), **kwargs, check_values=True)\n    warning_msgs = [w.message.args[0] for w in record]\n    assert expected_warning in warning_msgs", "idx": 1449}
{"project": "Scanpy", "commit_id": "121_scanpy_1.9.0__utils.py_check_datasetdir_exists.py", "target": 0, "func": "def check_datasetdir_exists(f):\n    @wraps(f)\n    def wrapper(*args, **kwargs):\n        settings.datasetdir.mkdir(exist_ok=True)\n        return f(*args, **kwargs)\n\n    return wrapper", "idx": 1454}
{"project": "Scanpy", "commit_id": "811_scanpy_1.4_scanpy_preprocessing__qc.py_top_segment_proportions_sparse_csr.py", "target": 1, "func": "def top_segment_proportions_sparse_csr(data, indptr, ns):\n    ns = np.sort(ns)\n    maxidx = ns[-1]\n    sums = np.zeros((indptr.size - 1), dtype=data.dtype)\n    values = np.zeros((indptr.size-1, len(ns)), dtype=np.float64)\n    # Just to keep it simple, as a dense matrix\n    partitioned = np.zeros((indptr.size-1, maxidx), dtype=data.dtype)\n    for i in numba.prange(indptr.size - 1):\n        start, end = indptr[i], indptr[i+1]\n        sums[i] = np.sum(data[start:end])\n        if end - start <= maxidx:\n            partitioned[i, :end-start] = data[start:end]\n        elif (end - start) > maxidx:\n            partitioned[i, :] = - \\\n                (np.partition(-data[start:end], maxidx))[:maxidx]\n    partitioned = np.apply_along_axis(\n        np.partition, 1, partitioned, maxidx - ns)[:, ::-1][:, :ns[-1]]\n    acc = np.zeros((indptr.size-1), dtype=data.dtype)\n    prev = 0\n    for j, n in enumerate(ns):\n        acc += partitioned[:, prev:n].sum(axis=1)\n        values[:, j] = acc\n        prev = n\n    return values / sums[:, None]", "idx": 1455}
{"project": "Scanpy", "commit_id": "641_scanpy_1.1_scanpy_plotting_tools_paga.py_paga_compare.py", "target": 1, "func": "def paga_compare(\n        adata,\n        basis='tsne',\n        edges=None,\n        color=None,\n        alpha=None,\n        groups=None,\n        components=None,\n        projection='2d',\n        legend_loc='on data',\n        legend_fontsize=None,\n        legend_fontweight=None,\n        color_map=None,\n        palette=None,\n        size=None,\n        title=None,\n        right_margin=None,\n        left_margin=0.05,\n        show=None,\n        save=None,\n        title_graph=None,\n        groups_graph=None,\n        color_graph=None,\n        **paga_graph_params):\n    \"\"\"Scatter and abstracted graph side-by-side.\n    Consists in a scatter plot and the abstracted graph. See\n    :func:`~scanpy.api.pl.paga` for all related parameters.\n    See :func:`~scanpy.api.pl.paga_path` for visualizing gene changes along paths\n    through the abstracted graph.\n    Additional parameters are as follows.\n    Parameters\n    ----------\n    adata : :class:`~scanpy.api.AnnData`\n        Annotated data matrix.\n    color : string or list of strings, optional (default: None)\n        Keys for observation/cell annotation either as list `[\"ann1\", \"ann2\"]` or\n        string `\"ann1,ann2,...\"`.\n    groups : str, optional (default: all groups)\n        Restrict to a few categories in categorical observation annotation.\n    legend_loc : str, optional (default: 'right margin')\n         Location of legend, either 'on data', 'right margin' or valid keywords\n         for matplotlib.legend.\n    legend_fontsize : int (default: None)\n         Legend font size.\n    color_map : str (default: `matplotlib.rcParams['image.cmap']`)\n         String denoting matplotlib color map.\n    palette : list of str (default: None)\n         Colors to use for plotting groups (categorical annotation).\n    size : float (default: None)\n         Point size.\n    title : str, optional (default: None)\n         Provide title for panels either as `[\"title1\", \"title2\", ...]`.\n    right_margin : float or list of floats (default: None)\n         Adjust the width of the space right of each plotting panel.\n    title : `str` or `None`, optional (default: `None`)\n        Title for the scatter panel, or, if `title_graph is None`, title for the\n        whole figure.\n    title_graph : `str` or `None`, optional (default: `None`)\n        Separate title for the abstracted graph.\n    show : bool, optional (default: None)\n         Show the plot, do not return axis.\n    save : `bool` or `str`, optional (default: `None`)\n        If `True` or a `str`, save the figure. A string is appended to the\n        default filename. Infer the filetype if ending on \\{'.pdf', '.png', '.svg'\\}.\n    ax : matplotlib.Axes\n         A matplotlib axes object.\n    \"\"\"\n    axs, _, _, _ = utils.setup_axes(panels=[0, 1],\n                                    right_margin=right_margin)  # dummy colors\n    # set a common title for the figure\n    suptitle = None\n    if title is not None and title_graph is None:\n        suptitle = title\n        title = ''\n        title_graph = ''\n    _paga_scatter(adata,\n                basis=basis,\n                edges=edges,\n                color=color,\n                alpha=alpha,\n                groups=groups,\n                components=components,\n                projection=projection,\n                legend_loc=legend_loc,\n                legend_fontsize=legend_fontsize,\n                legend_fontweight=legend_fontweight,\n                color_map=color_map,\n                palette=palette,\n                right_margin=None,\n                size=size,\n                title=title,\n                ax=axs[0],\n                show=False,\n                save=False)\n    paga(adata, ax=axs[1], show=False, save=False, title=title_graph,\n         labels=groups_graph, colors=color_graph, **paga_graph_params)\n    if suptitle is not None: pl.suptitle(suptitle)\n    utils.savefig_or_show('paga_compare', show=show, save=save)\n    if show == False: return axs", "idx": 1457}
{"project": "Scanpy", "commit_id": "114_scanpy_0.0_scanpy___init__.py_run_args.py", "target": 1, "func": "def run_args(toolkey, args):\n    \"\"\"\n    Run specified tool, do preprocessing and read/write outfiles.\n    Result files store the dictionary returned by the tool. File type is\n    determined by variable sett.extd allowed are 'h5' (hdf5), 'xlsx' (Excel) or\n    'csv' (comma separated value file).\n    If called twice with the same settings the existing result file is used.\n    Parameters\n    ----------\n    toolkey : str\n        Name of the tool.\n    args : dict containing\n        exkey : str\n            String that identifies the example use key.\n    \"\"\"\n    # help on plot parameters\n    if args['pparams']:\n        if args['pparams'][0] == 'help':\n            from sys import exit\n            exit(get_tool(toolkey).plot.__doc__)\n\n    # read parameters\n    if toolkey == 'sim':\n        if args['opfile'] != '':\n            oparams = read_params(args['opfile'])\n        else:\n            opfile_sim = 'sim/' + args['exkey'] + '_oparams.txt'\n            oparams = read_params(opfile_sim)\n            sett.m(0,'--> you can specify your custom params file using the option\\n'\n                     '    \"--opfile\" or provide parameters directly via \"--oparams\"')\n        if 'writedir' not in oparams:\n            oparams['writedir'] = sett.writedir + sett.basekey + '_' + toolkey\n    else:\n        adata, exmodule = example(args['exkey'], subsample=args['subsample'],\n                                  return_module=True)\n        oparams = {}\n        # try to load tool parameters from dexamples\n        try:\n            did_not_find_params_in_exmodule = False\n            dexample = exmodule.dexamples[args['exkey']]\n            oparams = {}\n            for key in dexample.keys():\n                if toolkey in key:\n                    oparams = dexample[key]\n                    sett.m(0, '... appending \"-o',\n                           ' '.join([' '.join([k, str(v)]) for k, v in oparams.items()])\n                           + '\"',\n                          'to call of', toolkey)\n                    break\n        except:\n            did_not_find_params_in_exmodule = True\n            pass\n        # if optional parameters have been specified in a parameter file update\n        # the current param dict with these\n        if args['opfile'] != '':\n            add_params = read_params(args['opfile'])\n            oparams = utils.update_params(oparams, add_params)\n        # same if optional parameters have been specified on the command line\n        if args['oparams']:\n            add_params = readwrite.get_params_from_list(args['oparams'])\n            sett.m(0, '... overwriting optional params', '\"' +\n                   ' '.join([' '.join([k, str(v)]) for k, v in add_params.items()])\n                   + '\"',\n                  'to call of', toolkey)\n            oparams = utils.update_params(oparams, add_params)\n        elif did_not_find_params_in_exmodule and args['opfile'] != '':\n            sett.m(0, 'using default parameters, change them using \"--oparams\"')\n\n    # read/write files\n    writekey = sett.basekey + '_' + toolkey\n    opfile = sett.writedir + writekey + '_oparams.txt'\n    if args['logfile']:\n        logfile = sett.writedir + writekey + '_log.txt'\n        sett.logfile(logfile)\n\n    # actual call of tool\n    from os.path import exists\n    if ((toolkey == 'sim'\n         or toolkey not in adata['tools'])\n        or sett.recompute != 'none'):\n        tool = get_tool(toolkey, func=True)\n        if toolkey == 'sim':\n            adata = tool(**oparams)\n        else:\n            adata = tool(adata, **oparams)\n        # append toolkey to tools in adata\n        if (toolkey != 'sim'\n            and toolkey not in adata['tools']):\n            import numpy as np\n            adata['tools'] = np.append(adata['tools'], toolkey)\n        write(sett.basekey, adata)\n        sett.m(0, 'updated file',\n               readwrite.get_filename_from_key(sett.basekey))\n        # save a copy of the changed parameters\n        readwrite.write_params(opfile, oparams)\n    # plotting and postprocessing\n    pparams = (readwrite.get_params_from_list(args['pparams'])\n               if args['pparams'] else {})\n    # post-processing specific to example and tool\n    # - only if we are not subsampling\n    if toolkey != 'sim':\n        postprocess = args['exkey'] + '_' + toolkey\n        if postprocess in dir(exmodule) and args['subsample'] == 1:\n            adata = getattr(exmodule, postprocess)(adata)\n            write(sett.basekey, adata)\n    plot(toolkey, adata, **pparams)", "idx": 1462}
{"project": "Scanpy", "commit_id": "604_scanpy_1.0.4_scanpy_plotting_utils.py_adjust_palette.py", "target": 1, "func": "def adjust_palette(palette, length):\n    islist = False\n    if isinstance(palette, list):\n        islist = True\n    if ((islist and len(palette) < length)\n       or (not isinstance(palette, list) and len(palette.by_key()['color']) < length)):\n        if length <= 28:\n            palette = palettes.default_26\n        else:\n            palette = palettes.default_64\n        logg.m('... updating the color palette to provide enough colors')\n        return palette if islist else cycler(color=palette)\n    elif islist:\n        return palette\n    elif not isinstance(palette, Cycler):\n        return cycler(color=palette)\n    else:\n        return palette", "idx": 1464}
{"project": "Scanpy", "commit_id": "536_scanpy_0.4.4_scanpy_neighbors___init__.py___init__.py", "target": 1, "func": "def __init__(self, adata):\n    self._adata = adata\n    self._init_iroot()\n    # use the graph in adata\n    info_str = ''\n    if 'neighbors' in adata.uns:\n        self.knn = issparse(adata.uns['neighbors']['distances'])\n        self._distances = adata.uns['neighbors']['distances']\n        self._connectivities = adata.uns['neighbors']['connectivities']\n        if self.knn:\n            self.n_neighbors = adata.uns[\n                                   'neighbors']['distances'][0].nonzero()[0].size + 1\n        else:\n            self.n_neighbors = None  # is unknown\n        info_str += '`.distances` `.connectivities` '\n    else:\n        self.knn = None\n        self._distances = None\n        self._connectivities = None\n    if 'X_diffmap' in adata.obsm_keys():\n        self._eigen_values = _backwards_compat_get_full_eval(adata)\n        self._eigen_basis = _backwards_compat_get_full_X_diffmap(adata)\n        self.n_dcs = len(self._eigen_values)\n        info_str += '`.eigen_values` `.eigen_basis` `.distances_dpt`'\n    else:\n        self._eigen_values = None\n        self._eigen_basis = None\n        self.n_dcs = None\n    if info_str != '':\n        logg.info('    initialized {}'.format(info_str))", "idx": 1470}
{"project": "Scanpy", "commit_id": "18_scanpy_0.0_scanpy___init__.py_run_args.py", "target": 1, "func": "def run_args(toolkey, args):\n    \"\"\"\n    Run specified tool, do preprocessing and read/write outfiles.\n    Output files store the dictionary returned by the tool. File type is\n    determined by variable sett.extd allowed are 'h5' (hdf5), 'xlsx' (Excel) or\n    'csv' (comma separated value file).\n    If called twice with the same settings the existing output file is returned.\n    Parameters\n    ----------\n    toolkey : str\n        Name of the tool.\n    args : dict containing\n        exkey : str\n            String that identifies the example use key.\n    Returns\n    -------\n    dfunc : dict of type toolkey\n    dadd : dict\n         Additional dict used for plotting in a later step.\n    \"\"\"\n    if args['plotparams']:\n        if args['plotparams'][0] == 'help':\n            from sys import exit\n            exit(get_tool(toolkey).plot.__doc__)\n    writekey = sett.basekey + '_' + toolkey + sett.fsig\n    resultfile = sett.writedir + writekey + '.' + sett.extd\n    paramsfile = sett.writedir + writekey + '_params.txt'\n    if args['logfile']:\n        logfile = sett.writedir + writekey + '_log.txt'\n        sett.logfile(logfile)\n    if toolkey == 'sim':\n        if args['paramsfile'] != '':\n            params = read_params(args['paramsfile'])\n        else:\n            paramsfile_sim = 'sim/' + args['exkey'] + '_params.txt'\n            params = read_params(paramsfile_sim)\n            sett.m(0,'--> you can specify your custom params file using the option\\n'\n                     '    \"--paramsfile\" or provide parameters directly via \"--params\"')\n        if 'writedir' not in params:\n            params['writedir'] = sett.writedir + sett.basekey + '_' + toolkey\n    else:\n        ddata, exmodule = example(args['exkey'], return_module=True)\n        params = {}\n        if args['params']:\n            params = utils.get_params_from_list(args['params'])\n        # if a parameter file has been specified, load the parameter file\n        elif args['paramsfile'] != '':\n            params = read_params(args['paramsfile'])\n        # otherwise, load tool parameters from dexamples\n        else:\n            try:\n                dexample = exmodule.dexamples[args['exkey']]\n                params = {}\n                for key in dexample.keys():\n                    if toolkey in key:\n                        params = dexample[key]\n            except:\n                sett.m(0, 'using default parameters')\n                pass\n\n    # subsampling\n    if args['subsample'] != 1:\n        ddata = subsample(ddata,args['subsample'])\n\n    # previous tool\n    if 'prev' in args:\n        prevkey = sett.basekey + '_' + args['prev'] + sett.fsig\n        dprev = read(prevkey)\n\n    # simply load resultfile\n    if os.path.exists(resultfile) and not sett.recompute:\n        dtool = read(writekey)\n    # call the tool resultfile\n    else:\n        # TODO: solve this in a nicer way, also get an ordered dict for params\n        from inspect import getcallargs\n        tool = get_tool(toolkey, func=True)\n        if toolkey == 'sim':\n            dtool = tool(**params)\n            params = getcallargs(tool, **params)\n        elif 'prev' in args:\n            dtool = tool(dprev, ddata, **params)\n            params = getcallargs(tool, dprev, ddata, **params)\n            # TODO: Would be good to name the first argument dprev_or_ddata\n            #       in difftest, but this doesn't work\n            del params['dprev']\n            del params['ddata']\n        else:\n            dtool = tool(ddata, **params)\n            params = getcallargs(tool, ddata, **params)\n            del params['ddata']\n        dtool['writekey'] = writekey\n        write(writekey, dtool)\n        # save a copy of the parameters to a file\n        utils.write_params(paramsfile, params)\n    # plotting and postprocessing\n    plotparams = {}\n    if args['plotparams']:\n        plotparams = utils.get_params_from_list(args['plotparams'])\n    if toolkey == 'sim':\n        plot(dtool, plotparams)\n    else:\n        # post-processing specific to example and tool\n        postprocess = args['exkey'] + '_' + toolkey\n        if postprocess in dir(exmodule) and args['subsample'] == 1:\n            dtool = getattr(exmodule, postprocess)(dtool)\n            write(writekey, dtool)\n        if args['plotkey'] != '':\n            plotwritekey = sett.basekey + '_' +  args['plotkey'] + sett.fsig\n            dplot = read(plotwritekey)\n            plot(dtool, ddata, dplot, **plotparams)\n        else:\n            plot(dtool, ddata, **plotparams)", "idx": 1474}
{"project": "Scanpy", "commit_id": "667_scanpy_1.9.0_test_plotting.py_test_scrublet_plots.py", "target": 0, "func": "def test_scrublet_plots(image_comparer, plt):\n    save_and_compare_images = image_comparer(ROOT, FIGS, tol=30)\n\n    adata = pbmc3k()\n    sc.external.pp.scrublet(adata, use_approx_neighbors=False)\n\n    sc.external.pl.scrublet_score_distribution(adata, return_fig=True)\n    save_and_compare_images('scrublet')\n\n    del adata.uns['scrublet']['threshold']\n    adata.obs['predicted_doublet'] = False\n\n    sc.external.pl.scrublet_score_distribution(adata, return_fig=True)\n    save_and_compare_images('scrublet_no_threshold')\n\n    adata.obs['batch'] = 1350 * ['a'] + 1350 * ['b']\n    sc.external.pp.scrublet(adata, use_approx_neighbors=False, batch_key='batch')\n\n    sc.external.pl.scrublet_score_distribution(adata, return_fig=True)\n    save_and_compare_images('scrublet_with_batches')", "idx": 1480}
{"project": "Scanpy", "commit_id": "492_scanpy_0.4.1_scanpy_tools_rank_genes_groups.py_rank_genes_groups.py", "target": 1, "func": "def rank_genes_groups(\n        adata,\n        group_by,\n        use_raw=True,\n        groups='all',\n        reference='rest',\n        n_genes=100,\n        compute_distribution=False,\n        only_positive=True,\n        copy=False,\n        test_type='t-test_overestim_var'):\n    \"\"\"Rank genes according to differential expression [Wolf17]_.\n    Rank genes by differential expression. By default, a t-test-like ranking is\n    used, in which means are normalized with variances.\n    Parameters\n    ----------\n    adata : :class:`~scanpy.api.AnnData`\n        Annotated data matrix.\n    group_by : `str`\n        The key of the sample grouping to consider.\n    use_raw : `bool`, optional (default: `True`)\n        Use `raw` attribute of `adata` if present.\n    groups : `str`, `list`, optional (default: `'all'`)\n        Subset of groups, e.g. `['g1', 'g2', 'g3']`, to which comparison shall\n        be restricted. If not passed, a ranking will be generated for all\n        groups.\n    reference : `str`, optional (default: `'rest'`)\n        If `'rest'`, compare each group to the union of the rest of the group.  If\n        a group identifier, compare with respect to this group.\n    n_genes : `int`, optional (default: 100)\n        How many genes to rank.\n    test_type : {'t-test_overestim_var', 't-test', 'wilcoxon'}, optional (default: 't-test_overestim_var')\n        If 't-test', use t-test to calculate test statistics. If 'wilcoxon', use\n        Wilcoxon-Rank-Sum to calculate test statistic. If\n        't-test_overestim_var', overestimate variance.\n    only_positive : bool, optional (default: `True`)\n        Only consider positive differences.\n    Returns\n    -------\n    rank_genes_groups_gene_scores : structured `np.ndarray` (adata.uns)\n        Structured array to be indexed by group id of shape storing the zscore\n        for each gene for each group.\n    rank_genes_groups_gene_names : structured `np.ndarray` (adata.uns)\n        Structured array to be indexed by group id for storing the gene names.\n    \"\"\"\n    logg.info('rank differentially expressed genes', r=True)\n    adata = adata.copy() if copy else adata\n    utils.sanitize_anndata(adata)\n    if compute_distribution:\n        logg.warn('`compute_distribution` is deprecated, as it requires storing'\n                  'a shifted and rescaled disribution for each gene'\n                  'You can now run `sc.pl.rank_genes_groups_violin` without it, '\n                  'which will show the original distribution of the gene.')\n    # for clarity, rename variable\n    n_genes_user = n_genes\n    groups_order = groups\n    if isinstance(groups_order, list) and isinstance(groups_order[0], int):\n        groups_order = [str(n) for n in groups_order]\n    if reference != 'rest' and reference not in set(groups_order):\n        groups_order += [reference]\n    if (reference != 'rest'\n        and reference not in set(adata.obs[group_by].cat.categories)):\n        raise ValueError('reference = {} needs to be one of group_by = {}.'\n                         .format(reference,\n                                 adata.obs[group_by].cat.categories.tolist()))\n    groups_order, groups_masks = utils.select_groups(\n        adata, groups_order, group_by)\n    adata.uns['rank_genes_groups_params'] = np.array(\n        (group_by, reference, test_type, use_raw),\n        dtype=[('group_by', 'U50'), ('reference', 'U50'), ('test_type', 'U50'), ('use_raw', np.bool_)])\n    # adata_comp mocks an AnnData object if use_raw is True\n    # otherwise it's just the AnnData object\n    adata_comp = adata\n    if adata.raw is not None and use_raw:\n        adata_comp = adata.raw\n    X = adata_comp.X\n\n    # Make sure indices are not OoB in case there are less genes than n_genes\n    if n_genes > X.shape[1]:\n        n_genes = X.shape[1]\n\n    rankings_gene_zscores = []\n    rankings_gene_names = []\n    n_groups = groups_masks.shape[0]\n    n_genes = X.shape[1]\n    ns = np.zeros(n_groups, dtype=int)\n    for imask, mask in enumerate(groups_masks):\n        ns[imask] = np.where(mask)[0].size\n    logg.info('    consider \\'{}\\':'.format(group_by), groups_order,\n              'with sample numbers', ns)\n    if reference != 'rest':\n        ireference = np.where(groups_order == reference)[0][0]\n    reference_indices = np.arange(adata_comp.n_vars, dtype=int)\n    avail_tests = {'t-test', 't-test_overestim_var', 'wilcoxon'}\n    if test_type not in avail_tests:\n        raise ValueError('test_type should be one of {}.'\n                         '\"t-test_overestim_var\" is being used as default.'\n                         .format(avail_tests))\n    if test_type in {'t-test', 't-test_overestim_var'}:\n        # loop over all masks and compute means, variances and sample numbers\n        means = np.zeros((n_groups, n_genes))\n        vars = np.zeros((n_groups, n_genes))\n        for imask, mask in enumerate(groups_masks):\n            means[imask], vars[imask] = simple._get_mean_var(X[mask])\n        # test each either against the union of all other groups or against a\n        # specific group\n        for igroup in range(n_groups):\n            if reference == 'rest':\n                mask_rest = ~groups_masks[igroup]\n            else:\n                if igroup == ireference: continue\n                else: mask_rest = groups_masks[ireference]\n            mean_rest, var_rest = simple._get_mean_var(X[mask_rest])\n            if test_type == 't-test':\n                ns_rest = np.where(mask_rest)[0].size\n            else:  # hack for overestimating the variance\n                ns_rest = ns[igroup]\n            denominator = np.sqrt(vars[igroup]/ns[igroup] + var_rest/ns_rest)\n            denominator[np.flatnonzero(denominator == 0)] = np.nan\n            zscores = (means[igroup] - mean_rest) / denominator\n            zscores[np.isnan(zscores)] = 0\n            zscores = zscores if only_positive else np.abs(zscores)\n            partition = np.argpartition(zscores, -n_genes_user)[-n_genes_user:]\n            partial_indices = np.argsort(zscores[partition])[::-1]\n            global_indices = reference_indices[partition][partial_indices]\n            rankings_gene_zscores.append(zscores[global_indices])\n            rankings_gene_names.append(adata_comp.var_names[global_indices])\n            if compute_distribution:\n                mask = groups_masks[igroup]\n                for gene_counter in range(n_genes_user):\n                    gene_idx = global_indices[gene_counter]\n                    X_col = X[mask, gene_idx]\n                    if issparse(X): X_col = X_col.toarray()[:, 0]\n                    identifier = _build_identifier(group_by, groups_order[igroup],\n                                                   gene_counter, adata_comp.var_names[gene_idx])\n                    full_col = np.empty(adata.n_obs)\n                    full_col[:] = np.nan\n                    full_col[mask] = (X_col - mean_rest[gene_idx]) / denominator[gene_idx]\n                    adata.obs[identifier] = full_col\n    elif test_type == 'wilcoxon':\n        # Wilcoxon-rank-sum test is usually more powerful in detecting marker genes\n        # Limit maximal RAM that is required by the calculation. Currently set fixed to roughly 100 MByte\n        CONST_MAX_SIZE = 10000000\n        ns_rest = np.zeros(n_groups, dtype=int)\n        # initialize space for z-scores\n        zscores = np.zeros(n_genes)\n        # First loop: Loop over all genes\n        if reference != 'rest':\n            for imask, mask in enumerate(groups_masks):\n                if imask == ireference: continue\n                else: mask_rest = groups_masks[ireference]\n                ns_rest[imask] = np.where(mask_rest)[0].size\n                if ns_rest[imask] <= 25 or ns[imask] <= 25:\n                    logg.hint('Few observations in a group for '\n                              'normal approximation (<=25). Lower test accuracy.')\n                n_active = ns[imask]\n                m_active = ns_rest[imask]\n                # Now calculate gene expression ranking in chunkes:\n                chunk = []\n                # Calculate chunk frames\n                n_genes_max_chunk = floor(CONST_MAX_SIZE / (n_active + m_active))\n                if n_genes_max_chunk < n_genes - 1:\n                    chunk_index = n_genes_max_chunk\n                    while chunk_index < n_genes - 1:\n                        chunk.append(chunk_index)\n                        chunk_index = chunk_index + n_genes_max_chunk\n                    chunk.append(n_genes - 1)\n                else:\n                    chunk.append(n_genes - 1)\n                left = 0\n                # Calculate rank sums for each chunk for the current mask\n                for chunk_index, right in enumerate(chunk):\n                    # Check if issparse is true: AnnData objects are currently sparse.csr or ndarray.\n                    if issparse(X):\n                        df1 = pd.DataFrame(data=X[mask, left:right].todense())\n                        df2 = pd.DataFrame(data=X[mask_rest, left:right].todense(),\n                                           index=np.arange(start=n_active, stop=n_active + m_active))\n                    else:\n                        df1 = pd.DataFrame(data=X[mask, left:right])\n                        df2 = pd.DataFrame(data=X[mask_rest, left:right],\n                                           index=np.arange(start=n_active, stop=n_active + m_active))\n                    df1 = df1.append(df2)\n                    ranks = df1.rank()\n                    # sum up adjusted_ranks to calculate W_m,n\n                    zscores[left:right] = np.sum(ranks.loc[0:n_active, :])\n                    left = right + 1\n                zscores = (zscores - (n_active * (n_active + m_active + 1) / 2)) / sqrt(\n                    (n_active * m_active * (n_active + m_active + 1) / 12))\n                zscores = zscores if only_positive else np.abs(zscores)\n                zscores[np.isnan(zscores)] = 0\n                partition = np.argpartition(zscores, -n_genes_user)[-n_genes_user:]\n                partial_indices = np.argsort(zscores[partition])[::-1]\n                global_indices = reference_indices[partition][partial_indices]\n                rankings_gene_zscores.append(zscores[global_indices])\n                rankings_gene_names.append(adata_comp.var_names[global_indices])\n                if compute_distribution:\n                    # Add calculation of means, var: (Unnecessary for wilcoxon if compute distribution=False)\n                    mean, vars = simple._get_mean_var(X[mask])\n                    mean_rest, var_rest = simple._get_mean_var(X[mask_rest])\n                    denominator = np.sqrt(vars / ns[imask] + var_rest / ns_rest[imask])\n                    denominator[np.flatnonzero(denominator == 0)] = np.nan\n                    for gene_counter in range(n_genes_user):\n                        gene_idx = global_indices[gene_counter]\n                        X_col = X[mask, gene_idx]\n                        if issparse(X): X_col = X_col.toarray()[:, 0]\n                        identifier = _build_identifier(group_by, groups_order[imask],\n                                                       gene_counter, adata_comp.var_names[gene_idx])\n                        full_col = np.empty(adata.n_obs)\n                        full_col[:] = np.nan\n                        full_col[mask] = (X_col - mean_rest[gene_idx]) / denominator[gene_idx]\n                        adata.obs[identifier] = full_col\n        # If no reference group exists, ranking needs only to be done once (full mask)\n        else:\n            zscores = np.zeros((n_groups, n_genes))\n            chunk = []\n            n_cells = X.shape[0]\n            n_genes_max_chunk = floor(CONST_MAX_SIZE / n_cells)\n            if n_genes_max_chunk < n_genes - 1:\n                chunk_index = n_genes_max_chunk\n                while chunk_index < n_genes - 1:\n                    chunk.append(chunk_index)\n                    chunk_index = chunk_index + n_genes_max_chunk\n                chunk.append(n_genes - 1)\n            else:\n                chunk.append(n_genes - 1)\n            left = 0\n            for chunk_index, right in enumerate(chunk):\n                # Check if issparse is true\n                if issparse(X):\n                    df1 = pd.DataFrame(data=X[:, left:right].todense())\n                else:\n                    df1 = pd.DataFrame(data=X[:, left:right])\n                ranks = df1.rank()\n                # sum up adjusted_ranks to calculate W_m,n\n                for imask, mask in enumerate(groups_masks):\n                    zscores[imask, left:right] = np.sum(ranks.loc[mask, :])\n                left = right + 1\n            for imask, mask in enumerate(groups_masks):\n                zscores[imask, :] = (zscores[imask, :] - (ns[imask] * (n_cells + 1) / 2)) / sqrt(\n                    (ns[imask] * (n_cells - ns[imask]) * (n_cells + 1) / 12))\n                zscores = zscores if only_positive else np.abs(zscores)\n                zscores[np.isnan(zscores)] = 0\n                partition = np.argpartition(zscores[imask, :], -n_genes_user)[-n_genes_user:]\n                partial_indices = np.argsort(zscores[imask, partition])[::-1]\n                global_indices = reference_indices[partition][partial_indices]\n                rankings_gene_zscores.append(zscores[imask, global_indices])\n                rankings_gene_names.append(adata_comp.var_names[global_indices])\n                if compute_distribution:\n                    mean, vars = simple._get_mean_var(X[mask])\n                    mean_rest, var_rest = simple._get_mean_var(X[~mask])\n                    denominator = np.sqrt(vars / ns[imask] + var_rest / (n_cells-ns[imask]))\n                    denominator[np.flatnonzero(denominator == 0)] = np.nan\n                    for gene_counter in range(n_genes_user):\n                        gene_idx = global_indices[gene_counter]\n                        X_col = X[mask, gene_idx]\n                        if issparse(X): X_col = X_col.toarray()[:, 0]\n                        identifier = _build_identifier(group_by, groups_order[imask],\n                                                       gene_counter, adata_comp.var_names[gene_idx])\n                        full_col = np.empty(adata.n_obs)\n                        full_col[:] = np.nan\n                        full_col[mask] = (X_col - mean_rest[gene_idx]) / denominator[gene_idx]\n                        adata.obs[identifier] = full_col\n    groups_order_save = [str(g) for g in groups_order]\n    if reference != 'rest':\n        groups_order_save = [g for g in groups_order if g != reference]\n    adata.uns['rank_genes_groups_gene_scores'] = np.rec.fromarrays(\n        [n for n in rankings_gene_zscores],\n        dtype=[(rn, 'float32') for rn in groups_order_save])\n    adata.uns['rank_genes_groups_gene_names'] = np.rec.fromarrays(\n        [n for n in rankings_gene_names],\n        dtype=[(rn, 'U50') for rn in groups_order_save])\n    logg.info('    finished', time=True, end=' ' if settings.verbosity > 2 else '\\n')\n    logg.hint('added\\n'\n           '    \\'rank_genes_groups_gene_names\\', np.recarray to be indexed by group ids (adata.uns)\\n'\n           '    \\'rank_genes_groups_gene_scores\\', np.recarray to be indexed by group ids (adata.uns)')\n    return adata if copy else None", "idx": 1488}
{"project": "Scanpy", "commit_id": "657_scanpy_1.9.0_test_plotting.py_test_scatter_no_basis_raw.py", "target": 0, "func": "def test_scatter_no_basis_raw(check_same_image, pbmc_filtered, tmpdir):\n    \"\"\"Test scatterplots of raw layer with no basis.\"\"\"\n    path1 = tmpdir / \"scatter_EGFL7_F12_FAM185A_rawNone.png\"\n    path2 = tmpdir / \"scatter_EGFL7_F12_FAM185A_rawTrue.png\"\n    path3 = tmpdir / \"scatter_EGFL7_F12_FAM185A_rawToAdata.png\"\n\n    sc.pl.scatter(pbmc_filtered, x='EGFL7', y='F12', color='FAM185A', use_raw=None)\n    plt.savefig(path1)\n    plt.close()\n\n    # is equivalent to:\n    sc.pl.scatter(pbmc_filtered, x='EGFL7', y='F12', color='FAM185A', use_raw=True)\n    plt.savefig(path2)\n    plt.close()\n\n    # and also to:\n    sc.pl.scatter(pbmc_filtered.raw.to_adata(), x='EGFL7', y='F12', color='FAM185A')\n    plt.savefig(path3)\n\n    check_same_image(path1, path2, tol=15)\n    check_same_image(path1, path3, tol=15)", "idx": 1489}
{"project": "Scanpy", "commit_id": "749_scanpy_1.3.3_docs_conf.py_handle_item.py", "target": 1, "func": "def handle_item(fieldarg: str, content: List[nodes.inline]) -> nodes.definition_list_item:\n    head = nodes.term()\n    head += makerefs(self.rolename, fieldarg, addnodes.literal_strong)\n    fieldtype = types.pop(fieldarg, None)\n    if fieldtype is not None:\n        head += nodes.Text(' : ')\n        if len(fieldtype) == 1 and isinstance(fieldtype[0], nodes.Text):\n            typename = ''.join(n.astext() for n in fieldtype)\n            head += makerefs(self.typerolename, typename, addnodes.literal_emphasis)\n        else:\n            head += fieldtype\n\n    body_content = nodes.paragraph('', '', *content)\n    body = nodes.definition('', body_content)\n    return nodes.definition_list_item('', head, body)", "idx": 1497}
{"project": "Scanpy", "commit_id": "190_scanpy_0.0_scanpy_readwrite.py_write_params.py", "target": 1, "func": "def write_params(filename, *args, **dicts):\n    \"\"\"\n    Write parameters to file, so that it's readable by read_params.\n    Uses INI file format.\n    \"\"\"\n    if not os.path.exists(filename):\n        os.makedirs(os.path.dirname(filename))\n    if len(args) == 1:\n        d = args[0]\n        with open(filename, 'w') as f:\n            for key in d:\n                f.write(key + ' = ' + str(d[key]) + '\\n')\n    else:\n        with open(filename, 'w') as f:\n            for k, d in dicts.items():\n                f.write('[' + k + ']\\n')\n                for key, val in d.items():\n                    f.write(key + ' = ' + str(val) + '\\n')", "idx": 1507}
{"project": "Scanpy", "commit_id": "915_scanpy_1.4.4_scanpy_tools__sim.py_sample_dynamic_data.py", "target": 1, "func": "def sample_dynamic_data(**params):\n    \"\"\"\n    Helper function.\n    \"\"\"\n    model_key = Path(params['model']).with_suffix('').name\n    if 'writedir' not in params or params['writedir'] is None:\n        params['writedir'] = settings.writedir / (model_key + '_sim')\n    params['writedir'].mkdir(parents=True, exist_ok=True)\n    readwrite.write_params(params['writedir'] / 'params.txt', params)\n    # init variables\n    writedir = Path(params['writedir'])\n    tmax = params['tmax']\n    branching = params['branching']\n    noiseObs = params['noiseObs']\n    noiseDyn = params['noiseDyn']\n    nrRealizations = params['nrRealizations']\n    step = params['step']  # step size for saving the figure\n    nrSamples = 1  # how many files?\n    maxRestarts = 1000\n    maxNrSamples = 1\n    # simple vector auto regressive process or\n    # hill kinetics process simulation\n    if 'krumsiek11' not in model_key:\n        # create instance, set seed\n        grnsim = GRNsim(model=model_key, params=params)\n        nrOffEdges_list = np.zeros(nrSamples)\n        for sample in range(nrSamples):\n            # random topology / for a given edge density\n            if 'hill' not in model_key:\n                Coupl = np.array(grnsim.Coupl)\n                for sampleCoupl in range(10):\n                    nrOffEdges = 0\n                    for gp in range(grnsim.dim):\n                        for g in range(grnsim.dim):\n                            # only consider off-diagonal edges\n                            if g != gp:\n                                Coupl[gp, g] = 0.7 if np.random.rand() < 0.4 else 0\n                                nrOffEdges += 1 if Coupl[gp, g] > 0 else 0\n                            else:\n                                Coupl[gp, g] = 0.7\n                    # check that the coupling matrix does not have eigenvalues\n                    # greater than 1, which would lead to an exploding var process\n                    if max(sp.linalg.eig(Coupl)[0]) < 1:\n                        break\n                nrOffEdges_list[sample] = nrOffEdges\n                grnsim.set_coupl(Coupl)\n            # init type\n            real = 0\n            X0 = np.random.rand(grnsim.dim)\n            Xsamples = []\n            for restart in range(nrRealizations+maxRestarts):\n                # slightly break symmetry in initial conditions\n                if 'toggleswitch' in model_key:\n                    X0 = (np.array([0.8 for i in range(grnsim.dim)])\n                          + 0.01*np.random.randn(grnsim.dim))\n                X = grnsim.sim_model(tmax=tmax, X0=X0,\n                                     noiseDyn=noiseDyn)\n                # check branching\n                check = True\n                if branching:\n                    check, Xsamples = _check_branching(X, Xsamples, restart)\n                if check:\n                    real += 1\n                    grnsim.write_data(X[::step], dir=writedir,\n                                      noiseObs=noiseObs,\n                                      append=(False if restart==0 else True),\n                                      branching=branching,\n                                      nrRealizations=nrRealizations)\n                # append some zeros\n                if 'zeros' in writedir.name and real == 2:\n                    grnsim.write_data(noiseDyn*np.random.randn(500,3), dir=writedir,\n                                      noiseObs=noiseObs,\n                                      append=(False if restart==0 else True),\n                                      branching=branching,\n                                      nrRealizations=nrRealizations)\n                if real >= nrRealizations:\n                    break\n        logg.debug(\n            f'mean nr of offdiagonal edges {nrOffEdges_list.mean()} '\n            f'compared to total nr {grnsim.dim*(grnsim.dim-1)/2.}'\n        )\n    # more complex models\n    else:\n        initType = 'random'\n        dim = 11\n        step = 5\n        grnsim = GRNsim(dim=dim,initType=initType,model=model_key,params=params)\n        curr_nrSamples = 0\n        Xsamples = []\n        for sample in range(maxNrSamples):\n            # choose initial conditions such that branchings result\n            if initType == 'branch':\n                X0mean = grnsim.branch_init_model1(tmax)\n                if X0mean is None:\n                    grnsim.set_coupl()\n                    continue\n            real = 0\n            for restart in range(nrRealizations+maxRestarts):\n                if initType == 'branch':\n                    # vary initial conditions around mean\n                    X0 = X0mean + (0.05*np.random.rand(dim) - 0.025*np.ones(dim))\n                else:\n                    # generate random initial conditions within [0.3,0.7]\n                    X0 = 0.4*np.random.rand(dim)+0.3\n                if model_key in [5,6]:\n                    X0 = np.array([0.3,0.3,0,0,0,0])\n                if model_key in [7,8,9,10]:\n                    X0 = 0.6*np.random.rand(dim)+0.2\n                    X0[2:] = np.zeros(4)\n                if 'krumsiek11' in model_key:\n                    X0 = np.zeros(dim)\n                    X0[grnsim.varNames['Gata2']] = 0.8\n                    X0[grnsim.varNames['Pu.1']] = 0.8\n                    X0[grnsim.varNames['Cebpa']] = 0.8\n                    X0 += 0.001*np.random.randn(dim)\n                    if False:\n                        switch_gene = restart - (nrRealizations - dim)\n                        if switch_gene >= dim:\n                            break\n                        X0[switch_gene] = 0 if X0[switch_gene] > 0.1 else 0.8\n                X = grnsim.sim_model(tmax,X0=X0,\n                                     noiseDyn=noiseDyn,\n                                     restart=restart)\n                # check branching\n                check = True\n                if branching:\n                    check, Xsamples = _check_branching(X,Xsamples,restart)\n                if check:\n                    real += 1\n                    grnsim.write_data(X[::step],dir=writedir,noiseObs=noiseObs,\n                                     append=(False if restart==0 else True),\n                                     branching=branching,\n                                     nrRealizations=nrRealizations)\n                if real >= nrRealizations:\n                    break\n    # load the last simulation file\n    filename = None\n    for filename in writedir.glob('sim*.txt'):\n        pass\n    logg.info(f'reading simulation results {filename}')\n    adata = readwrite._read(filename, first_column_names=True,\n                            suppress_cache_warning=True)\n    adata.uns['tmax_write'] = tmax/step\n    return adata", "idx": 1508}
{"project": "Scanpy", "commit_id": "115_scanpy_0.0_scanpy_plotting.py_timeseries_subplot.py", "target": 1, "func": "def timeseries_subplot(X,\n                       c,\n                       varnames=(),\n                       highlightsX=(),\n                       xlabel='segments / pseudotime order',\n                       ylabel='gene expression',\n                       yticks=None,\n                       xlim=None,\n                       legend=True,\n                       pal=None,\n                       cmap='viridis'):\n    \"\"\"\n    Plot X. Call this with:\n    X with one column, c categorical\n    X with one column, c continuous\n    X with n columns, c is of length n\n    \"\"\"\n\n    use_cmap = isinstance(c[0], float)\n    pal = default_pal(pal)\n    x_range = np.arange(X.shape[0])\n    if X.shape[1] > 1:\n        colors = pal[:X.shape[1]].by_key()['color']\n        subsets = [(x_range, X[:, 0])]\n    elif use_cmap:\n        colors = [c]\n        subsets = [(x_range, X[:, 0])]\n    else:\n        levels, _ = np.unique(c, return_inverse=True)\n        colors = np.array(pal[:len(levels)].by_key()['color'])\n        subsets = [(x_range[c == l], X[c == l, :]) for l in levels]\n    for i, (x, y) in enumerate(subsets):\n        pl.scatter(\n            x, y,\n            marker='.',\n            edgecolor='face',\n            s=rcParams['lines.markersize'],\n            c=colors[i],\n            label=varnames[i] if len(varnames) > 0 else '',\n            cmap=cmap,\n        )\n    ylim = pl.ylim()\n    for ih,h in enumerate(highlightsX):\n        pl.plot([h,h],[ylim[0],ylim[1]],\n                '--',color='black')\n    pl.ylim(ylim)\n    if xlim is not None:\n        pl.xlim(xlim)\n    pl.xlabel(xlabel)\n    pl.ylabel(ylabel)\n    if yticks is not None:\n        pl.yticks(yticks)\n    if len(varnames) > 0 and legend==True:\n        pl.legend(frameon=False)", "idx": 1513}
{"project": "Scanpy", "commit_id": "500_scanpy_0.4.3_scanpy_tools_louvain.py_louvain.py", "target": 1, "func": "def louvain(adata,\n            n_neighbors=None,\n            resolution=None,\n            n_pcs=50,\n            random_state=0,\n            restrict_to=None,\n            key_added=None,\n            flavor='vtraag',\n            directed=True,\n            recompute_pca=False,\n            recompute_distances=False,\n            recompute_graph=False,\n            n_dcs=None,\n            n_jobs=None,\n            copy=False):\n    \"\"\"Cluster cells into subgroups [Blondel08]_ [Levine15]_ [Traag17]_.\n    Cluster cells using the Louvain algorithm [Blondel08]_ in the implementation\n    of [Traag17]_. The Louvain algorithm has been proposed for single-cell\n    analysis by [Levine15]_.\n    Parameters\n    ----------\n    adata : :class:`~scanpy.api.AnnData`\n        The annotated data matrix.\n    n_neighbors : `int`, optional (default: 30)\n        Number of neighbors to use for construction of knn graph.\n    resolution : `float` or `None`, optional (default: 1)\n        For the default flavor ('vtraag'), you can provide a resolution (higher\n        resolution means finding more and smaller clusters), which defaults to\n        1.0.\n    n_pcs : int, optional (default: 50)\n        Number of PCs to use for computation of data point graph.\n    random_state : int, optional (default: 0)\n        Change the initialization of the optimization.\n    key_added : str, optional (default: `None`)\n        Key under which to add the cluster labels.\n    restrict_to : tuple, optional (default: None)\n        Restrict the clustering to the categories within the key for sample\n        annotation, tuple needs to contain (obs key, list of categories).\n    flavor : {'vtraag', 'igraph'}\n        Choose between to packages for computing the clustering. 'vtraag' is\n        much more powerful.\n    copy : `bool` (default: False)\n        Copy adata or modify it inplace.\n    Returns\n    -------\n    Depending on `copy`, returns or updates `adata` with the following fields.\n    louvain_groups : `pd.Series` (``adata.obs``, dtype `category`)\n        Array of dim (number of samples) that stores the subgroup id ('0',\n        '1', ...) for each cell.\n    \"\"\"\n    logg.info('running Louvain clustering', r=True)\n    adata = adata.copy() if copy else adata\n    add_or_update_graph_in_adata(\n        adata,\n        n_neighbors=n_neighbors,\n        n_pcs=n_pcs,\n        n_dcs=n_dcs,\n        recompute_pca=recompute_pca,\n        recompute_distances=recompute_distances,\n        recompute_graph=recompute_graph,\n        n_jobs=n_jobs)\n    adjacency = adata.uns['data_graph_norm_weights']\n    if restrict_to is not None:\n        restrict_key, restrict_categories = restrict_to\n        if not isinstance(restrict_categories[0], str):\n            raise ValueError('You need to use strings to label categories, '\n                             'e.g. \\'1\\' instead of 1.')\n        restrict_indices = adata.obs[restrict_key].isin(restrict_categories).values\n        adjacency = adjacency[restrict_indices, :]\n        adjacency = adjacency[:, restrict_indices]\n    if flavor in {'vtraag', 'igraph'}:\n        if flavor == 'igraph' and resolution is not None:\n            logg.warn('`resolution` parameter has no effect for flavor \"igraph\"')\n        if directed and flavor == 'igraph':\n            directed = False\n        if not directed: logg.m('    using the undirected graph', v=4)\n        g = utils.get_igraph_from_adjacency(adjacency, directed=directed)\n        if flavor == 'vtraag':\n            import louvain\n            if resolution is None: resolution = 1\n            try:\n                logg.info('    using the \"louvain\" package of Traag (2017)')\n                louvain.set_rng_seed(random_state)\n                part = louvain.find_partition(g, louvain.RBConfigurationVertexPartition,\n                                              resolution_parameter=resolution)\n                # adata.uns['louvain_quality'] = part.quality()\n            except AttributeError:\n                logg.warn('Did not find package louvain>=0.6, '\n                          'the clustering result will therefore not '\n                          'be 100% reproducible, '\n                          'but still meaningful. '\n                          'If you want 100% reproducible results, '\n                          'update via \"pip install louvain --upgrade\".')\n                part = louvain.find_partition(g, method='RBConfiguration',\n                                              resolution_parameter=resolution)\n        elif flavor == 'igraph':\n            part = g.community_multilevel()\n        groups = np.array(part.membership)\n    elif flavor == 'taynaud':\n        # this is deprecated\n        import networkx as nx\n        import community\n        g = nx.Graph(adata.uns['data_graph_distance_local'])\n        partition = community.best_partition(g)\n        groups = np.zeros(len(partition), dtype=int)\n        for k, v in partition.items(): groups[k] = v\n    else:\n        raise ValueError('`flavor` needs to be \"vtraag\" or \"igraph\" or \"taynaud\".')\n    unique_groups = np.unique(groups)\n    n_clusters = len(unique_groups)\n    if restrict_to is None:\n        groups = groups.astype('U')\n        adata.obs['louvain_groups'] = pd.Categorical(\n            values=groups,\n            categories=natsorted(unique_groups.astype('U')))\n        key_added = 'louvain_groups' if key_added is None else key_added\n    else:\n        key_added = restrict_key + '_R' if key_added is None else key_added\n        groups += 1\n        adata.obs[key_added] = adata.obs[restrict_key].astype('U')\n        adata.obs[key_added] += ','\n        adata.obs[key_added].iloc[restrict_indices] += groups.astype('U')\n        adata.obs[key_added].iloc[~restrict_indices] += '0'\n        adata.obs[key_added] = adata.obs[key_added].astype(\n            'category', categories=natsorted(adata.obs[key_added].unique()))\n    adata.uns['louvain_params'] = np.array((resolution, random_state,),\n                                           dtype=[('resolution', float), ('random_state', int)])\n    logg.info('    finished', time=True, end=' ' if settings.verbosity > 2 else '\\n')\n    logg.hint('found {} clusters and added\\n'\n              '    \\'{}\\', the cluster labels (adata.obs, dtype=category)'\n              .format(n_clusters, key_added))\n    return adata if copy else None", "idx": 1518}
{"project": "Scanpy", "commit_id": "424_scanpy_1.9.0__simple.py_sqrt.py", "target": 0, "func": "def sqrt(\n    data: AnnData,\n    copy: bool = False,\n    chunked: bool = False,\n    chunk_size: Optional[int] = None,\n) -> Optional[AnnData]:\n    \"\"\"\\\n    Square root the data matrix.\n\n    Computes :math:`X = \\\\sqrt(X)`.\n\n    Parameters\n    ----------\n    data\n        The (annotated) data matrix of shape `n_obs` \u00d7 `n_vars`.\n        Rows correspond to cells and columns to genes.\n    copy\n        If an :class:`~anndata.AnnData` object is passed,\n        determines whether a copy is returned.\n    chunked\n        Process the data matrix in chunks, which will save memory.\n        Applies only to :class:`~anndata.AnnData`.\n    chunk_size\n        `n_obs` of the chunks to process the data in.\n\n    Returns\n    -------\n    Returns or updates `data`, depending on `copy`.\n    \"\"\"\n    if isinstance(data, AnnData):\n        adata = data.copy() if copy else data\n        if chunked:\n            for chunk, start, end in adata.chunked_X(chunk_size):\n                adata.X[start:end] = sqrt(chunk)\n        else:\n            adata.X = sqrt(data.X)\n        return adata if copy else None\n    X = data  # proceed with data matrix\n    if not issparse(X):\n        return np.sqrt(X)\n    else:\n        return X.sqrt()", "idx": 1519}
{"project": "Scanpy", "commit_id": "86_scanpy_0.0_scanpy_readwrite.py_read_txt_as_floats.py", "target": 1, "func": "def read_txt_as_floats(filename, delim=None, first_column_names=None):\n    \"\"\"\n    Return data as list of lists of strings and the header as string.\n    Parameters\n    ----------\n    filename : str\n        Filename of data file.\n    delim : str, optional\n        Separator that separates data within text file. If None, will split at\n        arbitrary number of white spaces, which is different from enforcing\n        splitting at single white space ' '.\n\n    Returns\n    -------\n    data : list\n         List of lists of strings.\n    header : str\n         String storing the comment lines (those that start with '#').\n    \"\"\"\n    header = ''\n    data = []\n    length = -1\n    f = open(filename)\n    col_names = []\n    row_names = []\n    # read header and column names\n    for line in f:\n        if line.startswith('#'):\n            header += line\n        else:\n            line_list = line.split(delim)\n            if not is_float(line_list[0]):\n                col_names = line_list\n                sett.m(0, '--> assuming first line in file stores column names')\n            else:\n                if not is_float(line_list[0]) or first_column_names:\n                    first_column_names = True\n                    row_names.append(line_list[0])\n                    data.append(line_list[1:])\n                else:\n                    data.append(line_list)\n            break\n    if not col_names:\n        # try reading col_names from the last comment line\n        if len(header) > 0:\n            sett.m(0, '--> assuming last comment line stores variable names')\n            col_names = np.array(header.split('\\n')[-2].strip('#').split())\n        # just numbers as col_names\n        else:\n            sett.m(0, '--> did not find column names in file')\n            col_names = np.arange(len(data[0])).astype(str)\n    col_names = np.array(col_names, dtype=str)\n    # check if first column contains row names or not\n    if first_column_names is None:\n        first_column_names = False\n    for line in f:\n        line_list = line.split(delim)\n        if not is_float(line_list[0]) or first_column_names:\n            sett.m(0, '--> assuming first column in file stores row names')\n            first_column_names = True\n            row_names.append(line_list[0])\n            data.append(line_list[1:])\n        else:\n            data.append(line_list)\n        break\n    # parse the file\n    for line in f:\n        line_list = line.split(delim)\n        if first_column_names:\n            row_names.append(line_list[0])\n            data.append(line_list[1:])\n        else:\n            data.append(line_list)\n    sett.mt(0, 'read data into list of lists')\n    # transfrom to array, this takes a long time and a lot of memory\n    # but it's actually the same thing as np.genfromtext does\n    # - we don't use the latter as it would involve another slicing step\n    #   in the end, to separate row_names from float data, slicing takes\n    #   a lot of memory and cpu time\n    data = np.array(data, dtype=np.float64)\n    sett.mt(0, 'constructed array from list of list')\n    # transform row_names\n    if not row_names:\n        row_names = np.arange(len(data)).astype(str)\n        sett.m(0, '--> did not find row names in file')\n    else:\n        row_names = np.array(row_names)\n    # adapt col_names if necessary\n    if col_names.size > data.shape[1]:\n        col_names = col_names[1:]\n    ddata = {'X': data, 'row_names': row_names, 'col_names': col_names}\n    return ddata", "idx": 1520}
{"project": "Scanpy", "commit_id": "185_scanpy_0.0_scanpy___init__.py__run_command_line_args.py", "target": 1, "func": "def _run_command_line_args(toolkey, args):\n    \"\"\"\n    Run specified tool, do preprocessing and read/write outfiles.\n    Result files store the dictionary returned by the tool. File type is\n    determined by variable sett.file_format_data allowed are 'h5' (hdf5), 'xlsx' (Excel) or\n    'csv' (comma separated value file).\n    If called twice with the same settings the existing result file is used.\n    Parameters\n    ----------\n    toolkey : str\n        Name of the tool.\n    args : dict containing\n        exkey : str\n            String that identifies the example use key.\n    \"\"\"\n    # help on plot parameters\n    if args['pparams']:\n        if args['pparams'][0] == 'help':\n            sys.exit(getattr(get_tool(toolkey), 'plot_' + toolkey).__doc__)\n\n    # read parameters\n    adata = None\n    if os.path.exists(readwrite.get_filename_from_key(sett.basekey)) or toolkey != 'sim':\n        adata, exmodule = get_example(args['exkey'],\n                                      subsample=args['subsample'],\n                                      suffix=sett.suffix,\n                                      return_module=True,\n                                      recompute=sett.recompute == 'pp',\n                                      reread=sett.recompute == 'read')\n        params = {}\n        # try to load tool parameters from dexamples\n        try:\n            did_not_find_params_in_exmodule = False\n            dexample = exmodule.example_parameters[args['exkey']]\n            params = {}\n            for key in dexample.keys():\n                if toolkey in key:\n                    params = dexample[key]\n                    sett.m(0, '... using parameters', '\"-p ' +\n                           ' '.join(['='.join([k, str(v)]) for k, v in params.items()])\n                           + '\"')\n                    break\n        except:\n            did_not_find_params_in_exmodule = True\n            pass\n        # if optional parameters have been specified in a parameter file update\n        # the current param dict with these\n        if args['pfile'] != '':\n            add_params = read_params(args['pfile'])\n            params = utils.update_params(params, add_params)\n        # same if optional parameters have been specified on the command line\n        if args['params']:\n            add_params = readwrite.get_params_from_list(args['params'])\n            sett.m(0, '... overwriting params', '\"' +\n                   ' '.join(['='.join([k, str(v)]) for k, v in add_params.items()])\n                   + '\"', 'in call of', toolkey)\n            params = utils.update_params(params, add_params)\n        elif did_not_find_params_in_exmodule and args['pfile'] != '':\n            sett.m(0, 'using default parameters, change them using \"--params\"')\n    elif toolkey == 'sim':\n        if args['pfile'] != '':\n            params = read_params(args['pfile'])\n        else:\n            from . import sim_models\n            pfile_sim = os.path.dirname(sim_models.__file__) + '/' + args['exkey'] + '_params.txt'\n            params = read_params(pfile_sim)\n            sett.m(0, '--> you can specify your custom params file using the option\\n'\n                   '    \"--pfile\" or provide parameters directly via \"--params\"')\n        if 'writedir' not in params:\n            params['writedir'] = sett.writedir + sett.basekey + '_' + toolkey\n    # read/write files\n    writekey = sett.basekey + '_' + toolkey\n    pfile = sett.writedir + sett.basekey + '_params/' + toolkey + '.txt'\n    if args['logfile']:\n        logfile = sett.writedir + writekey + '_log.txt'\n        sett.logfile(logfile)\n\n    # actual call of tool\n    if (adata is None\n        or toolkey not in adata.add['tools']\n        or sett.recompute != 'none'):\n        tool = get_tool(toolkey, func=True)\n        if toolkey == 'sim':\n            adata = tool(**params)\n        elif toolkey == 'pca':\n            adata = tool(adata, recompute=False, **params)\n        else:\n            adata = tool(adata, **params)\n        # append toolkey to tools in adata\n        if toolkey not in adata.add['tools']:\n            adata.add['tools'] = np.append(adata.add['tools'], toolkey)\n        write(sett.basekey, adata)\n        if sett.file_format_data in {'h5', 'npz'}:\n            sett.m(0, 'updated file',\n                   readwrite.get_filename_from_key(sett.basekey))\n        # save a copy of the changed parameters\n        readwrite.write_params(pfile, params)\n    # plotting and postprocessing\n    pparams = (readwrite.get_params_from_list(args['pparams'])\n               if args['pparams'] else {})\n    # post-processing specific to example and tool\n    # - only if we are not subsampling\n    if toolkey != 'sim':\n        postprocess = args['exkey'] + '_' + toolkey\n        if postprocess in dir(exmodule) and args['subsample'] == 1:\n            adata = getattr(exmodule, postprocess)(adata)\n            write(sett.basekey, adata)\n    getattr(get_tool(toolkey), 'plot_' + toolkey)(adata, **pparams)", "idx": 1521}
{"project": "Scanpy", "commit_id": "278_scanpy_0.1_scanpy_tools_dpt.py_select_segment.py", "target": 1, "func": "def select_segment(self, segs, segs_tips, segs_undecided):\n    \"\"\"Out of a list of line segments, choose segment that has the most\n    distant second data point.\n    Assume the distance matrix Ddiff is sorted according to seg_idcs.\n    Compute all the distances.\n    Returns\n    -------\n    iseg : int\n        Index identifying the position within the list of line segments.\n    tips3 : int\n        Positions of tips within chosen segment.\n    \"\"\"\n    scores_tips = np.zeros((len(segs), 4))\n    allindices = np.arange(self.X.shape[0], dtype=int)\n    for iseg, seg in enumerate(segs):\n        # do not consider too small segments\n        if segs_tips[iseg][0] == -1: continue\n        # restrict distance matrix to points in segment\n        if not isinstance(self.Dchosen, data_graph.OnFlySymMatrix):\n            Dseg = self.Dchosen[np.ix_(seg, seg)]\n        else:\n            Dseg = self.Dchosen.restrict(seg)\n        third_maximizer = None\n        if segs_undecided[iseg]:\n            # check that none of our tips \"connects\" with a tip of the\n            # other segments\n            for jseg in range(len(segs)):\n                if jseg != iseg:\n                    # take the inner tip, the \"second tip\" of the segment\n                    for itip in range(2):\n                        if (self.Dchosen[segs_tips[jseg][1], segs_tips[iseg][itip]]\n                                < 0.5 * self.Dchosen[segs_tips[iseg][~itip], segs_tips[iseg][itip]]):\n                            # logg.m('... group', iseg, 'with tip', segs_tips[iseg][itip],\n                            #        'connects with', jseg, 'with tip', segs_tips[jseg][1], v=4)\n                            # logg.m('    do not use the tip for \"triangulation\"', v=4)\n                            third_maximizer = itip\n        # map the global position to the position within the segment\n        tips = [np.where(allindices[seg] == tip)[0][0]\n                for tip in segs_tips[iseg]]\n        # find the third point on the segment that has maximal\n        # added distance from the two tip points\n        dseg = Dseg[tips[0]] + Dseg[tips[1]]\n        # add this point to tips, it's a third tip, we store it at the first\n        # position in an array called tips3\n        third_tip = np.argmax(dseg)\n        if third_maximizer is not None:\n            # find a fourth point that has maximal distance to all three\n            dseg += Dseg[third_tip]\n            fourth_tip = np.argmax(dseg)\n            if fourth_tip != tips[0] and fourth_tip != third_tip:\n                tips[1] = fourth_tip\n                dseg -= Dseg[tips[1]]\n            else:\n                dseg -= Dseg[third_tip]\n        tips3 = np.append(tips, third_tip)\n        # compute the score as ratio of the added distance to the third tip,\n        # to what it would be if it were on the straight line between the\n        # two first tips, given by Dseg[tips[:2]]\n        # if we did not normalize, there would be a danger of simply\n        # assigning the highest score to the longest segment\n        if self.flavor != 'wolf17_bi':\n            score = dseg[tips3[2]] / Dseg[tips3[0], tips3[1]]\n        else:\n            score = Dseg[tips3[0], tips3[1]]\n        score = len(seg) if self.choose_largest_segment else score  # simply the number of points\n        # self.choose_largest_segment = False\n        logg.m('... group', iseg, 'score', score, 'n_points', len(seg),\n               '(too small)' if len(seg) < self.min_group_size else '')\n        if len(seg) < self.min_group_size: score = 0\n        # write result\n        scores_tips[iseg, 0] = score\n        scores_tips[iseg, 1:] = tips3\n    iseg = np.argmax(scores_tips[:, 0])\n    tips3 = scores_tips[iseg, 1:].astype(int)\n    return iseg, tips3", "idx": 1525}
{"project": "Scanpy", "commit_id": "935_scanpy_1.9.0___init__.py___getitem__.py", "target": 0, "func": "def __getitem__(self, key):\n        if key == 'distances':\n            if 'distances' not in self:\n                raise KeyError(f'No \"{self._dists_key}\" in .obsp')\n            return self._distances\n        elif key == 'connectivities':\n            if 'connectivities' not in self:\n                raise KeyError(f'No \"{self._conns_key}\" in .obsp')\n            return self._connectivities\n        else:\n            return self._neighbors_dict[key]", "idx": 1526}
{"project": "Scanpy", "commit_id": "721_scanpy_1.3.2_scanpy_plotting_tools_scatterplots.py__set_colors_for_categorical_obs.py", "target": 1, "func": "def _set_colors_for_categorical_obs(adata, value_to_plot, palette):\n    \"\"\"\n    Sets the adata.uns[value_to_plot + '_colors'] according to the given palette\n    Parameters\n    ----------\n    adata : annData object\n    value_to_plot : name of a valid categorical observation\n    palette : Palette should be either a valid `matplotlib.pyplot.colormaps()` string,\n              a list of colors (in a format that can be understood by matplotlib,\n              eg. RGB, RGBS, hex, or a cycler object with key='color'\n    Returns\n    -------\n    None\n    \"\"\"\n    from matplotlib.colors import to_hex\n    from cycler import Cycler, cycler\n\n    categories = adata.obs[value_to_plot].cat.categories\n    # check is palette is a valid color map\n    if isinstance(palette, str) and palette in pl.colormaps():\n        # this creates a palette from a colormap. E.g. 'Accent, Dark2, tab20'\n        cmap = pl.get_cmap(palette)\n        colors_list = [to_hex(x) for x in cmap(np.linspace(0, 1, len(categories)))]\n    else:\n        # check if palette is a list and convert it to a cycler, thus\n        # it doesnt matter if the list is shorter than the categories length:\n        if isinstance(palette, list):\n            if len(palette) < len(categories):\n                logg.warn(\"Length of palette colors is smaller than the number of \"\n                          \"categories (palette length: {}, categories length: {}. \"\n                          \"Some categories will have the same color.\"\n                          .format(len(palette), len(categories)))\n            palette = cycler(color=palette)\n        if not isinstance(palette, Cycler):\n            raise ValueError(\"Please check that the value of 'palette' is a \"\n                             \"valid matplotlib colormap string (eg. Set2), a \"\n                             \"list of color names or a cycler with a 'color' key.\")\n        if 'color' not in palette.keys:\n            raise ValueError(\"Please set the palette key 'color'.\")\n        cc = palette()\n        colors_list = [to_hex(next(cc)['color']) for x in range(len(categories))]\n    adata.uns[value_to_plot + '_colors'] = colors_list", "idx": 1538}
{"project": "Scanpy", "commit_id": "318_scanpy_0.1_scanpy_tools_draw_graph.py_draw_graph.py", "target": 1, "func": "def draw_graph(adata,\n               layout='fr',\n               n_neighbors=30,\n               n_pcs=50,\n               n_jobs=None,\n               random_state=0,\n               recompute_graph=False,\n               copy=False):\n    \"\"\"Visualize data using standard graph drawing algorithms.\n    Parameters\n    ----------\n    adata : AnnData\n        Annotated data matrix.\n    layout : str, optional (default: 'fruchterman_reingold')\n        Any valid igraph layout: http://igraph.org/c/doc/igraph-Layout.html.  Of\n        particular interest are 'fr' (Fruchterman Reingold), 'grid_fr' (Grid\n        Fruchterman Reingold, faster than 'fr'), 'kk' (Kamadi Kawai', slower\n        than 'fr'), 'lgl' (Large Graph, very fast), 'drl' (Distributed Recursive\n        Layout) and 'tree' (Reingold Tilford').\n    n_neighbors : int\n        Number of nearest neighbors in graph.\n    Returns\n    -------\n    Returns or updates adata depending on `copy` with\n         \"X_draw_graph\", graph-drawing coordinates (adata.smp)\n    References\n    ----------\n    - The package \"igraph\", which provides the drawing implementations used\n      here: Csardi & Nepusz, InterJournal Complex Systems, 1695 (2006)\n    - Suggestion to use the \"spring\" graph-drawing algorithm of the package D3js\n      for single-cell data: Weinreb et al., bioRxiv doi:10.1101/090332 (2016)\n    \"\"\"\n    import numpy as np\n    import igraph as ig\n    from .. import logging as logg\n    from .. import data_structs\n    avail_layouts = {'fr', 'drl', 'kk', 'grid_fr', 'lgl', 'tree'}\n    if layout not in avail_layouts:\n        raise ValueError('Provide a valid layout, one of {}.'.format(avail_layouts))\n    adata = adata.copy() if copy else adata\n    logg.info('drawing single-cell graph using layout \"{}\"'.format(layout))\n    if 'Ktilde' not in adata.add or recompute_graph:\n        graph = data_structs.DataGraph(adata,\n                                       k=n_neighbors,\n                                       n_pcs=n_pcs,\n                                       n_jobs=n_jobs)\n        graph.compute_transition_matrix()\n        adata.add['Ktilde'] = graph.Ktilde\n    adjacency = adata.add['Ktilde']\n    sources, targets = adjacency.nonzero()\n    weights = adjacency[sources, targets]\n    weights = np.array(weights)[0]  # need to convert sparse matrix into a form appropriate for igraph\n    g = ig.Graph(list(zip(sources, targets)),  #, edge_attrs={'weight': weights}\n                 directed=True)\n    if layout in {'fr', 'drl', 'kk', 'grid_fr'}:\n        np.random.seed(random_state)\n        init_coords = np.random.random((adjacency.shape[0], 2)).tolist()\n        ig_layout = g.layout(layout,  # weights='weight',\n                             seed=init_coords)\n    else:\n        ig_layout = g.layout(layout)\n    if 'draw_graph_layout' in adata.add:\n        adata.add['draw_graph_layout'] = list(adata.add['draw_graph_layout']) + [layout]\n    else:\n        adata.add['draw_graph_layout'] = [layout]\n    adata.smp['X_draw_graph_' + layout] = np.array(ig_layout.coords)\n    logg.m('    finished', t=True, end=' ')\n    logg.m('and added\\n'\n           '    \"X_draw_graph\", graph_drawing coordinates (adata.smp)\\n'\n           '    \"draw_graph_layout\", the chosen layout (adata.add)')\n    return adata if copy else None", "idx": 1540}
{"project": "Scanpy", "commit_id": "215_scanpy_1.9.0___init__.py__backwards_compat_get_full_X_diffmap.py", "target": 0, "func": "def _backwards_compat_get_full_X_diffmap(adata: AnnData) -> np.ndarray:\n    if 'X_diffmap0' in adata.obs:\n        return np.c_[adata.obs['X_diffmap0'].values[:, None], adata.obsm['X_diffmap']]\n    else:\n        return adata.obsm['X_diffmap']", "idx": 1544}
{"project": "Scanpy", "commit_id": "228_scanpy_1.9.0___init__.py_distances_dpt.py", "target": 0, "func": "def distances_dpt(self):\n        \"\"\"DPT distances (on-fly matrix).\n\n        This is yields [Haghverdi16]_, Eq. 15 from the supplement with the\n        extensions of [Wolf19]_, supplement on random-walk based distance\n        measures.\n        \"\"\"\n        return OnFlySymMatrix(self._get_dpt_row, shape=self._adata.shape)", "idx": 1545}
{"project": "Scanpy", "commit_id": "742_scanpy_1.3.2_scanpy_plotting_tools_paga.py_paga_path.py", "target": 1, "func": "def paga_path(\n        adata,\n        nodes,\n        keys,\n        use_raw=True,\n        annotations=['dpt_pseudotime'],\n        color_map=None,\n        color_maps_annotations={'dpt_pseudotime': 'Greys'},\n        palette_groups=None,\n        n_avg=1,\n        groups_key=None,\n        xlim=[None, None],\n        title=None,\n        left_margin=None,\n        ytick_fontsize=None,\n        title_fontsize=None,\n        show_node_names=True,\n        show_yticks=True,\n        show_colorbar=True,\n        legend_fontsize=None,\n        legend_fontweight=None,\n        normalize_to_zero_one=False,\n        as_heatmap=True,\n        return_data=False,\n        show=None,\n        save=None,\n        ax=None):\n    \"\"\"Gene expression and annotation changes along paths in the abstracted graph.\n    Parameters\n    ----------\n    adata : :class:`~anndata.AnnData`\n        An annotated data matrix.\n    nodes : list of group names or their category indices\n        A path through nodes of the abstracted graph, that is, names or indices\n        (within `.categories`) of groups that have been used to run PAGA.\n    keys : list of str\n        Either variables in `adata.var_names` or annotations in\n        `adata.obs`. They are plotted using `color_map`.\n    use_raw : `bool`, optional (default: `True`)\n        Use `adata.raw` for retrieving gene expressions if it has been set.\n    annotations : list of annotations, optional (default: ['dpt_pseudotime'])\n        Plot these keys with `color_maps_annotations`. Need to be keys for\n        `adata.obs`.\n    color_map : color map for plotting keys or `None`, optional (default: `None`)\n        Matplotlib colormap.\n    color_maps_annotations : dict storing color maps or `None`, optional (default: {'dpt_pseudotime': 'Greys'})\n        Color maps for plotting the annotations. Keys of the dictionary must\n        appear in `annotations`.\n    palette_groups : list of colors or `None`, optional (default: `None`)\n        Ususally, use the same `sc.pl.palettes...` as used for coloring the\n        abstracted graph.\n    n_avg : `int`, optional (default: 1)\n        Number of data points to include in computation of running average.\n    groups_key : `str`, optional (default: `None`)\n        Key of the grouping used to run PAGA. If `None`, defaults to\n        `adata.uns['paga']['groups']`.\n    as_heatmap : `bool`, optional (default: `True`)\n        Plot the timeseries as heatmap. If not plotting as heatmap,\n        `annotations` have no effect.\n    show_node_names : `bool`, optional (default: `True`)\n        Plot the node names on the nodes bar.\n    show_colorbar : `bool`, optional (default: `True`)\n        Show the colorbar.\n    show_yticks : `bool`, optional (default: `True`)\n        Show the y ticks.\n    normalize_to_zero_one : `bool`, optional (default: `True`)\n        Shift and scale the running average to [0, 1] per gene.\n    return_data : `bool`, optional (default: `False`)\n        Return the timeseries data in addition to the axes if `True`.\n    show : `bool`, optional (default: `None`)\n         Show the plot, do not return axis.\n    save : `bool` or `str`, optional (default: `None`)\n        If `True` or a `str`, save the figure. A string is appended to the\n        default filename. Infer the filetype if ending on \\{'.pdf', '.png', '.svg'\\}.\n    ax : `matplotlib.Axes`\n         A matplotlib axes object.\n    Returns\n    -------\n    A `matplotlib.Axes`, if `ax` is `None`, else `None`. If `return_data`,\n    return the timeseries data in addition to an axes.\n    \"\"\"\n    ax_was_none = ax is None\n    if groups_key is None:\n        if 'groups' not in adata.uns['paga']:\n            raise KeyError(\n                'Pass the key of the grouping with which you ran PAGA, '\n                'using the parameter `groups_key`.')\n        groups_key = adata.uns['paga']['groups']\n    groups_names = adata.obs[groups_key].cat.categories\n\n    if 'dpt_pseudotime' is not adata.obs.keys():\n        raise ValueError(\n            '`pl.paga_path` requires computation of a pseudotime `tl.dpt` '\n            'for ordering at single-cell resolution')\n    if palette_groups is None:\n        utils.add_colors_for_categorical_sample_annotation(adata, groups_key)\n        palette_groups = adata.uns[groups_key + '_colors']\n\n    ax = pl.gca() if ax is None else ax\n    from matplotlib import transforms\n    trans = transforms.blended_transform_factory(\n        ax.transData, ax.transAxes)\n    X = []\n    x_tick_locs = [0]\n    x_tick_labels = []\n    groups = []\n    anno_dict = {anno: [] for anno in annotations}\n    if isinstance(nodes[0], str):\n        nodes_ints = []\n        groups_names_set = set(groups_names)\n        for node in nodes:\n            if node not in groups_names_set:\n                raise ValueError(\n                    'Each node/group needs to be one of {} (`groups_key`=\\'{}\\') not \\'{}\\'.'\n                    .format(groups_names.tolist(), groups_key, node))\n            nodes_ints.append(groups_names.get_loc(node))\n        nodes_strs = nodes\n    else:\n        nodes_ints = nodes\n        nodes_strs = [groups_names[node] for node in nodes]\n    adata_X = adata\n    if use_raw and adata.raw is not None:\n        adata_X = adata.raw\n    for ikey, key in enumerate(keys):\n        x = []\n        for igroup, group in enumerate(nodes_ints):\n            idcs = np.arange(adata.n_obs)[\n                adata.obs[groups_key].values == nodes_strs[igroup]]\n            if len(idcs) == 0:\n                raise ValueError(\n                    'Did not find data points that match '\n                    '`adata.obs[{}].values == str({})`.'\n                    'Check whether adata.obs[{}] actually contains what you expect.'\n                    .format(groups_key, group, groups_key))\n            idcs_group = np.argsort(adata.obs['dpt_pseudotime'].values[\n                adata.obs[groups_key].values == nodes_strs[igroup]])\n            idcs = idcs[idcs_group]\n            if key in adata.obs_keys(): x += list(adata.obs[key].values[idcs])\n            else: x += list(adata_X[:, key].X[idcs])\n            if ikey == 0:\n                groups += [group for i in range(len(idcs))]\n                x_tick_locs.append(len(x))\n                for anno in annotations:\n                    series = adata.obs[anno]\n                    if is_categorical_dtype(series): series = series.cat.codes\n                    anno_dict[anno] += list(series.values[idcs])\n        if n_avg > 1:\n            old_len_x = len(x)\n            x = moving_average(x)\n            if ikey == 0:\n                for key in annotations:\n                    if not isinstance(anno_dict[key][0], str):\n                        anno_dict[key] = moving_average(anno_dict[key])\n        if normalize_to_zero_one:\n            x -= np.min(x)\n            x /= np.max(x)\n        X.append(x)\n        if not as_heatmap:\n            ax.plot(x[xlim[0]:xlim[1]], label=key)\n        if ikey == 0:\n            for igroup, group in enumerate(nodes):\n                if len(groups_names) > 0 and group not in groups_names:\n                    label = groups_names[group]\n                else:\n                    label = group\n                x_tick_labels.append(label)\n    X = np.array(X)\n    if as_heatmap:\n        img = ax.imshow(X, aspect='auto', interpolation='nearest',\n                        cmap=color_map)\n        if show_yticks:\n            ax.set_yticks(range(len(X)))\n            ax.set_yticklabels(keys, fontsize=ytick_fontsize)\n        else:\n            ax.set_yticks([])\n        ax.set_frame_on(False)\n        ax.set_xticks([])\n        ax.tick_params(axis='both', which='both', length=0)\n        ax.grid(False)\n        if show_colorbar:\n            pl.colorbar(img, ax=ax)\n        left_margin = 0.2 if left_margin is None else left_margin\n        pl.subplots_adjust(left=left_margin)\n    else:\n        left_margin = 0.4 if left_margin is None else left_margin\n        if len(keys) > 1:\n            pl.legend(frameon=False, loc='center left',\n                      bbox_to_anchor=(-left_margin, 0.5),\n                      fontsize=legend_fontsize)\n    xlabel = groups_key\n    if not as_heatmap:\n        ax.set_xlabel(xlabel)\n        pl.yticks([])\n        if len(keys) == 1: pl.ylabel(keys[0] + ' (a.u.)')\n    else:\n        import matplotlib.colors\n        # groups bar\n        ax_bounds = ax.get_position().bounds\n        groups_axis = pl.axes([ax_bounds[0],\n                               ax_bounds[1] - ax_bounds[3] / len(keys),\n                               ax_bounds[2],\n                               ax_bounds[3] / len(keys)])\n        groups = np.array(groups)[None, :]\n        groups_axis.imshow(groups, aspect='auto',\n                           interpolation=\"nearest\",\n                           cmap=matplotlib.colors.ListedColormap(\n                               # the following line doesn't work because of normalization\n                               # adata.uns['paga_groups_colors'])\n                               palette_groups[np.min(groups).astype(int):],\n                               N=int(np.max(groups)+1-np.min(groups))))\n        if show_yticks:\n            groups_axis.set_yticklabels(['', xlabel, ''], fontsize=ytick_fontsize)\n        else:\n            groups_axis.set_yticks([])\n        groups_axis.set_frame_on(False)\n        if show_node_names:\n            ypos = (groups_axis.get_ylim()[1] + groups_axis.get_ylim()[0])/2\n            x_tick_locs = sc_utils.moving_average(x_tick_locs, n=2)\n            for ilabel, label in enumerate(x_tick_labels):\n                groups_axis.text(x_tick_locs[ilabel], ypos, x_tick_labels[ilabel],\n                                 fontdict={'horizontalalignment': 'center',\n                                           'verticalalignment': 'center'})\n        groups_axis.set_xticks([])\n        groups_axis.grid(False)\n        groups_axis.tick_params(axis='both', which='both', length=0)\n        # further annotations\n        y_shift = ax_bounds[3] / len(keys)\n        for ianno, anno in enumerate(annotations):\n            if ianno > 0: y_shift = ax_bounds[3] / len(keys) / 2\n            anno_axis = pl.axes([ax_bounds[0],\n                                 ax_bounds[1] - (ianno+2) * y_shift,\n                                 ax_bounds[2],\n                                 y_shift])\n            arr = np.array(anno_dict[anno])[None, :]\n            if anno not in color_maps_annotations:\n                color_map_anno = ('Vega10' if is_categorical_dtype(adata.obs[anno])\n                                  else 'Greys')\n            else:\n                color_map_anno = color_maps_annotations[anno]\n            img = anno_axis.imshow(arr, aspect='auto',\n                                   interpolation='nearest',\n                                   cmap=color_map_anno)\n            if show_yticks:\n                anno_axis.set_yticklabels(['', anno, ''],\n                                          fontsize=ytick_fontsize)\n                anno_axis.tick_params(axis='both', which='both', length=0)\n            else:\n                anno_axis.set_yticks([])\n            anno_axis.set_frame_on(False)\n            anno_axis.set_xticks([])\n            anno_axis.grid(False)\n    if title is not None: ax.set_title(title, fontsize=title_fontsize)\n    if show is None and not ax_was_none: show = False\n    else: show = settings.autoshow if show is None else show\n    utils.savefig_or_show('paga_path', show=show, save=save)\n    if return_data:\n        df = pd.DataFrame(data=X.T, columns=keys)\n        df['groups'] = moving_average(groups)  # groups is without moving average, yet\n        if 'dpt_pseudotime' in anno_dict:\n            df['distance'] = anno_dict['dpt_pseudotime'].T\n        return ax, df if ax_was_none and show == False else df\n    else:\n        return ax if ax_was_none and show == False else None", "idx": 1550}
{"project": "Scanpy", "commit_id": "886_scanpy_1.4.3_scanpy_tools__rank_genes_groups.py_rank_genes_groups.py", "target": 1, "func": "def rank_genes_groups(\n    adata,\n    groupby,\n    use_raw=True,\n    groups: Union[str, Iterable[str]] = 'all',\n    reference='rest',\n    n_genes=100,\n    rankby_abs=False,\n    key_added=None,\n    copy=False,\n    method='t-test_overestim_var',\n    corr_method='benjamini-hochberg',\n    **kwds\n):\n    \"\"\"Rank genes for characterizing groups.\n    Parameters\n    ----------\n    adata : :class:`~anndata.AnnData`\n        Annotated data matrix.\n    groupby : `str`\n        The key of the observations grouping to consider.\n    use_raw : `bool`, optional (default: `True`)\n        Use `raw` attribute of `adata` if present.\n    groups\n        Subset of groups, e.g. `['g1', 'g2', 'g3']`, to which comparison shall\n        be restricted, or `'all'` (default), for all groups.\n    reference : `str`, optional (default: `'rest'`)\n        If `'rest'`, compare each group to the union of the rest of the group.  If\n        a group identifier, compare with respect to this group.\n    n_genes : `int`, optional (default: 100)\n        The number of genes that appear in the returned tables.\n    method : `{'logreg', 't-test', 'wilcoxon', 't-test_overestim_var'}`, optional (default: 't-test_overestim_var')\n        If 't-test', uses t-test, if 'wilcoxon', uses Wilcoxon-Rank-Sum. If\n        't-test_overestim_var', overestimates variance of each group. If\n        'logreg' uses logistic regression, see [Ntranos18]_, `here\n        <https://github.com/theislab/scanpy/issues/95>`__ and `here\n        <http://www.nxn.se/valent/2018/3/5/actionable-scrna-seq-clusters>`__, for\n        why this is meaningful.\n    corr_method : `{'benjamini-hochberg', 'bonferroni'}`, optional (default: 'benjamini-hochberg')\n        p-value correction method. Used only for 't-test', 't-test_overestim_var',\n        and 'wilcoxon' methods.\n    rankby_abs : `bool`, optional (default: `False`)\n        Rank genes by the absolute value of the score, not by the\n        score. The returned scores are never the absolute values.\n    **kwds : keyword parameters\n        Are passed to test methods. Currently this affects only parameters that\n        are passed to `sklearn.linear_model.LogisticRegression\n        <http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html>`__.\n        For instance, you can pass `penalty='l1'` to try to come up with a\n        minimal set of genes that are good predictors (sparse solution meaning\n        few non-zero fitted coefficients).\n    Returns\n    -------\n    **names** : structured `np.ndarray` (`.uns['rank_genes_groups']`)\n        Structured array to be indexed by group id storing the gene\n        names. Ordered according to scores.\n    **scores** : structured `np.ndarray` (`.uns['rank_genes_groups']`)\n        Structured array to be indexed by group id storing the z-score\n        underlying the computation of a p-value for each gene for each\n        group. Ordered according to scores.\n    **logfoldchanges** : structured `np.ndarray` (`.uns['rank_genes_groups']`)\n        Structured array to be indexed by group id storing the log2\n        fold change for each gene for each group. Ordered according to\n        scores. Only provided if method is 't-test' like.\n        Note: this is an approximation calculated from mean-log values.\n    **pvals** : structured `np.ndarray` (`.uns['rank_genes_groups']`)\n        p-values.\n    **pvals_adj** : structured `np.ndarray` (`.uns['rank_genes_groups']`)\n        Corrected p-values.\n    Notes\n    -----\n    There are slight inconsistencies depending on whether sparse\n    or dense data are passed. See `here <https://github.com/theislab/scanpy/blob/master/scanpy/tests/test_rank_genes_groups.py>`__.\n    Examples\n    --------\n    >>> adata = sc.datasets.pbmc68k_reduced()\n    >>> sc.tl.rank_genes_groups(adata, 'bulk_labels', method='wilcoxon')\n    # to visualize the results\n    >>> sc.pl.rank_genes_groups(adata)\n    \"\"\"\n    if 'only_positive' in kwds:\n        rankby_abs = not kwds.pop('only_positive')  # backwards compat\n    start = logg.info('ranking genes')\n    avail_methods = {'t-test', 't-test_overestim_var', 'wilcoxon', 'logreg'}\n    if method not in avail_methods:\n        raise ValueError('Method must be one of {}.'.format(avail_methods))\n    avail_corr = {'benjamini-hochberg', 'bonferroni'}\n    if corr_method not in avail_corr:\n        raise ValueError('Correction method must be one of {}.'.format(avail_corr))\n    adata = adata.copy() if copy else adata\n    utils.sanitize_anndata(adata)\n    # for clarity, rename variable\n    groups_order = groups if isinstance(groups, str) else list(groups)\n    if isinstance(groups_order, list) and isinstance(groups_order[0], int):\n        groups_order = [str(n) for n in groups_order]\n    if reference != 'rest' and reference not in set(groups_order):\n        groups_order += [reference]\n    if (reference != 'rest'\n        and reference not in set(adata.obs[groupby].cat.categories)):\n        raise ValueError('reference = {} needs to be one of groupby = {}.'\n                         .format(reference,\n                                 adata.obs[groupby].cat.categories.tolist()))\n    groups_order, groups_masks = utils.select_groups(\n        adata, groups_order, groupby)\n    if key_added is None:\n        key_added = 'rank_genes_groups'\n    adata.uns[key_added] = {}\n    adata.uns[key_added]['params'] = {\n        'groupby': groupby,\n        'reference': reference,\n        'method': method,\n        'use_raw': use_raw,\n        'corr_method': corr_method,\n    }\n    # adata_comp mocks an AnnData object if use_raw is True\n    # otherwise it's just the AnnData object\n    adata_comp = adata\n    if adata.raw is not None and use_raw:\n        adata_comp = adata.raw\n    X = adata_comp.X\n    # for clarity, rename variable\n    n_genes_user = n_genes\n    # make sure indices are not OoB in case there are less genes than n_genes\n    if n_genes_user > X.shape[1]:\n        n_genes_user = X.shape[1]\n    # in the following, n_genes is simply another name for the total number of genes\n    n_genes = X.shape[1]\n    n_groups = groups_masks.shape[0]\n    ns = np.zeros(n_groups, dtype=int)\n    for imask, mask in enumerate(groups_masks):\n        ns[imask] = np.where(mask)[0].size\n    logg.debug(f'consider {groupby!r} groups:')\n    logg.debug(f'with sizes: {ns}')\n    if reference != 'rest':\n        ireference = np.where(groups_order == reference)[0][0]\n    reference_indices = np.arange(adata_comp.n_vars, dtype=int)\n    rankings_gene_scores = []\n    rankings_gene_names = []\n    rankings_gene_logfoldchanges = []\n    rankings_gene_pvals = []\n    rankings_gene_pvals_adj = []\n    if method in {'t-test', 't-test_overestim_var'}:\n        from scipy import stats\n        from statsmodels.stats.multitest import multipletests\n        # loop over all masks and compute means, variances and sample numbers\n        means = np.zeros((n_groups, n_genes))\n        vars = np.zeros((n_groups, n_genes))\n        for imask, mask in enumerate(groups_masks):\n            means[imask], vars[imask] = _get_mean_var(X[mask])\n        # test each either against the union of all other groups or against a\n        # specific group\n        for igroup in range(n_groups):\n            if reference == 'rest':\n                mask_rest = ~groups_masks[igroup]\n            else:\n                if igroup == ireference: continue\n                else: mask_rest = groups_masks[ireference]\n            mean_group, var_group = means[igroup], vars[igroup]\n            mean_rest, var_rest = _get_mean_var(X[mask_rest])\n            ns_group = ns[igroup]  # number of observations in group\n            if method == 't-test': ns_rest = np.where(mask_rest)[0].size\n            elif method == 't-test_overestim_var': ns_rest = ns[igroup]  # hack for overestimating the variance for small groups\n            else: raise ValueError('Method does not exist.')\n\n            scores, pvals = stats.ttest_ind_from_stats(\n                mean1=mean_group, std1=np.sqrt(var_group), nobs1=ns_group,\n                mean2=mean_rest,  std2=np.sqrt(var_rest),  nobs2=ns_rest,\n                equal_var=False  # Welch's\n            )\n\n            # Fold change\n            foldchanges = (np.expm1(mean_group) + 1e-9) / (np.expm1(mean_rest) + 1e-9)  # add small value to remove 0's\n            scores[np.isnan(scores)] = 0  # I think it's only nan when means are the same and vars are 0\n            pvals[np.isnan(pvals)] = 1  # This also has to happen for Benjamini Hochberg\n            if corr_method == 'benjamini-hochberg':\n                _, pvals_adj, _, _ = multipletests(pvals, alpha=0.05, method='fdr_bh')\n            elif corr_method == 'bonferroni':\n                pvals_adj = np.minimum(pvals * n_genes, 1.0)\n            scores_sort = np.abs(scores) if rankby_abs else scores\n            partition = np.argpartition(scores_sort, -n_genes_user)[-n_genes_user:]\n            partial_indices = np.argsort(scores_sort[partition])[::-1]\n            global_indices = reference_indices[partition][partial_indices]\n            rankings_gene_scores.append(scores[global_indices])\n            rankings_gene_logfoldchanges.append(np.log2(foldchanges[global_indices]))\n            rankings_gene_names.append(adata_comp.var_names[global_indices])\n            rankings_gene_pvals.append(pvals[global_indices])\n            rankings_gene_pvals_adj.append(pvals_adj[global_indices])\n    elif method == 'logreg':\n        # if reference is not set, then the groups listed will be compared to the rest\n        # if reference is set, then the groups listed will be compared only to the other groups listed\n        from sklearn.linear_model import LogisticRegression\n        reference = groups_order[0]\n        if len(groups) == 1:\n            raise Exception('Cannot perform logistic regression on a single cluster.')\n        adata_copy = adata[adata.obs[groupby].isin(groups_order)]\n        adata_comp = adata_copy\n        if adata.raw is not None and use_raw:\n            adata_comp = adata_copy.raw\n        X = adata_comp.X\n        clf = LogisticRegression(**kwds)\n        clf.fit(X, adata_copy.obs[groupby].cat.codes)\n        scores_all = clf.coef_\n        for igroup, group in enumerate(groups_order):\n            if len(groups_order) <= 2:  # binary logistic regression\n                scores = scores_all[0]\n            else:\n                scores = scores_all[igroup]\n            partition = np.argpartition(scores, -n_genes_user)[-n_genes_user:]\n            partial_indices = np.argsort(scores[partition])[::-1]\n            global_indices = reference_indices[partition][partial_indices]\n            rankings_gene_scores.append(scores[global_indices])\n            rankings_gene_names.append(adata_comp.var_names[global_indices])\n            if len(groups_order) <= 2:\n                break\n    elif method == 'wilcoxon':\n        from scipy import stats\n        from statsmodels.stats.multitest import multipletests\n        CONST_MAX_SIZE = 10000000\n        means = np.zeros((n_groups, n_genes))\n        vars = np.zeros((n_groups, n_genes))\n        # initialize space for z-scores\n        scores = np.zeros(n_genes)\n        # First loop: Loop over all genes\n        if reference != 'rest':\n            for imask, mask in enumerate(groups_masks):\n                means[imask], vars[imask] = _get_mean_var(X[mask])  # for fold-change only\n                if imask == ireference: continue\n                else: mask_rest = groups_masks[ireference]\n                ns_rest = np.where(mask_rest)[0].size\n                mean_rest, var_rest = _get_mean_var(X[mask_rest]) # for fold-change only\n                if ns_rest <= 25 or ns[imask] <= 25:\n                    logg.hint('Few observations in a group for '\n                              'normal approximation (<=25). Lower test accuracy.')\n                n_active = ns[imask]\n                m_active = ns_rest\n                # Now calculate gene expression ranking in chunkes:\n                chunk = []\n                # Calculate chunk frames\n                n_genes_max_chunk = floor(CONST_MAX_SIZE / (n_active + m_active))\n                if n_genes_max_chunk < n_genes:\n                    chunk_index = n_genes_max_chunk\n                    while chunk_index < n_genes:\n                        chunk.append(chunk_index)\n                        chunk_index = chunk_index + n_genes_max_chunk\n                    chunk.append(n_genes)\n                else:\n                    chunk.append(n_genes)\n                left = 0\n                # Calculate rank sums for each chunk for the current mask\n                for chunk_index, right in enumerate(chunk):\n                    # Check if issparse is true: AnnData objects are currently sparse.csr or ndarray.\n                    if issparse(X):\n                        df1 = pd.DataFrame(data=X[mask, left:right].todense())\n                        df2 = pd.DataFrame(data=X[mask_rest, left:right].todense(),\n                                           index=np.arange(start=n_active, stop=n_active + m_active))\n                    else:\n                        df1 = pd.DataFrame(data=X[mask, left:right])\n                        df2 = pd.DataFrame(data=X[mask_rest, left:right],\n                                           index=np.arange(start=n_active, stop=n_active + m_active))\n                    df1 = df1.append(df2)\n                    ranks = df1.rank()\n                    # sum up adjusted_ranks to calculate W_m,n\n                    scores[left:right] = np.sum(ranks.loc[0:n_active, :])\n                    left = right\n                scores = (scores - (n_active * (n_active + m_active + 1) / 2)) / sqrt(\n                    (n_active * m_active * (n_active + m_active + 1) / 12))\n                scores[np.isnan(scores)] = 0\n                pvals = 2 * stats.distributions.norm.sf(np.abs(scores))\n                if corr_method == 'benjamini-hochberg':\n                    pvals[np.isnan(pvals)] = 1  # set Nan values to 1 to properly convert using Benhjamini Hochberg\n                    _, pvals_adj, _, _ = multipletests(pvals, alpha=0.05, method='fdr_bh')\n                elif corr_method == 'bonferroni':\n                    pvals_adj = np.minimum(pvals * n_genes, 1.0)\n                # Fold change\n                foldchanges = (np.expm1(means[imask]) + 1e-9) / (np.expm1(mean_rest) + 1e-9)  # add small value to remove 0's\n                scores_sort = np.abs(scores) if rankby_abs else scores\n                partition = np.argpartition(scores_sort, -n_genes_user)[-n_genes_user:]\n                partial_indices = np.argsort(scores_sort[partition])[::-1]\n                global_indices = reference_indices[partition][partial_indices]\n                rankings_gene_scores.append(scores[global_indices])\n                rankings_gene_names.append(adata_comp.var_names[global_indices])\n                rankings_gene_logfoldchanges.append(np.log2(foldchanges[global_indices]))\n                rankings_gene_pvals.append(pvals[global_indices])\n                rankings_gene_pvals_adj.append(pvals_adj[global_indices])\n        # If no reference group exists, ranking needs only to be done once (full mask)\n        else:\n            scores = np.zeros((n_groups, n_genes))\n            chunk = []\n            n_cells = X.shape[0]\n            n_genes_max_chunk = floor(CONST_MAX_SIZE / n_cells)\n            if n_genes_max_chunk < n_genes:\n                chunk_index = n_genes_max_chunk\n                while chunk_index < n_genes:\n                    chunk.append(chunk_index)\n                    chunk_index = chunk_index + n_genes_max_chunk\n                chunk.append(n_genes)\n            else:\n                chunk.append(n_genes)\n            left = 0\n            for chunk_index, right in enumerate(chunk):\n                # Check if issparse is true\n                if issparse(X):\n                    df1 = pd.DataFrame(data=X[:, left:right].todense())\n                else:\n                    df1 = pd.DataFrame(data=X[:, left:right])\n                ranks = df1.rank()\n                # sum up adjusted_ranks to calculate W_m,n\n                for imask, mask in enumerate(groups_masks):\n                    scores[imask, left:right] = np.sum(ranks.loc[mask, :])\n                left = right\n            for imask, mask in enumerate(groups_masks):\n                mask_rest = ~groups_masks[imask]\n                means[imask], vars[imask] = _get_mean_var(X[mask]) #for fold-change\n                mean_rest, var_rest = _get_mean_var(X[mask_rest])  # for fold-change\n                scores[imask, :] = (scores[imask, :] - (ns[imask] * (n_cells + 1) / 2)) / sqrt(\n                    (ns[imask] * (n_cells - ns[imask]) * (n_cells + 1) / 12))\n                scores[np.isnan(scores)] = 0\n                pvals = 2 * stats.distributions.norm.sf(np.abs(scores[imask,:]))\n                if corr_method == 'benjamini-hochberg':\n                    pvals[np.isnan(pvals)] = 1  # set Nan values to 1 to properly convert using Benhjamini Hochberg\n                    _, pvals_adj, _, _ = multipletests(pvals, alpha=0.05, method='fdr_bh')\n                elif corr_method == 'bonferroni':\n                    pvals_adj = np.minimum(pvals * n_genes, 1.0)\n                # Fold change\n                foldchanges = (np.expm1(means[imask]) + 1e-9) / (np.expm1(mean_rest) + 1e-9)  # add small value to remove 0's\n                scores_sort = np.abs(scores) if rankby_abs else scores\n                partition = np.argpartition(scores_sort[imask, :], -n_genes_user)[-n_genes_user:]\n                partial_indices = np.argsort(scores_sort[imask, partition])[::-1]\n                global_indices = reference_indices[partition][partial_indices]\n                rankings_gene_scores.append(scores[imask, global_indices])\n                rankings_gene_names.append(adata_comp.var_names[global_indices])\n                rankings_gene_logfoldchanges.append(np.log2(foldchanges[global_indices]))\n                rankings_gene_pvals.append(pvals[global_indices])\n                rankings_gene_pvals_adj.append(pvals_adj[global_indices])\n    groups_order_save = [str(g) for g in groups_order]\n    if (reference != 'rest' and method != 'logreg') or (method == 'logreg' and len(groups) == 2):\n        groups_order_save = [g for g in groups_order if g != reference]\n    adata.uns[key_added]['scores'] = np.rec.fromarrays(\n        [n for n in rankings_gene_scores],\n        dtype=[(rn, 'float32') for rn in groups_order_save])\n    adata.uns[key_added]['names'] = np.rec.fromarrays(\n        [n for n in rankings_gene_names],\n        dtype=[(rn, 'U50') for rn in groups_order_save])\n    if method in {'t-test', 't-test_overestim_var', 'wilcoxon'}:\n        adata.uns[key_added]['logfoldchanges'] = np.rec.fromarrays(\n            [n for n in rankings_gene_logfoldchanges],\n            dtype=[(rn, 'float32') for rn in groups_order_save])\n        adata.uns[key_added]['pvals'] = np.rec.fromarrays(\n            [n for n in rankings_gene_pvals],\n            dtype=[(rn, 'float64') for rn in groups_order_save])\n        adata.uns[key_added]['pvals_adj'] = np.rec.fromarrays(\n            [n for n in rankings_gene_pvals_adj],\n            dtype=[(rn, 'float64') for rn in groups_order_save])\n    logg.info(\n        '    finished',\n        time=start,\n        deep=(\n            f'added to `.uns[{key_added!r}]`\\n'\n            \"    'names', sorted np.recarray to be indexed by group ids\\n\"\n            \"    'scores', sorted np.recarray to be indexed by group ids\\n\"\n            + (\n                \"    'logfoldchanges', sorted np.recarray to be indexed by group ids\\n\"\n                \"    'pvals', sorted np.recarray to be indexed by group ids\\n\"\n                \"    'pvals_adj', sorted np.recarray to be indexed by group ids\"\n                if method in {'t-test', 't-test_overestim_var', 'wilcoxon'} else\n                ''\n            )\n        ),\n    )\n    return adata if copy else None", "idx": 1551}
{"project": "Scanpy", "commit_id": "20_scanpy_0.0_scanpy_utils.py_init_params.py", "target": 1, "func": "def init_params(params, default_params, check=True):\n    \"\"\"\n    Update default paramaters with params.\n    \"\"\"\n    _params = dict(default_params)\n    if params: # allow for params to be None\n        for key, val in params.items():\n            if key in default_params:\n                _params[key] = val\n            elif check:\n                raise ValueError('\\'' + key\n                                 + '\\' is not a valid parameter key, '\n                                 + 'consider one of \\n'\n                                 + str(list(default_params.keys())))\n    return _params", "idx": 1554}
{"project": "Scanpy", "commit_id": "779_scanpy_1.9.0__dpt.py__diffmap.py", "target": 0, "func": "def _diffmap(adata, n_comps=15, neighbors_key=None, random_state=0):\n    start = logg.info(f'computing Diffusion Maps using n_comps={n_comps}(=n_dcs)')\n    dpt = DPT(adata, neighbors_key=neighbors_key)\n    dpt.compute_transitions()\n    dpt.compute_eigen(n_comps=n_comps, random_state=random_state)\n    adata.obsm['X_diffmap'] = dpt.eigen_basis\n    adata.uns['diffmap_evals'] = dpt.eigen_values\n    logg.info(\n        '    finished',\n        time=start,\n        deep=(\n            'added\\n'", "idx": 1557}
{"project": "Scanpy", "commit_id": "434_scanpy_1.9.0__simple.py__downsample_per_cell.py", "target": 0, "func": "def _downsample_per_cell(X, counts_per_cell, random_state, replace):\n    n_obs = X.shape[0]\n    if isinstance(counts_per_cell, int):\n        counts_per_cell = np.full(n_obs, counts_per_cell)\n    else:\n        counts_per_cell = np.asarray(counts_per_cell)\n    # np.random.choice needs int arguments in numba code:\n    counts_per_cell = counts_per_cell.astype(np.int_, copy=False)\n    if not isinstance(counts_per_cell, np.ndarray) or len(counts_per_cell) != n_obs:\n        raise ValueError(\n            \"If provided, 'counts_per_cell' must be either an integer, or \"\n            \"coercible to an `np.ndarray` of length as number of observations\"\n            \" by `np.asarray(counts_per_cell)`.\"\n        )\n    if issparse(X):\n        original_type = type(X)\n        if not isspmatrix_csr(X):\n            X = csr_matrix(X)\n        totals = np.ravel(X.sum(axis=1))  # Faster for csr matrix\n        under_target = np.nonzero(totals > counts_per_cell)[0]\n        rows = np.split(X.data, X.indptr[1:-1])\n        for rowidx in under_target:\n            row = rows[rowidx]\n            _downsample_array(\n                row,\n                counts_per_cell[rowidx],\n                random_state=random_state,\n                replace=replace,\n                inplace=True,\n            )\n        X.eliminate_zeros()\n        if original_type is not csr_matrix:  # Put it back\n            X = original_type(X)\n    else:\n        totals = np.ravel(X.sum(axis=1))\n        under_target = np.nonzero(totals > counts_per_cell)[0]\n        for rowidx in under_target:\n            row = X[rowidx, :]\n            _downsample_array(\n                row,\n                counts_per_cell[rowidx],\n                random_state=random_state,\n                replace=replace,\n                inplace=True,\n            )\n    return X", "idx": 1558}
{"project": "Scanpy", "commit_id": "1014_scanpy_1.4.6_scanpy_plotting__tools_scatterplots.py_embedding.py", "target": 1, "func": "def embedding(\n    adata: AnnData,\n    basis: str,\n    *,\n    color: Union[str, Sequence[str], None] = None,\n    gene_symbols: Optional[str] = None,\n    use_raw: Optional[bool] = None,\n    sort_order: bool = True,\n    edges: bool = False,\n    edges_width: float = 0.1,\n    edges_color: Union[str, Sequence[float], Sequence[str]] = 'grey',\n    neighbors_key: Optional[str] = None,\n    arrows: bool = False,\n    arrows_kwds: Optional[Mapping[str, Any]] = None,\n    groups: Optional[str] = None,\n    components: Union[str, Sequence[str]] = None,\n    layer: Optional[str] = None,\n    projection: Literal['2d', '3d'] = '2d',\n    # image parameters\n    img_key: Optional[str] = None,\n    crop_coord: Tuple[int, int, int, int] = None,\n    alpha_img: float = 1.0,\n    bw: bool = False,\n    library_id: str = None,\n    #\n    color_map: Union[Colormap, str, None] = None,\n    palette: Union[str, Sequence[str], Cycler, None] = None,\n    size: Union[float, Sequence[float], None] = None,\n    frameon: Optional[bool] = None,\n    legend_fontsize: Union[int, float, _FontSize, None] = None,\n    legend_fontweight: Union[int, _FontWeight] = 'bold',\n    legend_loc: str = 'right margin',\n    legend_fontoutline: Optional[int] = None,\n    vmax: Union[VMinMax, Sequence[VMinMax], None] = None,\n    vmin: Union[VMinMax, Sequence[VMinMax], None] = None,\n    add_outline: Optional[bool] = False,\n    outline_width: Tuple[float, float] = (0.3, 0.05),\n    outline_color: Tuple[str, str] = ('black', 'white'),\n    ncols: int = 4,\n    hspace: float = 0.25,\n    wspace: Optional[float] = None,\n    title: Union[str, Sequence[str], None] = None,\n    show: Optional[bool] = None,\n    save: Union[bool, str, None] = None,\n    ax: Optional[Axes] = None,\n    return_fig: Optional[bool] = None,\n    **kwargs,\n) -> Union[Figure, Axes, None]:\n    \"\"\"\\\n    Scatter plot for user specified embedding basis (e.g. umap, pca, etc)\n    Parameters\n    ----------\n    basis\n        Name of the `obsm` basis to use.\n    {adata_color_etc}\n    {edges_arrows}\n    {scatter_bulk}\n    {show_save_ax}\n    Returns\n    -------\n    If `show==False` a :class:`~matplotlib.axes.Axes` or a list of it.\n    \"\"\"\n    sanitize_anndata(adata)\n    if color_map is not None:\n        kwargs['cmap'] = color_map\n    if size is not None:\n        kwargs['s'] = size\n    if 'edgecolor' not in kwargs:\n        # by default turn off edge color. Otherwise, for\n        # very small sizes the edge will not reduce its size\n        # (https://github.com/theislab/scanpy/issues/293)\n        kwargs['edgecolor'] = 'none'\n    if groups:\n        if isinstance(groups, str):\n            groups = [groups]\n    make_projection_available(projection)\n    args_3d = dict(projection='3d') if projection == '3d' else {}\n    # Deal with Raw\n    if use_raw is None:\n        # check if adata.raw is set\n        use_raw = layer is None and adata.raw is not None\n    if use_raw and layer is not None:\n        raise ValueError(\n            \"Cannot use both a layer and the raw representation. Was passed:\"\n            f\"use_raw={use_raw}, layer={layer}.\"\n        )\n    if wspace is None:\n        #  try to set a wspace that is not too large or too small given the\n        #  current figure size\n        wspace = 0.75 / rcParams['figure.figsize'][0] + 0.02\n    if adata.raw is None and use_raw:\n        raise ValueError(\n            \"`use_raw` is set to True but AnnData object does not have raw. \"\n            \"Please check.\"\n        )\n    # turn color into a python list\n    color = [color] if isinstance(color, str) or color is None else list(color)\n    if title is not None:\n        # turn title into a python list if not None\n        title = [title] if isinstance(title, str) else list(title)\n    # get the points position and the components list\n    # (only if components is not None)\n    data_points, components_list = _get_data_points(\n        adata, basis, projection, components, img_key, library_id\n    )\n    # Setup layout.\n    # Most of the code is for the case when multiple plots are required\n    # 'color' is a list of names that want to be plotted.\n    # Eg. ['Gene1', 'louvain', 'Gene2'].\n    # component_list is a list of components [[0,1], [1,2]]\n    if (\n        not isinstance(color, str)\n        and isinstance(color, cabc.Sequence)\n        and len(color) > 1\n    ) or len(components_list) > 1:\n        if ax is not None:\n            raise ValueError(\n                \"Cannot specify `ax` when plotting multiple panels \"\n                \"(each for a given value of 'color').\"\n            )\n        if len(components_list) == 0:\n            components_list = [None]\n        # each plot needs to be its own panel\n        num_panels = len(color) * len(components_list)\n        fig, grid = _panel_grid(hspace, wspace, ncols, num_panels)\n    else:\n        if len(components_list) == 0:\n            components_list = [None]\n        grid = None\n        if ax is None:\n            fig = pl.figure()\n            ax = fig.add_subplot(111, **args_3d)\n    # turn vmax and vmin into a sequence\n    if isinstance(vmax, str) or not isinstance(vmax, cabc.Sequence):\n        vmax = [vmax]\n    if isinstance(vmin, str) or not isinstance(vmin, cabc.Sequence):\n        vmin = [vmin]\n    if 's' in kwargs:\n        size = kwargs.pop('s')\n    if size is not None:\n        # check if size is any type of sequence, and if so\n        # set as ndarray\n        import pandas.core.series\n        if (\n            size is not None\n            and isinstance(\n                size, (cabc.Sequence, pandas.core.series.Series, np.ndarray,)\n            )\n            and len(size) == adata.shape[0]\n        ):\n            size = np.array(size, dtype=float)\n    elif basis is \"spatial\":\n        size = 1.0\n    else:\n        size = 120000 / adata.shape[0]\n\n    ###\n    # make the plots\n    axs = []\n    import itertools\n    idx_components = range(len(components_list))\n    # use itertools.product to make a plot for each color and for each component\n    # For example if color=[gene1, gene2] and components=['1,2, '2,3'].\n    # The plots are: [\n    #     color=gene1, components=[1,2], color=gene1, components=[2,3],\n    #     color=gene2, components = [1, 2], color=gene2, components=[2,3],\n    # ]\n    for count, (value_to_plot, component_idx) in enumerate(\n        itertools.product(color, idx_components)\n    ):\n        color_vector, categorical = _get_color_values(\n            adata,\n            value_to_plot,\n            layer=layer,\n            groups=groups,\n            palette=palette,\n            use_raw=use_raw,\n            gene_symbols=gene_symbols,\n        )\n        # check if higher value points should be plot on top\n        if sort_order is True and value_to_plot is not None and categorical is False:\n            order = np.argsort(color_vector)\n            color_vector = color_vector[order]\n            _data_points = data_points[component_idx][order, :]\n            # check if 'size' is given (stored in kwargs['s']\n            # and reorder it.\n            if isinstance(size, np.ndarray):\n                size = np.array(size)[order]\n        else:\n            _data_points = data_points[component_idx]\n        # if plotting multiple panels, get the ax from the grid spec\n        # else use the ax value (either user given or created previously)\n        if grid:\n            ax = pl.subplot(grid[count], **args_3d)\n            axs.append(ax)\n        if not (settings._frameon if frameon is None else frameon):\n            ax.axis('off')\n        if title is None:\n            if value_to_plot is not None:\n                ax.set_title(value_to_plot)\n            else:\n                ax.set_title('')\n        else:\n            try:\n                ax.set_title(title[count])\n            except IndexError:\n                logg.warning(\n                    \"The title list is shorter than the number of panels. \"\n                    \"Using 'color' value instead for some plots.\"\n                )\n                ax.set_title(value_to_plot)\n        # check vmin and vmax options\n        if categorical:\n            kwargs['vmin'] = kwargs['vmax'] = None\n        else:\n            kwargs['vmin'], kwargs['vmax'] = _get_vmin_vmax(\n                vmin, vmax, count, color_vector\n            )\n        # make the scatter plot\n        if projection == '3d':\n            cax = ax.scatter(\n                _data_points[:, 0],\n                _data_points[:, 1],\n                _data_points[:, 2],\n                marker=\".\",\n                c=color_vector,\n                rasterized=settings._vector_friendly,\n                **kwargs,\n            )\n        else:\n            if img_key is not None:\n                # had to return size_spot cause spot size is set according\n                # to the image to be plotted\n                img_processed, img_coord, size_spot, cmap_img = _process_image(\n                    adata, data_points, img_key, crop_coord, size, library_id, bw\n                )\n                ax.imshow(img_processed, cmap=cmap_img, alpha=alpha_img)\n                ax.set_xlim(img_coord[0], img_coord[1])\n                ax.set_ylim(img_coord[3], img_coord[2])\n            elif img_key is None and basis is \"spatial\":\n                # order of magnitude similar to public visium\n                size_spot = 70 * size\n\n            scatter = (\n                partial(ax.scatter, s=size)\n                if basis is not \"spatial\"\n                else partial(circles, s=size_spot, ax=ax)\n            )\n\n            if add_outline:\n                # the default outline is a black edge followed by a\n                # thin white edged added around connected clusters.\n                # To add an outline\n                # three overlapping scatter plots are drawn:\n                # First black dots with slightly larger size,\n                # then, white dots a bit smaller, but still larger\n                # than the final dots. Then the final dots are drawn\n                # with some transparency.\n                bg_width, gap_width = outline_width\n                point = np.sqrt(size)\n                gap_size = (point + (point * gap_width) * 2) ** 2\n                bg_size = (np.sqrt(gap_size) + (point * bg_width) * 2) ** 2\n                # the default black and white colors can be changes using\n                # the contour_config parameter\n                bg_color, gap_color = outline_color\n                # remove edge from kwargs if present\n                # because edge needs to be set to None\n                kwargs['edgecolor'] = 'none'\n                # remove alpha for outline\n                alpha = kwargs.pop('alpha') if 'alpha' in kwargs else None\n                ax.scatter(\n                    _data_points[:, 0],\n                    _data_points[:, 1],\n                    s=bg_size,\n                    marker=\".\",\n                    c=bg_color,\n                    rasterized=settings._vector_friendly,\n                    **kwargs,\n                )\n                ax.scatter(\n                    _data_points[:, 0],\n                    _data_points[:, 1],\n                    s=gap_size,\n                    marker=\".\",\n                    c=gap_color,\n                    rasterized=settings._vector_friendly,\n                    **kwargs,\n                )\n                # if user did not set alpha, set alpha to 0.7\n                kwargs['alpha'] = 0.7 if alpha is None else alpha\n            if groups:\n                # first plot non-groups and then plot the\n                # required groups on top\n                in_groups = np.array(adata.obs[value_to_plot].isin(groups))\n                if isinstance(size, np.ndarray):\n                    in_groups_size = size[in_groups]\n                    not_in_groups_size = size[~in_groups]\n                elif img_key is not None:\n                    in_groups_size = not_in_groups_size = size_spot\n                else:\n                    in_groups_size = not_in_groups_size = size\n\n                # only show grey points if no image is below\n                if basis is not \"spatial\":\n                    ax.scatter(\n                        _data_points[~in_groups, 0],\n                        _data_points[~in_groups, 1],\n                        s=not_in_groups_size,\n                        marker=\".\",\n                        c=color_vector[~in_groups],\n                        rasterized=settings._vector_friendly,\n                        **kwargs,\n                    )\n                cax = scatter(\n                    _data_points[in_groups, 0],\n                    _data_points[in_groups, 1],\n                    s=in_groups_size,\n                    marker=\".\",\n                    c=color_vector[in_groups],\n                    rasterized=settings._vector_friendly,\n                    **kwargs,\n                )\n            else:\n                cax = scatter(\n                    _data_points[:, 0],\n                    _data_points[:, 1],\n                    marker=\".\",\n                    c=color_vector,\n                    rasterized=settings._vector_friendly,\n                    **kwargs,\n                )\n        # remove y and x ticks\n        ax.set_yticks([])\n        ax.set_xticks([])\n        if projection == '3d':\n            ax.set_zticks([])\n        # set default axis_labels\n        name = _basis2name(basis)\n        if components is not None:\n            axis_labels = [name + str(x + 1) for x in components_list[component_idx]]\n        elif projection == '3d':\n            axis_labels = [name + str(x + 1) for x in range(3)]\n        else:\n            axis_labels = [name + str(x + 1) for x in range(2)]\n        ax.set_xlabel(axis_labels[0])\n        ax.set_ylabel(axis_labels[1])\n        if projection == '3d':\n            # shift the label closer to the axis\n            ax.set_zlabel(axis_labels[2], labelpad=-7)\n        ax.autoscale_view()\n        if edges:\n            _utils.plot_edges(ax, adata, basis, edges_width, edges_color, neighbors_key)\n        if arrows:\n            _utils.plot_arrows(ax, adata, basis, arrows_kwds)\n        if value_to_plot is None:\n            # if only dots were plotted without an associated value\n            # there is not need to plot a legend or a colorbar\n            continue\n        if legend_fontoutline is not None:\n            path_effect = [\n                patheffects.withStroke(linewidth=legend_fontoutline, foreground='w',)\n            ]\n        else:\n            path_effect = None\n        _add_legend_or_colorbar(\n            adata,\n            ax,\n            cax,\n            categorical,\n            value_to_plot,\n            legend_loc,\n            _data_points,\n            legend_fontweight,\n            legend_fontsize,\n            path_effect,\n            groups,\n            bool(grid),\n        )\n    if return_fig is True:\n        return fig\n    axs = axs if grid else ax\n    _utils.savefig_or_show(basis, show=show, save=save)\n    if show is False:\n        return axs", "idx": 1559}
{"project": "Scanpy", "commit_id": "353_scanpy_0.2_scanpy_data_structs_data_graph.py___init__.py", "target": 1, "func": "def __init__(self,\n             adata_or_X,\n             k=30,\n             knn=True,\n             n_jobs=None,\n             n_pcs=30,\n             n_dcs=10,\n             recompute_pca=None,\n             recompute_diffmap=None,\n             flavor='haghverdi16'):\n    logg.info('initializing data graph with `n_neighbors={}`'\n              .format(k))\n    self.k = k if k is not None else 30\n    self.knn = knn\n    self.n_jobs = sett.n_jobs if n_jobs is None else n_jobs\n    self.n_pcs = n_pcs\n    self.n_dcs = n_dcs\n    self.flavor = flavor  # this is to experiment around\n    self.sym = True  # we do not allow asymetric cases\n    self.iroot = None\n    isadata = isinstance(adata_or_X, AnnData)\n    if isadata:\n        adata = adata_or_X\n        X = adata_or_X.X\n    else:\n        X = adata_or_X\n    # retrieve xroot\n    xroot = None\n    if 'xroot' in adata.add:\n        xroot = adata.add['xroot']\n    elif 'xroot' in adata.var:\n        xroot = adata.var['xroot']\n    # set iroot directly\n    if 'iroot' in adata.add: self.iroot = adata.add['iroot']\n    # use the fulll data matrix X\n    if (self.n_pcs == 0  # use the full X as n_pcs == 0\n            or X.shape[1] <= self.n_pcs):\n        self.X = X\n        logg.m('    using data matrix X directly for building graph (no PCA)')\n        if xroot is not None: self.set_root(xroot)\n    # use the precomupted X_pca\n    elif (isadata\n          and not recompute_pca\n          and 'X_pca' in adata.smp\n          and adata.smp['X_pca'].shape[1] >= self.n_pcs):\n        logg.info('    using X_pca for building graph')\n        if xroot is not None and xroot.size == adata.X.shape[1]:\n            self.X = adata.X\n            self.set_root(xroot)\n        self.X = adata.smp['X_pca'][:, :n_pcs]\n        if xroot is not None and xroot.size == adata.smp['X_pca'].shape[1]:\n            self.set_root(xroot[:n_pcs])\n    # compute X_pca\n    else:\n        self.X = X\n        if (isadata\n                and xroot is not None\n                and xroot.size == adata.X.shape[1]):\n            self.set_root(xroot)\n        logg.m('    compute `X_pca` for building graph')\n        from ..preprocessing import pca\n        pca(adata, n_comps=self.n_pcs)\n        self.X = adata.smp['X_pca']\n        if xroot is not None and xroot.size == adata.smp['X_pca'].shape[1]:\n            self.set_root(xroot)\n    self.Dchosen = None\n    self.Dsq = adata.add['distance'] if knn and 'distance' in adata.add else None\n    # use diffmap from previous calculation\n    if (isadata and 'X_diffmap' in adata.smp and not recompute_diffmap\n            and adata.smp['X_diffmap'].shape[1] >= n_dcs - 1):\n        self.X_diffmap = adata.smp['X_diffmap'][:, :n_dcs - 1]\n        self.evals = np.r_[1, adata.add['diffmap_evals'][:n_dcs - 1]]\n        np.set_printoptions(precision=3)\n        logg.info('    using stored \"X_diffmap\" with spectrum\\n    {}'\n                  .format(str(self.evals).replace('\\n', '\\n    ')))\n        self.rbasis = np.c_[adata.smp['X_diffmap0'][:, None],\n        adata.smp['X_diffmap'][:, :n_dcs - 1]]\n        self.lbasis = self.rbasis\n        if knn: self.Ktilde = adata.add['Ktilde']\n        self.Dchosen = OnFlySymMatrix(self.get_Ddiff_row,\n                                      shape=(self.X.shape[0], self.X.shape[0]))\n    else:\n        self.evals = None\n        self.rbasis = None\n        self.lbasis = None\n        self.Dsq = None\n    # further attributes that might be written during the computation\n    self.M = None", "idx": 1563}
{"project": "Scanpy", "commit_id": "731_scanpy_1.3.2_scanpy_plotting_tools___init__.py__rank_genes_groups_plot.py", "target": 1, "func": "def _rank_genes_groups_plot(adata, plot_type='heatmap', groups=None,\n                            n_genes=10, groupby=None, key=None,\n                            show=None, save=None, **kwds):\n    \"\"\"\\\n    Plot ranking of genes using the specified plot type\n    Parameters\n    ----------\n    adata : :class:`~anndata.AnnData`\n        Annotated data matrix.\n    groups : `str` or `list` of `str`\n        The groups for which to show the gene ranking.\n    n_genes : `int`, optional (default: 10)\n        Number of genes to show.\n    groupby : `str` or `None`, optional (default: `None`)\n        The key of the observation grouping to consider. By default,\n        the groupby is chosen from the rank genes groups parameter but\n        other groupby options can be used.\n    {show_save_ax}\n    \"\"\"\n    if key is None:\n        key = 'rank_genes_groups'\n\n    if groupby is None:\n        groupby = str(adata.uns[key]['params']['groupby'])\n    group_names = (adata.uns[key]['names'].dtype.names\n                   if groups is None else groups)\n    # make a list of tuples containing the index for the start gene and the\n    # end gene that should be labelled\n    group_positions = [(x, x + n_genes - 1) for x in range(0, n_genes * len(group_names), n_genes)]\n    # sum(list, []) is used to flatten the gene list\n    gene_names = sum([list(adata.uns[key]['names'][x][:n_genes]) for x in group_names], [])\n    if plot_type == 'dotplot':\n        from ..anndata import dotplot\n        dotplot(adata, gene_names, groupby, var_group_labels=group_names,\n                var_group_positions=group_positions, show=show, save=save, **kwds)\n    elif plot_type == 'heatmap':\n        from ..anndata import heatmap\n        heatmap(adata, gene_names, groupby, var_group_labels=group_names,\n                var_group_positions=group_positions, show=show, save=save, **kwds)\n\n    elif plot_type == 'stacked_violin':\n        from ..anndata import stacked_violin\n        stacked_violin(adata, gene_names, groupby, var_group_labels=group_names,\n                       var_group_positions=group_positions, show=show, save=save, **kwds)\n\n    elif plot_type == 'matrixplot':\n        from ..anndata import matrixplot\n        matrixplot(adata, gene_names, groupby, var_group_labels=group_names,\n                   var_group_positions=group_positions, show=show, save=save, **kwds)", "idx": 1564}
{"project": "Scanpy", "commit_id": "267_scanpy_1.9.0__baseplot_class.py__plot_totals.py", "target": 0, "func": "def _plot_totals(\n        self, total_barplot_ax: Axes, orientation: Literal['top', 'right']\n    ):\n        \"\"\"\n        Makes the bar plot for totals\n        \"\"\"\n        params = self.plot_group_extra\n        counts_df = params['counts_df']\n        if self.categories_order is not None:\n            counts_df = counts_df.loc[self.categories_order]\n        if params['color'] is None:\n            if f'{self.groupby}_colors' in self.adata.uns:\n                color = self.adata.uns[f'{self.groupby}_colors']\n            else:\n                color = 'salmon'\n        else:\n            color = params['color']\n\n        if orientation == 'top':\n            counts_df.plot(\n                kind=\"bar\",\n                color=color,\n                position=0.5,\n                ax=total_barplot_ax,\n                edgecolor=\"black\",\n                width=0.65,\n            )\n            # add numbers to the top of the bars\n            max_y = max([p.get_height() for p in total_barplot_ax.patches])\n\n            for p in total_barplot_ax.patches:\n                p.set_x(p.get_x() + 0.5)\n                if p.get_height() >= 1000:\n                    display_number = f'{np.round(p.get_height()/1000, decimals=1)}k'\n                else:\n                    display_number = np.round(p.get_height(), decimals=1)\n                total_barplot_ax.annotate(\n                    display_number,\n                    (p.get_x() + p.get_width() / 2.0, (p.get_height() + max_y * 0.05)),\n                    ha=\"center\",\n                    va=\"top\",\n                    xytext=(0, 10),\n                    fontsize=\"x-small\",\n                    textcoords=\"offset points\",\n                )\n            # for k in total_barplot_ax.spines.keys():\n            #     total_barplot_ax.spines[k].set_visible(False)\n            total_barplot_ax.set_ylim(0, max_y * 1.4)\n\n        elif orientation == 'right':\n            counts_df.plot(\n                kind=\"barh\",\n                color=color,\n                position=-0.3,\n                ax=total_barplot_ax,\n                edgecolor=\"black\",\n                width=0.65,\n            )\n\n            # add numbers to the right of the bars\n            max_x = max([p.get_width() for p in total_barplot_ax.patches])\n            for p in total_barplot_ax.patches:\n                if p.get_width() >= 1000:\n                    display_number = f'{np.round(p.get_width()/1000, decimals=1)}k'\n                else:\n                    display_number = np.round(p.get_width(), decimals=1)\n                total_barplot_ax.annotate(\n                    display_number,\n                    ((p.get_width()), p.get_y() + p.get_height()),\n                    ha=\"center\",\n                    va=\"top\",\n                    xytext=(10, 10),\n                    fontsize=\"x-small\",\n                    textcoords=\"offset points\",\n                )\n            total_barplot_ax.set_xlim(0, max_x * 1.4)\n\n        total_barplot_ax.grid(False)\n        total_barplot_ax.axis(\"off\")", "idx": 1565}
{"project": "Scanpy", "commit_id": "720_scanpy_1.9.0_test_rank_genes_groups.py_test_emptycat.py", "target": 0, "func": "def test_emptycat():\n    pbmc = pbmc68k_reduced()\n    pbmc.obs['louvain'] = pbmc.obs['louvain'].cat.add_categories(['11'])\n\n    with pytest.raises(ValueError, match=rf\"Could not calculate statistics.*{'11'}\"):\n        rank_genes_groups(pbmc, groupby='louvain')", "idx": 1572}
{"project": "Scanpy", "commit_id": "60_scanpy_0.0_scanpy_exs_builtin.py_toggleswitch.py", "target": 1, "func": "def toggleswitch():\n    \"\"\"\n    Returns\n    -------\n    See paul15.\n    \"\"\"\n    filename = 'write/toggleswitch_sim/sim_000000.txt'\n    ddata = sc.read(filename, first_column_names=True)\n    ddata['xroot'] = ddata['X'][0]\n    return ddata", "idx": 1574}
{"project": "Scanpy", "commit_id": "884_scanpy_1.9.0__sim.py_sim_model_backwards.py", "target": 0, "func": "def sim_model_backwards(self, tmax, X0):\n        \"\"\"Simulate the model backwards in time.\"\"\"\n        X = np.zeros((tmax, self.dim))\n        X[tmax - 1] = X0\n        for t in range(tmax - 2, -1, -1):\n            sol = sp.optimize.root(\n                self.sim_model_back_help, X[t + 1], args=(X[t + 1]), method='hybr'\n            )\n            X[t] = sol.x\n        return X", "idx": 1583}
{"project": "Scanpy", "commit_id": "829_scanpy_1.4_scanpy_preprocessing_normalization.py_normalize_quantile.py", "target": 1, "func": "def normalize_quantile(adata, target_sum=None, quantile=1, key_added=None,\n                       layers=[], layer_norm=None, inplace=True):\n    \"\"\"\\\n    {norm_bulk}\n    {norm_quant}\n    {norm_return}\n    {ex_quant}\n    \"\"\"\n    if quantile < 0 or quantile > 1:\n        raise ValueError('Choose quantile between 0 and 1.')\n    X = adata.X\n    gene_subset = None\n    if not inplace:\n    # not recarray because need to support sparse\n        dat = {}\n    if quantile < 1:\n        logg.msg('normalizing by count per cell for \\\n                  genes that make up less than quantile * total count per cell', r=True)\n        X = adata.X\n        counts_per_cell = X.sum(1)\n        counts_per_cell = np.ravel(counts_per_cell)\n        gene_subset = (X>counts_per_cell[:, None]*quantile).sum(0)\n        gene_subset = (np.ravel(gene_subset) == 0)\n    else:\n        logg.msg('normalizing by total count per cell', r=True)\n    X = X if gene_subset is None else adata[:, gene_subset].X\n    counts_per_cell = X.sum(1)\n    # get rid of adata view\n    counts_per_cell = np.ravel(counts_per_cell).copy()\n    del X\n    del gene_subset\n    if key_added is not None:\n        adata.obs[key_added] = counts_per_cell\n\n    cell_subset = counts_per_cell>0\n    if not np.all(cell_subset):\n        logg.warning('Some cells have total count of genes equal to zero')\n\n    if layer_norm == 'after':\n        after = target_sum\n    elif layer_norm == 'X':\n        after = np.median(counts_per_cell[cell_subset])\n    elif layer_norm is None:\n        after = None\n    else: raise ValueError('layer_norm should be \"after\", \"X\" or None')\n    del cell_subset\n\n    if inplace:\n        _normalize_data(adata.X, counts_per_cell, target_sum)\n    else:\n        dat['X'] = _normalize_data(adata.X, counts_per_cell, target_sum, copy=True)\n\n    layers = adata.layers.keys() if layers == 'all' else layers\n    for layer in layers:\n        L = adata.layers[layer]\n        counts = np.ravel(L.sum(1))\n        if inplace:\n            _normalize_data(L, counts, after)\n        else:\n            dat[layer] = _normalize_data(L, counts, after, copy=True)\n\n    logg.msg('    finished', t=True, end=': ')\n    logg.msg('normalized adata.X')\n    if key_added is not None:\n        logg.msg('and added \\'{}\\', counts per cell before normalization (adata.obs)'\n            .format(key_added))\n    return dat if not inplace else None", "idx": 1588}
{"project": "Scanpy", "commit_id": "292_scanpy_1.9.0__qc.py_highest_expr_genes.py", "target": 0, "func": "def highest_expr_genes(\n    adata: AnnData,\n    n_top: int = 30,\n    show: Optional[bool] = None,\n    save: Optional[Union[str, bool]] = None,\n    ax: Optional[Axes] = None,\n    gene_symbols: Optional[str] = None,\n    log: bool = False,\n    **kwds,\n):\n    \"\"\"\\\n    Fraction of counts assigned to each gene over all cells.\n\n    Computes, for each gene, the fraction of counts assigned to that gene within\n    a cell. The `n_top` genes with the highest mean fraction over all cells are\n    plotted as boxplots.\n\n    This plot is similar to the `scater` package function `plotHighestExprs(type\n    = \"highest-expression\")`, see `here\n    <https://bioconductor.org/packages/devel/bioc/vignettes/scater/inst/doc/vignette-qc.html>`__. Quoting\n    from there:\n\n        *We expect to see the \u201cusual suspects\u201d, i.e., mitochondrial genes, actin,\n        ribosomal protein, MALAT1. A few spike-in transcripts may also be\n        present here, though if all of the spike-ins are in the top 50, it\n        suggests that too much spike-in RNA was added. A large number of\n        pseudo-genes or predicted genes may indicate problems with alignment.*\n        -- Davis McCarthy and Aaron Lun\n\n    Parameters\n    ----------\n    adata\n        Annotated data matrix.\n    n_top\n        Number of top\n    {show_save_ax}\n    gene_symbols\n        Key for field in .var that stores gene symbols if you do not want to use .var_names.\n    log\n        Plot x-axis in log scale\n    **kwds\n        Are passed to :func:`~seaborn.boxplot`.\n\n    Returns\n    -------\n    If `show==False` a :class:`~matplotlib.axes.Axes`.\n    \"\"\"\n    import seaborn as sns  # Slow import, only import if called\n    from scipy.sparse import issparse\n\n    # compute the percentage of each gene per cell\n    norm_dict = normalize_total(adata, target_sum=100, inplace=False)\n\n    # identify the genes with the highest mean\n    if issparse(norm_dict['X']):\n        mean_percent = norm_dict['X'].mean(axis=0).A1\n        top_idx = np.argsort(mean_percent)[::-1][:n_top]\n        counts_top_genes = norm_dict['X'][:, top_idx].A\n    else:\n        mean_percent = norm_dict['X'].mean(axis=0)\n        top_idx = np.argsort(mean_percent)[::-1][:n_top]\n        counts_top_genes = norm_dict['X'][:, top_idx]\n    columns = (\n        adata.var_names[top_idx]\n        if gene_symbols is None\n        else adata.var[gene_symbols][top_idx]\n    )\n    counts_top_genes = pd.DataFrame(\n        counts_top_genes, index=adata.obs_names, columns=columns\n    )\n\n    if not ax:\n        # figsize is hardcoded to produce a tall image. To change the fig size,\n        # a matplotlib.axes.Axes object needs to be passed.\n        height = (n_top * 0.2) + 1.5\n        fig, ax = plt.subplots(figsize=(5, height))\n    sns.boxplot(data=counts_top_genes, orient='h', ax=ax, fliersize=1, **kwds)\n    ax.set_xlabel('% of total counts')\n    if log:\n        ax.set_xscale('log')\n    _utils.savefig_or_show('highest_expr_genes', show=show, save=save)\n    if show is False:\n        return ax", "idx": 1589}
{"project": "Scanpy", "commit_id": "945_scanpy_1.4.4_docs_conf.py_scanpy_log_param_types.py", "target": 1, "func": "def scanpy_log_param_types(self, fields, field_role='param', type_role='type'):\n    for _name, _type, _desc in fields:\n        if not _type:\n            continue\n        set_item = r\"`'[a-z0-9_.-]+'`\"\n        if re.fullmatch(rf\"{{{set_item}(, {set_item})*}}\", _type):\n            continue\n        w_list = param_warnings.setdefault((self._name, self._obj), [])\n        if (_name, _type) not in w_list:\n            w_list.append((_name, _type))\n    return _format_docutils_params_orig(self, fields, field_role, type_role)", "idx": 1594}
{"project": "Scanpy", "commit_id": "932_scanpy_1.9.0___init__.py_decorator.py", "target": 0, "func": "def decorator(func):\n        @wraps(func)\n        def func_wrapper(*args, **kwargs):\n            warnings.simplefilter('always', DeprecationWarning)  # turn off filter\n            for old, new in arg_mapping.items():\n                if old in kwargs:\n                    warnings.warn(\n                        f\"Keyword argument '{old}' has been \"\n                        f\"deprecated in favour of '{new}'. \"\n                        f\"'{old}' will be removed in a future version.\",\n                        category=DeprecationWarning,\n                        stacklevel=2,\n                    )\n                    val = kwargs.pop(old)\n                    kwargs[new] = val\n            # reset filter\n            warnings.simplefilter('default', DeprecationWarning)\n            return func(*args, **kwargs)\n\n        return func_wrapper", "idx": 1602}
{"project": "Scanpy", "commit_id": "15_scanpy_0.0_scanpy_utils.py__interpret_as_floats.py", "target": 1, "func": "def _interpret_as_floats(data, header, first_column_names):\n    \"\"\"\n    Interpret as float array with optional colnames and rownames.\n    \"\"\"\n    # if the first element of the data list cannot be interpreted as float, the\n    # first row of the data is assumed to store variable names\n    if not is_float(data[0][0]):\n        sett.m(0,'--> assuming first line in file stores variable names')\n        # if the first row is one element shorter\n        colnames = np.array(data[0]).astype(str)\n        data = np.array(data[1:])\n    # try reading colnames from the last comment line\n    elif len(header) > 0 and type(header) == str:\n        sett.m(0,'--> assuming last comment line stores variable names')\n        potentialnames = header.split('\\n')[-2].strip('#').split()\n        colnames = np.array(potentialnames)\n        # skip the first column\n        colnames = colnames[1:]\n        data = np.array(data)\n    # just numbers as colnames\n    else:\n        sett.m(0,'--> did not find variable names in file')\n        data = np.array(data)\n        colnames = np.arange(data.shape[1]).astype(str)\n    # if the first element of the second row of the data cannot be interpreted\n    # as float, it is assumed to store sample names\n    if not is_float(data[1][0]) or first_column_names:\n        sett.m(0,'--> assuming first column stores sample names')\n        rownames = data[:,0].astype(str)\n        # skip the first column\n        X = data[:,1:].astype(float)\n        if colnames.size > X.shape[1]:\n            colnames = colnames[1:]\n    # just numbers as rownames\n    else:\n        sett.m(0,'--> did not find sample names in file')\n        X = data.astype(float)\n        rownames = np.arange(X.shape[0]).astype(str)\n    ddata = {\n        'X' : X, 'rownames' : rownames, 'colnames' : colnames\n    }\n    return ddata", "idx": 1605}
{"project": "Scanpy", "commit_id": "903_scanpy_1.4.4_scanpy_preprocessing__simple.py_normalize_per_cell.py", "target": 1, "func": "def normalize_per_cell(\n    data,\n    counts_per_cell_after=None,\n    counts_per_cell=None,\n    key_n_counts=None,\n    copy=False,\n    layers=[],\n    use_rep=None,\n    min_counts=1,\n) -> Optional[AnnData]:\n    \"\"\"Normalize total counts per cell.\n    .. warning::\n        .. deprecated:: 1.3.7\n            Use :func:`~scanpy.api.pp.normalize_total` instead.\n            The new function is equivalent to the present\n            function, except that\n            * the new function doesn't filter cells based on `min_counts`,\n              use :func:`~scanpy.api.pp.filter_cells` if filtering is needed.\n            * some arguments were renamed\n            * `copy` is replaced by `inplace`\n    Normalize each cell by total counts over all genes, so that every cell has\n    the same total count after normalization.\n    Similar functions are used, for example, by Seurat [Satija15]_, Cell Ranger\n    [Zheng17]_ or SPRING [Weinreb17]_.\n    Parameters\n    ----------\n    data : :class:`~anndata.AnnData`, `np.ndarray`, `sp.sparse`\n        The (annotated) data matrix of shape `n_obs` \u00d7 `n_vars`. Rows correspond\n        to cells and columns to genes.\n    counts_per_cell_after : `float` or `None`, optional (default: `None`)\n        If `None`, after normalization, each cell has a total count equal\n        to the median of the *counts_per_cell* before normalization.\n    counts_per_cell : `np.array`, optional (default: `None`)\n        Precomputed counts per cell.\n    key_n_counts : `str`, optional (default: `'n_counts'`)\n        Name of the field in `adata.obs` where the total counts per cell are\n        stored.\n    copy : `bool`, optional (default: `False`)\n        If an :class:`~anndata.AnnData` is passed, determines whether a copy\n        is returned.\n    min_counts : `int`, optional (default: 1)\n        Cells with counts less than `min_counts` are filtered out during\n        normalization.\n    Returns\n    -------\n    Returns or updates `adata` with normalized version of the original\n    `adata.X`, depending on `copy`.\n    Examples\n    --------\n    >>> adata = AnnData(\n    >>>     data=np.array([[1, 0], [3, 0], [5, 6]]))\n    >>> print(adata.X.sum(axis=1))\n    [  1.   3.  11.]\n    >>> sc.pp.normalize_per_cell(adata)\n    >>> print(adata.obs)\n    >>> print(adata.X.sum(axis=1))\n       n_counts\n    0       1.0\n    1       3.0\n    2      11.0\n    [ 3.  3.  3.]\n    >>> sc.pp.normalize_per_cell(adata, counts_per_cell_after=1,\n    >>>                          key_n_counts='n_counts2')\n    >>> print(adata.obs)\n    >>> print(adata.X.sum(axis=1))\n       n_counts  n_counts2\n    0       1.0        3.0\n    1       3.0        3.0\n    2      11.0        3.0\n    [ 1.  1.  1.]\n    \"\"\"\n    if key_n_counts is None: key_n_counts = 'n_counts'\n    if isinstance(data, AnnData):\n        start = logg.info('normalizing by total count per cell')\n        adata = data.copy() if copy else data\n        if counts_per_cell is None:\n            cell_subset, counts_per_cell = materialize_as_ndarray(\n                        filter_cells(adata.X, min_counts=min_counts))\n            adata.obs[key_n_counts] = counts_per_cell\n            adata._inplace_subset_obs(cell_subset)\n            counts_per_cell=counts_per_cell[cell_subset]\n        normalize_per_cell(adata.X, counts_per_cell_after, counts_per_cell)\n        layers = adata.layers.keys() if layers == 'all' else layers\n        if use_rep == 'after':\n            after = counts_per_cell_after\n        elif use_rep == 'X':\n            after = np.median(counts_per_cell[cell_subset])\n        elif use_rep is None:\n            after = None\n        else: raise ValueError('use_rep should be \"after\", \"X\" or None')\n        for layer in layers:\n            subset, counts = filter_cells(adata.layers[layer],\n                    min_counts=min_counts)\n            temp = normalize_per_cell(adata.layers[layer], after, counts, copy=True)\n            adata.layers[layer] = temp\n        logg.info(\n            '    finished ({time_passed}): normalized adata.X and added'\n            f'    {key_n_counts!r}, counts per cell before normalization (adata.obs)',\n            time=start,\n        )\n        return adata if copy else None\n    # proceed with data matrix\n    X = data.copy() if copy else data\n    if counts_per_cell is None:\n        if copy == False:\n            raise ValueError('Can only be run with copy=True')\n        cell_subset, counts_per_cell = filter_cells(X, min_counts=min_counts)\n        X = X[cell_subset]\n        counts_per_cell = counts_per_cell[cell_subset]\n    if counts_per_cell_after is None:\n        counts_per_cell_after = np.median(counts_per_cell)\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n        counts_per_cell += counts_per_cell == 0\n        counts_per_cell /= counts_per_cell_after\n        if not issparse(X): X /= materialize_as_ndarray(counts_per_cell[:, np.newaxis])\n        else: sparsefuncs.inplace_row_scale(X, 1/counts_per_cell)\n    return X if copy else None", "idx": 1607}
{"project": "Scanpy", "commit_id": "588_scanpy_1.0.4_scanpy_preprocessing_simple.py_filter_genes_dispersion.py", "target": 1, "func": "def filter_genes_dispersion(data,\n                            flavor='seurat',\n                            min_disp=None, max_disp=None,\n                            min_mean=None, max_mean=None,\n                            n_bins=20,\n                            n_top_genes=None,\n                            log=True,\n                            copy=False):\n    \"\"\"Extract highly variable genes [Satija15]_ [Zheng17]_.\n    If trying out parameters, pass the data matrix instead of AnnData.\n    Depending on `flavor`, this reproduces the R-implementations of Seurat\n    [Satija15]_ and Cell Ranger [Zheng17]_.\n    The normalized dispersion is obtained by scaling with the mean and standard\n    deviation of the dispersions for genes falling into a given bin for mean\n    expression of genes. This means that for each bin of mean expression, highly\n    variable genes are selected.\n    Use `flavor='cell_ranger'` with care and in the same way as in\n    :func:`~scanpy.api.pp.recipe_zheng17`.\n    Parameters\n    ----------\n    data : :class:`~scanpy.api.AnnData`, `np.ndarray`, `sp.sparse`\n        The (annotated) data matrix of shape `n_obs` \u00d7 `n_vars`. Rows correspond\n        to cells and columns to genes.\n    flavor : {'seurat', 'cell_ranger'}, optional (default: 'seurat')\n        Choose the flavor for computing normalized dispersion. If choosing\n        'seurat', this expects non-logarithmized data - the logarithm of mean\n        and dispersion is taken internally when `log` is at its default value\n        `True`. For 'cell_ranger', this is usually called for logarithmized data\n        - in this case you should set `log` to `False`. In their default\n        workflows, Seurat passes the cutoffs whereas Cell Ranger passes\n        `n_top_genes`.\n    min_mean=0.0125, max_mean=3, min_disp=0.5, max_disp=`None` : `float`, optional\n        If `n_top_genes` unequals `None`, these cutoffs for the means and the\n        normalized dispersions are ignored.\n    n_bins : `int` (default: 20)\n        Number of bins for binning the mean gene expression. Normalization is\n        done with respect to each bin. If just a single gene falls into a bin,\n        the normalized dispersion is artificially set to 1. You'll be informed\n        about this if you set `settings.verbosity = 4`.\n    n_top_genes : `int` or `None` (default: `None`)\n        Number of highly-variable genes to keep.\n    log : `bool`, optional (default: `True`)\n        Use the logarithm of the mean to variance ratio.\n    copy : `bool`, optional (default: `False`)\n        If an :class:`~scanpy.api.AnnData` is passed, determines whether a copy\n        is returned.\n    Returns\n    -------\n    If an AnnData `adata` is passed, returns or updates `adata` depending on \\\n    `copy`. It filters the `adata` and adds the annotations\n    means : adata.var\n        Means per gene. Logarithmized when `log` is `True`.\n    dispersions : adata.var\n        Dispersions per gene. Logarithmized when `log` is `True`.\n    dispersions_norm : adata.var\n        Normalized dispersions per gene. Logarithmized when `log` is `True`.\n    If a data matrix `X` is passed, the annotation is returned as `np.recarray` \\\n    with the same information stored in fields: `gene_subset`, `means`, `dispersions`, `dispersion_norm`.\n    \"\"\"\n    if n_top_genes is not None and not all([\n            min_disp is None, max_disp is None, min_mean is None, max_mean is None]):\n        logg.warn('If you pass `n_top_genes`, all cutoffs are ignored.')\n    if min_disp is None: min_disp = 0.5\n    if min_mean is None: min_mean = 0.0125\n    if max_mean is None: max_mean = 3\n    if isinstance(data, AnnData):\n        adata = data.copy() if copy else data\n        result = filter_genes_dispersion(adata.X, log=log,\n                                         min_disp=min_disp, max_disp=max_disp,\n                                         min_mean=min_mean, max_mean=max_mean,\n                                         n_top_genes=n_top_genes,\n                                         flavor=flavor)\n        adata.var['means'] = result['means']\n        adata.var['dispersions'] = result['dispersions']\n        adata.var['dispersions_norm'] = result['dispersions_norm']\n        adata._inplace_subset_var(result['gene_subset'])\n        return adata if copy else None\n    logg.msg('extracting highly variable genes',\n              r=True, v=4)\n    X = data  # no copy necessary, X remains unchanged in the following\n    mean, var = _get_mean_var(X)\n    # now actually compute the dispersion\n    mean[mean == 0] = 1e-12  # set entries equal to zero to small value\n    dispersion = var / mean\n    if log:  # logarithmized mean as in Seurat\n        dispersion[dispersion == 0] = np.nan\n        dispersion = np.log(dispersion)\n        mean = np.log1p(mean)\n    # all of the following quantities are \"per-gene\" here\n    import pandas as pd\n    df = pd.DataFrame()\n    df['mean'] = mean\n    df['dispersion'] = dispersion\n    if flavor == 'seurat':\n        df['mean_bin'] = pd.cut(df['mean'], bins=n_bins)\n        disp_grouped = df.groupby('mean_bin')['dispersion']\n        disp_mean_bin = disp_grouped.mean()\n        disp_std_bin = disp_grouped.std(ddof=1)\n        # retrieve those genes that have nan std, these are the ones where\n        # only a single gene fell in the bin and implicitly set them to have\n        # a normalized disperion of 1\n        one_gene_per_bin = disp_std_bin.isnull()\n        gen_indices = np.where(one_gene_per_bin[df['mean_bin']])[0].tolist()\n        if len(gen_indices) > 0:\n            logg.msg(\n                'Gene indices {} fell into a single bin: their '\n                'normalized dispersion was set to 1.\\n    '\n                'Decreasing `n_bins` will likely avoid this effect.'\n                .format(gen_indices), v=4)\n        disp_std_bin[one_gene_per_bin] = disp_mean_bin[one_gene_per_bin]\n        disp_mean_bin[one_gene_per_bin] = 0\n        # actually do the normalization\n        df['dispersion_norm'] = (df['dispersion'].values  # use values here as index differs\n                                 - disp_mean_bin[df['mean_bin']].values) \\\n                                 / disp_std_bin[df['mean_bin']].values\n    elif flavor == 'cell_ranger':\n        from statsmodels import robust\n        df['mean_bin'] = pd.cut(df['mean'], np.r_[-np.inf,\n            np.percentile(df['mean'], np.arange(10, 105, 5)), np.inf])\n        disp_grouped = df.groupby('mean_bin')['dispersion']\n        disp_median_bin = disp_grouped.median()\n        # the next line raises the warning: \"Mean of empty slice\"\n        with warnings.catch_warnings():\n            warnings.simplefilter('ignore')\n            disp_mad_bin = disp_grouped.apply(robust.mad)\n        df['dispersion_norm'] = np.abs((df['dispersion'].values\n                                 - disp_median_bin[df['mean_bin']].values)) \\\n                                / disp_mad_bin[df['mean_bin']].values\n    else:\n        raise ValueError('`flavor` needs to be \"seurat\" or \"cell_ranger\"')\n    dispersion_norm = df['dispersion_norm'].values.astype('float32')\n    if n_top_genes is not None:\n        dispersion_norm[::-1].sort()  # interestingly, np.argpartition is slightly slower\n        disp_cut_off = dispersion_norm[n_top_genes-1]\n        gene_subset = df['dispersion_norm'].values >= disp_cut_off\n        logg.msg('the {} top genes correspond to a normalized dispersion cutoff of'\n                 .format(n_top_genes, disp_cut_off), v=5)\n    else:\n        max_disp = np.inf if max_disp is None else max_disp\n        dispersion_norm[np.isnan(dispersion_norm)] = 0  # similar to Seurat\n        gene_subset = np.logical_and.reduce((mean > min_mean, mean < max_mean,\n                                             dispersion_norm > min_disp,\n                                             dispersion_norm < max_disp))\n    logg.msg('    finished', time=True, v=4)\n    return np.rec.fromarrays((gene_subset,\n                              df['mean'].values,\n                              df['dispersion'].values,\n                              df['dispersion_norm'].values.astype('float32', copy=False)),\n                              dtype=[('gene_subset', bool),\n                                     ('means', 'float32'),\n                                     ('dispersions', 'float32'),\n                                     ('dispersions_norm', 'float32')])", "idx": 1609}
{"project": "Scanpy", "commit_id": "137_scanpy_0.0_scanpy_examples___init__.py__check_adata.py", "target": 1, "func": "def _check_adata(adata):\n    \"\"\"\n    Do sanity checks on adata object.\n    Checks whether adata contains annotation.\n    \"\"\"\n    import numpy as np\n    import sys\n    if 'tools' not in adata:\n        adata['tools'] = np.array([], dtype=str)\n    if len(adata.smp_keys()) == 0:\n        sett.m(0, _howto_specify_subgroups)\n    else:\n        for smp in adata.smp_keys():\n            # ordered unique categories\n            if not smp + '_names' in adata:\n                adata[smp + '_names'] = np.unique(adata.smp[smp])\n                try:\n                    from natsort import natsorted\n                    adata[smp + '_names'] = np.array(natsorted(adata[smp + '_names']))\n                except:\n                    pass\n                if adata[smp + '_names'].dtype.char == 'U':\n                    adata[smp + '_names'] = np.setdiff1d(adata[smp + '_names'],\n                                                       np.array(_ignore_groups))\n            # output\n            ann_info = adata[smp + '_names']\n            if len(adata[smp + '_names']) > 20:\n                ann_info = (str(adata[smp + '_names'][0:3]).replace(']','')\n                            + ' ...' + str(adata[smp + '_names'][-2:]).replace('[',''))\n            sett.m(0,'sample annotation','\"' + smp + '\"', 'with',  ann_info)\n    return adata", "idx": 1616}
{"project": "Scanpy", "commit_id": "920_scanpy_1.9.0___init__.py_moving_average.py", "target": 0, "func": "def moving_average(a: np.ndarray, n: int):\n    \"\"\"Moving average over one-dimensional array.\n\n    Parameters\n    ----------\n    a\n        One-dimensional array.\n    n\n        Number of entries to average over. n=2 means averaging over the currrent\n        the previous entry.\n\n    Returns\n    -------\n    An array view storing the moving average.\n    \"\"\"\n    ret = np.cumsum(a, dtype=float)\n    ret[n:] = ret[n:] - ret[:-n]\n    return ret[n - 1 :] / n", "idx": 1627}
{"project": "Scanpy", "commit_id": "356_scanpy_1.9.0_scatterplots.py__components_to_dimensions.py", "target": 0, "func": "def _components_to_dimensions(\n    components: Optional[Union[str, Collection[str]]],\n    dimensions: Optional[Union[Collection[int], Collection[Collection[int]]]],\n    *,\n    projection: Literal[\"2d\", \"3d\"] = \"2d\",\n    total_dims: int,\n) -> List[Collection[int]]:\n    \"\"\"Normalize components/ dimensions args for embedding plots.\"\"\"\n    # TODO: Deprecate components kwarg\n    ndims = {\"2d\": 2, \"3d\": 3}[projection]\n    if components is None and dimensions is None:\n        dimensions = [tuple(i for i in range(ndims))]\n    elif components is not None and dimensions is not None:\n        raise ValueError(\"Cannot provide both dimensions and components\")\n\n    # TODO: Consider deprecating this\n    # If components is not None, parse them and set dimensions\n    if components == \"all\":\n        dimensions = list(combinations(range(total_dims), ndims))\n    elif components is not None:\n        if isinstance(components, str):\n            components = [components]\n        # Components use 1 based indexing\n        dimensions = [[int(dim) - 1 for dim in c.split(\",\")] for c in components]\n\n    if all(isinstance(el, Integral) for el in dimensions):\n        dimensions = [dimensions]\n    # if all(isinstance(el, Collection) for el in dimensions):\n    for dims in dimensions:\n        if len(dims) != ndims or not all(isinstance(d, Integral) for d in dims):\n            raise ValueError()\n\n    return dimensions", "idx": 1644}
{"project": "Scanpy", "commit_id": "967_scanpy_1.4.5_scanpy_tools__leiden.py_leiden.py", "target": 1, "func": "def leiden(\n    adata: AnnData,\n    resolution: float = 1,\n    *,\n    restrict_to: Optional[Tuple[str, Sequence[str]]] = None,\n    random_state: Optional[Union[int, RandomState]] = 0,\n    key_added: str = 'leiden',\n    adjacency: Optional[sparse.spmatrix] = None,\n    directed: bool = True,\n    use_weights: bool = True,\n    n_iterations: int = -1,\n    partition_type: Optional[Type[MutableVertexPartition]] = None,\n    copy: bool = False,\n    **partition_kwargs,\n) -> Optional[AnnData]:\n    \"\"\"\\\n    Cluster cells into subgroups [Traag18]_.\n    Cluster cells using the Leiden algorithm [Traag18]_,\n    an improved version of the Louvain algorithm [Blondel08]_.\n    It has been proposed for single-cell analysis by [Levine15]_.\n    This requires having ran :func:`~scanpy.pp.neighbors` or\n    :func:`~scanpy.external.pp.bbknn` first.\n    Parameters\n    ----------\n    adata\n        The annotated data matrix.\n    resolution\n        A parameter value controlling the coarseness of the clustering.\n        Higher values lead to more clusters.\n        Set to `None` if overriding `partition_type`\n        to one that doesn\u2019t accept a `resolution_parameter`.\n    random_state\n        Change the initialization of the optimization.\n    restrict_to\n        Restrict the clustering to the categories within the key for sample\n        annotation, tuple needs to contain `(obs_key, list_of_categories)`.\n    key_added\n        `adata.obs` key under which to add the cluster labels.\n    adjacency\n        Sparse adjacency matrix of the graph, defaults to\n        `adata.uns['neighbors']['connectivities']`.\n    directed\n        Whether to treat the graph as directed or undirected.\n    use_weights\n        If `True`, edge weights from the graph are used in the computation\n        (placing more emphasis on stronger edges).\n    n_iterations\n        How many iterations of the Leiden clustering algorithm to perform.\n        Positive values above 2 define the total number of iterations to perform,\n        -1 has the algorithm run until it reaches its optimal clustering.\n    partition_type\n        Type of partition to use.\n        Defaults to :class:`~leidenalg.RBConfigurationVertexPartition`.\n        For the available options, consult the documentation for\n        :func:`~leidenalg.find_partition`.\n    copy\n        Whether to copy `adata` or modify it inplace.\n    **partition_kwargs\n        Any further arguments to pass to `~leidenalg.find_partition`\n        (which in turn passes arguments to the `partition_type`).\n    Returns\n    -------\n    `adata.obs[key_added]`\n        Array of dim (number of samples) that stores the subgroup id\n        (`'0'`, `'1'`, ...) for each cell.\n    `adata.uns['leiden']['params']`\n        A dict with the values for the parameters `resolution`, `random_state`,\n        and `n_iterations`.\n    \"\"\"\n    try:\n        import leidenalg\n    except ImportError:\n        raise ImportError(\n            'Please install the leiden algorithm: `conda install -c conda-forge leidenalg` or `pip3 install leidenalg`.'\n        )\n    partition_kwargs = dict(partition_kwargs)\n    start = logg.info('running Leiden clustering')\n    adata = adata.copy() if copy else adata\n    # are we clustering a user-provided graph or the default AnnData one?\n    if adjacency is None:\n        if 'neighbors' not in adata.uns:\n            raise ValueError(\n                'You need to run `pp.neighbors` first '\n                'to compute a neighborhood graph.'\n            )\n        adjacency = adata.uns['neighbors']['connectivities']\n    if restrict_to is not None:\n        restrict_key, restrict_categories = restrict_to\n        adjacency, restrict_indices = restrict_adjacency(\n            adata,\n            restrict_key,\n            restrict_categories,\n            adjacency,\n        )\n    # convert it to igraph\n    g = _utils.get_igraph_from_adjacency(adjacency, directed=directed)\n    # flip to the default partition type if not overriden by the user\n    if partition_type is None:\n        partition_type = leidenalg.RBConfigurationVertexPartition\n    # Prepare find_partition arguments as a dictionary,\n    # appending to whatever the user provided. It needs to be this way\n    # as this allows for the accounting of a None resolution\n    # (in the case of a partition variant that doesn't take it on input)\n    if use_weights:\n        partition_kwargs['weights'] = np.array(g.es['weight']).astype(np.float64)\n    partition_kwargs['n_iterations'] = n_iterations\n    partition_kwargs['seed'] = random_state\n    if resolution is not None:\n        partition_kwargs['resolution_parameter'] = resolution\n    # clustering proper\n    part = leidenalg.find_partition(g, partition_type, **partition_kwargs)\n    # store output into adata.obs\n    groups = np.array(part.membership)\n    if restrict_to is not None:\n        if key_added == 'louvain':\n            key_added += '_R'\n        groups = rename_groups(\n            adata,\n            key_added,\n            restrict_key,\n            restrict_categories,\n            restrict_indices,\n            groups,\n        )\n    adata.obs[key_added] = pd.Categorical(\n        values=groups.astype('U'),\n        categories=natsorted(np.unique(groups).astype('U')),\n    )\n    # store information on the clustering parameters\n    adata.uns['leiden'] = {}\n    adata.uns['leiden']['params'] = dict(\n        resolution=resolution,\n        random_state=random_state,\n        n_iterations=n_iterations,\n    )\n    logg.info(\n        '    finished',\n        time=start,\n        deep=(\n            f'found {len(np.unique(groups))} clusters and added\\n'\n            f'    {key_added!r}, the cluster labels (adata.obs, categorical)'\n        ),\n    )\n    return adata if copy else None", "idx": 1645}
{"project": "Scanpy", "commit_id": "387_scanpy_0.2.8_docs_conf.py_modurl.py", "target": 1, "func": "def modurl(qualname):\n    \"\"\"Get the full GitHub URL for some object\u2019s qualname\"\"\"\n    obj, module = get_obj_module(qualname)\n    path = Path(module.__file__).relative_to(project_dir)\n    fragment = '#L{}-L{}'.format(*get_linenos(obj)) if obj else ''\n    return '{}/{}{}'.format(github_url, path, fragment)", "idx": 1652}
{"project": "Scanpy", "commit_id": "623_scanpy_1.9.0_test_pca.py_test_pca_reproducible.py", "target": 0, "func": "def test_pca_reproducible(pbmc3k_normalized, array_type, float_dtype):\n    pbmc = pbmc3k_normalized\n    pbmc.X = array_type(pbmc.X)\n\n    a = sc.pp.pca(pbmc, copy=True, dtype=float_dtype, random_state=42)\n    b = sc.pp.pca(pbmc, copy=True, dtype=float_dtype, random_state=42)\n    c = sc.pp.pca(pbmc, copy=True, dtype=float_dtype, random_state=0)\n\n    assert_equal(a, b)\n    # Test that changing random seed changes result\n    assert not np.array_equal(a.obsm[\"X_pca\"], c.obsm[\"X_pca\"])", "idx": 1654}
{"project": "Scanpy", "commit_id": "379_scanpy_0.2.6_scanpy_utils.py_get_graph_tool_from_adjacency.py", "target": 1, "func": "def get_graph_tool_from_adjacency(adjacency, directed=None):\n    \"\"\"Get graph_tool graph from adjacency matrix.\"\"\"\n    import graph_tool as gt\n    g = gt.Graph(directed=directed)\n    g.add_vertex(adjacency.shape[0])\n    g.add_edge_list(np.transpose(adjacency.nonzero()))\n    return g", "idx": 1657}
{"project": "Scanpy", "commit_id": "1090_scanpy_1.8.1_scanpy_plotting__tools___init__.py__rank_genes_groups_plot.py", "target": 1, "func": "def _rank_genes_groups_plot(\n    adata: AnnData,\n    plot_type: str = 'heatmap',\n    groups: Union[str, Sequence[str]] = None,\n    n_genes: Optional[int] = None,\n    groupby: Optional[str] = None,\n    values_to_plot: Optional[str] = None,\n    var_names: Optional[Union[Sequence[str], Mapping[str, Sequence[str]]]] = None,\n    min_logfoldchange: Optional[float] = None,\n    key: Optional[str] = None,\n    show: Optional[bool] = None,\n    save: Optional[bool] = None,\n    return_fig: Optional[bool] = False,\n    gene_symbols: Optional[str] = None,\n    **kwds,\n):\n    \"\"\"\\\n    Common function to call the different rank_genes_groups_* plots\n    \"\"\"\n    if var_names is not None and n_genes is not None:\n        raise ValueError(\n            \"The arguments n_genes and var_names are mutually exclusive. Please \"\n            \"select only one.\"\n        )\n    if var_names is None and n_genes is None:\n        # set n_genes = 10 as default when none of the options is given\n        n_genes = 10\n    if key is None:\n        key = 'rank_genes_groups'\n    if groupby is None:\n        groupby = str(adata.uns[key]['params']['groupby'])\n    group_names = adata.uns[key]['names'].dtype.names if groups is None else groups\n    if var_names is not None:\n        if isinstance(var_names, Mapping):\n            # get a single list of all gene names in the dictionary\n            var_names_list = sum([list(x) for x in var_names.values()], [])\n        elif isinstance(var_names, str):\n            var_names_list = [var_names]\n        else:\n            var_names_list = var_names\n    else:\n        # dict in which each group is the key and the n_genes are the values\n        var_names = {}\n        var_names_list = []\n        for group in group_names:\n            df = rank_genes_groups_df(\n                adata,\n                group,\n                key=key,\n                gene_symbols=gene_symbols,\n                log2fc_min=min_logfoldchange,\n            )\n            if gene_symbols is not None:\n                df['names'] = df[gene_symbols]\n\n            genes_list = df.names.tolist()\n\n            if len(genes_list) == 0:\n                logg.warning(f'No genes found for group {group}')\n                continue\n            if n_genes < 0:\n                genes_list = genes_list[n_genes:]\n            else:\n                genes_list = genes_list[:n_genes]\n            var_names[group] = genes_list\n            var_names_list.extend(genes_list)\n    # by default add dendrogram to plots\n    kwds.setdefault('dendrogram', True)\n    if plot_type in ['dotplot', 'matrixplot']:\n        # these two types of plots can also\n        # show score, logfoldchange and pvalues, in general any value from rank\n        # genes groups\n        title = None\n        values_df = None\n        if values_to_plot is not None:\n            values_df = _get_values_to_plot(\n                adata,\n                values_to_plot,\n                var_names_list,\n                key=key,\n                gene_symbols=gene_symbols,\n            )\n            title = values_to_plot\n            if values_to_plot == 'logfoldchanges':\n                title = 'log fold change'\n            else:\n                title = values_to_plot.replace(\"_\", \" \").replace('pvals', 'p-value')\n        if plot_type == 'dotplot':\n            from .._dotplot import dotplot\n            _pl = dotplot(\n                adata,\n                var_names,\n                groupby,\n                dot_color_df=values_df,\n                return_fig=True,\n                gene_symbols=gene_symbols,\n                **kwds,\n            )\n            if title is not None and 'colorbar_title' not in kwds:\n                _pl.legend(colorbar_title=title)\n        elif plot_type == 'matrixplot':\n            from .._matrixplot import matrixplot\n            _pl = matrixplot(\n                adata,\n                var_names,\n                groupby,\n                values_df=values_df,\n                return_fig=True,\n                gene_symbols=gene_symbols,\n                **kwds,\n            )\n            if title is not None and 'colorbar_title' not in kwds:\n                _pl.legend(title=title)\n        return _fig_show_save_or_axes(_pl, return_fig, show, save)\n    elif plot_type == 'stacked_violin':\n        from .._stacked_violin import stacked_violin\n        _pl = stacked_violin(\n            adata,\n            var_names,\n            groupby,\n            return_fig=True,\n            gene_symbols=gene_symbols,\n            **kwds,\n        )\n        return _fig_show_save_or_axes(_pl, return_fig, show, save)\n    elif plot_type == 'heatmap':\n        from .._anndata import heatmap\n        return heatmap(\n            adata,\n            var_names,\n            groupby,\n            show=show,\n            save=save,\n            gene_symbols=gene_symbols,\n            **kwds,\n        )\n    elif plot_type == 'tracksplot':\n        from .._anndata import tracksplot\n        return tracksplot(\n            adata,\n            var_names,\n            groupby,\n            show=show,\n            save=save,\n            gene_symbols=gene_symbols,\n            **kwds,\n        )", "idx": 1660}
{"project": "Scanpy", "commit_id": "904_scanpy_1.4.4_scanpy_tools__umap.py_umap.py", "target": 1, "func": "def umap(\n    adata,\n    min_dist=0.5,\n    spread=1.0,\n    n_components=2,\n    maxiter=None,\n    alpha=1.0,\n    gamma=1.0,\n    negative_sample_rate=5,\n    init_pos='spectral',\n    random_state=0,\n    a=None,\n    b=None,\n    copy=False,\n):\n    \"\"\"Embed the neighborhood graph using UMAP [McInnes18]_.\n    UMAP (Uniform Manifold Approximation and Projection) is a manifold learning\n    technique suitable for visualizing high-dimensional data. Besides tending to\n    be faster than tSNE, it optimizes the embedding such that it best reflects\n    the topology of the data, which we represent throughout Scanpy using a\n    neighborhood graph. tSNE, by contrast, optimizes the distribution of\n    nearest-neighbor distances in the embedding such that these best match the\n    distribution of distances in the high-dimensional space.  We use the\n    implementation of `umap-learn <https://github.com/lmcinnes/umap>`__\n    [McInnes18]_. For a few comparisons of UMAP with tSNE, see this `preprint\n    <https://doi.org/10.1101/298430>`__.\n    Parameters\n    ----------\n    adata : :class:`~anndata.AnnData`\n        Annotated data matrix.\n    min_dist : `float`, optional (default: 0.5)\n        The effective minimum distance between embedded points. Smaller values\n        will result in a more clustered/clumped embedding where nearby points on\n        the manifold are drawn closer together, while larger values will result\n        on a more even dispersal of points. The value should be set relative to\n        the ``spread`` value, which determines the scale at which embedded\n        points will be spread out. The default of in the `umap-learn` package is\n        0.1.\n    spread : `float` (optional, default 1.0)\n        The effective scale of embedded points. In combination with `min_dist`\n        this determines how clustered/clumped the embedded points are.\n    n_components : `int`, optional (default: 2)\n        The number of dimensions of the embedding.\n    maxiter : `int`, optional (default: `None`)\n        The number of iterations (epochs) of the optimization. Called `n_epochs`\n        in the original UMAP.\n    alpha : `float`, optional (default: 1.0)\n        The initial learning rate for the embedding optimization.\n    gamma : `float` (optional, default 1.0)\n        Weighting applied to negative samples in low dimensional embedding\n        optimization. Values higher than one will result in greater weight\n        being given to negative samples.\n    negative_sample_rate : `int` (optional, default 5)\n        The number of negative edge/1-simplex samples to use per positive\n        edge/1-simplex sample in optimizing the low dimensional embedding.\n    init_pos : `string` or `np.array`, optional (default: 'spectral')\n        How to initialize the low dimensional embedding. Called `init` in the\n        original UMAP.\n        Options are:\n        * Any key for `adata.obsm`.\n        * 'paga': positions from :func:`~scanpy.api.pl.paga`.\n        * 'spectral': use a spectral embedding of the graph.\n        * 'random': assign initial embedding positions at random.\n        * A numpy array of initial embedding positions.\n    random_state : `int`, `RandomState` or `None`, optional (default: 0)\n        If `int`, `random_state` is the seed used by the random number generator;\n        If `RandomState`, `random_state` is the random number generator;\n        If `None`, the random number generator is the `RandomState` instance used\n        by `np.random`.\n    a : `float` (optional, default `None`)\n        More specific parameters controlling the embedding. If `None` these\n        values are set automatically as determined by `min_dist` and\n        `spread`.\n    b : `float` (optional, default `None`)\n        More specific parameters controlling the embedding. If `None` these\n        values are set automatically as determined by `min_dist` and\n        `spread`.\n    copy : `bool` (default: `False`)\n        Return a copy instead of writing to adata.\n    Returns\n    -------\n    Depending on `copy`, returns or updates `adata` with the following fields.\n    **X_umap** : `adata.obsm` field\n        UMAP coordinates of data.\n    \"\"\"\n    adata = adata.copy() if copy else adata\n    if 'neighbors' not in adata.uns:\n        raise ValueError(\n            'Did not find \\'neighbors/connectivities\\'. Run `sc.pp.neighbors` first.')\n    start = logg.info('computing UMAP')\n    if ('params' not in adata.uns['neighbors']\n        or adata.uns['neighbors']['params']['method'] != 'umap'):\n        logg.warning('neighbors/connectivities have not been computed using umap')\n    from umap.umap_ import find_ab_params, simplicial_set_embedding\n    if a is None or b is None:\n        a, b = find_ab_params(spread, min_dist)\n    else:\n        a = a\n        b = b\n    if isinstance(init_pos, str) and init_pos in adata.obsm.keys():\n        init_coords = adata.obsm[init_pos]\n    elif isinstance(init_pos, str) and init_pos == 'paga':\n        init_coords = get_init_pos_from_paga(adata, random_state=random_state)\n    else:\n        init_coords = init_pos  # Let umap handle it\n    if hasattr(init_coords, \"dtype\") and is_numeric_dtype(init_pos):\n        init_coords = check_array(init_coords, dtype=np.float32, accept_sparse=False)\n\n    random_state = check_random_state(random_state)\n    n_epochs = 0 if maxiter is None else maxiter\n    neigh_params = adata.uns['neighbors']['params']\n    X = choose_representation(\n        adata, neigh_params.get('use_rep', None), neigh_params.get('n_pcs', None), silent=True)\n    # the data matrix X is really only used for determining the number of connected components\n    # for the init condition in the UMAP embedding\n    X_umap = simplicial_set_embedding(\n        X,\n        adata.uns['neighbors']['connectivities'].tocoo(),\n        n_components,\n        alpha,\n        a,\n        b,\n        gamma,\n        negative_sample_rate,\n        n_epochs,\n        init_coords,\n        random_state,\n        neigh_params.get('metric', 'euclidean'),\n        neigh_params.get('metric_kwds', {}),\n        verbose=settings.verbosity > 3,\n    )\n    adata.obsm['X_umap'] = X_umap  # annotate samples with UMAP coordinates\n    logg.info(\n        '    finished',\n        time=start,\n        deep=(\n            'added\\n'\n            \"    'X_umap', UMAP coordinates (adata.obsm)\"\n        ),\n    )\n    return adata if copy else None", "idx": 1665}
{"project": "Scanpy", "commit_id": "500_scanpy_1.9.0_test_dendrogram_key_added.py_test_dendrogram_key_added.py", "target": 0, "func": "def test_dendrogram_key_added(adata, groupby, key_added):\n    sc.tl.dendrogram(adata, groupby=groupby, key_added=key_added, use_rep=\"X_pca\")\n    if isinstance(groupby, list):\n        dendrogram_key = f'dendrogram_{\"_\".join(groupby)}'\n    else:\n        dendrogram_key = f'dendrogram_{groupby}'\n\n    if key_added is None:\n        key_added = dendrogram_key\n    assert key_added in adata.uns", "idx": 1682}
{"project": "Scanpy", "commit_id": "180_scanpy_0.0_scanpy_classes_ann_data.py_copy_.py", "target": 1, "func": "def copy(self):\n    new = super(BoundStructArray, self).copy()\n    new._is_attr_of = self._is_attr_of\n    new._keys_multicol = self._keys_multicol\n    return new", "idx": 1685}
{"project": "Scanpy", "commit_id": "855_scanpy_1.9.0__rank_genes_groups.py_compute_statistics.py", "target": 0, "func": "def compute_statistics(\n        self,\n        method,\n        corr_method='benjamini-hochberg',\n        n_genes_user=None,\n        rankby_abs=False,\n        tie_correct=False,\n        **kwds,\n    ):\n\n        if method in {'t-test', 't-test_overestim_var'}:\n            generate_test_results = self.t_test(method)\n        elif method == 'wilcoxon':\n            generate_test_results = self.wilcoxon(tie_correct)\n        elif method == 'logreg':\n            generate_test_results = self.logreg(**kwds)\n\n        self.stats = None\n\n        n_genes = self.X.shape[1]\n\n        for group_index, scores, pvals in generate_test_results:\n            group_name = str(self.groups_order[group_index])\n\n            if n_genes_user is not None:\n                scores_sort = np.abs(scores) if rankby_abs else scores\n                global_indices = _select_top_n(scores_sort, n_genes_user)\n                first_col = 'names'\n            else:\n                global_indices = slice(None)\n                first_col = 'scores'\n\n            if self.stats is None:\n                idx = pd.MultiIndex.from_tuples([(group_name, first_col)])\n                self.stats = pd.DataFrame(columns=idx)\n\n            if n_genes_user is not None:\n                self.stats[group_name, 'names'] = self.var_names[global_indices]\n\n            self.stats[group_name, 'scores'] = scores[global_indices]\n\n            if pvals is not None:\n                self.stats[group_name, 'pvals'] = pvals[global_indices]\n                if corr_method == 'benjamini-hochberg':\n                    from statsmodels.stats.multitest import multipletests\n\n                    pvals[np.isnan(pvals)] = 1\n                    _, pvals_adj, _, _ = multipletests(\n                        pvals, alpha=0.05, method='fdr_bh'\n                    )\n                elif corr_method == 'bonferroni':\n                    pvals_adj = np.minimum(pvals * n_genes, 1.0)\n                self.stats[group_name, 'pvals_adj'] = pvals_adj[global_indices]\n\n            if self.means is not None:\n                mean_group = self.means[group_index]\n                if self.ireference is None:\n                    mean_rest = self.means_rest[group_index]\n                else:\n                    mean_rest = self.means[self.ireference]\n                foldchanges = (self.expm1_func(mean_group) + 1e-9) / (\n                    self.expm1_func(mean_rest) + 1e-9\n                )  # add small value to remove 0's\n                self.stats[group_name, 'logfoldchanges'] = np.log2(\n                    foldchanges[global_indices]\n                )\n\n        if n_genes_user is None:\n            self.stats.index = self.var_names", "idx": 1694}
{"project": "Scanpy", "commit_id": "286_scanpy_1.9.0__matrixplot.py_matrixplot.py", "target": 0, "func": "def matrixplot(\n    adata: AnnData,\n    var_names: Union[_VarNames, Mapping[str, _VarNames]],\n    groupby: Union[str, Sequence[str]],\n    use_raw: Optional[bool] = None,\n    log: bool = False,\n    num_categories: int = 7,\n    figsize: Optional[Tuple[float, float]] = None,\n    dendrogram: Union[bool, str] = False,\n    title: Optional[str] = None,\n    cmap: Optional[str] = MatrixPlot.DEFAULT_COLORMAP,\n    colorbar_title: Optional[str] = MatrixPlot.DEFAULT_COLOR_LEGEND_TITLE,\n    gene_symbols: Optional[str] = None,\n    var_group_positions: Optional[Sequence[Tuple[int, int]]] = None,\n    var_group_labels: Optional[Sequence[str]] = None,\n    var_group_rotation: Optional[float] = None,\n    layer: Optional[str] = None,\n    standard_scale: Literal['var', 'group'] = None,\n    values_df: Optional[pd.DataFrame] = None,\n    swap_axes: bool = False,\n    show: Optional[bool] = None,\n    save: Union[str, bool, None] = None,\n    ax: Optional[_AxesSubplot] = None,\n    return_fig: Optional[bool] = False,\n    vmin: Optional[float] = None,\n    vmax: Optional[float] = None,\n    vcenter: Optional[float] = None,\n    norm: Optional[Normalize] = None,\n    **kwds,\n) -> Union[MatrixPlot, dict, None]:\n    \"\"\"\\\n    Creates a heatmap of the mean expression values per group of each var_names.\n\n    This function provides a convenient interface to the :class:`~scanpy.pl.MatrixPlot`\n    class. If you need more flexibility, you should use :class:`~scanpy.pl.MatrixPlot`\n    directly.\n\n    Parameters\n    ----------\n    {common_plot_args}\n    {groupby_plots_args}\n    {show_save_ax}\n    {vminmax}\n    kwds\n        Are passed to :func:`matplotlib.pyplot.pcolor`.\n\n    Returns\n    -------\n    If `return_fig` is `True`, returns a :class:`~scanpy.pl.MatrixPlot` object,\n    else if `show` is false, return axes dict\n\n    See also\n    --------\n    :class:`~scanpy.pl.MatrixPlot`: The MatrixPlot class can be used to to control\n        several visual parameters not available in this function.\n    :func:`~scanpy.pl.rank_genes_groups_matrixplot`: to plot marker genes\n        identified using the :func:`~scanpy.tl.rank_genes_groups` function.\n\n    Examples\n    --------\n\n    .. plot::\n        :context: close-figs\n\n        import scanpy as sc\n        adata = sc.datasets.pbmc68k_reduced()\n        markers = ['C1QA', 'PSAP', 'CD79A', 'CD79B', 'CST3', 'LYZ']\n        sc.pl.matrixplot(adata, markers, groupby='bulk_labels', dendrogram=True)\n\n    Using var_names as dict:\n\n    .. plot::\n        :context: close-figs\n\n        markers = {{'T-cell': 'CD3D', 'B-cell': 'CD79A', 'myeloid': 'CST3'}}\n        sc.pl.matrixplot(adata, markers, groupby='bulk_labels', dendrogram=True)\n\n    Get Matrix object for fine tuning:\n\n    .. plot::\n        :context: close-figs\n\n        mp = sc.pl.matrixplot(adata, markers, 'bulk_labels', return_fig=True)\n        mp.add_totals().style(edge_color='black').show()\n\n    The axes used can be obtained using the get_axes() method\n\n    .. plot::\n        :context: close-figs\n\n        axes_dict = mp.get_axes()\n    \"\"\"\n\n    mp = MatrixPlot(\n        adata,\n        var_names,\n        groupby=groupby,\n        use_raw=use_raw,\n        log=log,\n        num_categories=num_categories,\n        standard_scale=standard_scale,\n        title=title,\n        figsize=figsize,\n        gene_symbols=gene_symbols,\n        var_group_positions=var_group_positions,\n        var_group_labels=var_group_labels,\n        var_group_rotation=var_group_rotation,\n        layer=layer,\n        values_df=values_df,\n        ax=ax,\n        vmin=vmin,\n        vmax=vmax,\n        vcenter=vcenter,\n        norm=norm,\n        **kwds,\n    )\n\n    if dendrogram:\n        mp.add_dendrogram(dendrogram_key=dendrogram)\n    if swap_axes:\n        mp.swap_axes()\n\n    mp = mp.style(cmap=cmap).legend(title=colorbar_title)\n    if return_fig:\n        return mp\n    else:\n        mp.make_figure()\n        savefig_or_show(MatrixPlot.DEFAULT_SAVE_PREFIX, show=show, save=save)\n        show = settings.autoshow if show is None else show\n        if not show:\n            return mp.get_axes()", "idx": 1708}
{"project": "Scanpy", "commit_id": "300_scanpy_0.1_scanpy_readwrite.py_read_txt_as_floats.py", "target": 1, "func": "def read_txt_as_floats(filename, delim=None, first_column_names=None, dtype='float32', return_dict=True):\n    \"\"\"Return data as list of lists of strings and the header as string.\n    Parameters\n    ----------\n    filename : str, Path\n        Filename of data file.\n    delim : str, optional\n        Separator that separates data within text file. If None, will split at\n        arbitrary number of white spaces, which is different from enforcing\n        splitting at single white space ' '.\n    Returns\n    -------\n    data : list\n         List of lists of strings.\n    header : str\n         String storing the comment lines (those that start with '#').\n    \"\"\"\n    filename = str(filename)  # allow passing pathlib.Path objects\n    header = ''\n    data = []\n    length = -1\n    f = open(filename)\n    col_names = []\n    row_names = []\n    # read header and column names\n    for line in f:\n        if line.startswith('#'):\n            header += line\n        else:\n            if delim is not None and delim not in line:\n                logg.warn('did not find delimiter \"{}\" in first line'.format(delim))\n            line_list = line.split(delim)\n            if not is_float(line_list[0]):\n                col_names = line_list\n                logg.m('... assuming first line in file stores column names')\n            else:\n                if not is_float(line_list[0]) or first_column_names:\n                    first_column_names = True\n                    row_names.append(line_list[0])\n                    data.append(line_list[1:])\n                else:\n                    data.append(line_list)\n            break\n    if not col_names:\n        # try reading col_names from the last comment line\n        if len(header) > 0:\n            logg.m('... assuming last comment line stores variable names')\n            col_names = np.array(header.split('\\n')[-2].strip('#').split())\n        # just numbers as col_names\n        else:\n            logg.m('... did not find column names in file')\n            col_names = np.arange(len(data[0])).astype(str)\n    col_names = np.array(col_names, dtype=str)\n    # check if first column contains row names or not\n    if first_column_names is None:\n        first_column_names = False\n    for line in f:\n        line_list = line.split(delim)\n        if not is_float(line_list[0]) or first_column_names:\n            logg.m('... assuming first column in file stores row names')\n            first_column_names = True\n            row_names.append(line_list[0])\n            data.append(line_list[1:])\n        else:\n            data.append(line_list)\n        break\n    # parse the file\n    for line in f:\n        line_list = line.split(delim)\n        if first_column_names:\n            row_names.append(line_list[0])\n            data.append(line_list[1:])\n        else:\n            data.append(line_list)\n    logg.m('... read data into list of lists', t=True)\n    # transfrom to array, this takes a long time and a lot of memory\n    # but it's actually the same thing as np.genfromtext does\n    # - we don't use the latter as it would involve another slicing step\n    #   in the end, to separate row_names from float data, slicing takes\n    #   a lot of memory and cpu time\n    data = np.array(data, dtype=dtype)\n    logg.m('... constructed array from list of list', t=True)\n    # transform row_names\n    if not row_names:\n        row_names = np.arange(len(data)).astype(str)\n        logg.m('... did not find row names in file')\n    else:\n        row_names = np.array(row_names)\n        for iname, name in enumerate(row_names):\n            row_names[iname] = name.strip('\"')\n    # adapt col_names if necessary\n    if col_names.size > data.shape[1]:\n        col_names = col_names[1:]\n    for iname, name in enumerate(col_names):\n        col_names[iname] = name.strip('\"')\n    if return_dict:\n        return {'X': data, 'col_names': col_names, 'row_names': row_names}\n    else:\n        return AnnData(data, smp={'smp_names': row_names}, var={'var_names': col_names})", "idx": 1709}
{"project": "Scanpy", "commit_id": "695_scanpy_1.9.0_test_preprocessing_distributed.py_test_log1p.py", "target": 0, "func": "def test_log1p(self, adata, adata_dist):\n        log1p(adata_dist)\n        result = materialize_as_ndarray(adata_dist.X)\n        log1p(adata)\n        assert result.shape == adata.shape\n        assert result.shape == (adata.n_obs, adata.n_vars)\n        npt.assert_allclose(result, adata.X)", "idx": 1711}
{"project": "Scanpy", "commit_id": "230_scanpy_0.1_scanpy_plotting_toplevel.py_scatter.py", "target": 1, "func": "def scatter(adata,\n            x=None,\n            y=None,\n            color='grey',\n            basis=None,\n            names=None,\n            comps=None,\n            cont=None,\n            layout='2d',\n            legendloc='right margin',\n            cmap=None,\n            pal=None,\n            right_margin=None,\n            size=3,\n            titles=None,\n            show=True):\n    \"\"\"Scatter plots.\n    Color with sample annotation (`color in adata.smp_keys()`) or gene\n    expression (`color in adata.var_names`).\n    Parameters\n    ----------\n    adata : AnnData\n        Annotated data matrix.\n    x : str or None\n        x coordinate.\n    y : str or None\n        y coordinate.\n    color : str or list of strings, optional (default: 'grey')\n        Sample/Cell annotation key for coloring (either a key for adata.smp or a\n        var_name or a uniform matplotlib color). String annotation is plotted assuming categorical annotation,\n        float and integer annotation is plotted assuming continuous\n        annoation.\n    basis : {'pca', 'tsne', 'diffmap'}\n        String that denotes a plotting tool.\n    names : str, optional (default: all names in color)\n        Allows to restrict groups in sample annotation (color) to a few.\n    comps : str, optional (default: '1,2')\n         String in the form '1,2,3'.\n    cont : bool, None (default: None)\n        Switch on continuous layout, switch off categorical layout.\n    layout : {'2d', '3d'}, optional (default: '2d')\n         Layout of plot.\n    legendloc : see matplotlib.legend, optional (default: 'lower right')\n         Options for keyword argument 'loc'.\n    cmap : str (default: 'viridis')\n         String denoting matplotlib color map.\n    pal : list of str (default: matplotlib.rcParams['axes.prop_cycle'].by_key()['color'])\n         Colors cycle to use for categorical groups.\n    right_margin : float (default: None)\n         Adjust how far the plotting panel extends to the right.\n    size : float (default: 3)\n         Point size.\n    titles : str, optional (default: None)\n         Provide titles for panels as \"my title1,another title,...\".\n    Returns\n    -------\n    A list of matplotlib.Axis objects.\n    \"\"\"\n    # write params to a config file\n    params = locals()\n    del params['adata']\n    if os.path.exists('.scanpy/config_plotting.txt'):\n        params = sc_utils.update_params(readwrite.read_params('.scanpy/config_plotting.txt', verbosity=1), params)\n        if right_margin != params['right_margin']:\n            right_margin = params['right_margin']\n            sett.m(1, '... setting right_margin to saved value', right_margin)\n    readwrite.write_params('.scanpy/config_plotting.txt', params)\n    del params\n    # compute components\n    if comps is None: comps = '1,2' if '2d' in layout else '1,2,3'\n    if isinstance(comps, str): comps = comps.split(',')\n    comps = np.array(comps).astype(int) - 1\n    titles = None if titles is None else titles.split(',') if isinstance(titles, str) else titles\n    color_keys = [None] if color is None else color.split(',') if isinstance(color, str) else color\n    names = None if names is None else names.split(',') if isinstance(names, str) else names\n    # highlights\n    highlights = adata.add['highlights'] if 'highlights' in adata.smp else []\n    if basis is not None:\n        try:\n            Y = adata.smp['X_' + basis][:, comps]\n        except KeyError:\n            sett.mi('--> compute the basis using plotting tool', basis, 'first')\n            raise\n    else:\n        x_arr = adata.get_smp_array(x)\n        y_arr = adata.get_smp_array(y)\n        Y = np.c_[x_arr[:, None], y_arr[:, None]]\n    pal = utils.default_pal(pal)\n    component_name = ('DC' if basis == 'diffmap'\n                      else 'Spring' if basis == 'spring'\n                      else 'tSNE' if basis == 'tsne'\n                      else 'PC' if basis == 'pca'\n                      else None)\n    axis_labels = (x, y) if component_name is None else None\n    show_ticks = True if component_name is None else False\n    # the actual color ids, e.g. 'grey' or '#109482'\n    color_ids = [None if not is_color_like(color_key) else color_key for color_key in color_keys]\n    categoricals = []\n    colorbars = []\n    sizes = []\n    for icolor_key, color_key in enumerate(color_keys):\n        if color_ids[icolor_key] is not None:\n            c = color_ids[icolor_key]\n            continuous = True\n            categorical = False\n            colorbars.append(False)\n        else:\n            c = 'grey' if layout == '2d' else 'white'\n            categorical = False\n            continuous = False\n            if len(adata.smp_keys()) > 0:\n                if color_key is None:\n                    for color_key_ in adata.smp_keys():\n                        if color_key_ not in color_keys:\n                            color_key = color_key_\n                            color_keys[icolor_key] = color_key\n                            break\n                # test whether we have categorial or continuous annotation\n                if color_key in adata.smp_keys():\n                    if adata.smp[color_key].dtype.char in ['S', 'U']:\n                        categorical = True\n                        if cont is True:\n                            c = adata.smp[color_key]\n                    else:\n                        continuous = True\n                        c = adata.smp[color_key]\n                    # sett.m(0, '... coloring according to', color_key)\n                # coloring according to gene expression\n                elif color_key in set(adata.var_names):\n                    c = adata[:, color_key].X\n                    continuous = True\n                    # sett.m(0, '... coloring according to expression of gene', color_key)\n                else:\n                    raise ValueError('\"' + color_key + '\" is invalid!'\n                                     + ' specify valid sample annotation, one of '\n                                     + str(adata.smp_keys()) + ' or a gene name '\n                                     + str(adata.var_names))\n            if cont is not None:\n                categorical = not cont\n                continuous = cont\n            colorbars.append(True if continuous else False)\n        sizes.append(size if continuous else 0.66*size)\n        if categorical:\n            categoricals.append(icolor_key)\n        color_ids[icolor_key] = c\n\n    if right_margin is None and legendloc == 'right margin':\n        right_margin = 0.3\n    if titles is None and color_keys[0] is not None:\n        titles = [color_key.replace('_', ' ') if not is_color_like(color_key) else '' for color_key in color_keys]\n    axs = scatter_base(Y,\n                       titles=titles,\n                       component_name=component_name,\n                       axis_labels=axis_labels,\n                       component_indexnames=comps + 1,\n                       layout=layout,\n                       colors=color_ids,\n                       highlights=highlights,\n                       colorbars=colorbars,\n                       right_margin=right_margin,\n                       sizes=sizes,\n                       cmap='viridis' if cmap is None else cmap,\n                       show_ticks=show_ticks)\n    for icolor_key in categoricals:\n        color_key = color_keys[icolor_key]\n        if (color_key != 'groups' and 'groups_names' in adata.add\n            and len(np.setdiff1d(adata.add['groups_names'], adata.add[color_key + '_names']))\n                < len(adata.add['groups_names'])):\n            # if there is a correspondence between color_key and the 'groups' defined\n            # in adata, that is, if color_key has corresponding categories with those\n            # in adata.add['groups_names']\n            adata.add[color_key + '_colors'] = pal[:len(adata.add['groups_names'])].by_key()['color']\n        elif not color_key + '_colors' in adata.add:\n            adata.add[color_key + '_colors'] = pal[:len(adata.add[color_key + '_names'])].by_key()['color']\n        if len(adata.add[color_key + '_names']) > len(adata.add[color_key + '_colors']):\n            sett.m(0, 'number of categories/names in', color_key, 'so large that color map \"jet\" is used')\n            adata.add[color_key + '_colors'] = pl.cm.get_cmap(cmap)(\n                pl.Normalize()(np.arange(len(adata.add[color_key + '_names']), dtype=int)))\n        for iname, name in enumerate(adata.add[color_key + '_names']):\n            if names is None or (names is not None and name in names):\n                scatter_group(axs[icolor_key], color_key, iname, adata, Y, layout, size=size)\n        if legendloc == 'right margin':\n            legend = axs[icolor_key].legend(frameon=False, loc='center left', bbox_to_anchor=(1, 0.5), markerscale=2)\n        elif legendloc != 'none':\n            axs[icolor_key].legend(frameon=False, loc=legendloc)\n    if show: pl.show()\n    return axs", "idx": 1715}
{"project": "Scanpy", "commit_id": "883_scanpy_1.4.3_scanpy_tools__utils.py_choose_representation.py", "target": 1, "func": "def choose_representation(adata, use_rep=None, n_pcs=None):\n    if use_rep is None and n_pcs == 0:  # backwards compat for specifying `.X`\n        use_rep = 'X'\n    if use_rep is None:\n        if adata.n_vars > N_PCS:\n            if 'X_pca' in adata.obsm.keys():\n                if n_pcs is not None and n_pcs > adata.obsm['X_pca'].shape[1]:\n                    raise ValueError(\n                        '`X_pca` does not have enough PCs. Rerun `sc.pp.pca` with adjusted `n_comps`.')\n                X = adata.obsm['X_pca'][:, :n_pcs]\n                logg.info('    using \\'X_pca\\' with n_pcs = {}'\n                          .format(X.shape[1]))\n                return X\n            else:\n                logg.warn(\n                    'You\\'re trying to run this on {} dimensions of `.X`, '\n                    'if you really want this, set `use_rep=\\'X\\'`.\\n         '\n                    'Falling back to preprocessing with `sc.pp.pca` and default params.'\n                    .format(adata.n_vars))\n                X = pca(adata.X)\n                adata.obsm['X_pca'] = X[:, :n_pcs]\n                return X\n        else:\n            logg.info('    using data matrix X directly')\n            return adata.X\n    else:\n        if use_rep in adata.obsm.keys():\n            return adata.obsm[use_rep]\n        elif use_rep == 'X':\n            return adata.X\n        else:\n            raise ValueError(\n                'Did not find {} in `.obsm.keys()`. '\n                'You need to compute it first.'.format(use_rep))", "idx": 1737}
{"project": "Scanpy", "commit_id": "132_scanpy_1.9.0_exporting.py_write_hdf5_genes.py", "target": 0, "func": "def write_hdf5_genes(E, gene_list, filename):\n    '''SPRING standard: filename = main_spring_dir + \"counts_norm_sparse_genes.hdf5\"'''\n\n    E = E.tocsc()\n\n    hf = h5py.File(filename, 'w')\n    counts_group = hf.create_group('counts')\n    cix_group = hf.create_group('cell_ix')\n\n    hf.attrs['ncells'] = E.shape[0]\n    hf.attrs['ngenes'] = E.shape[1]\n\n    for iG, g in enumerate(gene_list):\n        counts = E[:, iG].A.squeeze()\n        cell_ix = np.nonzero(counts)[0]\n        counts = counts[cell_ix]\n        counts_group.create_dataset(g, data=counts)\n        cix_group.create_dataset(g, data=cell_ix)\n\n    hf.close()", "idx": 1751}
{"project": "Scanpy", "commit_id": "174_scanpy_1.9.0__pypairs.py_cyclone.py", "target": 0, "func": "def cyclone(\n    adata: AnnData,\n    marker_pairs: Optional[Mapping[str, Collection[Tuple[str, str]]]] = None,\n    *,\n    iterations: int = 1000,\n    min_iter: int = 100,\n    min_pairs: int = 50,\n) -> pd.DataFrame:\n    \"\"\"\\\n    Assigns scores and predicted class to observations [Scialdone15]_ [Fechtner18]_.\n\n    Calculates scores for each observation and each phase and assigns prediction\n    based on marker pairs indentified by :func:`~scanpy.external.tl.sandbag`.\n\n    This reproduces the approach of [Scialdone15]_ in the implementation of\n    [Fechtner18]_.\n\n    Parameters\n    ----------\n    adata\n        The annotated data matrix.\n    marker_pairs\n        Mapping of categories to lists of marker pairs.\n        See :func:`~scanpy.external.tl.sandbag` output.\n    iterations\n        An integer scalar specifying the number of\n        iterations for random sampling to obtain a cycle score.\n    min_iter\n        An integer scalar specifying the minimum number of iterations\n        for score estimation.\n    min_pairs\n        An integer scalar specifying the minimum number of pairs\n        for score estimation.\n\n    Returns\n    -------\n    A :class:`~pandas.DataFrame` with samples as index and categories as columns\n    with scores for each category for each sample and a additional column with\n    the name of the max scoring category for each sample.\n\n    If `marker_pairs` contains only the cell cycle categories G1, S and G2M an\n    additional column `pypairs_cc_prediction` will be added.\n    Where category S is assigned to samples where G1 and G2M score are < 0.5.\n    \"\"\"\n    _check_import()\n    from pypairs.pairs import cyclone\n    from pypairs import settings as pp_settings\n\n    pp_settings.verbosity = settings.verbosity\n    pp_settings.n_jobs = settings.n_jobs\n    pp_settings.writedir = settings.writedir\n    pp_settings.cachedir = settings.cachedir\n    pp_settings.logfile = settings.logfile\n\n    return cyclone(\n        data=adata,\n        marker_pairs=marker_pairs,\n        iterations=iterations,\n        min_iter=min_iter,\n        min_pairs=min_pairs,", "idx": 1752}
{"project": "Scanpy", "commit_id": "1113_scanpy_1.9.0_scanpy_plotting__tools_scatterplots.py_embedding.py", "target": 1, "func": "def embedding(\n    adata: AnnData,\n    basis: str,\n    *,\n    color: Union[str, Sequence[str], None] = None,\n    gene_symbols: Optional[str] = None,\n    use_raw: Optional[bool] = None,\n    sort_order: bool = True,\n    edges: bool = False,\n    edges_width: float = 0.1,\n    edges_color: Union[str, Sequence[float], Sequence[str]] = 'grey',\n    neighbors_key: Optional[str] = None,\n    arrows: bool = False,\n    arrows_kwds: Optional[Mapping[str, Any]] = None,\n    groups: Optional[str] = None,\n    components: Union[str, Sequence[str]] = None,\n    dimensions: Optional[Union[Tuple[int, int], Sequence[Tuple[int, int]]]] = None,\n    layer: Optional[str] = None,\n    projection: Literal['2d', '3d'] = '2d',\n    scale_factor: Optional[float] = None,\n    color_map: Union[Colormap, str, None] = None,\n    cmap: Union[Colormap, str, None] = None,\n    palette: Union[str, Sequence[str], Cycler, None] = None,\n    na_color: ColorLike = \"lightgray\",\n    na_in_legend: bool = True,\n    size: Union[float, Sequence[float], None] = None,\n    frameon: Optional[bool] = None,\n    legend_fontsize: Union[int, float, _FontSize, None] = None,\n    legend_fontweight: Union[int, _FontWeight] = 'bold',\n    legend_loc: str = 'right margin',\n    legend_fontoutline: Optional[int] = None,\n    vmax: Union[VBound, Sequence[VBound], None] = None,\n    vmin: Union[VBound, Sequence[VBound], None] = None,\n    vcenter: Union[VBound, Sequence[VBound], None] = None,\n    norm: Union[Normalize, Sequence[Normalize], None] = None,\n    add_outline: Optional[bool] = False,\n    outline_width: Tuple[float, float] = (0.3, 0.05),\n    outline_color: Tuple[str, str] = ('black', 'white'),\n    ncols: int = 4,\n    hspace: float = 0.25,\n    wspace: Optional[float] = None,\n    title: Union[str, Sequence[str], None] = None,\n    show: Optional[bool] = None,\n    save: Union[bool, str, None] = None,\n    ax: Optional[Axes] = None,\n    return_fig: Optional[bool] = None,\n    **kwargs,\n) -> Union[Figure, Axes, None]:\n    \"\"\"\\\n    Scatter plot for user specified embedding basis (e.g. umap, pca, etc)\n    Parameters\n    ----------\n    basis\n        Name of the `obsm` basis to use.\n    {adata_color_etc}\n    {edges_arrows}\n    {scatter_bulk}\n    {show_save_ax}\n    Returns\n    -------\n    If `show==False` a :class:`~matplotlib.axes.Axes` or a list of it.\n    \"\"\"\n    #####################\n    # Argument handling #\n    #####################\n    check_projection(projection)\n    sanitize_anndata(adata)\n    basis_values = _get_basis(adata, basis)\n    dimensions = _components_to_dimensions(\n        components, dimensions, projection=projection, total_dims=basis_values.shape[1]\n    )\n    args_3d = dict(projection='3d') if projection == '3d' else {}\n    # Figure out if we're using raw\n    if use_raw is None:\n        # check if adata.raw is set\n        use_raw = layer is None and adata.raw is not None\n    if use_raw and layer is not None:\n        raise ValueError(\n            \"Cannot use both a layer and the raw representation. Was passed:\"\n            f\"use_raw={use_raw}, layer={layer}.\"\n        )\n    if use_raw and adata.raw is None:\n        raise ValueError(\n            \"`use_raw` is set to True but AnnData object does not have raw. \"\n            \"Please check.\"\n        )\n    if isinstance(groups, str):\n        groups = [groups]\n    # Color map\n    if color_map is not None:\n        if cmap is not None:\n            raise ValueError(\"Cannot specify both `color_map` and `cmap`.\")\n        else:\n            cmap = color_map\n    cmap = copy(get_cmap(cmap))\n    cmap.set_bad(na_color)\n    kwargs[\"cmap\"] = cmap\n    # Prevents warnings during legend creation\n    na_color = colors.to_hex(na_color, keep_alpha=True)\n    if 'edgecolor' not in kwargs:\n        # by default turn off edge color. Otherwise, for\n        # very small sizes the edge will not reduce its size\n        # (https://github.com/theislab/scanpy/issues/293)\n        kwargs['edgecolor'] = 'none'\n    # Vectorized arguments\n    # turn color into a python list\n    color = [color] if isinstance(color, str) or color is None else list(color)\n    if title is not None:\n        # turn title into a python list if not None\n        title = [title] if isinstance(title, str) else list(title)\n    # turn vmax and vmin into a sequence\n    if isinstance(vmax, str) or not isinstance(vmax, cabc.Sequence):\n        vmax = [vmax]\n    if isinstance(vmin, str) or not isinstance(vmin, cabc.Sequence):\n        vmin = [vmin]\n    if isinstance(vcenter, str) or not isinstance(vcenter, cabc.Sequence):\n        vcenter = [vcenter]\n    if isinstance(norm, Normalize) or not isinstance(norm, cabc.Sequence):\n        norm = [norm]\n    # Size\n    if 's' in kwargs and size is None:\n        size = kwargs.pop('s')\n    if size is not None:\n        # check if size is any type of sequence, and if so\n        # set as ndarray\n        if (\n            size is not None\n            and isinstance(size, (cabc.Sequence, pd.Series, np.ndarray))\n            and len(size) == adata.shape[0]\n        ):\n            size = np.array(size, dtype=float)\n    else:\n        size = 120000 / adata.shape[0]\n    ##########\n    # Layout #\n    ##########\n    # Most of the code is for the case when multiple plots are required\n    if wspace is None:\n        #  try to set a wspace that is not too large or too small given the\n        #  current figure size\n        wspace = 0.75 / rcParams['figure.figsize'][0] + 0.02\n    if components is not None:\n        color, dimensions = list(zip(*product(color, dimensions)))\n    color, dimensions = _broadcast_args(color, dimensions)\n    # 'color' is a list of names that want to be plotted.\n    # Eg. ['Gene1', 'louvain', 'Gene2'].\n    # component_list is a list of components [[0,1], [1,2]]\n    if (\n        not isinstance(color, str)\n        and isinstance(color, cabc.Sequence)\n        and len(color) > 1\n    ) or len(dimensions) > 1:\n        if ax is not None:\n            raise ValueError(\n                \"Cannot specify `ax` when plotting multiple panels \"\n                \"(each for a given value of 'color').\"\n            )\n        # each plot needs to be its own panel\n        fig, grid = _panel_grid(hspace, wspace, ncols, len(color))\n    else:\n        grid = None\n        if ax is None:\n            fig = pl.figure()\n            ax = fig.add_subplot(111, **args_3d)\n    ############\n    # Plotting #\n    ############\n    axs = []\n    # use itertools.product to make a plot for each color and for each component\n    # For example if color=[gene1, gene2] and components=['1,2, '2,3'].\n    # The plots are: [\n    #     color=gene1, components=[1,2], color=gene1, components=[2,3],\n    #     color=gene2, components = [1, 2], color=gene2, components=[2,3],\n    # ]\n    for count, (value_to_plot, dims) in enumerate(zip(color, dimensions)):\n        color_source_vector = _get_color_source_vector(\n            adata,\n            value_to_plot,\n            layer=layer,\n            use_raw=use_raw,\n            gene_symbols=gene_symbols,\n            groups=groups,\n        )\n        color_vector, categorical = _color_vector(\n            adata,\n            value_to_plot,\n            color_source_vector,\n            palette=palette,\n            na_color=na_color,\n        )\n        # Order points\n        order = slice(None)\n        if sort_order is True and value_to_plot is not None and categorical is False:\n            # Higher values plotted on top, null values on bottom\n            order = np.argsort(-color_vector, kind=\"stable\")[::-1]\n        elif sort_order and categorical:\n            # Null points go on bottom\n            order = np.argsort(~pd.isnull(color_source_vector), kind=\"stable\")\n        # Set orders\n        if isinstance(size, np.ndarray):\n            size = np.array(size)[order]\n        color_source_vector = color_source_vector[order]\n        color_vector = color_vector[order]\n        coords = basis_values[:, dims][order, :]\n        # if plotting multiple panels, get the ax from the grid spec\n        # else use the ax value (either user given or created previously)\n        if grid:\n            ax = pl.subplot(grid[count], **args_3d)\n            axs.append(ax)\n        if not (settings._frameon if frameon is None else frameon):\n            ax.axis('off')\n        if title is None:\n            if value_to_plot is not None:\n                ax.set_title(value_to_plot)\n            else:\n                ax.set_title('')\n        else:\n            try:\n                ax.set_title(title[count])\n            except IndexError:\n                logg.warning(\n                    \"The title list is shorter than the number of panels. \"\n                    \"Using 'color' value instead for some plots.\"\n                )\n                ax.set_title(value_to_plot)\n        if not categorical:\n            vmin_float, vmax_float, vcenter_float, norm_obj = _get_vboundnorm(\n                vmin, vmax, vcenter, norm, count, color_vector\n            )\n            normalize = check_colornorm(\n                vmin_float,\n                vmax_float,\n                vcenter_float,\n                norm_obj,\n            )\n        else:\n            normalize = None\n        # make the scatter plot\n        if projection == '3d':\n            cax = ax.scatter(\n                coords[:, 0],\n                coords[:, 1],\n                coords[:, 2],\n                marker=\".\",\n                c=color_vector,\n                rasterized=settings._vector_friendly,\n                norm=normalize,\n                **kwargs,\n            )\n        else:\n            scatter = (\n                partial(ax.scatter, s=size, plotnonfinite=True)\n                if scale_factor is None\n                else partial(\n                    circles, s=size, ax=ax, scale_factor=scale_factor\n                )  # size in circles is radius\n            )\n            if add_outline:\n                # the default outline is a black edge followed by a\n                # thin white edged added around connected clusters.\n                # To add an outline\n                # three overlapping scatter plots are drawn:\n                # First black dots with slightly larger size,\n                # then, white dots a bit smaller, but still larger\n                # than the final dots. Then the final dots are drawn\n                # with some transparency.\n                bg_width, gap_width = outline_width\n                point = np.sqrt(size)\n                gap_size = (point + (point * gap_width) * 2) ** 2\n                bg_size = (np.sqrt(gap_size) + (point * bg_width) * 2) ** 2\n                # the default black and white colors can be changes using\n                # the contour_config parameter\n                bg_color, gap_color = outline_color\n                # remove edge from kwargs if present\n                # because edge needs to be set to None\n                kwargs['edgecolor'] = 'none'\n                # remove alpha for outline\n                alpha = kwargs.pop('alpha') if 'alpha' in kwargs else None\n                ax.scatter(\n                    coords[:, 0],\n                    coords[:, 1],\n                    s=bg_size,\n                    marker=\".\",\n                    c=bg_color,\n                    rasterized=settings._vector_friendly,\n                    norm=normalize,\n                    **kwargs,\n                )\n                ax.scatter(\n                    coords[:, 0],\n                    coords[:, 1],\n                    s=gap_size,\n                    marker=\".\",\n                    c=gap_color,\n                    rasterized=settings._vector_friendly,\n                    norm=normalize,\n                    **kwargs,\n                )\n                # if user did not set alpha, set alpha to 0.7\n                kwargs['alpha'] = 0.7 if alpha is None else alpha\n            cax = scatter(\n                coords[:, 0],\n                coords[:, 1],\n                marker=\".\",\n                c=color_vector,\n                rasterized=settings._vector_friendly,\n                norm=normalize,\n                **kwargs,\n            )\n        # remove y and x ticks\n        ax.set_yticks([])\n        ax.set_xticks([])\n        if projection == '3d':\n            ax.set_zticks([])\n        # set default axis_labels\n        name = _basis2name(basis)\n        axis_labels = [name + str(d + 1) for d in dims]\n        ax.set_xlabel(axis_labels[0])\n        ax.set_ylabel(axis_labels[1])\n        if projection == '3d':\n            # shift the label closer to the axis\n            ax.set_zlabel(axis_labels[2], labelpad=-7)\n        ax.autoscale_view()\n        if edges:\n            _utils.plot_edges(ax, adata, basis, edges_width, edges_color, neighbors_key)\n        if arrows:\n            _utils.plot_arrows(ax, adata, basis, arrows_kwds)\n        if value_to_plot is None:\n            # if only dots were plotted without an associated value\n            # there is not need to plot a legend or a colorbar\n            continue\n        if legend_fontoutline is not None:\n            path_effect = [\n                patheffects.withStroke(linewidth=legend_fontoutline, foreground='w')\n            ]\n        else:\n            path_effect = None\n        # Adding legends\n        if categorical:\n            _add_categorical_legend(\n                ax,\n                color_source_vector,\n                palette=_get_palette(adata, value_to_plot),\n                scatter_array=coords,\n                legend_loc=legend_loc,\n                legend_fontweight=legend_fontweight,\n                legend_fontsize=legend_fontsize,\n                legend_fontoutline=path_effect,\n                na_color=na_color,\n                na_in_legend=na_in_legend,\n                multi_panel=bool(grid),\n            )\n        else:\n            # TODO: na_in_legend should have some effect here\n            pl.colorbar(cax, ax=ax, pad=0.01, fraction=0.08, aspect=30)\n\n    if return_fig is True:\n        return fig\n    axs = axs if grid else ax\n    _utils.savefig_or_show(basis, show=show, save=save)\n    if show is False:\n        return axs", "idx": 1753}
{"project": "Scanpy", "commit_id": "764_scanpy_1.3.5_scanpy_plotting_anndata.py_dotplot.py", "target": 1, "func": "def dotplot(adata, var_names, groupby=None, use_raw=None, log=False, num_categories=7,\n            color_map='Reds', figsize=None, dendrogram=False, var_group_positions=None,\n            var_group_labels=None, var_group_rotation=None, layer=None, show=None, save=None, **kwds):\n    \"\"\"\\\n    Makes a *dot plot* of the expression values of `var_names`.\n    For each var_name and each `groupby` category a dot is plotted. Each dot\n    represents two values: mean expression within each category (visualized by\n    color) and fraction of cells expressing the var_name in the\n    category. (visualized by the size of the dot).  If groupby is not given, the\n    dotplot assumes that all data belongs to a single category. A gene is not\n    considered expressed if the expression value in the adata (or adata.raw) is\n    equal to zero.\n    For instance, for each marker gene, the mean value and the percentage of cells\n    expressing the gene can be visualized for each cluster.\n    Parameters\n    ----------\n    {common_plot_args}\n    color_map : `str`, optional (default: `Reds`)\n        String denoting matplotlib color map.\n    {show_save_ax}\n    **kwds : keyword arguments\n        Are passed to `matplotlib.pyplot.scatter`.\n    Returns\n    -------\n    List of `matplotlib.Axes`\n    Examples\n    -------\n    >>> adata = sc.datasets.pbmc68k_reduced()\n    >>> sc.pl.dotplot(adata, ['C1QA', 'PSAP', 'CD79A', 'CD79B', 'CST3', 'LYZ'],\n    ...               groupby='bulk_labels', dendrogram=True)\n    \"\"\"\n    if use_raw is None and adata.raw is not None: use_raw = True\n    if isinstance(var_names, str):\n        var_names = [var_names]\n    categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer)\n    # for if category defined by groupby (if any) compute for each var_name\n    # 1. the mean value over the category\n    # 2. the fraction of cells in the category having a value > 0\n    # 1. compute mean value\n    mean_obs = obs_tidy.groupby(level=0).mean()\n    # 2. compute fraction of cells having value >0\n    # transform obs_tidy into boolean matrix\n    obs_bool = obs_tidy.astype(bool)\n    # compute the sum per group which in the boolean matrix this is the number\n    # of values >0, and divide the result by the total number of values in the group\n    # (given by `count()`)\n    fraction_obs = obs_bool.groupby(level=0).sum() / obs_bool.groupby(level=0).count()\n    dendro_width = 0.8 if dendrogram else 0\n    colorbar_width = 0.2\n    colorbar_width_spacer = 0.5\n    size_legend_width = 0.25\n    if figsize is None:\n        height = len(categories) * 0.3 + 1  # +1 for labels\n        # if the number of categories is small (eg 1 or 2) use\n        # a larger height\n        height = max([1.5, height])\n        heatmap_width = len(var_names) * 0.35\n        width = heatmap_width + colorbar_width + size_legend_width + dendro_width + colorbar_width_spacer\n    else:\n        width, height = figsize\n        heatmap_width = width - (colorbar_width + size_legend_width + dendro_width + colorbar_width_spacer)\n    # colorbar ax width should not change with differences in the width of the image\n    # otherwise can become too small\n    if var_group_positions is not None and len(var_group_positions) > 0:\n        # add some space in case 'brackets' want to be plotted on top of the image\n        height_ratios = [0.5, 10]\n    else:\n        height_ratios = [0, 10.5]\n    # define a layout of 2 rows x 5 columns\n    # first row is for 'brackets' (if no brackets needed, the height of this row is zero)\n    # second row is for main content. This second row\n    # is divided into 4 axes:\n    #   first ax is for the main figure\n    #   second ax is for dendrogram (if present)\n    #   third ax is for the color bar legend\n    #   fourth ax is for an spacer that avoids the ticks\n    #             from the color bar to be hidden beneath the size lengend axis\n    #   fifth ax is to plot the size legend\n    fig = pl.figure(figsize=(width, height))\n    axs = gridspec.GridSpec(nrows=2, ncols=5, wspace=0.02, hspace=0.04,\n                            width_ratios=[heatmap_width, dendro_width, colorbar_width, colorbar_width_spacer, size_legend_width],\n                            height_ratios=height_ratios)\n    if len(categories) < 4:\n        # when few categories are shown, the colorbar and size legend\n        # need to be larger than the main plot, otherwise they would look\n        # compressed. For this, the dotplot ax is split into two:\n        axs2 = gridspec.GridSpecFromSubplotSpec(2, 1, subplot_spec=axs[1, 0],\n                                                height_ratios=[len(categories) * 0.3, 1])\n        dot_ax = fig.add_subplot(axs2[0])\n    else:\n        dot_ax = fig.add_subplot(axs[1, 0])\n    color_legend = fig.add_subplot(axs[1, 2])\n    if groupby is None or len(categories) <= 1:\n        # dendrogram can only be computed  between groupby categories\n        dendrogram = False\n    if dendrogram:\n        dendro_data = _compute_dendrogram(adata, groupby, var_names=var_names,\n                                          categories=categories,\n                                          var_group_labels=var_group_labels,\n                                          var_group_positions=var_group_positions,\n                                          use_raw=use_raw, log=log, num_categories=num_categories)\n        var_group_labels = dendro_data['var_group_labels']\n        var_group_positions = dendro_data['var_group_positions']\n        # reorder matrix\n        if dendro_data['var_names_idx_ordered'] is not None:\n            # reorder columns (usually genes) if needed. This only happens when\n            # var_group_positions and var_group_labels is set\n            mean_obs = mean_obs.iloc[:,dendro_data['var_names_idx_ordered']]\n            fraction_obs = fraction_obs.iloc[:, dendro_data['var_names_idx_ordered']]\n        # reorder rows (categories) to match the dendrogram order\n        mean_obs = mean_obs.iloc[dendro_data['categories_idx_ordered'], :]\n        fraction_obs = fraction_obs.iloc[dendro_data['categories_idx_ordered'], :]\n        y_ticks = range(mean_obs.shape[0])\n        dendro_ax = fig.add_subplot(axs[1, 1], sharey=dot_ax)\n        _plot_dendrogram(dendro_ax, adata, ticks=y_ticks)\n    # to keep the size_legen of about the same height, irrespective\n    # of the number of categories, the fourth ax is subdivided into two parts\n    size_legend_height = min(1.3, height)\n    # wspace is proportional to the width but a constant value is\n    # needed such that the spacing is the same for thinner or wider images.\n    wspace = 10.5 / width\n    axs3 = gridspec.GridSpecFromSubplotSpec(2, 1, subplot_spec=axs[1, 4], wspace=wspace,\n                                            height_ratios=[size_legend_height / height,\n                                                           (height - size_legend_height) / height])\n    size_legend = fig.add_subplot(axs3[0])\n    # make scatter plot in which\n    # x = var_names\n    # y = groupby category\n    # size = fraction\n    # color = mean expression\n    y, x = np.indices(mean_obs.shape)\n    y = y.flatten()\n    x = x.flatten()\n    frac = fraction_obs.values.flatten()\n    mean_flat = mean_obs.values.flatten()\n    cmap = pl.get_cmap(color_map)\n    size = (frac * 10) ** 2\n\n    import matplotlib.colors\n    normalize = matplotlib.colors.Normalize(vmin=min(mean_flat), vmax=max(mean_flat))\n    colors = [cmap(normalize(value)) for value in mean_flat]\n\n    dot_ax.scatter(x, y, color=colors, s=size, cmap=cmap, norm=None, edgecolor='none', **kwds)\n    y_ticks = range(mean_obs.shape[0])\n    dot_ax.set_yticks(y_ticks)\n    dot_ax.set_yticklabels([mean_obs.index[idx] for idx in y_ticks])\n    x_ticks = range(mean_obs.shape[1])\n    dot_ax.set_xticks(x_ticks)\n    dot_ax.set_xticklabels([mean_obs.columns[idx] for idx in x_ticks], rotation=90)\n    dot_ax.tick_params(axis='both', labelsize='small')\n    dot_ax.grid(False)\n    dot_ax.set_xlim(-0.5, len(var_names) + 0.5)\n    dot_ax.set_ylabel(groupby)\n    # to be consistent with the heatmap plot, is better to\n    # invert the order of the y-axis, such that the first group is on\n    # top\n    ymin, ymax = dot_ax.get_ylim()\n    dot_ax.set_ylim(ymax+0.5, ymin - 0.5)\n    dot_ax.set_xlim(-1, len(var_names))\n    # plot group legends on top of dot_ax (if given)\n    if var_group_positions is not None and len(var_group_positions) > 0:\n        gene_groups_ax = fig.add_subplot(axs[0, 0], sharex=dot_ax)\n        _plot_gene_groups_brackets(gene_groups_ax, group_positions=var_group_positions,\n                                   group_labels=var_group_labels,\n                                   rotation=var_group_rotation)\n    # plot colorbar\n    import matplotlib.colorbar\n    matplotlib.colorbar.ColorbarBase(color_legend, cmap=cmap, norm=normalize)\n    # plot size bar\n    fracs_legend = np.array([0.25, 0.50, 0.75, 1])\n    size = (fracs_legend * 10) ** 2\n    color = [cmap(normalize(value)) for value in np.repeat(max(mean_flat) * 0.7, len(size))]\n    size_legend.scatter(np.repeat(0, len(size)), range(len(size)), s=size, color=color)\n    size_legend.set_yticks(range(len(size)))\n    size_legend.set_yticklabels([\"{:.0%}\".format(x) for x in fracs_legend])\n    size_legend.tick_params(axis='y', left=False, labelleft=False, labelright=True)\n    # remove x ticks and labels\n    size_legend.tick_params(axis='x', bottom=False, labelbottom=False)\n    # remove surrounding lines\n    size_legend.spines['right'].set_visible(False)\n    size_legend.spines['top'].set_visible(False)\n    size_legend.spines['left'].set_visible(False)\n    size_legend.spines['bottom'].set_visible(False)\n    size_legend.grid(False)\n    ymin, ymax = size_legend.get_ylim()\n    size_legend.set_ylim(ymin, ymax+0.5)\n    utils.savefig_or_show('dotplot', show=show, save=save)\n    return axs", "idx": 1754}
{"project": "Scanpy", "commit_id": "842_scanpy_1.9.0__paga.py_compute_transitions.py", "target": 0, "func": "def compute_transitions(self):\n        vkey = 'velocity_graph'\n        if vkey not in self._adata.uns:\n            if 'velocyto_transitions' in self._adata.uns:\n                self._adata.uns[vkey] = self._adata.uns['velocyto_transitions']\n                logg.debug(\n                    \"The key 'velocyto_transitions' has been changed to 'velocity_graph'.\"\n                )\n            else:\n                raise ValueError(\n                    'The passed AnnData needs to have an `uns` annotation '\n                    \"with key 'velocity_graph' - a sparse matrix from RNA velocity.\"\n                )\n        if self._adata.uns[vkey].shape != (self._adata.n_obs, self._adata.n_obs):\n            raise ValueError(\n                f\"The passed 'velocity_graph' have shape {self._adata.uns[vkey].shape} \"\n                f\"but shoud have shape {(self._adata.n_obs, self._adata.n_obs)}\"\n            )\n        # restore this at some point\n        # if 'expected_n_edges_random' not in self._adata.uns['paga']:\n        #     raise ValueError(\n        #         'Before running PAGA with `use_rna_velocity=True`, run it with `False`.')\n        import igraph\n\n        g = _utils.get_igraph_from_adjacency(\n            self._adata.uns[vkey].astype('bool'),\n            directed=True,\n        )\n        vc = igraph.VertexClustering(\n            g, membership=self._adata.obs[self._groups_key].cat.codes.values\n        )\n        # set combine_edges to False if you want self loops\n        cg_full = vc.cluster_graph(combine_edges='sum')\n        transitions = _utils.get_sparse_from_igraph(cg_full, weight_attr='weight')\n        transitions = transitions - transitions.T\n        transitions_conf = transitions.copy()\n        transitions = transitions.tocoo()\n        total_n = self._neighbors.n_neighbors * np.array(vc.sizes())\n        # total_n_sum = sum(total_n)\n        # expected_n_edges_random = self._adata.uns['paga']['expected_n_edges_random']\n        for i, j, v in zip(transitions.row, transitions.col, transitions.data):\n            # if expected_n_edges_random[i, j] != 0:\n            #     # factor 0.5 because of asymmetry\n            #     reference = 0.5 * expected_n_edges_random[i, j]\n            # else:\n            #     # approximate\n            #     reference = self._neighbors.n_neighbors * total_n[i] * total_n[j] / total_n_sum\n            reference = np.sqrt(total_n[i] * total_n[j])\n            transitions_conf[i, j] = 0 if v < 0 else v / reference\n        transitions_conf.eliminate_zeros()\n        # transpose in order to match convention of stochastic matrices\n        # entry ij means transition from j to i\n        self.transitions_confidence = transitions_conf.T", "idx": 1764}
{"project": "Scanpy", "commit_id": "402_scanpy_0.2.8_scanpy_plotting_tools.py_aga_path.py", "target": 1, "func": "def aga_path(\n        adata,\n        nodes=[0],\n        keys=[0],\n        normalize_to_zero_one=False,\n        as_heatmap=True,\n        color_map=None,\n        xlim=[None, None],\n        n_avg=1,\n        title=None,\n        left_margin=None,\n        show_left_y_ticks=None,\n        ytick_fontsize=None,\n        show_nodes_twin=True,\n        legend_fontsize=None,\n        legend_fontweight=None,\n        save=None,\n        show=None,\n        ax=None):\n    \"\"\"Gene expression changes along paths in the abstracted graph.\n    Parameters\n    ----------\n    normalize_to_zero_one : bool, optional (default: True)\n        Shift and scale the running average to [0, 1] per gene.\n    \"\"\"\n    ax_was_none = ax is None\n    if show_left_y_ticks is None:\n        show_left_y_ticks = False if show_nodes_twin else True\n    orig_node_names = []\n    if ('aga_groups_order_original' in adata.add\n        and adata.add['aga_groups_original'] != 'louvain_groups'):\n        orig_node_names = adata.add['aga_groups_order_original']\n    else:\n        logg.m('did not find field \"aga_groups_order_original\" in adata.add, '\n               'using aga_group integer ids instead', v=4)\n\n    ax = pl.gca() if ax is None else ax\n    from matplotlib import transforms\n    trans = transforms.blended_transform_factory(\n        ax.transData, ax.transAxes)\n    if as_heatmap:\n        X = []\n    x_tick_locs = []\n    x_tick_labels = []\n    for ikey, key in enumerate(keys):\n        x = []\n        for igroup, group in enumerate(nodes):\n            if ikey == 0: x_tick_locs.append(len(x))\n            idcs = np.arange(adata.n_smps)[adata.smp['aga_groups'] == str(group)]\n            idcs_group = np.argsort(adata.smp['aga_pseudotime'][adata.smp['aga_groups'] == str(group)])\n            idcs = idcs[idcs_group]\n            if key in adata.smp_keys(): x += list(adata.smp[key][idcs])\n            else: x += list(adata[:, key].X[idcs])\n        if n_avg > 1:\n            old_len_x = len(x)\n            x = moving_average(x)\n            if ikey == 0: x_tick_locs = len(x)/old_len_x * np.array(x_tick_locs)\n        if normalize_to_zero_one:\n            x -= np.min(x)\n            x /= np.max(x)\n        if not as_heatmap:\n            ax.plot(x[xlim[0]:xlim[1]], label=key)\n        else:\n            X.append(x)\n        if ikey == 0:\n            for igroup, group in enumerate(nodes):\n                if len(orig_node_names) > 0 and group not in orig_node_names:\n                    label = orig_node_names[int(group)]\n                else:\n                    label = group\n                if not isinstance(label, int):\n                    pl.text(x_tick_locs[igroup], -0.05*(igroup+1),\n                            label, transform=trans)\n                else:\n                    x_tick_labels.append(label)\n    if as_heatmap:\n        img = ax.imshow(np.array(X), aspect='auto', interpolation='nearest',\n                        cmap=color_map)\n        ax.set_yticks(range(len(X)))\n        ax.set_yticklabels(keys, fontsize=ytick_fontsize)\n        ax.set_frame_on(False)\n        pl.colorbar(img, ax=ax)\n        left_margin = 0.2 if left_margin is None else left_margin\n        pl.subplots_adjust(left=left_margin)\n    else:\n        left_margin = 0.4 if left_margin is None else left_margin\n        if len(keys) > 1:\n            pl.legend(frameon=False, loc='center left',\n                      bbox_to_anchor=(-left_margin, 0.5),\n                      fontsize=legend_fontsize)\n    ax.set_xticks(x_tick_locs)\n    ax.set_xticklabels(x_tick_labels)\n    xlabel = (adata.add['aga_groups_original'] if ('aga_groups_original' in adata.add\n              and adata.add['aga_groups_original'] != 'louvain_groups')\n              else 'aga groups')\n    if as_heatmap:\n        ax.set_xlabel(xlabel)\n    else:\n        ax.set_xlabel(xlabel)\n    if show_left_y_ticks:\n        utils.pimp_axis(pl.gca().get_yaxis())\n        if len(keys) > 1: pl.ylabel('as indicated on legend')\n        else: pl.ylabel(keys[0])\n    elif not as_heatmap:\n        pl.yticks([])\n        pl.ylabel('as indicated on legend (a.u.)')\n    if show_nodes_twin and not as_heatmap:\n        pl.twinx()\n        x = []\n        for g in nodes:\n            x += list(adata.smp['aga_groups'][adata.smp['aga_groups'] == str(g)].astype(int))\n        if n_avg > 1: x = moving_average(x)\n        pl.plot(x[xlim[0]:xlim[1]], '--', color='black')\n        label = 'aga groups' + (' / original groups' if len(orig_node_names) > 0 else '')\n        pl.ylabel(label)\n        utils.pimp_axis(pl.gca().get_yaxis())\n    if title is not None: pl.title(title)\n    if show is None and not ax_was_none: show = False\n    else: show = settings.autoshow if show is None else show\n    utils.savefig_or_show('aga_path', show=show, save=save)\n    return ax if ax_was_none else None", "idx": 1769}
{"project": "Scanpy", "commit_id": "779_scanpy_1.3.7_scanpy_readwrite.py__read_legacy_10x_h5.py", "target": 1, "func": "def _read_legacy_10x_h5(filename, genome=None):\n    \"\"\"\n    Read hdf5 file from Cell Ranger v2 or earlier versions.\n    \"\"\"\n    with tables.open_file(str(filename), 'r') as f:\n        try:\n            if not genome:\n                children = [x._v_name for x in f.list_nodes(f.root)]\n                if len(children) > 1:\n                    raise ValueError(\"This file contains more than one genome.\"\n                                    \" For legacy 10x h5 files you must specify\"\n                                    \" the genome if more than one is present. \"\n                                    \"Available genomes are: {}\".format(children))\n                genome = children[0]\n            dsets = {}\n            for node in f.walk_nodes('/' + genome, 'Array'):\n                dsets[node.name] = node.read()\n            # AnnData works with csr matrices\n            # 10x stores the transposed data, so we do the transposition right away\n            from scipy.sparse import csr_matrix\n            M, N = dsets['shape']\n            data = dsets['data']\n            if dsets['data'].dtype == np.dtype('int32'):\n                data = dsets['data'].view('float32')\n                data[:] = dsets['data']\n            matrix = csr_matrix((data, dsets['indices'], dsets['indptr']),\n                                shape=(N, M))\n            # the csc matrix is automatically the transposed csr matrix\n            # as scanpy expects it, so, no need for a further transpostion\n            adata = AnnData(matrix,\n                            {'obs_names': dsets['barcodes'].astype(str)},\n                            {'var_names': dsets['gene_names'].astype(str),\n                             'gene_ids': dsets['genes'].astype(str)})\n            logg.info(t=True)\n            return adata\n        except tables.NoSuchNodeError:\n            raise Exception('Genome %s does not exist in this file.' % genome)\n        except KeyError:\n            raise Exception('File is missing one or more required datasets.')", "idx": 1771}
{"project": "Scanpy", "commit_id": "218_scanpy_1.9.0___init__.py___init__.py", "target": 0, "func": "def __init__(\n        self,\n        adata: AnnData,\n        n_dcs: Optional[int] = None,\n        neighbors_key: Optional[str] = None,\n    ):\n        self._adata = adata\n        self._init_iroot()\n        # use the graph in adata\n        info_str = ''\n        self.knn: Optional[bool] = None\n        self._distances: Union[np.ndarray, csr_matrix, None] = None\n        self._connectivities: Union[np.ndarray, csr_matrix, None] = None\n        self._transitions_sym: Union[np.ndarray, csr_matrix, None] = None\n        self._number_connected_components: Optional[int] = None\n        self._rp_forest: Optional[RPForestDict] = None\n        if neighbors_key is None:\n            neighbors_key = 'neighbors'\n        if neighbors_key in adata.uns:\n            neighbors = NeighborsView(adata, neighbors_key)\n            if 'distances' in neighbors:\n                self.knn = issparse(neighbors['distances'])\n                self._distances = neighbors['distances']\n            if 'connectivities' in neighbors:\n                self.knn = issparse(neighbors['connectivities'])\n                self._connectivities = neighbors['connectivities']\n            if 'rp_forest' in neighbors:\n                self._rp_forest = neighbors['rp_forest']\n            if 'params' in neighbors:\n                self.n_neighbors = neighbors['params']['n_neighbors']\n            else:\n\n                def count_nonzero(a: Union[np.ndarray, csr_matrix]) -> int:\n                    return a.count_nonzero() if issparse(a) else np.count_nonzero(a)\n\n                # estimating n_neighbors\n                if self._connectivities is None:\n                    self.n_neighbors = int(\n                        count_nonzero(self._distances) / self._distances.shape[0]\n                    )\n                else:\n                    self.n_neighbors = int(\n                        count_nonzero(self._connectivities)\n                        / self._connectivities.shape[0]\n                        / 2\n                    )\n            info_str += '`.distances` `.connectivities` '\n            self._number_connected_components = 1\n            if issparse(self._connectivities):\n                from scipy.sparse.csgraph import connected_components\n\n                self._connected_components = connected_components(self._connectivities)\n                self._number_connected_components = self._connected_components[0]\n        if 'X_diffmap' in adata.obsm_keys():\n            self._eigen_values = _backwards_compat_get_full_eval(adata)\n            self._eigen_basis = _backwards_compat_get_full_X_diffmap(adata)\n            if n_dcs is not None:\n                if n_dcs > len(self._eigen_values):\n                    raise ValueError(\n                        'Cannot instantiate using `n_dcs`={}. '\n                        'Compute diffmap/spectrum with more components first.'.format(\n                            n_dcs\n                        )\n                    )\n                self._eigen_values = self._eigen_values[:n_dcs]\n                self._eigen_basis = self._eigen_basis[:, :n_dcs]\n            self.n_dcs = len(self._eigen_values)\n            info_str += '`.eigen_values` `.eigen_basis` `.distances_dpt`'\n        else:\n            self._eigen_values = None\n            self._eigen_basis = None\n            self.n_dcs = None\n        if info_str != '':\n            logg.debug(f'    initialized {info_str}')", "idx": 1773}
{"project": "Scanpy", "commit_id": "141_scanpy_0.0_scanpy_classes_data_graph.py___init__.py", "target": 1, "func": "def __init__(self, adata_or_X, params):\n    \"\"\"\n    \"\"\"\n    isadata = isinstance(adata_or_X, AnnData)\n    if isadata:\n        adata = adata_or_X\n        X = adata_or_X.X\n    else:\n        X = adata_or_X\n    # decide on whether to compute PCA or not\n    if (params['n_pcs_pre'] == 0\n            or X.shape[1] < params['n_pcs_pre']):\n        self.X = X\n        if 'xroot' in adata:\n            self.set_root(adata['xroot'])\n        sett.m(0, '... using X for building graph')\n    elif (isadata\n          and 'X_pca' in adata\n          and adata['X_pca'].shape[1] >= params['n_pcs_pre']):\n        if 'xroot' in adata and adata['xroot'].size == adata.X.shape[1]:\n            self.X = adata.X\n            self.set_root(adata['xroot'])\n        self.X = adata['X_pca']\n        if 'xroot' in adata and adata['xroot'].size == adata['X_pca'].shape[1]:\n            self.set_root(adata['xroot'])\n        sett.m(0, '... using X_pca for building graph')\n    else:\n        if isadata and 'xroot' in adata:\n            self.X = X\n            self.set_root(adata['xroot'])\n        from ..preprocess import pca\n        self.X = pca(X, n_comps=params['n_pcs_pre'])\n        if isadata:\n            adata['X_pca'] = self.X\n    self.params = params\n    if self.params['sigma'] > 0:\n        self.params['method'] = 'global'\n    else:\n        self.params['method'] = 'local'", "idx": 1788}
{"project": "Scanpy", "commit_id": "581_scanpy_1.9.0_test_marker_gene_overlap.py_test_marker_overlap_subsetting.py", "target": 0, "func": "def test_marker_overlap_subsetting():\n    test_data, marker_genes = generate_test_data()\n\n    t6 = sc.tl.marker_gene_overlap(test_data, marker_genes, top_n_markers=2)\n    t7 = sc.tl.marker_gene_overlap(test_data, marker_genes, adj_pval_threshold=0.01)\n\n    assert t6['c0']['type 1'] == 2.0\n    assert t7['c0']['type 1'] == 1.0", "idx": 1790}
{"project": "Scanpy", "commit_id": "132_scanpy_0.0_scanpy_utils.py_select_groups.py", "target": 1, "func": "def select_groups(adata, groups_names_subset='all', smp='groups'):\n    \"\"\"\n    Get subset of groups in adata.smp[smp].\n    \"\"\"\n    groups_names = adata[smp + '_names']\n    if smp + '_masks' in adata:\n        groups_masks = adata[smp + '_masks']\n    else:\n        groups_masks = np.zeros((len(adata[smp + '_names']),\n                                        adata.smp[smp].size), dtype=bool)\n        for iname, name in enumerate(adata[smp + '_names']):\n            # if the name is not found, fallback to index retrieval\n            if adata[smp + '_names'][iname] in adata.smp[smp]:\n                mask = adata[name + '_names'][iname] == adata.smp[smp]\n            else:\n                mask = str(iname) == adata.smp[smp]\n            groups_masks[iname] = mask\n    groups_ids = list(range(len(groups_names)))\n    if groups_names_subset != 'all':\n        # get list from string\n        if isinstance(groups_names_subset, str):\n            groups_names_subset = groups_names_subset.split(',')\n        groups_ids = np.where(np.in1d(adata[smp + '_names'], np.array(groups_names_subset)))[0]\n        if len(groups_ids) == 0:\n            sett.m(0, np.array(groups_names_subset),\n                   'invalid! specify valid groups_names for testing, one of',\n                   adata[smp + '_names'])\n            from sys import exit\n            exit(0)\n        groups_masks = groups_masks[groups_ids]\n    else:\n        groups_names_subset = groups_names\n    return groups_names_subset, groups_masks", "idx": 1801}
{"project": "Scanpy", "commit_id": "186_scanpy_1.9.0_get.py__set_obs_rep.py", "target": 0, "func": "def _set_obs_rep(adata, val, *, use_raw=False, layer=None, obsm=None, obsp=None):\n    \"\"\"\n    Set value for observation rep.\n    \"\"\"\n    is_layer = layer is not None\n    is_raw = use_raw is not False\n    is_obsm = obsm is not None\n    is_obsp = obsp is not None\n    choices_made = sum((is_layer, is_raw, is_obsm, is_obsp))\n    assert choices_made <= 1\n    if choices_made == 0:\n        adata.X = val\n    elif is_layer:\n        adata.layers[layer] = val\n    elif use_raw:\n        adata.raw.X = val\n    elif is_obsm:\n        adata.obsm[obsm] = val\n    elif is_obsp:\n        adata.obsp[obsp] = val\n    else:\n        assert False, (\n            \"That was unexpected. Please report this bug at:\\n\\n\\t\"", "idx": 1803}
{"project": "Scanpy", "commit_id": "884_scanpy_1.4.3_scanpy__settings.py__type_check.py", "target": 1, "func": "def _type_check(var, varname, types):\n    if not isinstance(types, list):\n        types = [types]\n    if not any(isinstance(var, t) for t in types):\n        type_names = [t.__name__ for t in types]\n        if len(types) == 1:\n            possible_types_str = type_names[0]\n        else:\n            possible_types_str = \"{} or {}\".format(\n                \", \".join(type_names[:-1]), type_names[-1]\n            )\n        raise TypeError(\"{} must be of type {}\".format(varname, possible_types_str))", "idx": 1809}
{"project": "Scanpy", "commit_id": "1097_scanpy_1.9.0_scanpy_plotting__anndata.py_scatter.py", "target": 1, "func": "def scatter(\n    adata: AnnData,\n    x: Optional[str] = None,\n    y: Optional[str] = None,\n    color: Union[str, Collection[str]] = None,\n    use_raw: Optional[bool] = None,\n    layers: Union[str, Collection[str]] = None,\n    sort_order: bool = True,\n    alpha: Optional[float] = None,\n    basis: Optional[_Basis] = None,\n    groups: Union[str, Iterable[str]] = None,\n    components: Union[str, Collection[str]] = None,\n    projection: Literal['2d', '3d'] = '2d',\n    legend_loc: str = 'right margin',\n    legend_fontsize: Union[int, float, _FontSize, None] = None,\n    legend_fontweight: Union[int, _FontWeight, None] = None,\n    legend_fontoutline: float = None,\n    color_map: Union[str, Colormap] = None,\n    palette: Union[Cycler, ListedColormap, ColorLike, Sequence[ColorLike]] = None,\n    frameon: Optional[bool] = None,\n    right_margin: Optional[float] = None,\n    left_margin: Optional[float] = None,\n    size: Union[int, float, None] = None,\n    title: Optional[str] = None,\n    show: Optional[bool] = None,\n    save: Union[str, bool, None] = None,\n    ax: Optional[Axes] = None,\n):\n    \"\"\"\\\n    Scatter plot along observations or variables axes.\n    Color the plot using annotations of observations (`.obs`), variables\n    (`.var`) or expression of genes (`.var_names`).\n    Parameters\n    ----------\n    adata\n        Annotated data matrix.\n    x\n        x coordinate.\n    y\n        y coordinate.\n    color\n        Keys for annotations of observations/cells or variables/genes,\n        or a hex color specification, e.g.,\n        `'ann1'`, `'#fe57a1'`, or `['ann1', 'ann2']`.\n    use_raw\n        Whether to use `raw` attribute of `adata`. Defaults to `True` if `.raw` is present.\n    layers\n        Use the `layers` attribute of `adata` if present: specify the layer for\n        `x`, `y` and `color`. If `layers` is a string, then it is expanded to\n        `(layers, layers, layers)`.\n    basis\n        String that denotes a plotting tool that computed coordinates.\n    {scatter_temp}\n    {show_save_ax}\n    Returns\n    -------\n    If `show==False` a :class:`~matplotlib.axes.Axes` or a list of it.\n    \"\"\"\n    args = locals()\n    if basis is not None:\n        return _scatter_obs(**args)\n    if x is None or y is None:\n        raise ValueError('Either provide a `basis` or `x` and `y`.')\n    if (\n        (x in adata.obs.keys() or x in adata.var.index)\n        and (y in adata.obs.keys() or y in adata.var.index)\n        and (color is None or color in adata.obs.keys() or color in adata.var.index)\n    ):\n        return _scatter_obs(**args)\n    if (\n        (x in adata.var.keys() or x in adata.obs.index)\n        and (y in adata.var.keys() or y in adata.obs.index)\n        and (color is None or color in adata.var.keys() or color in adata.obs.index)\n    ):\n        adata_T = adata.T\n        axs = _scatter_obs(\n            adata=adata_T,\n            **{name: val for name, val in args.items() if name != 'adata'},\n        )\n        # store .uns annotations that were added to the new adata object\n        adata.uns = adata_T.uns\n        return axs\n    raise ValueError(\n        '`x`, `y`, and potential `color` inputs must all '\n        'come from either `.obs` or `.var`'\n    )", "idx": 1823}
{"project": "Scanpy", "commit_id": "843_scanpy_1.9.0__paga.py_compute_transitions_old.py", "target": 0, "func": "def compute_transitions_old(self):\n        import igraph\n\n        g = _utils.get_igraph_from_adjacency(\n            self._adata.uns['velocyto_transitions'],\n            directed=True,\n        )\n        vc = igraph.VertexClustering(\n            g, membership=self._adata.obs[self._groups_key].cat.codes.values\n        )\n        # this stores all single-cell edges in the cluster graph\n        cg_full = vc.cluster_graph(combine_edges=False)\n        # this is the boolean version that simply counts edges in the clustered graph\n        g_bool = _utils.get_igraph_from_adjacency(\n            self._adata.uns['velocyto_transitions'].astype('bool'),\n            directed=True,\n        )\n        vc_bool = igraph.VertexClustering(\n            g_bool, membership=self._adata.obs[self._groups_key].cat.codes.values\n        )\n        cg_bool = vc_bool.cluster_graph(combine_edges='sum')  # collapsed version\n        transitions = _utils.get_sparse_from_igraph(cg_bool, weight_attr='weight')\n        total_n = self._neighbors.n_neighbors * np.array(vc_bool.sizes())\n        transitions_ttest = transitions.copy()\n        transitions_confidence = transitions.copy()\n        from scipy.stats import ttest_1samp\n\n        for i in range(transitions.shape[0]):\n            neighbors = transitions[i].nonzero()[1]\n            for j in neighbors:\n                forward = cg_full.es.select(_source=i, _target=j)['weight']\n                backward = cg_full.es.select(_source=j, _target=i)['weight']\n                # backward direction: add minus sign\n                values = np.array(list(forward) + list(-np.array(backward)))\n                # require some minimal number of observations\n                if len(values) < 5:\n                    transitions_ttest[i, j] = 0\n                    transitions_ttest[j, i] = 0\n                    transitions_confidence[i, j] = 0\n                    transitions_confidence[j, i] = 0\n                    continue\n                t, prob = ttest_1samp(values, 0.0)\n                if t > 0:\n                    # number of outgoing edges greater than number of ingoing edges\n                    # i.e., transition from i to j\n                    transitions_ttest[i, j] = -np.log10(max(prob, 1e-10))\n                    transitions_ttest[j, i] = 0\n                else:\n                    transitions_ttest[j, i] = -np.log10(max(prob, 1e-10))\n                    transitions_ttest[i, j] = 0\n                # geom_mean\n                geom_mean = np.sqrt(total_n[i] * total_n[j])\n                diff = (len(forward) - len(backward)) / geom_mean\n                if diff > 0:\n                    transitions_confidence[i, j] = diff\n                    transitions_confidence[j, i] = 0\n                else:\n                    transitions_confidence[j, i] = -diff\n                    transitions_confidence[i, j] = 0\n        transitions_ttest.eliminate_zeros()\n        transitions_confidence.eliminate_zeros()\n        # transpose in order to match convention of stochastic matrices\n        # entry ij means transition from j to i\n        self.transitions_ttest = transitions_ttest.T\n        self.transitions_confidence = transitions_confidence.T", "idx": 1825}
{"project": "Scanpy", "commit_id": "772_scanpy_1.3.7_scanpy_plotting__anndata.py_ranking.py", "target": 1, "func": "def ranking(adata, attr, keys, dictionary=None, indices=None,\n            labels=None, color='black', n_points=30,\n            log=False, show=None):\n    \"\"\"Plot rankings.\n    See, for example, how this is used in pl.pca_ranking.\n    Parameters\n    ----------\n    adata : AnnData\n        The data.\n    attr : {'var', 'obs', 'uns', 'varm', 'obsm'}\n        The attribute of AnnData that contains the score.\n    keys : str or list of str\n        The scores to look up an array from the attribute of adata.\n    Returns\n    -------\n    Returns matplotlib gridspec with access to the axes.\n    \"\"\"\n    if isinstance(keys, str) and indices is not None:\n        scores = getattr(adata, attr)[keys][:, indices]\n        keys = ['{}{}'.format(keys[:-1], i+1) for i in indices]\n    else:\n        if dictionary is None:\n            scores = getattr(adata, attr)[keys]\n        else:\n            scores = getattr(adata, attr)[dictionary][keys]\n    n_panels = len(keys) if isinstance(keys, list) else 1\n    if n_panels == 1: scores, keys = scores[:, None], [keys]\n    if log: scores = np.log(scores)\n    if labels is None:\n        labels = adata.var_names if attr in {'var', 'varm'} else np.arange(scores.shape[0]).astype(str)\n    if isinstance(labels, str):\n        labels = [labels + str(i+1) for i in range(scores.shape[0])]\n    if n_panels <= 5: n_rows, n_cols = 1, n_panels\n    else: n_rows, n_cols = 2, int(n_panels/2 + 0.5)\n    fig = pl.figure(figsize=(n_cols * rcParams['figure.figsize'][0],\n                             n_rows * rcParams['figure.figsize'][1]))\n    left, bottom = 0.2/n_cols, 0.13/n_rows\n    gs = gridspec.GridSpec(nrows=n_rows, ncols=n_cols, wspace=0.2,\n                           left=left, bottom=bottom,\n                           right=1-(n_cols-1)*left-0.01/n_cols,\n                           top=1-(n_rows-1)*bottom-0.1/n_rows)\n    for iscore, score in enumerate(scores.T):\n        pl.subplot(gs[iscore])\n        indices = np.argsort(score)[::-1][:n_points+1]\n        for ig, g in enumerate(indices):\n            pl.text(ig, score[g], labels[g], color=color,\n                    rotation='vertical', verticalalignment='bottom',\n                    horizontalalignment='center', fontsize=8)\n        pl.title(keys[iscore].replace('_', ' '))\n        if n_panels <= 5 or count > n_cols: pl.xlabel('ranking')\n        pl.xlim(-0.9, ig + 0.9)\n        score_min, score_max = np.min(score[indices]), np.max(score[indices])\n        pl.ylim((0.95 if score_min > 0 else 1.05) * score_min,\n                (1.05 if score_max > 0 else 0.95) * score_max)\n    if show == False: return gs", "idx": 1828}
{"project": "Scanpy", "commit_id": "753_scanpy_1.9.0_test_utils.py_test_is_constant.py", "target": 0, "func": "def test_is_constant(array_type):\n    from scanpy._utils import is_constant\n\n    constant_inds = [1, 3]\n    A = np.arange(20).reshape(5, 4)\n    A[constant_inds, :] = 10\n    A = array_type(A)\n    AT = array_type(A.T)\n\n    assert not is_constant(A)\n    assert not np.any(is_constant(A, axis=0))\n    np.testing.assert_array_equal(\n        [False, True, False, True, False], is_constant(A, axis=1)\n    )\n\n    assert not is_constant(AT)\n    assert not np.any(is_constant(AT, axis=1))\n    np.testing.assert_array_equal(\n        [False, True, False, True, False], is_constant(AT, axis=0)", "idx": 1829}
{"project": "Scanpy", "commit_id": "596_scanpy_1.0.4_scanpy_tools_paga.py_compute_transitions_coarse.py", "target": 1, "func": "def compute_transitions_coarse(self):\n    import igraph\n    g = utils.get_igraph_from_adjacency(\n        self._adata.uns['rna_velocity']['graph'], directed=True)\n    self.vc = igraph.VertexClustering(\n        g, membership=self._adata.obs[self._groups].cat.codes.values)\n    cg = self.vc.cluster_graph(combine_edges='sum')\n    self.transitions_coarse = utils.get_sparse_from_igraph(cg, weight_attr='weight')", "idx": 1832}
{"project": "Scanpy", "commit_id": "552_scanpy_0.4.4_scanpy_plotting_tools___init__.py_draw_graph.py", "target": 1, "func": "def draw_graph(\n        adata,\n        edges=False,\n        edges_width=0.1,\n        edges_color='grey',\n        layout=None,\n        color=None,\n        use_raw=True,\n        sort_order=True,\n        alpha=None,\n        groups=None,\n        components=None,\n        legend_loc='right margin',\n        legend_fontsize=None,\n        legend_fontweight=None,\n        color_map=None,\n        palette=None,\n        right_margin=None,\n        size=None,\n        title=None,\n        show=None,\n        save=None,\n        ax=None):\n    \"\"\"Scatter plot in graph-drawing basis.\n    Parameters\n    ----------\n    adata : :class:`~scanpy.api.AnnData`\n        Annotated data matrix.\n    edges : `bool`, optional (default: `False`)\n        Show edges.\n    edges_width : `float`, optional (default: 0.1)\n        Width of edges.\n    edges_color : matplotlib color, optional (default: 'grey')\n        Color of edges.\n    layout : {'fr', 'drl', ...}, optional (default: last computed)\n        One of the `draw_graph` layouts, see sc.tl.draw_graph. By default,\n        the last computed layout is taken.\n    color : string or list of strings, optional (default: None)\n        Keys for observation/cell annotation either as list `[\"ann1\", \"ann2\"]` or\n        string `\"ann1,ann2,...\"`.\n    use_raw : `bool`, optional (default: `True`)\n        Use `raw` attribute of `adata` if present.\n    sort_order : `bool`, optional (default: `True`)\n        For continuous annotations used as color parameter, plot data points\n        with higher values on top of others.\n    groups : str, optional (default: all groups)\n        Restrict to a few categories in categorical observation annotation.\n    components : str or list of str, optional (default: '1,2')\n         String of the form '1,2' or ['1,2', '2,3'].\n    legend_loc : str, optional (default: 'right margin')\n         Location of legend, either 'on data', 'right margin' or valid keywords\n         for matplotlib.legend.\n    legend_fontsize : int (default: None)\n         Legend font size.\n    color_map : str (default: `matplotlib.rcParams['image.cmap']`)\n         String denoting matplotlib color map.\n    palette : list of str (default: None)\n         Colors to use for plotting groups (categorical annotation).\n    right_margin : float or list of floats (default: None)\n         Adjust the width of the space right of each plotting panel.\n    size : float (default: None)\n         Point size.\n    title : str, optional (default: None)\n         Provide title for panels either as `[\"title1\", \"title2\", ...]` or\n         `\"title1,title2,...\"`.\n    show : bool, optional (default: None)\n         Show the plot, do not return axis.\n    save : `bool` or `str`, optional (default: `None`)\n        If `True` or a `str`, save the figure. A string is appended to the\n        default filename. Infer the filetype if ending on \\{'.pdf', '.png', '.svg'\\}.\n    ax : matplotlib.Axes\n         A matplotlib axes object.\n    Returns\n    -------\n    If `show==False`, a list of `matplotlib.Axis` objects. Every second element\n    corresponds to the 'right margin' drawing area for color bars and legends.\n    \"\"\"\n    if layout is None: layout = str(adata.uns['draw_graph_params']['layout'])\n    basis = 'draw_graph_' + layout\n    if 'X_' + basis not in adata.obsm_keys():\n        raise ValueError('Did not find {} in adata.obs. Did you compute layout {}?'\n                         .format('draw_graph_' + layout, layout))\n    axs = scatter(\n        adata,\n        basis=basis,\n        color=color,\n        use_raw=use_raw,\n        sort_order=sort_order,\n        alpha=alpha,\n        groups=groups,\n        components=components,\n        projection='2d',\n        legend_loc=legend_loc,\n        legend_fontsize=legend_fontsize,\n        legend_fontweight=legend_fontweight,\n        color_map=color_map,\n        palette=palette,\n        right_margin=right_margin,\n        size=size,\n        title=title,\n        show=False,\n        save=False,\n        ax=ax)\n    if edges:\n        for ax in axs[::2]:\n            g = nx.Graph(adata.uns['neighbors']['connectivities'])\n            edge_collection = nx.draw_networkx_edges(\n                g, adata.obsm['X_' + basis],\n                ax=ax, width=edges_width, edge_color=edges_color)\n            edge_collection.set_zorder(-2)\n    utils.savefig_or_show('scatter' if basis is None else basis, show=show, save=save)\n    if show == False: return axs", "idx": 1852}
{"project": "Scanpy", "commit_id": "184_scanpy_1.9.0_get.py_var_df.py", "target": 0, "func": "def var_df(\n    adata: AnnData,\n    keys: Iterable[str] = (),\n    varm_keys: Iterable[Tuple[str, int]] = (),\n    *,\n    layer: str = None,\n) -> pd.DataFrame:\n    \"\"\"\\\n    Return values for observations in adata.\n\n    Params\n    ------\n    adata\n        AnnData object to get values from.\n    keys\n        Keys from either `.obs_names`, or `.var.columns`.\n    varm_keys\n        Tuple of `(key from varm, column index of varm[key])`.\n    layer\n        Layer of `adata` to use as expression values.\n\n    Returns\n    -------\n    A dataframe with `adata.var_names` as index, and values specified by `keys`\n    and `varm_keys`.\n    \"\"\"\n    # Argument handling\n    var_cols, obs_idx_keys, _ = _check_indices(adata.var, adata.obs_names, \"var\", keys)\n\n    # initialize df\n    df = pd.DataFrame(index=adata.var.index)\n\n    if len(obs_idx_keys) > 0:\n        matrix = _get_array_values(\n            _get_obs_rep(adata, layer=layer),\n            adata.obs_names,\n            obs_idx_keys,\n            axis=0,\n            backed=adata.isbacked,\n        ).T\n        df = pd.concat(\n            [df, pd.DataFrame(matrix, columns=obs_idx_keys, index=adata.var_names)],\n            axis=1,\n        )\n\n    # add obs values\n    if len(var_cols) > 0:\n        df = pd.concat([df, adata.var[var_cols]], axis=1)\n\n    # reorder columns to given order\n    if keys:\n        df = df[keys]\n\n    for k, idx in varm_keys:\n        added_k = f\"{k}-{idx}\"\n        val = adata.varm[k]\n        if isinstance(val, np.ndarray):\n            df[added_k] = np.ravel(val[:, idx])\n        elif isinstance(val, spmatrix):\n            df[added_k] = np.ravel(val[:, idx].toarray())\n        elif isinstance(val, pd.DataFrame):\n            df[added_k] = val.loc[:, idx]\n    return df", "idx": 1859}
{"project": "Scanpy", "commit_id": "741_scanpy_1.3.2_scanpy_plotting_tools_paga.py_paga_path.py", "target": 1, "func": "def paga_path(\n        adata,\n        nodes,\n        keys,\n        use_raw=True,\n        annotations=['dpt_pseudotime'],\n        color_map=None,\n        color_maps_annotations={'dpt_pseudotime': 'Greys'},\n        palette_groups=None,\n        n_avg=1,\n        groups_key=None,\n        xlim=[None, None],\n        title=None,\n        left_margin=None,\n        ytick_fontsize=None,\n        title_fontsize=None,\n        show_node_names=True,\n        show_yticks=True,\n        show_colorbar=True,\n        legend_fontsize=None,\n        legend_fontweight=None,\n        normalize_to_zero_one=False,\n        as_heatmap=True,\n        return_data=False,\n        show=None,\n        save=None,\n        ax=None):\n    \"\"\"Gene expression and annotation changes along paths in the abstracted graph.\n    Parameters\n    ----------\n    adata : :class:`~anndata.AnnData`\n        An annotated data matrix.\n    nodes : list of group names or their category indices\n        A path through nodes of the abstracted graph, that is, names or indices\n        (within `.categories`) of groups that have been used to run PAGA.\n    keys : list of str\n        Either variables in `adata.var_names` or annotations in\n        `adata.obs`. They are plotted using `color_map`.\n    use_raw : `bool`, optional (default: `True`)\n        Use `adata.raw` for retrieving gene expressions if it has been set.\n    annotations : list of annotations, optional (default: ['dpt_pseudotime'])\n        Plot these keys with `color_maps_annotations`. Need to be keys for\n        `adata.obs`.\n    color_map : color map for plotting keys or `None`, optional (default: `None`)\n        Matplotlib colormap.\n    color_maps_annotations : dict storing color maps or `None`, optional (default: {'dpt_pseudotime': 'Greys'})\n        Color maps for plotting the annotations. Keys of the dictionary must\n        appear in `annotations`.\n    palette_groups : list of colors or `None`, optional (default: `None`)\n        Ususally, use the same `sc.pl.palettes...` as used for coloring the\n        abstracted graph.\n    n_avg : `int`, optional (default: 1)\n        Number of data points to include in computation of running average.\n    groups_key : `str`, optional (default: `None`)\n        Key of the grouping used to run PAGA. If `None`, defaults to\n        `adata.uns['paga']['groups']`.\n    as_heatmap : `bool`, optional (default: `True`)\n        Plot the timeseries as heatmap. If not plotting as heatmap,\n        `annotations` have no effect.\n    show_node_names : `bool`, optional (default: `True`)\n        Plot the node names on the nodes bar.\n    show_colorbar : `bool`, optional (default: `True`)\n        Show the colorbar.\n    show_yticks : `bool`, optional (default: `True`)\n        Show the y ticks.\n    normalize_to_zero_one : `bool`, optional (default: `True`)\n        Shift and scale the running average to [0, 1] per gene.\n    return_data : `bool`, optional (default: `False`)\n        Return the timeseries data in addition to the axes if `True`.\n    show : `bool`, optional (default: `None`)\n         Show the plot, do not return axis.\n    save : `bool` or `str`, optional (default: `None`)\n        If `True` or a `str`, save the figure. A string is appended to the\n        default filename. Infer the filetype if ending on \\{'.pdf', '.png', '.svg'\\}.\n    ax : `matplotlib.Axes`\n         A matplotlib axes object.\n    Returns\n    -------\n    A `matplotlib.Axes`, if `ax` is `None`, else `None`. If `return_data`,\n    return the timeseries data in addition to an axes.\n    \"\"\"\n    ax_was_none = ax is None\n    if groups_key is None:\n        if 'groups' not in adata.uns['paga']:\n            raise KeyError(\n                'Pass the key of the grouping with which you ran PAGA, '\n                'using the parameter `groups_key`.')\n        groups_key = adata.uns['paga']['groups']\n    groups_names = adata.obs[groups_key].cat.categories\n\n    if palette_groups is None:\n        utils.add_colors_for_categorical_sample_annotation(adata, groups_key)\n        palette_groups = adata.uns[groups_key + '_colors']\n\n    ax = pl.gca() if ax is None else ax\n    from matplotlib import transforms\n    trans = transforms.blended_transform_factory(\n        ax.transData, ax.transAxes)\n    X = []\n    x_tick_locs = [0]\n    x_tick_labels = []\n    groups = []\n    anno_dict = {anno: [] for anno in annotations}\n    if isinstance(nodes[0], str):\n        nodes_ints = []\n        groups_names_set = set(groups_names)\n        for node in nodes:\n            if node not in groups_names_set:\n                raise ValueError(\n                    'Each node/group needs to be one of {} (`groups_key`=\\'{}\\') not \\'{}\\'.'\n                    .format(groups_names.tolist(), groups_key, node))\n            nodes_ints.append(groups_names.get_loc(node))\n        nodes_strs = nodes\n    else:\n        nodes_ints = nodes\n        nodes_strs = [groups_names[node] for node in nodes]\n    adata_X = adata\n    if use_raw and adata.raw is not None:\n        adata_X = adata.raw\n    for ikey, key in enumerate(keys):\n        x = []\n        for igroup, group in enumerate(nodes_ints):\n            idcs = np.arange(adata.n_obs)[\n                adata.obs[groups_key].values == nodes_strs[igroup]]\n            if len(idcs) == 0:\n                raise ValueError(\n                    'Did not find data points that match '\n                    '`adata.obs[{}].values == str({})`.'\n                    'Check whether adata.obs[{}] actually contains what you expect.'\n                    .format(groups_key, group, groups_key))\n            idcs_group = np.argsort(adata.obs['dpt_pseudotime'].values[\n                adata.obs[groups_key].values == nodes_strs[igroup]])\n            idcs = idcs[idcs_group]\n            if key in adata.obs_keys(): x += list(adata.obs[key].values[idcs])\n            else: x += list(adata_X[:, key].X[idcs])\n            if ikey == 0:\n                groups += [group for i in range(len(idcs))]\n                x_tick_locs.append(len(x))\n                for anno in annotations:\n                    series = adata.obs[anno]\n                    if is_categorical_dtype(series): series = series.cat.codes\n                    anno_dict[anno] += list(series.values[idcs])\n        if n_avg > 1:\n            old_len_x = len(x)\n            x = moving_average(x)\n            if ikey == 0:\n                for key in annotations:\n                    if not isinstance(anno_dict[key][0], str):\n                        anno_dict[key] = moving_average(anno_dict[key])\n        if normalize_to_zero_one:\n            x -= np.min(x)\n            x /= np.max(x)\n        X.append(x)\n        if not as_heatmap:\n            ax.plot(x[xlim[0]:xlim[1]], label=key)\n        if ikey == 0:\n            for igroup, group in enumerate(nodes):\n                if len(groups_names) > 0 and group not in groups_names:\n                    label = groups_names[group]\n                else:\n                    label = group\n                x_tick_labels.append(label)\n    X = np.array(X)\n    if as_heatmap:\n        img = ax.imshow(X, aspect='auto', interpolation='nearest',\n                        cmap=color_map)\n        if show_yticks:\n            ax.set_yticks(range(len(X)))\n            ax.set_yticklabels(keys, fontsize=ytick_fontsize)\n        else:\n            ax.set_yticks([])\n        ax.set_frame_on(False)\n        ax.set_xticks([])\n        ax.tick_params(axis='both', which='both', length=0)\n        ax.grid(False)\n        if show_colorbar:\n            pl.colorbar(img, ax=ax)\n        left_margin = 0.2 if left_margin is None else left_margin\n        pl.subplots_adjust(left=left_margin)\n    else:\n        left_margin = 0.4 if left_margin is None else left_margin\n        if len(keys) > 1:\n            pl.legend(frameon=False, loc='center left',\n                      bbox_to_anchor=(-left_margin, 0.5),\n                      fontsize=legend_fontsize)\n    xlabel = groups_key\n    if not as_heatmap:\n        ax.set_xlabel(xlabel)\n        pl.yticks([])\n        if len(keys) == 1: pl.ylabel(keys[0] + ' (a.u.)')\n    else:\n        import matplotlib.colors\n        # groups bar\n        ax_bounds = ax.get_position().bounds\n        groups_axis = pl.axes([ax_bounds[0],\n                               ax_bounds[1] - ax_bounds[3] / len(keys),\n                               ax_bounds[2],\n                               ax_bounds[3] / len(keys)])\n        groups = np.array(groups)[None, :]\n        groups_axis.imshow(groups, aspect='auto',\n                           interpolation=\"nearest\",\n                           cmap=matplotlib.colors.ListedColormap(\n                               # the following line doesn't work because of normalization\n                               # adata.uns['paga_groups_colors'])\n                               palette_groups[np.min(groups).astype(int):],\n                               N=int(np.max(groups)+1-np.min(groups))))\n        if show_yticks:\n            groups_axis.set_yticklabels(['', xlabel, ''], fontsize=ytick_fontsize)\n        else:\n            groups_axis.set_yticks([])\n        groups_axis.set_frame_on(False)\n        if show_node_names:\n            ypos = (groups_axis.get_ylim()[1] + groups_axis.get_ylim()[0])/2\n            x_tick_locs = sc_utils.moving_average(x_tick_locs, n=2)\n            for ilabel, label in enumerate(x_tick_labels):\n                groups_axis.text(x_tick_locs[ilabel], ypos, x_tick_labels[ilabel],\n                                 fontdict={'horizontalalignment': 'center',\n                                           'verticalalignment': 'center'})\n        groups_axis.set_xticks([])\n        groups_axis.grid(False)\n        groups_axis.tick_params(axis='both', which='both', length=0)\n        # further annotations\n        y_shift = ax_bounds[3] / len(keys)\n        for ianno, anno in enumerate(annotations):\n            if ianno > 0: y_shift = ax_bounds[3] / len(keys) / 2\n            anno_axis = pl.axes([ax_bounds[0],\n                                 ax_bounds[1] - (ianno+2) * y_shift,\n                                 ax_bounds[2],\n                                 y_shift])\n            arr = np.array(anno_dict[anno])[None, :]\n            if anno not in color_maps_annotations:\n                color_map_anno = ('Vega10' if is_categorical_dtype(adata.obs[anno])\n                                  else 'Greys')\n            else:\n                color_map_anno = color_maps_annotations[anno]\n            img = anno_axis.imshow(arr, aspect='auto',\n                                   interpolation='nearest',\n                                   cmap=color_map_anno)\n            if show_yticks:\n                anno_axis.set_yticklabels(['', anno, ''],\n                                          fontsize=ytick_fontsize)\n                anno_axis.tick_params(axis='both', which='both', length=0)\n            else:\n                anno_axis.set_yticks([])\n            anno_axis.set_frame_on(False)\n            anno_axis.set_xticks([])\n            anno_axis.grid(False)\n    if title is not None: ax.set_title(title, fontsize=title_fontsize)\n    if show is None and not ax_was_none: show = False\n    else: show = settings.autoshow if show is None else show\n    utils.savefig_or_show('paga_path', show=show, save=save)\n    if return_data:\n        df = pd.DataFrame(data=X.T, columns=keys)\n        df['groups'] = moving_average(groups)  # groups is without moving average, yet\n        if 'dpt_pseudotime' in anno_dict:\n            df['distance'] = anno_dict['dpt_pseudotime'].T\n        return ax, df if ax_was_none and show == False else df\n    else:\n        return ax if ax_was_none and show == False else None", "idx": 1860}
{"project": "Scanpy", "commit_id": "314_scanpy_0.1_scanpy_plotting___init__.py_aga_timeseries.py", "target": 1, "func": "def aga_timeseries(\n        adata,\n        nodes=[0],\n        keys=[0],\n        xlim=[None, None],\n        n_avg=1,\n        left_margin=0.4,\n        show_left_y_ticks=None,\n        show_nodes_twin=True,\n        legend_fontsize=None,\n        ax=None,\n        show=None):\n    ax_was_none = ax is None\n    if show_left_y_ticks is None:\n        show_left_y_ticks = False if show_nodes_twin else True\n    orig_node_names = []\n    if 'aga_groups_names_original' in adata.add:\n        orig_node_names = adata.add['aga_groups_names_original']\n    else:\n        logg.warn('did not find field \"aga_groups_names_original\" in adata.add, '\n                  'using aga_group integer ids instead')\n\n    ax = pl.gca()\n    from matplotlib import transforms\n    trans = transforms.blended_transform_factory(\n        ax.transData, ax.transAxes)\n    for ikey, key in enumerate(keys):\n        x = []\n        for igroup, group in enumerate(nodes):\n            if ikey == 0:\n                if len(orig_node_names) > 0 and group not in orig_node_names:\n                    label = orig_node_names[int(group)]\n                else:\n                    label = group\n                pl.text(len(x), -0.05*(igroup+1), label, transform=trans)\n            idcs = np.arange(adata.n_smps)[adata.smp['aga_groups'] == str(group)]\n            idcs_group = np.argsort(adata.smp['aga_pseudotime'][adata.smp['aga_groups'] == str(group)])\n            idcs = idcs[idcs_group]\n            if key in adata.smp_keys(): x += list(adata.smp[key][idcs])\n            else: x += list(adata[:, key].X[idcs])\n        if n_avg > 1: x = moving_average(x)\n        pl.plot(x[xlim[0]:xlim[1]], label=key)\n    pl.legend(frameon=False, loc='center left',\n              bbox_to_anchor=(-left_margin, 0.5),\n              fontsize=legend_fontsize)\n    pl.xticks([])\n    if show_left_y_ticks:\n        utils.pimp_axis(pl.gca().get_yaxis())\n        pl.ylabel('as indicated on legend')\n    else:\n        pl.yticks([])\n        pl.ylabel('as indicated on legend (a.u.)')\n    if show_nodes_twin:\n        pl.twinx()\n        x = []\n        for g in nodes:\n            x += list(adata.smp['aga_groups'][adata.smp['aga_groups'] == str(g)].astype(int))\n        if n_avg > 1: x = moving_average(x)\n        pl.plot(x[xlim[0]:xlim[1]], '--', color='black')\n        label = 'aga groups' + (' / original groups' if len(orig_node_names) > 0 else '')\n        pl.ylabel(label)\n    if show is None and not ax_was_none: show = False\n    else: show = sett.autoshow if show is None else show\n    savefig_or_show('aga_timeseries', show)\n    return ax if ax_was_none else None", "idx": 1861}
{"project": "Scanpy", "commit_id": "64_scanpy_1.9.0_readwrite.py__read.py", "target": 0, "func": "def _read(\n    filename: Path,\n    backed=None,\n    sheet=None,\n    ext=None,\n    delimiter=None,\n    first_column_names=None,\n    backup_url=None,\n    cache=False,\n    cache_compression=None,\n    suppress_cache_warning=False,\n    **kwargs,\n):\n    if ext is not None and ext not in avail_exts:\n        raise ValueError(\n            'Please provide one of the available extensions.\\n' f'{avail_exts}'\n        )\n    else:\n        ext = is_valid_filename(filename, return_ext=True)\n    is_present = _check_datafile_present_and_download(filename, backup_url=backup_url)\n    if not is_present:\n        logg.debug(f'... did not find original file {filename}')\n    # read hdf5 files\n    if ext in {'h5', 'h5ad'}:\n        if sheet is None:\n            return read_h5ad(filename, backed=backed)\n        else:\n            logg.debug(f'reading sheet {sheet} from file {filename}')\n            return read_hdf(filename, sheet)\n    # read other file types\n    path_cache = settings.cachedir / _slugify(filename).replace(\n        '.' + ext, '.h5ad'\n    )  # type: Path\n    if path_cache.suffix in {'.gz', '.bz2'}:\n        path_cache = path_cache.with_suffix('')\n    if cache and path_cache.is_file():\n        logg.info(f'... reading from cache file {path_cache}')\n        return read_h5ad(path_cache)\n\n    if not is_present:\n        raise FileNotFoundError(f'Did not find file {filename}.')\n    logg.debug(f'reading {filename}')\n    if not cache and not suppress_cache_warning:\n        logg.hint(\n            'This might be very slow. Consider passing `cache=True`, '\n            'which enables much faster reading from a cache file.'\n        )\n    # do the actual reading\n    if ext == 'xlsx' or ext == 'xls':\n        if sheet is None:\n            raise ValueError(\"Provide `sheet` parameter when reading '.xlsx' files.\")\n        else:\n            adata = read_excel(filename, sheet)\n    elif ext in {'mtx', 'mtx.gz'}:\n        adata = read_mtx(filename)\n    elif ext == 'csv':\n        adata = read_csv(filename, first_column_names=first_column_names)\n    elif ext in {'txt', 'tab', 'data', 'tsv'}:\n        if ext == 'data':\n            logg.hint(\n                \"... assuming '.data' means tab or white-space \" 'separated text file',\n            )\n            logg.hint('change this by passing `ext` to sc.read')\n        adata = read_text(filename, delimiter, first_column_names)\n    elif ext == 'soft.gz':\n        adata = _read_softgz(filename)\n    elif ext == 'loom':\n        adata = read_loom(filename=filename, **kwargs)\n    else:\n        raise ValueError(f'Unknown extension {ext}.')\n    if cache:\n        logg.info(\n            f'... writing an {settings.file_format_data} '\n            'cache file to speedup reading next time'\n        )\n        if cache_compression is _empty:\n            cache_compression = settings.cache_compression\n        if not path_cache.parent.is_dir():\n            path_cache.parent.mkdir(parents=True)\n        # write for faster reading when calling the next time\n        adata.write(path_cache, compression=cache_compression)\n    return adata", "idx": 1865}
{"project": "Scanpy", "commit_id": "929_scanpy_1.9.0___init__.py__fallback_to_uns.py", "target": 0, "func": "def _fallback_to_uns(dct, conns, dists, conns_key, dists_key):\n    if conns is None and conns_key in dct:\n        conns = dct[conns_key]\n    if dists is None and dists_key in dct:\n        dists = dct[dists_key]\n\n    return conns, dists", "idx": 1867}
{"project": "Scanpy", "commit_id": "1092_scanpy_1.9.0_scanpy_tools__score_genes.py__sparse_nanmean.py", "target": 1, "func": "def _sparse_nanmean(X, axis):\n    \"\"\"\n    np.nanmean equivalent for sparse matrices\n    \"\"\"\n    if not issparse(X):\n        raise TypeError(\"X must be a sparse matrix\")\n    # count the number of nan elements per row/column (dep. on axis)\n    Z = X.copy()\n    Z.data = np.isnan(Z.data)\n    Z.eliminate_zeros()\n    n_elements = Z.shape[axis] - Z.sum(axis)\n    # set the nans to 0, so that a normal .sum() works\n    Y = X.copy()\n    Y.data[np.isnan(Y.data)] = 0\n    Y.eliminate_zeros()\n\n    # the average\n    s = Y.sum(axis)\n    m = s / n_elements.astype(\n        'float32'\n    )  # if we dont cast the int32 to float32, this will result in float64...\n\n    return m", "idx": 1869}
{"project": "Scanpy", "commit_id": "1046_scanpy_1.6.1_scanpy_tools__marker_gene_overlap.py_marker_gene_overlap.py", "target": 1, "func": "def marker_gene_overlap(\n    adata: AnnData,\n    reference_markers: Union[Dict[str, set], Dict[str, list]],\n    *,\n    key: str = 'rank_genes_groups',\n    method: _Method = 'overlap_count',\n    normalize: Optional[Literal['reference', 'data']] = None,\n    top_n_markers: Optional[int] = None,\n    adj_pval_threshold: Optional[float] = None,\n    key_added: str = 'marker_gene_overlap',\n    inplace: bool = False,\n):\n    \"\"\"\\\n    Calculate an overlap score between data-deriven marker genes and\n    provided markers\n    Marker gene overlap scores can be quoted as overlap counts, overlap\n    coefficients, or jaccard indices. The method returns a pandas dataframe\n    which can be used to annotate clusters based on marker gene overlaps.\n    This function was written by Malte Luecken.\n    Parameters\n    ----------\n    adata\n        The annotated data matrix.\n    reference_markers\n        A marker gene dictionary object. Keys should be strings with the\n        cell identity name and values are sets or lists of strings which match\n        format of `adata.var_name`.\n    key\n        The key in `adata.uns` where the rank_genes_groups output is stored.\n        By default this is `'rank_genes_groups'`.\n    method\n        (default: `overlap_count`)\n        Method to calculate marker gene overlap. `'overlap_count'` uses the\n        intersection of the gene set, `'overlap_coef'` uses the overlap\n        coefficient, and `'jaccard'` uses the Jaccard index.\n    normalize\n        Normalization option for the marker gene overlap output. This parameter\n        can only be set when `method` is set to `'overlap_count'`. `'reference'`\n        normalizes the data by the total number of marker genes given in the\n        reference annotation per group. `'data'` normalizes the data by the\n        total number of marker genes used for each cluster.\n    top_n_markers\n        The number of top data-derived marker genes to use. By default all\n        calculated marker genes are used. If `adj_pval_threshold` is set along\n        with `top_n_markers`, then `adj_pval_threshold` is ignored.\n    adj_pval_threshold\n        A significance threshold on the adjusted p-values to select marker\n        genes. This can only be used when adjusted p-values are calculated by\n        `sc.tl.rank_genes_groups()`. If `adj_pval_threshold` is set along with\n        `top_n_markers`, then `adj_pval_threshold` is ignored.\n    key_added\n        Name of the `.uns` field that will contain the marker overlap scores.\n    inplace\n        Return a marker gene dataframe or store it inplace in `adata.uns`.\n    Returns\n    -------\n    A pandas dataframe with the marker gene overlap scores if `inplace=False`.\n    For `inplace=True` `adata.uns` is updated with an additional field\n    specified by the `key_added` parameter (default = 'marker_gene_overlap').\n    Examples\n    --------\n    >>> import scanpy as sc\n    >>> adata = sc.datasets.pbmc68k_reduced()\n    >>> sc.pp.pca(adata, svd_solver='arpack')\n    >>> sc.pp.neighbors(adata)\n    >>> sc.tl.louvain(adata)\n    >>> sc.tl.rank_genes_groups(adata, groupby='louvain')\n    >>> marker_genes = {\n    ...     'CD4 T cells': {'IL7R'},\n    ...     'CD14+ Monocytes': {'CD14', 'LYZ'},\n    ...     'B cells': {'MS4A1'},\n    ...     'CD8 T cells': {'CD8A'},\n    ...     'NK cells': {'GNLY', 'NKG7'},\n    ...     'FCGR3A+ Monocytes': {'FCGR3A', 'MS4A7'},\n    ...     'Dendritic Cells': {'FCER1A', 'CST3'},\n    ...     'Megakaryocytes': {'PPBP'}\n    ... }\n    >>> marker_matches = sc.tl.marker_gene_overlap(adata, marker_genes)\n    \"\"\"\n    # Test user inputs\n    if inplace:\n        raise NotImplementedError(\n            'Writing Pandas dataframes to h5ad is currently under development.'\n            '\\nPlease use `inplace=False`.'\n        )\n    if key not in adata.uns:\n        raise ValueError(\n            'Could not find marker gene data. '\n            'Please run `sc.tl.rank_genes_groups()` first.'\n        )\n    avail_methods = {'overlap_count', 'overlap_coef', 'jaccard', 'enrich'}\n    if method not in avail_methods:\n        raise ValueError(f'Method must be one of {avail_methods}.')\n    if normalize == 'None':\n        normalize = None\n    avail_norm = {'reference', 'data', None}\n    if normalize not in avail_norm:\n        raise ValueError(f'Normalize must be one of {avail_norm}.')\n    if normalize is not None and method != 'overlap_count':\n        raise ValueError('Can only normalize with method=`overlap_count`.')\n    if not all(isinstance(val, cabc.Set) for val in reference_markers.values()):\n        try:\n            reference_markers = {\n                key: set(val) for key, val in reference_markers.items()\n            }\n        except Exception:\n            raise ValueError(\n                'Please ensure that `reference_markers` contains '\n                'sets or lists of markers as values.'\n            )\n    if adj_pval_threshold is not None:\n        if 'pvals_adj' not in adata.uns[key]:\n            raise ValueError(\n                'Could not find adjusted p-value data. '\n                'Please run `sc.tl.rank_genes_groups()` with a '\n                'method that outputs adjusted p-values.'\n            )\n        if adj_pval_threshold < 0:\n            logg.warning(\n                '`adj_pval_threshold` was set below 0. Threshold will be set to 0.'\n            )\n            adj_pval_threshold = 0\n        elif adj_pval_threshold > 1:\n            logg.warning(\n                '`adj_pval_threshold` was set above 1. Threshold will be set to 1.'\n            )\n            adj_pval_threshold = 1\n        if top_n_markers is not None:\n            logg.warning(\n                'Both `adj_pval_threshold` and `top_n_markers` is set. '\n                '`adj_pval_threshold` will be ignored.'\n            )\n    if top_n_markers is not None and top_n_markers < 1:\n        logg.warning(\n            '`top_n_markers` was set below 1. `top_n_markers` will be set to 1.'\n        )\n        top_n_markers = 1\n    # Get data-derived marker genes in a dictionary of sets\n    data_markers = dict()\n    cluster_ids = adata.uns[key]['names'].dtype.names\n    for group in cluster_ids:\n        if top_n_markers is not None:\n            n_genes = min(top_n_markers, adata.uns[key]['names'].shape[0])\n            data_markers[group] = set(adata.uns[key]['names'][group][:n_genes])\n        elif adj_pval_threshold is not None:\n            n_genes = (adata.uns[key]['pvals_adj'][group] < adj_pval_threshold).sum()\n            data_markers[group] = set(adata.uns[key]['names'][group][:n_genes])\n            if n_genes == 0:\n                logg.warning(\n                    'No marker genes passed the significance threshold of '\n                    f'{adj_pval_threshold} for cluster {group!r}.'\n                )\n        else:\n            data_markers[group] = set(adata.uns[key]['names'][group])\n\n    # Find overlaps\n    if method == 'overlap_count':\n        marker_match = _calc_overlap_count(reference_markers, data_markers)\n        if normalize == 'reference':\n            # Ensure rows sum to 1\n            ref_lengths = np.array(\n                [len(reference_markers[m_group]) for m_group in reference_markers]\n            )\n            marker_match = marker_match / ref_lengths[:, np.newaxis]\n            marker_match = np.nan_to_num(marker_match)\n        elif normalize == 'data':\n            # Ensure columns sum to 1\n            data_lengths = np.array(\n                [len(data_markers[dat_group]) for dat_group in data_markers]\n            )\n            marker_match = marker_match / data_lengths\n            marker_match = np.nan_to_num(marker_match)\n    elif method == 'overlap_coef':\n        marker_match = _calc_overlap_coef(reference_markers, data_markers)\n    elif method == 'jaccard':\n        marker_match = _calc_jaccard(reference_markers, data_markers)\n    # Note:\n    # Could add an 'enrich' option here\n    # (fisher's exact test or hypergeometric test),\n    # but that would require knowledge of the size of the space from which\n    # the reference marker gene set was taken.\n    # This is at best approximately known.\n    # Create a pandas dataframe with the results\n    marker_groups = list(reference_markers.keys())\n    clusters = list(cluster_ids)\n    marker_matching_df = pd.DataFrame(\n        marker_match, index=marker_groups, columns=clusters\n    )\n    # Store the results\n    if inplace:\n        adata.uns[key_added] = marker_matching_df\n        logg.hint(f'added\\n    {key_added!r}, marker overlap scores (adata.uns)')\n    else:\n        return marker_matching_df", "idx": 1877}
{"project": "Scanpy", "commit_id": "199_scanpy_1.9.0__morans_i.py_morans_i.py", "target": 0, "func": "def morans_i(\n    adata: AnnData,\n    *,\n    vals: Optional[Union[np.ndarray, sparse.spmatrix]] = None,\n    use_graph: Optional[str] = None,\n    layer: Optional[str] = None,\n    obsm: Optional[str] = None,\n    obsp: Optional[str] = None,\n    use_raw: bool = False,\n) -> Union[np.ndarray, float]:\n    r\"\"\"\n    Calculate Moran\u2019s I Global Autocorrelation Statistic.\n\n    Moran\u2019s I is a global autocorrelation statistic for some measure on a graph. It is commonly used in\n    spatial data analysis to assess autocorrelation on a 2D grid. It is closely related to Geary's C,\n    but not identical. More info can be found `here <https://en.wikipedia.org/wiki/Moran%27s_I>`_.\n\n    .. math::\n\n        I =\n            \\frac{\n                N \\sum_{i, j} w_{i, j} z_{i} z_{j}\n            }{\n                S_{0} \\sum_{i} z_{i}^{2}\n            }\n\n    Params\n    ------\n    adata\n    vals\n        Values to calculate Moran's I for. If this is two dimensional, should\n        be of shape `(n_features, n_cells)`. Otherwise should be of shape\n        `(n_cells,)`. This matrix can be selected from elements of the anndata\n        object by using key word arguments: `layer`, `obsm`, `obsp`, or\n        `use_raw`.\n    use_graph\n        Key to use for graph in anndata object. If not provided, default\n        neighbors connectivities will be used instead.\n    layer\n        Key for `adata.layers` to choose `vals`.\n    obsm\n        Key for `adata.obsm` to choose `vals`.\n    obsp\n        Key for `adata.obsp` to choose `vals`.\n    use_raw\n        Whether to use `adata.raw.X` for `vals`.\n\n\n    This function can also be called on the graph and values directly. In this case\n    the signature looks like:\n\n    Params\n    ------\n    g\n        The graph\n    vals\n        The values\n\n\n    See the examples for more info.\n\n    Returns\n    -------\n    If vals is two dimensional, returns a 1 dimensional ndarray array. Returns\n    a scalar if `vals` is 1d.\n\n\n    Examples\n    --------\n\n    Calculate Morans I for each components of a dimensionality reduction:\n\n    .. code:: python\n\n        import scanpy as sc, numpy as np\n\n        pbmc = sc.datasets.pbmc68k_processed()\n        pc_c = sc.metrics.morans_i(pbmc, obsm=\"X_pca\")\n\n\n    It's equivalent to call the function directly on the underlying arrays:\n\n    .. code:: python\n\n        alt = sc.metrics.morans_i(pbmc.obsp[\"connectivities\"], pbmc.obsm[\"X_pca\"].T)\n        np.testing.assert_array_equal(pc_c, alt)\n    \"\"\"\n    if use_graph is None:\n        # Fix for anndata<0.7\n        if hasattr(adata, \"obsp\") and \"connectivities\" in adata.obsp:\n            g = adata.obsp[\"connectivities\"]\n        elif \"neighbors\" in adata.uns:\n            g = adata.uns[\"neighbors\"][\"connectivities\"]\n        else:\n            raise ValueError(\"Must run neighbors first.\")\n    else:\n        raise NotImplementedError()\n    if vals is None:\n        vals = _get_obs_rep(adata, use_raw=use_raw, layer=layer, obsm=obsm, obsp=obsp).T\n    return morans_i(g, vals)", "idx": 1889}
{"project": "Scanpy", "commit_id": "252_scanpy_1.9.0__anndata.py__format_first_three_categories.py", "target": 0, "func": "def _format_first_three_categories(categories):\n    categories = list(categories)\n    if len(categories) > 3:\n        categories = categories[:3] + ['etc.']\n    return ', '.join(categories)", "idx": 1903}
{"project": "Scanpy", "commit_id": "662_scanpy_1.2.2_scanpy_plotting_tools___init__.py_rank_genes_groups_violin.py", "target": 1, "func": "def rank_genes_groups_violin(\n        adata, groups=None, n_genes=20,\n        gene_names=None, gene_symbols=None,\n        use_raw=None,\n        key=None,\n        split=True,\n        scale='width',\n        strip=True, jitter=True, size=1,\n        ax=None, show=None, save=None):\n    \"\"\"\\\n    Plot ranking of genes for all tested comparisons.\n    Parameters\n    ----------\n    adata : :class:`~anndata.AnnData`\n        Annotated data matrix.\n    groups : list of `str`, optional (default: `None`)\n        List of group names.\n    n_genes : `int`, optional (default: 20)\n        Number of genes to show. Is ignored if `gene_names` is passed.\n    gene_names : `None` or list of `str` (default: `None`)\n        List of genes to plot. Is only useful if interested in a custom gene list,\n        which is not the result of :func:`scanpy.api.tl.rank_genes_groups`.\n    gene_symbols : `str`, optional (default: `None`)\n        Key for field in `.var` that stores gene symbols if you do not want to\n        use `.var_names` displayed in the plot.\n    use_raw : `bool`, optional (default: `None`)\n        Use `raw` attribute of `adata` if present. Defaults to the value that\n        was used in :func:`~scanpy.api.tl.rank_genes_groups`.\n    split : `bool`, optional (default: `True`)\n        Whether to split the violins or not.\n    scale : `str`, optional (default: 'width')\n        See `seaborn.violinplot`.\n    strip : `bool`, optional (default: `True`)\n        Show a strip plot on top of the violin plot.\n    jitter : `int`, `float`, `bool`, optional (default: `True`)\n        If set to 0, no points are drawn. See `seaborn.stripplot`.\n    size : `int`, optional (default: 1)\n        Size of the jitter points.\n    {show_save_ax}\n    \"\"\"\n    if key is None:\n        key = 'rank_genes_groups'\n    groups_key = str(adata.uns[key]['params']['groupby'])\n    if use_raw is None:\n        use_raw = bool(adata.uns[key]['params']['use_raw'])\n    reference = str(adata.uns[key]['params']['reference'])\n    groups_names = (adata.uns[key]['names'].dtype.names\n                    if groups is None else groups)\n    if isinstance(groups_names, str): groups_names = [groups_names]\n    for group_name in groups_names:\n        if gene_names is None:\n            gene_names = adata.uns[\n                key]['names'][group_name][:n_genes]\n        df = pd.DataFrame()\n        new_gene_names = []\n        for g in gene_names:\n            if adata.raw is not None and use_raw:\n                X_col = adata.raw[:, g].X\n            else:\n                X_col = adata[:, g].X\n            if issparse(X_col): X_col = X_col.toarray().flatten()\n            new_gene_names.append(\n                g if gene_symbols is None else adata.var[gene_symbols][g])\n            df[g] = X_col\n        df['hue'] = adata.obs[groups_key].astype(str).values\n        if reference == 'rest':\n            df.loc[df['hue'] != group_name, 'hue'] = 'rest'\n        else:\n            df.loc[~df['hue'].isin([group_name, reference]), 'hue'] = np.nan\n        df['hue'] = df['hue'].astype('category')\n        df_tidy = pd.melt(df, id_vars='hue', value_vars=new_gene_names)\n        x = 'variable'\n        y = 'value'\n        hue_order = [group_name, reference]\n        import seaborn as sns\n        _ax = sns.violinplot(x=x, y=y, data=df_tidy, inner=None,\n                             hue_order=hue_order, hue='hue', split=split,\n                             scale=scale, orient='vertical', ax=ax)\n        if strip:\n            _ax = sns.stripplot(x=x, y=y, data=df_tidy,\n                                hue='hue', dodge=True, hue_order=hue_order,\n                                jitter=jitter, color='black', size=size, ax=_ax)\n        _ax.set_xlabel('genes')\n        _ax.set_title('{} vs. {}'.format(group_name, reference))\n        _ax.legend_.remove()\n        _ax.set_ylabel('expression')\n        _ax.set_xticklabels(gene_names, rotation='vertical')\n        writekey = ('rank_genes_groups_'\n                    + str(adata.uns[key]['params']['groupby'])\n                    + '_' + group_name)\n        utils.savefig_or_show(writekey, show=show, save=save)\n        return _ax", "idx": 1908}
{"project": "Scanpy", "commit_id": "900_scanpy_1.9.0__utils.py_preprocess_with_pca.py", "target": 0, "func": "def preprocess_with_pca(adata, n_pcs: Optional[int] = None, random_state=0):\n    \"\"\"\n    Parameters\n    ----------\n    n_pcs\n        If `n_pcs=0`, do not preprocess with PCA.\n        If `None` and there is a PCA version of the data, use this.\n        If an integer, compute the PCA.\n    \"\"\"\n    if n_pcs == 0:\n        logg.info('    using data matrix X directly (no PCA)')\n        return adata.X\n    elif n_pcs is None and 'X_pca' in adata.obsm_keys():\n        logg.info(f'    using \\'X_pca\\' with n_pcs = {adata.obsm[\"X_pca\"].shape[1]}')\n        return adata.obsm['X_pca']\n    elif 'X_pca' in adata.obsm_keys() and adata.obsm['X_pca'].shape[1] >= n_pcs:\n        logg.info(f'    using \\'X_pca\\' with n_pcs = {n_pcs}')\n        return adata.obsm['X_pca'][:, :n_pcs]\n    else:\n        n_pcs = settings.N_PCS if n_pcs is None else n_pcs\n        if adata.X.shape[1] > n_pcs:\n            logg.info(f'    computing \\'X_pca\\' with n_pcs = {n_pcs}')\n            logg.hint('avoid this by setting n_pcs = 0')\n            X = pca(adata.X, n_comps=n_pcs, random_state=random_state)\n            adata.obsm['X_pca'] = X\n            return X\n        else:\n            logg.info('    using data matrix X directly (no PCA)')\n            return adata.X", "idx": 1911}
{"project": "Scanpy", "commit_id": "631_scanpy_1.1_scanpy_preprocessing_mnn_correct.py_mnn_correct.py", "target": 1, "func": "def mnn_correct(*datas, var_index=None, var_subset=None, batch_key='batch', index_unique='-',\n                batch_categories=None, k=20, sigma=1., cos_norm_in=True, cos_norm_out=True,\n                svd_dim=None, var_adj=True, compute_angle=False, mnn_order=None, svd_mode='rsvd',\n                do_concatenate=True, save_raw=False, n_jobs=None, **kwargs):\n    \"\"\"Correct batch effects by matching mutual nearest neighbors [Haghverdi18]_ [Kang18]_.\n    This uses the implementatino of `mnnpy\n    <https://github.com/chriscainx/mnnpy>`_ [Kang18]_.\n    Depending on `do_concatenate`, returns matrices or `AnnData` objects in the\n    original order containing corrected expression values or a concatenated\n    matrix or AnnData object.\n    Be reminded that it is not advised to use the corrected data matrices for\n    differential expression testing.\n    More information and bug reports `here <https://github.com/chriscainx/mnnpy>`_.\n    Parameters\n    ----------\n    datas : `numpy.ndarray` or :class:`~scanpy.api.AnnData`\n        Expression matrices or AnnData objects. Matrices should be shaped like\n        n_obs * n_vars (n_cell * n_gene) and have consistent number of\n        columns. AnnData objects should have same number of variables.\n    var_index : `list` or `None`, optional (default: None)\n        The index (list of str) of vars (genes). Necessary when using only a\n        subset of vars to perform MNN correction, and should be supplied with\n        var_subset. When datas are AnnData objects, var_index is ignored.\n    var_subset : `list` or `None`, optional (default: None)\n        The subset of vars (list of str) to be used when performing MNN\n        correction. Typically, a list of highly variable genes (HVGs). When set\n        to None, uses all vars.\n    batch_key : `str`, optional (default: 'batch')\n        The batch_key for AnnData.concatenate. Only valid when do_concatenate\n        and supplying AnnData objects.\n    index_unique : `str`, optional (default: '-')\n        The index_unique for AnnData.concatenate. Only valid when do_concatenate\n        and supplying AnnData objects.\n    batch_categories : `list` or `None`, optional (default: None)\n        The batch_categories for AnnData.concatenate. Only valid when\n        do_concatenate and supplying AnnData objects.\n    k : `int`, optional (default: 20)\n        Number of mutual nearest neighbors.\n    sigma : `float`, optional (default: 1)\n        The bandwidth of the Gaussian smoothing kernel used to compute the\n        correction vectors. Default is 1.\n    cos_norm_in : `bool`, optional (default: True)\n        Whether cosine normalization should be performed on the input data prior\n        to calculating distances between cells.\n    cos_norm_out : `bool`, optional (default: True)\n        Whether cosine normalization should be performed prior to computing corrected expression values.\n    svd_dim : `int` or `None`, optional (default: None)\n        The number of dimensions to use for summarizing biological substructure\n        within each batch. If None, biological components will not be removed\n        from the correction vectors.\n    var_adj : `bool`, optional (default: True)\n        Whether to adjust variance of the correction vectors. Note this step\n        takes most computing time.\n    compute_angle : `bool`, optional (default: False)\n        Whether to compute the angle between each cell\u2019s correction vector and\n        the biological subspace of the reference batch.\n    mnn_order : `list` or `None`, optional (default: None)\n        The order in which batches are to be corrected. When set to None, datas\n        are corrected sequentially.\n    svd_mode : `str`, optional (default: 'rsvd')\n        One of 'svd', 'rsvd', and 'irlb'. 'svd' computes SVD using a\n        non-randomized SVD-via-ID algorithm, while 'rsvd' uses a randomized\n        version. 'irlb' performes truncated SVD by implicitly restarted Lanczos\n        bidiagonalization (forked from https://github.com/airysen/irlbpy).\n    do_concatenate : `bool`, optional (default: True)\n        Whether to concatenate the corrected matrices or AnnData objects. Default is True.\n    save_raw : `bool`, optional (default: False)\n        Whether to save the original expression data in the .raw attribute of AnnData objects.\n    n_jobs : `int` or `None`, optional (default: None)\n        The number of jobs. When set to None, automatically uses the number of cores.\n    kwargs : `dict` or `None`, optional (default: None)\n        optional keyword arguments for irlb.\n    Returns\n    -------\n    datas : `numpy.ndarray` or :class:`~scanpy.api.AnnData`\n        Corrected matrix/matrices or AnnData object/objects, depending on the\n        input type and `do_concatenate`.\n    mnn_list : `list`\n        A list containing MNN pairing information as DataFrames in each iteration step.\n    angle_list : `list`\n        A list containing angles of each batch.\n    \"\"\"\n    try:\n        from mnnpy import mnn_correct as mnn_cor\n        n_jobs = settings.n_jobs if n_jobs is None else n_jobs\n        datas, mnn_list, angle_list = mnn_cor(*datas, var_index, var_subset, batch_key, index_unique,\n                                                batch_categories, k, sigma, cos_norm_in, cos_norm_out,\n                                                svd_dim, var_adj, compute_angle, mnn_order, svd_mode,\n                                                do_concatenate, save_raw, n_jobs, **kwargs)\n        return datas, mnn_list, angle_list\n    except ImportError:\n        raise ImportError(\n            'Please install the package mnnpy '\n            '(https://github.com/chriscainx/mnnpy). ')", "idx": 1912}
{"project": "Scanpy", "commit_id": "704_scanpy_1.9.0_test_qc_metrics.py_test_top_segments.py", "target": 0, "func": "def test_top_segments(cls):\n    a = cls(np.ones((300, 100)))\n    seg = top_segment_proportions(a, [50, 100])\n    assert (seg[:, 0] == 0.5).all()\n    assert (seg[:, 1] == 1.0).all()\n    segfull = top_segment_proportions(a, np.arange(100) + 1)\n    propfull = top_proportions(a, 100)\n    assert (segfull == propfull).all()", "idx": 1920}
{"project": "Scanpy", "commit_id": "598_scanpy_1.9.0_test_neighbors_key_added.py_adata.py", "target": 0, "func": "def adata():\n    return sc.AnnData(pbmc68k_reduced().X)", "idx": 1925}
{"project": "Scanpy", "commit_id": "583_scanpy_1.0.4_scanpy_tools_rna_velocity.py_rna_velocity.py", "target": 1, "func": "def rna_velocity(adata, loomfile, copy=False):\n    \"\"\"Estimate RNA velocity [LaManno17]_\n    This requires generating a loom file with Velocyto, which will store the\n    counts of spliced, unspliced and ambiguous RNA for every cell and every\n    gene.\n    In contrast to Velocyto, here, we neither use RNA velocities for\n    extrapolation nor for constructing a Markov process. Instead, we directly\n    orient and weight edges in the nearest neighbor graph by computing\n    ``cosine_similarity((x_i - x_j), v_i)``, where `i` labels a cell, `j` a\n    neighbor of the cell, `x` a gene expression vector and `v` a velocity\n    vector.\n    \"\"\"\n    import loompy\n    adata = adata.copy() if copy else adata\n    # this is n_genes x n_cells\n    ds = loompy.connect(loomfile)\n    row_attrs = dict(ds.row_attrs.items())\n    col_attrs = dict(ds.col_attrs.items())\n    gene_names = [gene for gene in row_attrs['Gene'] if gene in adata.var_names]\n    # cell_names = [cell for cell in col_attrs['CellID'] if cell in adata.obs_names]\n    # subset the spliced and unspliced matrices to the genes in adata\n    from anndata.base import _normalize_index\n    gene_index = _normalize_index(gene_names, adata.var_names)\n    # cell_index = _normalize_index(cell_names, adata.obs_names)\n\n    X_spliced = ds.layer['spliced'][gene_index].T\n    X_unspliced = ds.layer['unspliced'][gene_index].T\n\n    print(X_spliced.shape)\n    print(X_unspliaced.shape)\n    quit()\n\n    # for now, take non-normalized values\n    from ..preprocessing.simple import normalize_per_cell\n    X_spliced = normalize_per_cell(X_spliced.T, copy=True).T\n    X_unspliced = normalize_per_cell(X_unspliced.T, copy=True).T\n\n    # loop over genes\n    offset = np.zeros(s.shape[0], dtype='float32')\n    gamma = np.zeros(s.shape[0], dtype='float32')\n    for i in range(s.shape[0]):\n        gamma[i], offset[i] = opt.minimize(\n            lambda m: np.sum((-X_unspliced[i] + X_spliced[i] * m[0] + m[1])**2),\n            x0=(0.1, 1e-16),\n            method='L-BFGS-B',\n            bounds=[(1e-8, 30), (0, 1.5)]).x\n\n    velocity = X_unspliced - (gamma[:, None] * X_spliced + offset[:, None])\n\n    from ..neighbors import Neighbors, get_indices_distances_from_sparse_matrix\n    neigh = Neighbors(adata)\n    knn_indices, knn_distances = get_indices_distances_from_sparse_matrix(\n        neigh.distances, self.n_neighbors)\n\n    n_obs = adata.n_obs\n    n_neighbors = neigh.n_neighbors\n    from scipy.sparse import dok_matrix\n    graph = dok_matrix((n_obs, n_obs), dtype='float32')\n    from scipy.spatial.distance import cosine\n    for i in range(knn_indices.shape[0]):\n        for j in range(n_neighbors):\n            if knn_indices[i, j] != i:\n                val = 1 - cosine((X_spliced[:, i] - X_spliced[:, j]), velocity[:, i])\n                if val > 0:\n                    # transition from i to j\n                    graph[j, i] = val\n                else:\n                    # transition from j to i\n                    graph[i, j] = val\n    graph = graph.tocoo().tocsr()\n    adata.uns['rna_velocity'] = {}\n    adata.uns['rna_velocity']['graph'] = graph\n    adata.var['rna_velocity_gamma'] = gamma\n    adata.var['rna_velocity_offset'] = offset\n    return adata if copy else None", "idx": 1927}
{"project": "Scanpy", "commit_id": "82_scanpy_1.9.0__settings.py_level.py", "target": 0, "func": "def level(self) -> int:\n        # getLevelName(str) returns the int level\u2026\n        return getLevelName(_VERBOSITY_TO_LOGLEVEL[self])", "idx": 1936}
{"project": "Scanpy", "commit_id": "206_scanpy_0.0_scanpy_preprocess_simple.py_filter_cells.py", "target": 1, "func": "def filter_cells(data, min_counts=None, min_genes=None):\n    \"\"\"\n    Keep cells that have at least `min_counts` UMI counts or `min_genes`\n    genes expressed.\n    This is to filter measurement outliers, i.e., \"unreliable\" samples.\n    Paramaters\n    ----------\n    data : np.ndarray or AnnData\n        Data matrix of shape n_sample x n_variables. Rows correspond to cells\n        and columns to genes.\n    min_counts : int\n        Minimum number of counts required for a cell to pass filtering.\n    Returns\n    -------\n    If data is a data matrix X\n        cell_filter : np.ndarray\n            Boolean index mask that does filtering. True means that the cell is\n            kept. False means the cell is removed.\n        number_per_cell: np.ndarray\n            Either n_counts or n_genes per cell.\n    otherwise:\n        adata : AnnData\n            The filtered adata object, with the count info stored in adata.smp.\n    \"\"\"\n    if min_genes is not None and min_counts is not None:\n        raise ValueError('Either provide min_counts or min_genes, but not both.')\n    if min_genes is None and min_counts is None:\n        raise ValueError('Provide one of min_counts or min_genes.')\n    if isinstance(data, AnnData):\n        adata = data\n        cell_filter, number = filter_cells(adata.X, min_counts, min_genes)\n        if min_genes is None:\n            adata.smp['n_counts'] = number\n        else:\n            adata.smp['n_genes'] = number\n        return adata[cell_filter]\n    X = data  # proceed with processing the data matrix\n    min_number = min_counts if min_genes is None else min_genes\n    number_per_cell = np.sum(X if min_genes is None else X > 0, axis=1)\n    if issparse(X):\n       number_per_cell = number_per_cell.A1\n    cell_filter = number_per_cell >= min_number\n    sett.m(0, '... filtered out', np.sum(~cell_filter), 'outlier cells')\n    return cell_filter, number_per_cell", "idx": 1946}
{"project": "Scanpy", "commit_id": "176_scanpy_0.0_scanpy_classes_ann_data.py_test_creation.py", "target": 1, "func": "def test_creation():\n    AnnData(np.array([[1, 2], [3, 4]]))\n    AnnData(ma.array([[1, 2], [3, 4]], add={'mask': [0, 1, 1, 0]})\n    AnnData(sp.eye(2))\n    AnnData(\n        np.array([[1, 2, 3], [4, 5, 6]]),\n        dict(Smp=['A', 'B']),\n        dict(Feat=['a', 'b', 'c']))\n    assert AnnData(np.array([1, 2])).X.shape == (2, 1)\n    from pytest import raises\n    raises(ValueError, AnnData,\n           np.array([[1, 2], [3, 4]]),\n           dict(TooLong=[1, 2, 3, 4]))", "idx": 1950}
{"project": "Scanpy", "commit_id": "844_scanpy_1.9.0__rank_genes_groups.py__select_top_n.py", "target": 0, "func": "def _select_top_n(scores, n_top):\n    n_from = scores.shape[0]\n    reference_indices = np.arange(n_from, dtype=int)\n    partition = np.argpartition(scores, -n_top)[-n_top:]\n    partial_indices = np.argsort(scores[partition])[::-1]\n    global_indices = reference_indices[partition][partial_indices]\n\n    return global_indices", "idx": 1957}
{"project": "Scanpy", "commit_id": "100_scanpy_1.9.0__settings.py_categories_to_ignore.py", "target": 0, "func": "def categories_to_ignore(self, categories_to_ignore: Iterable[str]):\n        categories_to_ignore = list(categories_to_ignore)\n        for i, cat in enumerate(categories_to_ignore):\n            _type_check(cat, f\"categories_to_ignore[{i}]\", str)\n        self._categories_to_ignore = categories_to_ignore", "idx": 1986}
{"project": "Scanpy", "commit_id": "926_scanpy_1.4.4_scanpy_plotting__tools___init__.py_pca_loadings.py", "target": 1, "func": "def pca_loadings(\n    adata: AnnData,\n    components: Union[str, Sequence[int], None] = None,\n    show: Optional[bool] = None,\n    save: Union[str, bool, None] = None,\n):\n    \"\"\"\\\n    Rank genes according to contributions to PCs.\n    Parameters\n    ----------\n    adata\n        Annotated data matrix.\n    components\n        For example, ``'1,2,3'`` means ``[1, 2, 3]``, first, second, third\n        principal component.\n    show\n        Show the plot, do not return axis.\n    save\n        If `True` or a `str`, save the figure.\n        A string is appended to the default filename.\n        Infer the filetype if ending on {`'.pdf'`, `'.png'`, `'.svg'`}.\n    \"\"\"\n    if components is None: components = [1, 2, 3]\n    elif isinstance(components, str): components = components.split(',')\n    components = np.array(components) - 1\n    if np.any(components < 0):\n        logg.error(\"Component indices must be greater than zero.\")\n        return\n    ranking(adata, 'varm', 'PCs', indices=components)\n    utils.savefig_or_show('pca_loadings', show=show, save=save)", "idx": 1991}
{"project": "Scanpy", "commit_id": "857_scanpy_1.4_scanpy_plotting__anndata.py_matrixplot.py", "target": 1, "func": "def matrixplot(adata, var_names, groupby=None, use_raw=None, log=False, num_categories=7,\n               figsize=None, dendrogram=False, gene_symbols=None, var_group_positions=None, var_group_labels=None,\n               var_group_rotation=None, layer=None, standard_scale=None, swap_axes=False, show=None,\n               save=None, **kwds):\n    \"\"\"\\\n    Creates a heatmap of the mean expression values per cluster of each var_names\n    If groupby is not given, the matrixplot assumes that all data belongs to a single\n    category.\n    Parameters\n    ----------\n    {common_plot_args}\n    standard_scale : {{'var', 'group'}}, optional (default: None)\n        Whether or not to standardize that dimension between 0 and 1, meaning for each variable or group,\n        subtract the minimum and divide each by its maximum.\n    {show_save_ax}\n    **kwds : keyword arguments\n        Are passed to `matplotlib.pyplot.pcolor`.\n    Returns\n    -------\n    List of `matplotlib.Axes`\n    Examples\n    --------\n    >>> adata = sc.datasets.pbmc68k_reduced()\n    >>> sc.pl.matrixplot(adata, ['C1QA', 'PSAP', 'CD79A', 'CD79B', 'CST3', 'LYZ'],\n    ... groupby='bulk_labels', dendrogram=True)\n    \"\"\"\n    if use_raw is None and adata.raw is not None: use_raw = True\n    if isinstance(var_names, str):\n        var_names = [var_names]\n    categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories,\n                                              gene_symbols=gene_symbols, layer=layer)\n    if groupby is None or len(categories) <= 1:\n        # dendrogram can only be computed  between groupby categories\n        dendrogram = False\n    mean_obs = obs_tidy.groupby(level=0).mean()\n    if standard_scale == 'group':\n        mean_obs = mean_obs.sub(mean_obs.min(1), axis=0)\n        mean_obs = mean_obs.div(mean_obs.max(1), axis=0).fillna(0)\n    elif standard_scale == 'var':\n        mean_obs -= mean_obs.min(0)\n        mean_obs /= mean_obs.max(0).fillna(0)\n    elif standard_scale is None:\n        pass\n    else:\n        logg.warn('Unknown type for standard_scale, ignored')\n    if dendrogram:\n        dendro_data = _reorder_categories_after_dendrogram(adata, groupby, dendrogram,\n                                                           var_names=var_names,\n                                                           var_group_labels=var_group_labels,\n                                                           var_group_positions=var_group_positions)\n        var_group_labels = dendro_data['var_group_labels']\n        var_group_positions = dendro_data['var_group_positions']\n        # reorder matrix\n        if dendro_data['var_names_idx_ordered'] is not None:\n            # reorder columns (usually genes) if needed. This only happens when\n            # var_group_positions and var_group_labels is set\n            mean_obs = mean_obs.iloc[:,dendro_data['var_names_idx_ordered']]\n        # reorder rows (categories) to match the dendrogram order\n        mean_obs = mean_obs.iloc[dendro_data['categories_idx_ordered'], :]\n    colorbar_width = 0.2\n    if not swap_axes:\n        dendro_width = 0.8 if dendrogram else 0\n        if figsize is None:\n            height = len(categories) * 0.2 + 1  # +1 for labels\n            heatmap_width = len(var_names) * 0.32\n            width = heatmap_width + dendro_width + colorbar_width  # +1.6 to account for the colorbar and  + 1 to account for labels\n        else:\n            width, height = figsize\n            heatmap_width = width - (dendro_width + colorbar_width)\n        if var_group_positions is not None and len(var_group_positions) > 0:\n            # add some space in case 'brackets' want to be plotted on top of the image\n            height_ratios = [0.5, 10]\n            height += 0.5\n        else:\n            height_ratios = [0, 10.5]\n        # define a layout of 2 rows x 3 columns\n        # first row is for 'brackets' (if no brackets needed, the height of this row is zero)\n        # second row is for main content. This second row\n        # is divided into three axes:\n        #   first ax is for the main matrix figure\n        #   second ax is for the dendrogram\n        #   third ax is for the color bar legend\n        fig = pl.figure(figsize=(width, height))\n        axs = gridspec.GridSpec(nrows=2, ncols=3, wspace=0.02, hspace=0.04,\n                                width_ratios=[heatmap_width, dendro_width, colorbar_width],\n                                height_ratios=height_ratios)\n        matrix_ax = fig.add_subplot(axs[1, 0])\n        y_ticks = np.arange(mean_obs.shape[0]) + 0.5\n        matrix_ax.set_yticks(y_ticks)\n        matrix_ax.set_yticklabels([mean_obs.index[idx] for idx in range(mean_obs.shape[0])])\n        if dendrogram:\n            dendro_ax = fig.add_subplot(axs[1, 1], sharey=matrix_ax)\n            _plot_dendrogram(dendro_ax, adata, groupby, dendrogram_key=dendrogram, ticks=y_ticks)\n        pc = matrix_ax.pcolor(mean_obs, edgecolor='gray', **kwds)\n        # invert y axis to show categories ordered from top to bottom\n        matrix_ax.set_ylim(mean_obs.shape[0], 0)\n        x_ticks = np.arange(mean_obs.shape[1]) + 0.5\n        matrix_ax.set_xticks(x_ticks)\n        matrix_ax.set_xticklabels([mean_obs.columns[idx] for idx in range(mean_obs.shape[1])], rotation=90)\n        matrix_ax.tick_params(axis='both', labelsize='small')\n        matrix_ax.grid(False)\n        matrix_ax.set_xlim(-0.5, len(var_names) + 0.5)\n        matrix_ax.set_ylabel(groupby)\n        matrix_ax.set_xlim(0, mean_obs.shape[1])\n        # plot group legends on top of matrix_ax (if given)\n        if var_group_positions is not None and len(var_group_positions) > 0:\n            gene_groups_ax = fig.add_subplot(axs[0, 0], sharex=matrix_ax)\n            _plot_gene_groups_brackets(gene_groups_ax, group_positions=var_group_positions,\n                                       group_labels=var_group_labels, rotation=var_group_rotation,\n                                       left_adjustment=0.2, right_adjustment=0.8)\n        # plot colorbar\n        _plot_colorbar(pc, fig, axs[1, 2])\n    else:\n        dendro_height = 0.5 if dendrogram else 0\n        if var_group_positions is not None and len(var_group_positions) > 0:\n            # add some space in case 'color blocks' want to be plotted on the right of the image\n            vargroups_width = 0.4\n        else:\n            vargroups_width = 0\n        if figsize is None:\n            heatmap_height = len(var_names) * 0.2\n            height = dendro_height + heatmap_height + 1  # +1 for labels\n            heatmap_width = len(categories) * 0.3\n            width = heatmap_width + vargroups_width + colorbar_width\n        else:\n            width, height = figsize\n            heatmap_width = width - (vargroups_width + colorbar_width)\n            heatmap_height = height - dendro_height\n        # define a layout of 2 rows x 3 columns\n        # first row is for 'dendrogram' (if no dendrogram is plotted, the height of this row is zero)\n        # second row is for main content. This row\n        # is divided into three axes:\n        #   first ax is for the main matrix figure\n        #   second ax is for the groupby categories (eg. brackets)\n        #   third ax is for the color bar legend\n        fig = pl.figure(figsize=(width, height))\n        axs = gridspec.GridSpec(nrows=2, ncols=3, wspace=0.05, hspace=0.005,\n                                width_ratios=[heatmap_width, vargroups_width, colorbar_width],\n                                height_ratios=[dendro_height, heatmap_height])\n        mean_obs = mean_obs.T\n        matrix_ax = fig.add_subplot(axs[1, 0])\n        pc = matrix_ax.pcolor(mean_obs, edgecolor='gray', **kwds)\n        y_ticks = np.arange(mean_obs.shape[0]) + 0.5\n        matrix_ax.set_yticks(y_ticks)\n        matrix_ax.set_yticklabels([mean_obs.index[idx] for idx in range(mean_obs.shape[0])])\n        x_ticks = np.arange(mean_obs.shape[1]) + 0.5\n        matrix_ax.set_xticks(x_ticks)\n        matrix_ax.set_xticklabels([mean_obs.columns[idx] for idx in range(mean_obs.shape[1])], rotation=90)\n        matrix_ax.tick_params(axis='both', labelsize='small')\n        matrix_ax.grid(False)\n        matrix_ax.set_xlim(0, len(categories))\n        matrix_ax.set_xlabel(groupby)\n        # invert y axis to show var_names ordered from top to bottom\n        matrix_ax.set_ylim(mean_obs.shape[0], 0)\n        if dendrogram:\n            dendro_ax = fig.add_subplot(axs[0, 0], sharex=matrix_ax)\n            _plot_dendrogram(dendro_ax, adata, groupby, dendrogram_key=dendrogram, ticks=x_ticks, orientation='top')\n        # plot group legends on top of matrix_ax (if given)\n        if var_group_positions is not None and len(var_group_positions) > 0:\n            gene_groups_ax = fig.add_subplot(axs[1, 1], sharey=matrix_ax)\n            _plot_gene_groups_brackets(gene_groups_ax, group_positions=var_group_positions,\n                                       group_labels=var_group_labels, rotation=var_group_rotation,\n                                       left_adjustment=0.2, right_adjustment=0.8, orientation='right')\n        # plot colorbar\n        _plot_colorbar(pc, fig, axs[1, 2])\n    utils.savefig_or_show('matrixplot', show=show, save=save)\n    return axs", "idx": 1992}
{"project": "Scanpy", "commit_id": "742_scanpy_1.9.0_test_score_genes.py_test_score_genes_sparse_vs_dense.py", "target": 0, "func": "def test_score_genes_sparse_vs_dense():\n    \"\"\"\n    score_genes() should give the same result for dense and sparse matrices\n    \"\"\"\n    adata_sparse = _create_adata(100, 1000, p_zero=0.3, p_nan=0.3)\n\n    adata_dense = adata_sparse.copy()\n    adata_dense.X = adata_dense.X.A\n\n    gene_set = adata_dense.var_names[:10]\n\n    sc.tl.score_genes(adata_sparse, gene_list=gene_set, score_name='Test')\n    sc.tl.score_genes(adata_dense, gene_list=gene_set, score_name='Test')\n\n    np.testing.assert_allclose(\n        adata_sparse.obs['Test'].values, adata_dense.obs['Test'].values", "idx": 1993}
{"project": "Scanpy", "commit_id": "287_scanpy_0.1_scanpy_plotting___init__.py_dpt.py", "target": 1, "func": "def dpt(adata,\n        basis='diffmap',\n        color=None,\n        names=None,\n        comps=None,\n        cont=None,\n        layout='2d',\n        legendloc='right margin',\n        cmap=None,\n        pal=None,\n        right_margin=None,\n        size=None,\n        title=None,\n        show=None):\n    \"\"\"Plot results of DPT analysis.\n    Parameters\n    ----------\n    adata : AnnData\n        Annotated data matrix.\n    basis : {'diffmap', 'pca', 'tsne', 'spring'}\n        Choose the basis in which to plot.\n    color : string or list of strings, optional (default: first annotation)\n        Sample/ cell annotation for coloring in the form \"ann1,ann2,...\". String\n        annotation is plotted assuming categorical annotation, float and integer\n        annotation is plotted assuming continuous annoation. Option 'cont'\n        allows to switch between these default choices.\n    names : str, optional (default: all names)\n        Restrict to a few categories in categorical sample annotation.\n    comps : str, optional (default: '1,2')\n         String of the form '1,2' or 'all'.\n    cont : bool, None (default: None)\n        Switch on continuous layout, switch off categorical layout.\n    layout : {'2d', '3d', 'unfolded 3d'}, optional (default: '2d')\n         Layout of plot.\n    legendloc : {'right margin', see matplotlib.legend}, optional (default: 'right margin')\n         Options for keyword argument 'loc'.\n    cmap : str (default: 'viridis')\n         String denoting matplotlib color map.\n    pal : list of str (default: matplotlib.rcParams['axes.prop_cycle'].by_key()['color'])\n         Colors cycle to use for categorical groups.\n    right_margin : float or list of floats (default: None)\n         Adjust the width of the space right of each plotting panel.\n    size : float (default: None)\n         Point size.\n    title : str, optional (default: None)\n         Provide title for panels as \"my title1,another title,...\".\n    \"\"\"\n    dpt_scatter(adata,\n                basis=basis,\n                color=color,\n                names=names,\n                comps=comps,\n                cont=cont,\n                layout=layout,\n                legendloc=legendloc,\n                cmap=cmap,\n                pal=pal,\n                right_margin=right_margin,\n                size=size,\n                title=title,\n                show=False)\n    colors = ['dpt_pseudotime']\n    if len(np.unique(adata.smp['dpt_groups'])) > 1: colors += ['dpt_groups']\n    if color is not None:\n        if not isinstance(color, list): colors = color.split(',')\n        else: colors = color\n    if 'dpt_groups' in colors: dpt_tree(adata, show=False)\n    dpt_timeseries(adata, cmap=cmap, show=show)", "idx": 1994}
{"project": "Scanpy", "commit_id": "580_scanpy_1.0.4_scanpy_tools_paga.py_paga.py", "target": 1, "func": "def paga(\n        adata,\n        groups='louvain',\n        n_jobs=None,\n        copy=False):\n    \"\"\"\\\n    Generate cellular maps of differentiation manifolds with complex\n    topologies [Wolf17i]_.\n    Partition-based graph abstraction (PAGA) quantifies the connectivities of\n    partitions of a neighborhood graph of single cells, thereby generating a\n    much simpler abstracted graph whose nodes label the partitions. Together\n    with a random walk-based distance measure, this generates a partial\n    coordinatization of data useful for exploring and explaining its variation.\n    Parameters\n    ----------\n    adata : :class:`~scanpy.api.AnnData`\n        Annotated data matrix.\n    groups : categorical annotation of observations or 'louvain_groups', optional (default: 'louvain_groups')\n        Criterion to determine the resulting partitions of the single-cell\n        graph. 'louvain_groups' uses the Louvain algorithm and optimizes\n        modularity of the graph. You can also pass your predefined groups by\n        choosing any categorical annotation of observations (`adata.obs`).\n    copy : `bool`, optional (default: `False`)\n        Copy `adata` before computation and return a copy. Otherwise, perform\n        computation inplace and return None.\n    Returns\n    -------\n    Returns or updates `adata` depending on `copy` with\n    connectivities : np.ndarray (adata.uns['connectivities'])\n        The full adjacency matrix of the abstracted graph, weights\n        correspond to connectivities.\n    confidence : np.ndarray (adata.uns['confidence'])\n        The full adjacency matrix of the abstracted graph, weights\n        correspond to confidence in the presence of an edge.\n    confidence_tree : sc.sparse csr matrix (adata.uns['confidence_tree'])\n        The adjacency matrix of the tree-like subgraph that best explains\n        the topology.\n    \"\"\"\n    if 'neighbors' not in adata.uns:\n        raise ValueError(\n            'You need to run `pp.neighbors` first to compute a neighborhood graph.')\n    adata = adata.copy() if copy else adata\n    utils.sanitize_anndata(adata)\n    logg.info('running partition-based graph abstraction (PAGA)', reset=True)\n    paga = PAGA(adata, groups)\n    paga.compute()\n    adata.uns['paga'] = {}\n    adata.uns['paga']['connectivities'] = paga.connectivities_coarse\n    adata.uns['paga']['confidence'] = paga.confidence\n    adata.uns['paga']['confidence_tree'] = paga.confidence_tree\n    adata.uns['paga']['groups'] = groups\n    adata.uns[groups + '_sizes'] = np.array(paga.vc.sizes())\n    logg.info('    finished', time=True, end=' ' if settings.verbosity > 2 else '\\n')\n    logg.hint(\n        'added\\n'\n        '    \\'paga/connectivities\\', connectivities adjacency (adata.uns)\\n'\n        '    \\'paga/confidence\\', confidence adjacency (adata.uns)\\n'\n        '    \\'paga/confidence_tree\\', confidence subtree (adata.uns)')\n    return adata if copy else None", "idx": 1996}
{"project": "Scanpy", "commit_id": "791_scanpy_1.4_scanpy_plotting__tools_paga.py_paga.py", "target": 1, "func": "def paga(\n        adata,\n        threshold=None,\n        color=None,\n        layout=None,\n        layout_kwds={},\n        init_pos=None,\n        root=0,\n        labels=None,\n        single_component=False,\n        solid_edges='connectivities',\n        dashed_edges=None,\n        transitions=None,\n        fontsize=None,\n        fontweight='bold',\n        text_kwds={},\n        node_size_scale=1,\n        node_size_power=0.5,\n        edge_width_scale=1,\n        min_edge_width=None,\n        max_edge_width=None,\n        arrowsize=30,\n        title=None,\n        left_margin=0.01,\n        random_state=0,\n        pos=None,\n        normalize_to_color=False,\n        cmap=None,\n        cax=None,\n        colorbar=None,\n        cb_kwds={},\n        frameon=None,\n        add_pos=True,\n        export_to_gexf=False,\n        use_raw=True,\n        colors=None,  # backwards compat\n        groups=None,  # backwards compat\n        plot=True,\n        show=None,\n        save=None,\n        ax=None):\n    \"\"\"Plot the PAGA graph through thresholding low-connectivity edges.\n    Compute a coarse-grained layout of the data. Reuse this by passing\n    `init_pos='paga'` to :func:`~scanpy.api.tl.umap` or\n    :func:`~scanpy.api.tl.draw_graph` and obtain embeddings with more meaningful\n    global topology [Wolf17i]_.\n    This uses ForceAtlas2 or igraph's layout algorithms for most layouts [Csardi06]_.\n    Parameters\n    ----------\n    adata : :class:`~anndata.AnnData`\n        Annotated data matrix.\n    threshold : `float` or `None`, optional (default: 0.01)\n        Do not draw edges for weights below this threshold. Set to 0 if you want\n        all edges. Discarding low-connectivity edges helps in getting a much\n        clearer picture of the graph.\n    color : gene name or obs. annotation, optional (default: `None`)\n        The node colors. Also plots the degree of the abstracted graph when\n        passing {'degree_dashed', 'degree_solid'}.\n    labels : `None`, `str`, `list`, `dict`, optional (default: `None`)\n        The node labels. If `None`, this defaults to the group labels stored in\n        the categorical for which :func:`~scanpy.api.tl.paga` has been computed.\n    pos : `np.ndarray`, filename of `.gdf` file,  optional (default: `None`)\n        Two-column array-like storing the x and y coordinates for drawing.\n        Otherwise, path to a `.gdf` file that has been exported from Gephi or\n        a similar graph visualization software.\n    layout : {'fa', 'fr', 'rt', 'rt_circular', 'eq_tree', ...}, optional (default: 'fr')\n        Plotting layout that computes positions. 'fa' stands for ForceAtlas2, 'fr' stands for\n        Fruchterman-Reingold, 'rt' stands for Reingold Tilford. 'eq_tree' stands\n        for 'eqally spaced tree'. All but 'fa' and 'eq_tree' are igraph\n        layouts. All other igraph layouts are also permitted. See also parameter\n        `pos` and :func:`~scanpy.api.tl.draw_graph`.\n    init_pos : `np.ndarray`, optional (default: `None`)\n        Two-column array storing the x and y coordinates for initializing the\n        layout.\n    random_state : `int` or `None`, optional (default: 0)\n        For layouts with random initialization like 'fr', change this to use\n        different intial states for the optimization. If `None`, the initial\n        state is not reproducible.\n    root : `int`, `str` or list of `int`, optional (default: 0)\n        If choosing a tree layout, this is the index of the root node or a list\n        of root node indices. If this is a non-empty vector then the supplied\n        node IDs are used as the roots of the trees (or a single tree if the\n        graph is connected). If this is `None` or an empty list, the root\n        vertices are automatically calculated based on topological sorting.\n    transitions : `str` or `None`, optional (default: `None`)\n        Key for `.uns['paga']` that specifies the matrix that - for instance\n        `'transistions_confidence'` - that specifies the matrix that stores the\n        arrows.\n    solid_edges : `str`, optional (default: 'paga_connectivities')\n        Key for `.uns['paga']` that specifies the matrix that stores the edges\n        to be drawn solid black.\n    dashed_edges : `str` or `None`, optional (default: `None`)\n        Key for `.uns['paga']` that specifies the matrix that stores the edges\n        to be drawn dashed grey. If `None`, no dashed edges are drawn.\n    single_component : `bool`, optional (default: `False`)\n        Restrict to largest connected component.\n    fontsize : `int` (default: `None`)\n        Font size for node labels.\n    text_kwds : keywords for `matplotlib.text`\n        See `here\n        <https://matplotlib.org/api/_as_gen/matplotlib.axes.Axes.text.html#matplotlib.axes.Axes.text>`_.\n    node_size_scale : `float` (default: 1.0)\n        Increase or decrease the size of the nodes.\n    node_size_power : `float` (default: 0.5)\n        The power with which groups sizes influence the radius of the nodes.\n    edge_width_scale : `float`, optional (default: 5)\n        Edge with scale in units of `rcParams['lines.linewidth']`.\n    min_edge_width : `float`, optional (default: `None`)\n        Min width of solid edges.\n    max_edge_width : `float`, optional (default: `None`)\n        Max width of solid and dashed edges.\n    arrowsize : `int`, optional (default: 30)\n       For directed graphs, choose the size of the arrow head head's length and\n       width. See :py:class: `matplotlib.patches.FancyArrowPatch` for attribute\n       `mutation_scale` for more info.\n    export_to_gexf : `bool`, optional (default: `None`)\n        Export to gexf format to be read by graph visualization programs such as\n        Gephi.\n    normalize_to_color : `bool`, optional (default: `False`)\n        Whether to normalize categorical plots to `color` or the underlying\n        grouping.\n    cmap : color map\n        The color map.\n    cax : `matplotlib.Axes`\n        A matplotlib axes object for a potential colorbar.\n    cb_kwds : colorbar keywords\n        See `here\n        <https://matplotlib.org/api/colorbar_api.html#matplotlib.colorbar.ColorbarBase>`__,\n        for instance, `ticks`.\n    add_pos : `bool`, optional (default: `True`)\n        Add the positions to `adata.uns['paga']`.\n    title : `str`, optional (default: `None`)\n        Provide a title.\n    frameon : `bool`, optional (default: `None`)\n        Draw a frame around the PAGA graph.\n    hide : `bool`, optional (default: `False`)\n        Do not create a plot.\n    plot : `bool`, optional (default: `True`)\n        If `False`, do not create the figure, simply compute the layout.\n    save : `bool` or `str`, optional (default: `None`)\n        If `True` or a `str`, save the figure. A string is appended to the\n        default filename. Infer the filetype if ending on \\\\{'.pdf', '.png', '.svg'\\\\}.\n    ax : `matplotlib.Axes`\n        A matplotlib axes object.\n    Returns\n    -------\n    `None`, `axs`\n        If `show==False`, one or more `matplotlib.Axis` objects.\n        Adds `'pos'` to `adata.uns['paga']` if `add_pos` is `True`.\n    Notes\n    -----\n    When initializing the positions, note that - for some reason - igraph\n    mirrors coordinates along the x axis... that is, you should increase the\n    `maxiter` parameter by 1 if the layout is flipped.\n    See also\n    --------\n    tl.paga\n    pl.paga_compare\n    pl.paga_path\n    \"\"\"\n    if groups is not None:  # backwards compat\n        labels = groups\n        logg.warn('`groups` is deprecated in `pl.paga`: use `labels` instead')\n    if colors is None:\n        colors = color\n    # colors is a list that contains no lists\n    groups_key = adata.uns['paga']['groups']\n    if ((isinstance(colors, Iterable) and len(colors) == len(adata.obs[groups_key].cat.categories))\n            or colors is None or isinstance(colors, str)):\n        colors = [colors]\n    if frameon is None:\n        frameon = settings._frameon\n    # labels is a list that contains no lists\n    if ((isinstance(labels, Iterable) and len(labels) == len(adata.obs[groups_key].cat.categories))\n            or labels is None or isinstance(labels, (str, dict))):\n        labels = [labels for i in range(len(colors))]\n    if title is None and len(colors) > 1:\n        title = [c for c in colors]\n    elif isinstance(title, str):\n        title = [title for c in colors]\n    elif title is None:\n        title = [None for c in colors]\n    if colorbar is None:\n        var_names = adata.var_names if adata.raw is None else adata.raw.var_names\n        colorbars = [True if c in var_names else False for c in colors]\n    else:\n        colorbars = [False for c in colors]\n    if isinstance(root, str):\n        if root in node_labels:\n            root = list(node_labels).index(root)\n        else:\n            raise ValueError(\n                'If `root` is a string, it needs to be one of {} not \\'{}\\'.'\n                .format(node_labels.tolist(), root))\n    if isinstance(root, list) and root[0] in node_labels:\n        root = [list(node_labels).index(r) for r in root]\n    # define the adjacency matrices\n    adjacency_solid = adata.uns['paga'][solid_edges].copy()\n    adjacency_dashed = None\n    if threshold is None:\n        threshold = 0.01  # default threshold\n    if threshold > 0:\n        adjacency_solid.data[adjacency_solid.data < threshold] = 0\n        adjacency_solid.eliminate_zeros()\n    if dashed_edges is not None:\n        adjacency_dashed = adata.uns['paga'][dashed_edges].copy()\n        if threshold > 0:\n            adjacency_dashed.data[adjacency_dashed.data < threshold] = 0\n            adjacency_dashed.eliminate_zeros()\n\n    # compute positions\n    if pos is None:\n        pos = _compute_pos(\n            adjacency_solid, layout=layout, random_state=random_state, init_pos=init_pos, layout_kwds=layout_kwds)\n\n    if plot:\n        if ax is None:\n            axs, panel_pos, draw_region_width, figure_width = utils.setup_axes(\n                panels=colors, colorbars=colorbars)\n        else:\n            axs = ax\n        if len(colors) == 1 and not isinstance(axs, list):\n            axs = [axs]\n\n        for icolor, c in enumerate(colors):\n            if title[icolor] is not None:\n                axs[icolor].set_title(title[icolor])\n            sct = _paga_graph(\n                adata,\n                axs[icolor],\n                colors=c,\n                solid_edges=solid_edges,\n                dashed_edges=dashed_edges,\n                transitions=transitions,\n                threshold=threshold,\n                adjacency_solid=adjacency_solid,\n                adjacency_dashed=adjacency_dashed,\n                root=root,\n                labels=labels[icolor],\n                fontsize=fontsize,\n                fontweight=fontweight,\n                text_kwds=text_kwds,\n                node_size_scale=node_size_scale,\n                node_size_power=node_size_power,\n                edge_width_scale=edge_width_scale,\n                min_edge_width=min_edge_width,\n                max_edge_width=max_edge_width,\n                normalize_to_color=normalize_to_color,\n                frameon=frameon,\n                cmap=cmap,\n                cax=cax,\n                colorbar=colorbars[icolor],\n                cb_kwds=cb_kwds,\n                use_raw=use_raw,\n                title=title[icolor],\n                export_to_gexf=export_to_gexf,\n                single_component=single_component,\n                arrowsize=arrowsize,\n                pos=pos)\n            if colorbars[icolor]:\n                bottom = panel_pos[0][0]\n                height = panel_pos[1][0] - bottom\n                width = 0.006 * draw_region_width / len(colors)\n                left = panel_pos[2][2 * icolor + 1] + 0.2 * width\n                rectangle = [left, bottom, width, height]\n                fig = pl.gcf()\n                ax_cb = fig.add_axes(rectangle)\n                cb = pl.colorbar(sct, format=ticker.FuncFormatter(utils.ticks_formatter),\n                                 cax=ax_cb)\n    if add_pos:\n        adata.uns['paga']['pos'] = pos\n        logg.hint('added \\'pos\\', the PAGA positions (adata.uns[\\'paga\\'])')\n    if plot:\n        utils.savefig_or_show('paga', show=show, save=save)\n        if len(colors) == 1 and isinstance(axs, list): axs = axs[0]\n        return axs if show == False else None", "idx": 1999}
{"project": "Scanpy", "commit_id": "478_scanpy_0.3.2_scanpy_datasets_builtin.py_moignard15.py", "target": 1, "func": "def moignard15():\n    \"\"\"Hematopoiesis in early mouse embryos [Moignard15]_.\n    Returns\n    -------\n    adata : :class:`~scanpy.api.AnnData`\n        Annotated data matrix.\n    \"\"\"\n    filename = 'data/moignard15/nbt.3154-S3.xlsx'\n    backup_url = 'http://www.nature.com/nbt/journal/v33/n3/extref/nbt.3154-S3.xlsx'\n    adata = sc.read(filename, sheet='dCt_values.txt', cache=True, backup_url=backup_url)\n    # filter out 4 genes as in Haghverdi et al. (2016)\n    gene_subset = ~np.in1d(adata.var_names, ['Eif2b1', 'Mrpl19', 'Polr2a', 'Ubc'])\n    adata = adata[:, gene_subset]  # retain non-removed genes\n    # choose root cell for DPT analysis as in Haghverdi et al. (2016)\n    adata.uns['iroot'] = 532  # note that in Matlab/R, counting starts at 1\n    # annotate with Moignard et al. (2015) experimental cell groups\n    groups_order = ['HF', 'NP', 'PS', '4SG', '4SFG']\n    # annotate each sample/cell\n    adata.obs['exp_groups'] = [\n        next(gname for gname in groups_order if sname.startswith(gname))\n        for sname in adata.obs_names]\n    # fix the order and colors of names in \"groups\"\n    adata.uns['exp_groups_order'] = groups_order\n    adata.uns['exp_groups_colors'] = ['#D7A83E', '#7AAE5D', '#497ABC', '#AF353A', '#765099']\n    return adata", "idx": 2001}
{"project": "Scanpy", "commit_id": "463_scanpy_1.9.0_conftest.py_fmt_descr.py", "target": 0, "func": "def fmt_descr(descr):\n            if basename != \"\":\n                return f\"{descr} ({basename})\"\n            else:\n                return descr", "idx": 2002}
{"project": "Scanpy", "commit_id": "88_scanpy_0.0_scanpy_tools_dpt.py_detect_branching.py", "target": 1, "func": "def detect_branching(self, segs, segstips, iseg, tips3):\n    \"\"\"\n    Detect branching on given segment.\n    Call function _detect_branching and perform bookkeeping on segs and\n    segstips.\n    Parameters\n    ----------\n    segs : list of np.ndarray\n        Dchosen distance matrix restricted to segment.\n    segstips : list of np.ndarray\n        Stores all tip points for the segments in segs.\n    iseg : int\n        Position of segment under study in segs.\n    tips3 : np.ndarray\n        The three tip points. They form a 'triangle' that contains the data.\n\n    Returns\n    -------\n    segs : list of np.ndarray\n        Updated list of segments.\n    segstips : list of np.ndarray\n        Updated list of segstips.\n    \"\"\"\n    seg = segs[iseg]\n    # restrict distance matrix to points in chosen segment seg\n    Dseg = self.Dchosen[np.ix_(seg, seg)]\n    # given the three tip points and the distance matrix detect the\n    # branching on the segment, return the list ssegs of segments that\n    # are defined by splitting this segment\n    ssegs, ssegs_tips = self._detect_branching(Dseg, tips3)\n    # map back to global indices\n    for iseg_new, seg_new in enumerate(ssegs):\n        ssegs[iseg_new] = seg[seg_new]\n        if ssegs_tips[iseg_new][0] != -1:\n            ssegs_tips[iseg_new] = seg[ssegs_tips[iseg_new]]\n            # remove previous segment\n    segs.pop(iseg)\n    segstips.pop(iseg)\n    # append new segments\n    segs += ssegs\n    segstips += ssegs_tips\n    return segs, segstips", "idx": 2004}
{"project": "Scanpy", "commit_id": "671_scanpy_1.9.0_test_preprocessing.py_test_log1p_rep.py", "target": 0, "func": "def test_log1p_rep(count_matrix_format, base, dtype):\n    X = count_matrix_format(\n        np.abs(sp.random(100, 200, density=0.3, dtype=dtype)).toarray()\n    )\n    check_rep_mutation(sc.pp.log1p, X, base=base)\n    check_rep_results(sc.pp.log1p, X, base=base)", "idx": 2007}
{"project": "Scanpy", "commit_id": "272_scanpy_0.1_scanpy_plotting___init__.py_spring.py", "target": 1, "func": "def spring(adata,\n           color=None,\n           names=None,\n           comps='1,2',\n           cont=None,\n           layout='2d',\n           legendloc='right margin',\n           cmap=None,\n           pal=None,\n           right_margin=None,\n           size=None,\n           titles=None,\n           show=None):\n    \"\"\"Plot spring scatter plot.\n    Parameters\n    ----------\n    adata : AnnData\n        Annotated data matrix.\n    color : str, optional (default: first annotation)\n        Sample/Cell annotation for coloring in the form \"ann1,ann2,...\". String\n        annotation is plotted assuming categorical annotation, float and integer\n        annotation is plotted assuming continuous annoation. Option 'cont'\n        allows to switch between these default choices.\n    names : str, optional (default: all names in color)\n        Allows to restrict groups in sample annotation (color) to a few.\n    comps : str, optional (default: '1,2')\n         String in the form '1,2,3'.\n    cont : bool, None (default: None)\n        Switch on continuous layout, switch off categorical layout.\n    layout : {'2d', '3d', 'unfolded 3d'}, optional (default: '2d')\n         Layout of plot.\n    legendloc : see matplotlib.legend, optional (default: 'lower right')\n         Options for keyword argument 'loc'.\n    cmap : str (default: 'viridis')\n         String denoting matplotlib color map.\n    pal : list of str (default: matplotlib.rcParams['axes.prop_cycle'].by_key()['color'])\n         Colors cycle to use for categorical groups.\n    right_margin : float (default: 0.2)\n         Adjust how far the plotting panel extends to the right.\n    size : float (default: None)\n         Point size.\n    titles : str, optional (default: None)\n         Provide titles for panels as \"my title1,another title,...\".\n    \"\"\"\n    from ..examples import check_adata\n    adata = check_adata(adata)\n    Y = adata.smp['X_spring']\n    axs = scatter(adata,\n                  basis='spring',\n                  color=color,\n                  names=names,\n                  comps=comps,\n                  cont=cont,\n                  layout=layout,\n                  legendloc=legendloc,\n                  cmap=cmap,\n                  pal=pal,\n                  right_margin=right_margin,\n                  size=size,\n                  # defined in plotting\n                  titles=titles,\n                  show=False)\n    savefig_or_show('spring', show=show)", "idx": 2011}
{"project": "Scanpy", "commit_id": "900_scanpy_1.4.3_scanpy_plotting__tools_scatterplots.py_plot_scatter.py", "target": 1, "func": "def plot_scatter(\n    adata: AnnData,\n    basis: str,\n    *,\n    color: Union[str, Sequence[str], None] = None,\n    gene_symbols: Optional[str] = None,\n    use_raw: Optional[bool] = None,\n    sort_order: bool = True,\n    edges: bool = False,\n    edges_width: float = 0.1,\n    edges_color: Union[str, Sequence[float], Sequence[str]] = 'grey',\n    arrows: bool = False,\n    arrows_kwds: Optional[Mapping[str, Any]] = None,\n    groups: Optional[str] = None,\n    components: Union[str, Sequence[str]] = None,\n    layer: Optional[str] = None,\n    projection: str = '2d',\n    color_map: Union[Colormap, str, None] = None,\n    palette: Union[str, Sequence[str], Cycler, None] = None,\n    size: Optional[float] = None,\n    frameon: Optional[bool] = None,\n    legend_fontsize: Optional[int] = None,\n    legend_fontweight: str = 'bold',\n    legend_loc: str = 'right margin',\n    legend_fontoutline: Optional[int] = None,\n    ncols: int = 4,\n    hspace: float = 0.25,\n    wspace: Optional[float] = None,\n    title: Union[str, Sequence[str], None] = None,\n    show: Optional[bool] = None,\n    save: Union[bool, str, None] = None,\n    ax: Optional[Axes] = None,\n    return_fig: Optional[bool] = None,\n    **kwargs\n) -> Union[Figure, Axes, None]:\n    sanitize_anndata(adata)\n    if color_map is not None:\n        kwargs['cmap'] = color_map\n    if size is not None:\n        kwargs['s'] = size\n    if 'edgecolor' not in kwargs:\n        # by default turn off edge color. Otherwise, for\n        # very small sizes the edge will not reduce its size\n        # (https://github.com/theislab/scanpy/issues/293)\n        kwargs['edgecolor'] = 'none'\n    if projection == '3d':\n        from mpl_toolkits.mplot3d import Axes3D\n        args_3d = {'projection': '3d'}\n    else:\n        args_3d = {}\n\n    if use_raw is None:\n        # check if adata.raw is set\n        use_raw = adata.raw is not None\n\n    if wspace is None:\n        #  try to set a wspace that is not too large or too small given the\n        #  current figure size\n        wspace = 0.75 / rcParams['figure.figsize'][0] + 0.02\n    if adata.raw is None and use_raw:\n        raise ValueError(\n            \"`use_raw` is set to True but AnnData object does not have raw. \"\n            \"Please check.\"\n        )\n    # turn color into a python list\n    color = [color] if isinstance(color, str) or color is None else list(color)\n    if title is not None:\n        # turn title into a python list if not None\n        title = [title] if isinstance(title, str) else list(title)\n    ####\n    # get the points position and the components list (only if components is not 'None)\n    data_points, components_list = _get_data_points(adata, basis, projection, components)\n    ###\n    # setup layout. Most of the code is for the case when multiple plots are required\n    # 'color' is a list of names that want to be plotted. Eg. ['Gene1', 'louvain', 'Gene2'].\n    # component_list is a list of components [[0,1], [1,2]]\n    if (isinstance(color, abc.Sequence) and len(color) > 1) or len(components_list) > 1:\n        if ax is not None:\n            raise ValueError(\n                \"When plotting multiple panels (each for a given value of 'color') \"\n                \"a given ax can not be used\"\n            )\n        if len(components_list) == 0:\n            components_list = [None]\n        multi_panel = True\n        # each plot needs to be its own panel\n        from matplotlib import gridspec\n        # set up the figure\n        num_panels = len(color) * len(components_list)\n        n_panels_x = min(ncols, num_panels)\n        n_panels_y = np.ceil(num_panels / n_panels_x).astype(int)\n        # each panel will have the size of rcParams['figure.figsize']\n        fig = pl.figure(figsize=(n_panels_x * rcParams['figure.figsize'][0] * (1 + wspace),\n                                 n_panels_y * rcParams['figure.figsize'][1]))\n        left = 0.2 / n_panels_x\n        bottom = 0.13 / n_panels_y\n        gs = gridspec.GridSpec(\n            nrows=n_panels_y, ncols=n_panels_x,\n            left=left, right=1-(n_panels_x-1)*left-0.01/n_panels_x,\n            bottom=bottom, top=1-(n_panels_y-1)*bottom-0.1/n_panels_y,\n            hspace=hspace, wspace=wspace,\n        )\n    else:\n        if len(components_list) == 0:\n            components_list = [None]\n        multi_panel = False\n        if ax is None:\n            fig = pl.figure()\n            ax = fig.add_subplot(111, **args_3d)\n    ###\n    # make the plots\n    axs = []\n    import itertools\n    idx_components = range(len(components_list))\n    # use itertools.product to make a plot for each color and for each component\n    # For example if color=[gene1, gene2] and components=['1,2, '2,3'].\n    # The plots are: [color=gene1, components=[1,2], color=gene1, components=[2,3],\n    #                 color=gene2, components = [1, 2], color=gene2, components=[2,3]]\n    for count, (value_to_plot, component_idx) in enumerate(itertools.product(color, idx_components)):\n        color_vector, categorical = _get_color_values(\n            adata, value_to_plot, layer=layer,\n            groups=groups, palette=palette,\n            use_raw=use_raw, gene_symbols=gene_symbols,\n        )\n        # check if higher value points should be plot on top\n        if sort_order is True and value_to_plot is not None and categorical is False:\n            order = np.argsort(color_vector)\n            color_vector = color_vector[order]\n            _data_points = data_points[component_idx][order, :]\n            # check if 'size' is given (stored in kwargs['s']\n            # and reorder it.\n            import pandas.core.series\n            if 's' in kwargs and kwargs['s'] is not None \\\n                and isinstance(kwargs['s'],(list, pandas.core.series.Series, np.ndarray)) \\\n                and len(kwargs['s']) == len(color_vector):\n                kwargs['s'] = np.array(kwargs['s'])[order]\n        else:\n            _data_points = data_points[component_idx]\n        # if plotting multiple panels, get the ax from the grid spec\n        # else use the ax value (either user given or created previously)\n        if multi_panel is True:\n            ax = pl.subplot(gs[count], **args_3d)\n            axs.append(ax)\n        if not (settings._frameon if frameon is None else frameon):\n            ax.axis('off')\n        if title is None:\n            if value_to_plot is not None:\n                ax.set_title(value_to_plot)\n            else:\n                ax.set_title('')\n        else:\n            try:\n                ax.set_title(title[count])\n            except IndexError:\n                logg.warning(\n                    \"The title list is shorter than the number of panels. \"\n                    \"Using 'color' value instead for some plots.\"\n                )\n                ax.set_title(value_to_plot)\n        if 's' not in kwargs:\n            kwargs['s'] = 120000 / _data_points.shape[0]\n        # make the scatter plot\n        if projection == '3d':\n            cax = ax.scatter(\n                _data_points[:, 0], _data_points[:, 1], _data_points[:, 2],\n                marker=\".\", c=color_vector, rasterized=settings._vector_friendly,\n                **kwargs,\n            )\n        else:\n            cax = ax.scatter(\n                _data_points[:, 0], _data_points[:, 1],\n                marker=\".\", c=color_vector, rasterized=settings._vector_friendly,\n                **kwargs,\n            )\n        # remove y and x ticks\n        ax.set_yticks([])\n        ax.set_xticks([])\n        if projection == '3d':\n            ax.set_zticks([])\n        # set default axis_labels\n        name = _basis2name(basis)\n        if components is not None:\n            axis_labels = [name + str(x + 1) for x in components_list[component_idx]]\n        elif projection == '3d':\n            axis_labels = [name + str(x + 1) for x in range(3)]\n        else:\n            axis_labels = [name + str(x + 1) for x in range(2)]\n        ax.set_xlabel(axis_labels[0])\n        ax.set_ylabel(axis_labels[1])\n        if projection == '3d':\n            # shift the label closer to the axis\n            ax.set_zlabel(axis_labels[2], labelpad=-7)\n        ax.autoscale_view()\n        if edges:\n            utils.plot_edges(ax, adata, basis, edges_width, edges_color)\n        if arrows:\n            utils.plot_arrows(ax, adata, basis, arrows_kwds)\n        if value_to_plot is None:\n            # if only dots were plotted without an associated value\n            # there is not need to plot a legend or a colorbar\n            continue\n        if legend_fontoutline is not None:\n            legend_fontoutline = [patheffects.withStroke(linewidth=legend_fontoutline,\n                                                         foreground='w')]\n        _add_legend_or_colorbar(\n            adata, ax, cax, categorical, value_to_plot, legend_loc,\n            _data_points, legend_fontweight, legend_fontsize, legend_fontoutline,\n            groups, multi_panel,\n        )\n    if return_fig is True:\n        return fig\n    axs = axs if multi_panel else ax\n    utils.savefig_or_show(basis, show=show, save=save)\n    if show is False:\n        return axs", "idx": 2012}
{"project": "Scanpy", "commit_id": "84_scanpy_1.9.0__settings.py___init__.py", "target": 0, "func": "def __init__(\n        self,\n        *,\n        verbosity: str = \"warning\",\n        plot_suffix: str = \"\",\n        file_format_data: str = \"h5ad\",\n        file_format_figs: str = \"pdf\",\n        autosave: bool = False,\n        autoshow: bool = True,\n        writedir: Union[str, Path] = \"./write/\",\n        cachedir: Union[str, Path] = \"./cache/\",\n        datasetdir: Union[str, Path] = \"./data/\",\n        figdir: Union[str, Path] = \"./figures/\",\n        cache_compression: Union[str, None] = 'lzf',\n        max_memory=15,\n        n_jobs=1,\n        logfile: Union[str, Path, None] = None,\n        categories_to_ignore: Iterable[str] = (\"N/A\", \"dontknow\", \"no_gate\", \"?\"),\n        _frameon: bool = True,\n        _vector_friendly: bool = False,\n        _low_resolution_warning: bool = True,\n        n_pcs=50,\n    ):\n        # logging\n        self._root_logger = _RootLogger(logging.INFO)  # level will be replaced\n        self.logfile = logfile\n        self.verbosity = verbosity\n        # rest\n        self.plot_suffix = plot_suffix\n        self.file_format_data = file_format_data\n        self.file_format_figs = file_format_figs\n        self.autosave = autosave\n        self.autoshow = autoshow\n        self.writedir = writedir\n        self.cachedir = cachedir\n        self.datasetdir = datasetdir\n        self.figdir = figdir\n        self.cache_compression = cache_compression\n        self.max_memory = max_memory\n        self.n_jobs = n_jobs\n        self.categories_to_ignore = categories_to_ignore\n        self._frameon = _frameon\n        \"\"\"bool: See set_figure_params.\"\"\"\n\n        self._vector_friendly = _vector_friendly\n        \"\"\"Set to true if you want to include pngs in svgs and pdfs.\"\"\"\n\n        self._low_resolution_warning = _low_resolution_warning\n        \"\"\"Print warning when saving a figure with low resolution.\"\"\"\n\n        self._start = time()\n        \"\"\"Time when the settings module is first imported.\"\"\"\n\n        self._previous_time = self._start\n        \"\"\"Variable for timing program parts.\"\"\"\n\n        self._previous_memory_usage = -1\n        \"\"\"Stores the previous memory usage.\"\"\"\n\n        self.N_PCS = n_pcs\n        \"\"\"Default number of principal components to use.\"\"\"", "idx": 2013}
{"project": "Scanpy", "commit_id": "547_scanpy_0.4.4_scanpy_tools_diffmap.py_diffmap.py", "target": 1, "func": "def diffmap(adata, n_comps=15, n_jobs=None, copy=False):\n    \"\"\"Diffusion Maps [Coifman05]_ [Haghverdi15]_ [Wolf17]_.\n    Diffusion maps [Coifman05]_ has been proposed for visualizing single-cell\n    data by [Haghverdi15]_. The tool uses the adapted Gaussian kernel suggested\n    by [Haghverdi16]_ in the implementation of [Wolf17]_.\n    Parameters\n    ----------\n    adata : :class:`~scanpy.api.AnnData`\n        Annotated data matrix.\n    n_comps : `int`, optional (default: 15)\n        The number of dimensions of the representation.\n    n_jobs : `int` or `None`\n        Number of CPUs to use (default: `sc.settings.n_jobs`).\n    copy : `bool` (default: `False`)\n        Return a copy instead of writing to adata.\n    Returns\n    -------\n    Depending on `copy`, returns or updates `adata` with the following fields.\n    X_diffmap : `adata.obsm`\n        Diffusion map representation of data, which is the right eigen basis of\n        the transition matrix with eigenvectors as columns.\n    diffmap_evals : `np.ndarray` (`adata.uns`)\n        Array of size (number of eigen vectors). Eigenvalues of transition matrix.\n    \"\"\"\n    logg.info('computing Diffusion Maps', r=True)\n    if n_comps <= 2:\n        raise ValueError(\n            'Provide any value greater than 2. ')\n    adata = adata.copy() if copy else adata\n    dmap = dpt.DPT(adata, n_jobs=n_jobs)\n    dmap.compute_transitions()\n    dmap.compute_eigen(n_comps=n_comps)\n    adata.obsm['X_diffmap'] = dmap.rbasis\n    adata.uns['diffmap_evals'] = dmap.evals\n    logg.info('    finished', time=True, end=' ' if settings.verbosity > 2 else '\\n')\n    logg.hint('added\\n'\n              '    \\'X_diffmap\\', diffmap coordinates (adata.obsm)\\n'\n              '    \\'diffmap_evals\\', eigenvalues of transition matrix (adata.uns)')\n    return adata if copy else None", "idx": 2016}
{"project": "Scanpy", "commit_id": "255_scanpy_1.9.0__anndata.py__plot_categories_as_colorblocks.py", "target": 0, "func": "def _plot_categories_as_colorblocks(\n    groupby_ax: Axes,\n    obs_tidy: pd.DataFrame,\n    colors=None,\n    orientation: Literal['top', 'bottom', 'left', 'right'] = 'left',\n    cmap_name: str = 'tab20',\n):\n    \"\"\"\\\n    Plots categories as colored blocks. If orientation is 'left', the categories\n    are plotted vertically, otherwise they are plotted horizontally.\n\n    Parameters\n    ----------\n    groupby_ax\n    obs_tidy\n    colors\n        Sequence of valid color names to use for each category.\n    orientation\n    cmap_name\n        Name of colormap to use, in case colors is None\n\n    Returns\n    -------\n    ticks position, labels, colormap\n    \"\"\"\n\n    groupby = obs_tidy.index.name\n    from matplotlib.colors import ListedColormap, BoundaryNorm\n\n    if colors is None:\n        groupby_cmap = pl.get_cmap(cmap_name)\n    else:\n        groupby_cmap = ListedColormap(colors, groupby + '_cmap')\n    norm = BoundaryNorm(np.arange(groupby_cmap.N + 1) - 0.5, groupby_cmap.N)\n\n    # determine groupby label positions such that they appear\n    # centered next/below to the color code rectangle assigned to the category\n    value_sum = 0\n    ticks = []  # list of centered position of the labels\n    labels = []\n    label2code = {}  # dictionary of numerical values asigned to each label\n    for code, (label, value) in enumerate(\n        obs_tidy.index.value_counts(sort=False).iteritems()\n    ):\n        ticks.append(value_sum + (value / 2))\n        labels.append(label)\n        value_sum += value\n        label2code[label] = code\n\n    groupby_ax.grid(False)\n\n    if orientation == 'left':\n        groupby_ax.imshow(\n            np.array([[label2code[lab] for lab in obs_tidy.index]]).T,\n            aspect='auto',\n            cmap=groupby_cmap,\n            norm=norm,\n        )\n        if len(labels) > 1:\n            groupby_ax.set_yticks(ticks)\n            groupby_ax.set_yticklabels(labels)\n\n        # remove y ticks\n        groupby_ax.tick_params(axis='y', left=False, labelsize='small')\n        # remove x ticks and labels\n        groupby_ax.tick_params(axis='x', bottom=False, labelbottom=False)\n\n        # remove surrounding lines\n        groupby_ax.spines['right'].set_visible(False)\n        groupby_ax.spines['top'].set_visible(False)\n        groupby_ax.spines['left'].set_visible(False)\n        groupby_ax.spines['bottom'].set_visible(False)\n\n        groupby_ax.set_ylabel(groupby)\n    else:\n        groupby_ax.imshow(\n            np.array([[label2code[lab] for lab in obs_tidy.index]]),\n            aspect='auto',\n            cmap=groupby_cmap,\n            norm=norm,\n        )\n        if len(labels) > 1:\n            groupby_ax.set_xticks(ticks)\n            if max([len(str(x)) for x in labels]) < 3:\n                # if the labels are small do not rotate them\n                rotation = 0\n            else:\n                rotation = 90\n            groupby_ax.set_xticklabels(labels, rotation=rotation)\n\n        # remove x ticks\n        groupby_ax.tick_params(axis='x', bottom=False, labelsize='small')\n        # remove y ticks and labels\n        groupby_ax.tick_params(axis='y', left=False, labelleft=False)\n\n        # remove surrounding lines\n        groupby_ax.spines['right'].set_visible(False)\n        groupby_ax.spines['top'].set_visible(False)\n        groupby_ax.spines['left'].set_visible(False)\n        groupby_ax.spines['bottom'].set_visible(False)\n\n        groupby_ax.set_xlabel(groupby)\n\n    return label2code, ticks, labels, groupby_cmap, norm", "idx": 2020}
{"project": "Scanpy", "commit_id": "660_scanpy_1.9.0_test_plotting.py_test_scatter_rep.py", "target": 0, "func": "def test_scatter_rep(tmpdir):\n    \"\"\"\n    Test to make sure I can predict when scatter reps should be the same\n    \"\"\"\n    TESTDIR = Path(tmpdir)\n    rep_args = {\n        \"raw\": {\"use_raw\": True},\n        \"layer\": {\"layer\": \"layer\", \"use_raw\": False},\n        \"X\": {\"use_raw\": False},\n    }\n    states = pd.DataFrame.from_records(\n        zip(\n            list(chain.from_iterable(repeat(x, 3) for x in [\"X\", \"raw\", \"layer\"])),\n            list(chain.from_iterable(repeat(\"abc\", 3))),\n            [1, 2, 3, 3, 1, 2, 2, 3, 1],\n        ),\n        columns=[\"rep\", \"gene\", \"result\"],\n    )\n    states[\"outpth\"] = [\n        TESTDIR / f\"{state.gene}_{state.rep}_{state.result}.png\"\n        for state in states.itertuples()\n    ]\n    pattern = np.array(list(chain.from_iterable(repeat(i, 5) for i in range(3))))\n    coords = np.c_[np.arange(15) % 5, pattern]\n\n    adata = AnnData(\n        X=np.zeros((15, 3)),\n        layers={\"layer\": np.zeros((15, 3))},\n        obsm={\"X_pca\": coords},\n        var=pd.DataFrame(index=[x for x in list(\"abc\")]),\n        obs=pd.DataFrame(index=[f\"cell{i}\" for i in range(15)]),\n    )\n    adata.raw = adata.copy()\n    adata.X[np.arange(15), pattern] = 1\n    adata.raw.X[np.arange(15), (pattern + 1) % 3] = 1\n    adata.layers[\"layer\"][np.arange(15), (pattern + 2) % 3] = 1\n\n    for state in states.itertuples():\n        sc.pl.pca(adata, color=state.gene, **rep_args[state.rep], show=False)\n        plt.savefig(state.outpth, dpi=60)\n        plt.close()\n\n    for s1, s2 in combinations(states.itertuples(), 2):\n        comp = compare_images(str(s1.outpth), str(s2.outpth), tol=5)\n        if s1.result == s2.result:\n            assert comp is None, comp\n        else:\n            assert \"Error\" in comp, f\"{s1.outpth}, {s2.outpth} aren't supposed to match\"", "idx": 2023}
{"project": "Scanpy", "commit_id": "755_scanpy_1.3.4_scanpy_plotting_anndata.py__prepare_dataframe.py", "target": 1, "func": "def _prepare_dataframe(adata, var_names, groupby=None, use_raw=None, log=False,\n                       num_categories=7, layer=None):\n    \"\"\"\n    Given the anndata object, prepares a data frame in which the row index are the categories\n    defined by group by and the columns correspond to var_names.\n    Parameters\n    ----------\n    adata : :class:`~anndata.AnnData`\n        Annotated data matrix.\n    var_names : `str` or list of `str`\n        `var_names` should be a valid subset of  `adata.var_names`.\n    groupby : `str` or `None`, optional (default: `None`)\n        The key of the observation grouping to consider. It is expected that\n        groupby is a categorical. If groupby is not a categorical observation,\n        it would be subdivided into `num_categories`.\n    log : `bool`, optional (default: `False`)\n        Use the log of the values\n    use_raw : `bool`, optional (default: `None`)\n        Use `raw` attribute of `adata` if present.\n    num_categories : `int`, optional (default: `7`)\n        Only used if groupby observation is not categorical. This value\n        determines the number of groups into which the groupby observation\n        should be subdivided.\n    Returns\n    -------\n    Tuple of `pandas.DataFrame` and list of categories.\n    \"\"\"\n    from scipy.sparse import issparse\n    sanitize_anndata(adata)\n    if use_raw is None and adata.raw is not None: use_raw = True\n    if isinstance(var_names, str):\n        var_names = [var_names]\n    if groupby is not None:\n        if groupby not in adata.obs_keys():\n            raise ValueError('groupby has to be a valid observation. Given value: {}, '\n                             'valid observations: {}'.format(groupby, adata.obs_keys()))\n\n    if use_raw:\n        matrix = adata.raw[:, var_names].X\n    else:\n        if layer is None:\n            matrix = adata[:, var_names].X\n        else:\n            if layer not in adata.layers.keys():\n                raise KeyError('Selected layer: {} is not in the layers list. The list of '\n                               'valid layers is: {}'.format(layer, adata.layers.keys()))\n            else:\n                matrix = adata[:, var_names].layers[layer]\n    if issparse(matrix):\n        matrix = matrix.toarray()\n    if log:\n        matrix = np.log1p(matrix)\n    obs_tidy = pd.DataFrame(matrix, columns=var_names)\n    if groupby is None:\n        groupby = ''\n        categorical = pd.Series(np.repeat('', len(obs_tidy))).astype('category')\n    else:\n        if not is_categorical_dtype(adata.obs[groupby]):\n            # if the groupby column is not categorical, turn it into one\n            # by subdividing into  `num_categories` categories\n            categorical = pd.cut(adata.obs[groupby], num_categories)\n        else:\n            categorical = adata.obs[groupby]\n    obs_tidy.set_index(categorical, groupby, inplace=True)\n    categories = obs_tidy.index.categories\n    return categories, obs_tidy", "idx": 2032}
{"project": "Scanpy", "commit_id": "800_scanpy_1.9.0__embedding_density.py__calc_density.py", "target": 0, "func": "def _calc_density(x: np.ndarray, y: np.ndarray):\n    \"\"\"\\\n    Calculates the density of points in 2 dimensions.\n    \"\"\"\n    from scipy.stats import gaussian_kde\n\n    # Calculate the point density\n    xy = np.vstack([x, y])\n    z = gaussian_kde(xy)(xy)\n\n    min_z = np.min(z)\n    max_z = np.max(z)\n\n    # Scale between 0 and 1\n    scaled_z = (z - min_z) / (max_z - min_z)\n\n    return scaled_z", "idx": 2033}
{"project": "Scanpy", "commit_id": "710_scanpy_1.9.0_test_qc_metrics.py_test_inner_methods.py", "target": 0, "func": "def test_inner_methods(anndata):\n    adata = anndata.copy()\n    full_inplace = adata.copy()\n    partial_inplace = adata.copy()\n    obs_orig, var_orig = sc.pp.calculate_qc_metrics(adata)\n    assert np.all(obs_orig == describe_obs(adata))\n    assert np.all(var_orig == describe_var(adata))\n    sc.pp.calculate_qc_metrics(full_inplace, inplace=True)\n    describe_obs(partial_inplace, inplace=True)\n    describe_var(partial_inplace, inplace=True)\n    assert np.all(full_inplace.obs == partial_inplace.obs)\n    assert np.all(full_inplace.var == partial_inplace.var)\n    assert np.all(partial_inplace.obs[obs_orig.columns] == obs_orig)\n    assert np.all(partial_inplace.var[var_orig.columns] == var_orig)", "idx": 2037}
{"project": "Scanpy", "commit_id": "313_scanpy_1.9.0__utils.py_plot_arrows.py", "target": 0, "func": "def plot_arrows(axs, adata, basis, arrows_kwds=None):\n    if not isinstance(axs, cabc.Sequence):\n        axs = [axs]\n    v_prefix = next(\n        (p for p in ['velocity', 'Delta'] if f'{p}_{basis}' in adata.obsm), None\n    )\n    if v_prefix is None:\n        raise ValueError(\n            \"`arrows=True` requires \"\n            f\"`'velocity_{basis}'` from scvelo or \"\n            f\"`'Delta_{basis}'` from velocyto.\"\n        )\n    if v_prefix == 'velocity':\n        logg.warning(\n            'The module `scvelo` has improved plotting facilities. '\n            'Prefer using `scv.pl.velocity_embedding` to `arrows=True`.'\n        )\n\n    basis_key = _get_basis(adata, basis)\n    X = adata.obsm[basis_key]\n    V = adata.obsm[f'{v_prefix}_{basis}']\n    for ax in axs:\n        quiver_kwds = arrows_kwds if arrows_kwds is not None else {}\n        ax.quiver(\n            X[:, 0],\n            X[:, 1],\n            V[:, 0],\n            V[:, 1],\n            **quiver_kwds,\n            rasterized=settings._vector_friendly,", "idx": 2039}
{"project": "Scanpy", "commit_id": "304_scanpy_1.9.0__utils.py_timeseries_as_heatmap.py", "target": 0, "func": "def timeseries_as_heatmap(\n    X: np.ndarray, var_names: Collection[str] = (), highlights_x=(), color_map=None\n):\n    \"\"\"\\\n    Plot timeseries as heatmap.\n\n    Parameters\n    ----------\n    X\n        Data array.\n    var_names\n        Array of strings naming variables stored in columns of X.\n    \"\"\"\n    if len(var_names) == 0:\n        var_names = np.arange(X.shape[1])\n    if var_names.ndim == 2:\n        var_names = var_names[:, 0]\n\n    # transpose X\n    X = X.T\n    min_x = np.min(X)\n\n    # insert space into X\n    if False:\n        # generate new array with highlights_x\n        space = 10  # integer\n        x_new = np.zeros((X.shape[0], X.shape[1] + space * len(highlights_x)))\n        hold = 0\n        _hold = 0\n        space_sum = 0\n        for ih, h in enumerate(highlights_x):\n            _h = h + space_sum\n            x_new[:, _hold:_h] = X[:, hold:h]\n            x_new[:, _h : _h + space] = min_x * np.ones((X.shape[0], space))\n            # update variables\n            space_sum += space\n            _hold = _h + space\n            hold = h\n        x_new[:, _hold:] = X[:, hold:]\n\n    _, ax = pl.subplots(figsize=(1.5 * 4, 2 * 4))\n    img = ax.imshow(\n        np.array(X, dtype=np.float_),\n        aspect='auto',\n        interpolation='nearest',\n        cmap=color_map,\n    )\n    pl.colorbar(img, shrink=0.5)\n    pl.yticks(range(X.shape[0]), var_names)\n    for h in highlights_x:\n        pl.plot([h, h], [0, X.shape[0]], '--', color='black')\n    pl.xlim([0, X.shape[1] - 1])\n    pl.ylim([0, X.shape[0] - 1])", "idx": 2046}
{"project": "Scanpy", "commit_id": "688_scanpy_1.2.2_scanpy_plotting_anndata.py__scatter_obs.py", "target": 1, "func": "def _scatter_obs(\n        adata,\n        x=None,\n        y=None,\n        color=None,\n        use_raw=True,\n        sort_order=True,\n        alpha=None,\n        basis=None,\n        groups=None,\n        components=None,\n        projection='2d',\n        legend_loc='right margin',\n        legend_fontsize=None,\n        legend_fontweight=None,\n        color_map=None,\n        palette=None,\n        frameon=None,\n        right_margin=None,\n        left_margin=None,\n        size=None,\n        title=None,\n        show=None,\n        save=None,\n        ax=None,\n        layers='X'):\n    \"\"\"See docstring of scatter.\"\"\"\n    sanitize_anndata(adata)\n    from scipy.sparse import issparse\n    if isinstance(layers, str) and (layers == 'X' or layers in adata.layers.keys()):\n        layers = (layers, layers, layers)\n    elif isinstance(layers, (tuple, list)) and len(layers) == 3:\n        for layer in layers:\n            if layer not in adata.layers.keys() and layer != 'X':\n                raise ValueError('layers should have elements that are either \"X\" or in adata.layers.keys()')\n    else: raise ValueError('layers should be a string or a list/tuple of length 3')\n    if use_raw and (layers != ('X', 'X', 'X') or layers != ['X', 'X', 'X']):\n        ValueError('use_raw should be False if layers different from \"X\" are used')\n    if legend_loc not in VALID_LEGENDLOCS:\n        raise ValueError(\n            'Invalid `legend_loc`, need to be one of: {}.'.format(VALID_LEGENDLOCS))\n    if components is None: components = '1,2' if '2d' in projection else '1,2,3'\n    if isinstance(components, str): components = components.split(',')\n    components = np.array(components).astype(int) - 1\n    keys = ['grey'] if color is None else [color] if isinstance(color, str) else color\n    if title is not None and isinstance(title, str):\n        title = [title]\n    highlights = adata.uns['highlights'] if 'highlights' in adata.uns else []\n    if basis is not None:\n        try:\n            # ignore the '0th' diffusion component\n            if basis == 'diffmap': components += 1\n            Y = adata.obsm['X_' + basis][:, components]\n            # correct the component vector for use in labeling etc.\n            if basis == 'diffmap': components -= 1\n        except KeyError:\n            raise KeyError('compute coordinates using visualization tool {} first'\n                           .format(basis))\n    elif x is not None and y is not None:\n        x_arr = adata._get_obs_array(x, use_raw=use_raw, layer=layers[0])\n        y_arr = adata._get_obs_array(y, use_raw=use_raw, layer=layers[1])\n        Y = np.c_[x_arr[:, None], y_arr[:, None]]\n    else:\n        raise ValueError('Either provide a `basis` or `x` and `y`.')\n    if size is None:\n        n = Y.shape[0]\n        size = 120000 / n\n    if legend_loc.startswith('on data') and legend_fontsize is None:\n        legend_fontsize = rcParams['legend.fontsize']\n    elif legend_fontsize is None:\n        legend_fontsize = rcParams['legend.fontsize']\n    palette_was_none = False\n    if palette is None: palette_was_none = True\n    if isinstance(palette, list):\n        if not is_color_like(palette[0]):\n            palettes = palette\n        else:\n            palettes = [palette]\n    else:\n        palettes = [palette for i in range(len(keys))]\n    for i, palette in enumerate(palettes):\n        palettes[i] = utils.default_palette(palette)\n    if basis is not None:\n        component_name = (\n            'DC' if basis == 'diffmap'\n            else 'tSNE' if basis == 'tsne'\n            else 'UMAP' if basis == 'umap'\n            else 'PC' if basis == 'pca'\n            else basis.replace('draw_graph_', '').upper() if 'draw_graph' in basis\n            else basis)\n    else:\n        component_name = None\n    axis_labels = (x, y) if component_name is None else None\n    show_ticks = True if component_name is None else False\n    # the actual color ids, e.g. 'grey' or '#109482'\n    color_ids = [None if not is_color_like(key)\n                 else key for key in keys]\n    categoricals = []\n    colorbars = []\n    for ikey, key in enumerate(keys):\n        if color_ids[ikey] is not None:\n            c = color_ids[ikey]\n            continuous = True\n            categorical = False\n            colorbars.append(False)\n        else:\n            c = 'white' if projection == '2d' else 'white'\n            categorical = False\n            continuous = False\n            # test whether we have categorial or continuous annotation\n            if key in adata.obs_keys():\n                if is_categorical_dtype(adata.obs[key]):\n                    categorical = True\n                else:\n                    continuous = True\n                    c = adata.obs[key]\n            # coloring according to gene expression\n            elif (use_raw\n                  and adata.raw is not None\n                  and key in adata.raw.var_names):\n                c = adata.raw[:, key].X\n                continuous = True\n            elif key in adata.var_names:\n                c = adata[:, key].X if layers[2]=='X' else adata[:, key].layers[layers[2]]\n                c = c.toarray().flatten() if issparse(c) else c\n                continuous = True\n            else:\n                raise ValueError(\n                    'key \\'{}\\' is invalid! pass valid observation annotation, '\n                    'one of {} or a gene name {}'\n                    .format(key, adata.obs_keys(), adata.var_names))\n            colorbars.append(True if continuous else False)\n        if categorical: categoricals.append(ikey)\n        color_ids[ikey] = c\n    if right_margin is None and len(categoricals) > 0:\n        if legend_loc == 'right margin': right_margin = 0.5\n    if title is None and keys[0] is not None:\n        title = [key.replace('_', ' ') if not is_color_like(key) else '' for key in keys]\n    axs = scatter_base(Y,\n                       title=title,\n                       alpha=alpha,\n                       component_name=component_name,\n                       axis_labels=axis_labels,\n                       component_indexnames=components + 1,\n                       projection=projection,\n                       colors=color_ids,\n                       highlights=highlights,\n                       colorbars=colorbars,\n                       right_margin=right_margin,\n                       left_margin=left_margin,\n                       sizes=[size for c in keys],\n                       color_map=color_map,\n                       show_ticks=show_ticks,\n                       ax=ax)\n\n    # loop over all categorical annotation and plot it\n    for i, ikey in enumerate(categoricals):\n        palette = palettes[i]\n        key = keys[ikey]\n        utils.add_colors_for_categorical_sample_annotation(\n            adata, key, palette, force_update_colors=not palette_was_none)\n        # actually plot the groups\n        mask_remaining = np.ones(Y.shape[0], dtype=bool)\n        centroids = {}\n        if groups is None:\n            for iname, name in enumerate(adata.obs[key].cat.categories):\n                if name not in settings.categories_to_ignore:\n                    mask = scatter_group(axs[ikey], key, iname,\n                                         adata, Y, projection, size=size, alpha=alpha)\n                    mask_remaining[mask] = False\n                    if legend_loc.startswith('on data'): add_centroid(centroids, name, Y, mask)\n        else:\n            for name in groups:\n                if name not in set(adata.obs[key].cat.categories):\n                    raise ValueError('\"' + name + '\" is invalid!'\n                                     + ' specify valid name, one of '\n                                     + str(adata.obs[key].cat.categories))\n                else:\n                    iname = np.flatnonzero(adata.obs[key].cat.categories.values == name)[0]\n                    mask = scatter_group(axs[ikey], key, iname,\n                                         adata, Y, projection, size=size, alpha=alpha)\n                    if legend_loc.startswith('on data'): add_centroid(centroids, name, Y, mask)\n                    mask_remaining[mask] = False\n        if mask_remaining.sum() > 0:\n            data = [Y[mask_remaining, 0], Y[mask_remaining, 1]]\n            if projection == '3d': data.append(Y[mask_remaining, 2])\n            axs[ikey].scatter(*data, marker='.', c='lightgrey', s=size,\n                                    edgecolors='none', zorder=-1)\n        legend = None\n        if legend_loc.startswith('on data'):\n            if legend_fontweight is None:\n                legend_fontweight = 'bold'\n            for name, pos in centroids.items():\n                axs[ikey].text(pos[0], pos[1], name,\n                               weight=legend_fontweight,\n                               verticalalignment='center',\n                               horizontalalignment='center',\n                               fontsize=legend_fontsize)\n            all_pos = np.zeros((len(adata.obs[key].cat.categories), 2))\n            for iname, name in enumerate(adata.obs[key].cat.categories):\n                if name in centroids:\n                    all_pos[iname] = centroids[name]\n                else:\n                    all_pos[iname] = [np.nan, np.nan]\n            utils._tmp_cluster_pos = all_pos\n            if legend_loc == 'on data export':\n                filename = settings.writedir + 'pos.csv'\n                logg.msg('exporting label positions to {}'.format(filename), v=1)\n                if settings.writedir != '' and not os.path.exists(settings.writedir):\n                    os.makedirs(settings.writedir)\n                np.savetxt(filename, all_pos, delimiter=',')\n        elif legend_loc == 'right margin':\n            legend = axs[ikey].legend(\n                frameon=False, loc='center left',\n                bbox_to_anchor=(1, 0.5),\n                ncol=(1 if len(adata.obs[key].cat.categories) <= 14\n                      else 2 if len(adata.obs[key].cat.categories) <= 30 else 3),\n                fontsize=legend_fontsize)\n        elif legend_loc != 'none':\n            legend = axs[ikey].legend(\n                frameon=False, loc=legend_loc, fontsize=legend_fontsize)\n        if legend is not None:\n            for handle in legend.legendHandles: handle.set_sizes([300.0])\n    # draw a frame around the scatter\n    frameon = settings._frameon if frameon is None else frameon\n    if not frameon:\n        for ax in axs:\n            ax.set_xlabel('')\n            ax.set_ylabel('')\n            ax.set_frame_on(False)\n    utils.savefig_or_show('scatter' if basis is None else basis, show=show, save=save)\n    if show == False: return axs if len(keys) > 1 else axs[0]", "idx": 2050}
{"project": "Scanpy", "commit_id": "89_scanpy_0.0_scanpy_tools_dpt.py__detect_branching.py", "target": 1, "func": "def _detect_branching(self, Dseg, tips):\n    \"\"\"\n    Detect branching on given segment.\n    Call function __detect_branching three times for all three orderings of\n    tips. Points that do not belong to the same segment in all three\n    orderings are assigned to a fourth segment. The latter is, by Haghverdi\n    et al. (2016) referred to as 'undecided cells'.\n    Parameters\n    ----------\n    Dseg : np.ndarray\n        Dchosen distance matrix restricted to segment.\n    tips : np.ndarray\n        The three tip points. They form a 'triangle' that contains the data.\n\n    Returns\n    -------\n    ssegs : list of np.ndarray\n        List of segments obtained from splitting the single segment defined\n        via the first two tip cells.\n    ssegstips : list of np.ndarray\n        List of tips of segments in ssegs.\n    \"\"\"\n    if False:\n        ssegs = self._detect_branching_versions(Dseg, tips)\n    if True:\n        ssegs = self._detect_branching_single(Dseg, tips)\n    # make sure that each data point has a unique association with a segment\n    masks = np.zeros((3, Dseg.shape[0]), dtype=bool)\n    for iseg, seg in enumerate(ssegs):\n        masks[iseg][seg] = True\n    nonunique = np.sum(masks, axis=0) > 1\n    # obtain the corresponding index arrays from masks\n    ssegs = []\n    for iseg, mask in enumerate(masks):\n        mask[nonunique] = False\n        ssegs.append(np.arange(Dseg.shape[0], dtype=int)[mask])\n    # compute new tips within new segments\n    ssegstips = []\n    for inewseg, newseg in enumerate(ssegs):\n        # get tip point position within segment\n        tip = np.where(np.arange(Dseg.shape[0])[newseg]\n                       == tips[inewseg])[0][0]\n        # new tip within restricted distance matrix\n        secondtip = np.argmax(Dseg[np.ix_(newseg, newseg)][tip])\n        # map back to position within segment\n        secondtip = np.arange(Dseg.shape[0])[newseg][secondtip]\n        # add to list\n        ssegstips.append([tips[inewseg], secondtip])\n    # for the points that cannot be assigned to the three segments of the\n    # branching, hence have no tip cells, but form a subset of their own,\n    # add dummy tips [-1,-1]\n    # this is not a good solution, but it ensures that we can easily write\n    # to hdf5 as ssegstips can be transformed to np.ndarray with dtype = int\n    ssegstips.append(np.array([-1, -1]))\n    # the following would be preferrable, but then ssegstips results in\n    # a np.ndarray with dtype = object, for which there is no straight\n    # forward hdf5 format, a solution via masks seems too much work\n    #     ssegstips.append(np.array([],dtype=int))\n    # also add the points not associated with a clear seg to ssegs\n    mask = np.zeros(Dseg.shape[0], dtype=bool)\n    # all points assigned to segments (flatten ssegs)\n    mask[[i for l in ssegs for i in l]] = True\n    # append all the points that have not been assigned. in Haghverdi et\n    # al. (2016), we call them 'undecided cells'\n    ssegs.append(np.arange(Dseg.shape[0], dtype=int)[mask == False])\n    return ssegs, ssegstips", "idx": 2053}
